{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "'input'(question)\n",
    "'context'\n",
    "'dataset'\n",
    "'label'\n",
    "'answers'\n",
    "'_id'\n",
    "'length'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- agriculture: 100\n",
    "- art: 200\n",
    "- biography: 180\n",
    "- biology: 220\n",
    "- cooking: 120\n",
    "- cs: 100\n",
    "- fiction: 220\n",
    "- fin: 345\n",
    "- health: 180\n",
    "- history: 180\n",
    "- legal: 438\n",
    "- literature: 180\n",
    "- mathematics: 160\n",
    "- mix: 130\n",
    "- music: 200\n",
    "- philosophy: 200\n",
    "- physics: 160\n",
    "- politics: 180\n",
    "- psychology: 200\n",
    "- technology: 240\n",
    "- Total Examples:3933\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\tools\\Anaconda\\envs\\lightrag\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapath=r\"datasets\\UltraDomain\"\n",
    "\n",
    "data=load_dataset(\"json\",data_files=f\"{datapath}/fin.jsonl\")\n",
    "#load_dataset 函数的返回值是一个 DatasetDict 类型，它是一个字典（dict）对象"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'context', 'dataset', 'label', 'answers', '_id', 'length'],\n",
      "        num_rows: 345\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input', 'context', 'dataset', 'label', 'answers', '_id', 'length'],\n",
      "    num_rows: 345\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'What regulatory milestones has CytoSorb achieved in the United States?', 'context': 'Item 1. Business.\\nOverview\\nWe are a leader in critical care immunotherapy, investigating and commercializing our CytoSorb blood purification technology to reduce deadly uncontrolled inflammation in hospitalized patients around the world, with the goal of preventing or treating multiple organ failure in life-threatening illnesses and cardiac surgery. Organ failure is the cause of nearly half of all deaths in the intensive care unit (“ICU”), with little to improve clinical outcome. CytoSorb, our flagship product, is approved in the European Union (“EU”) as an effective extracorporeal cytokine filter and is designed to reduce the “cytokine storm” that could otherwise cause massive inflammation, organ failure and death in common critical illnesses such as sepsis, burn injury, trauma, lung injury, and pancreatitis. These are conditions where the mortality is extremely high, yet no effective treatments exist. In May 2018, we received a label expansion for CytoSorb covering use of the device for the removal of bilirubin and myoglobin in the treatment of liver disease and trauma, respectively. In January 2020, we received CE-Mark label expansion for CytoSorb covering the use of the device for the removal of the anti-platelet agent, ticagrelor, in patients undergoing surgery requiring cardiopulmonary bypass. In April 2020, the United States Food and Drug Administration (the “FDA”) granted Breakthrough Designation to CytoSorb for the removal of ticagrelor in a cardiopulmonary bypass circuit during emergent and urgent cardiothoracic surgery. In April 2020, we announced that the FDA has granted Emergency Use Authorization (“EUA”) of CytoSorb for use in patients with COVID-19 infection. In May 2020, we received a CE-Mark label expansion for CytoSorb for the removal of rivaroxaban during cardiothoracic surgery requiring cardiopulmonary bypass. CytoSorb is used during and after cardiac surgery to remove inflammatory mediators, such as cytokines, activated compliment and free hemoglobin that can lead to post-operative complications, such as acute kidney injury, lung injury, shock, and stroke. We believe CytoSorb has the potential to be used in many other inflammatory conditions, including the treatment of autoimmune disease flares, cytokine release syndrome in cancer immunotherapy, and other applications in cancer, such as cancer cachexia. CytoSorb has been used globally in more than 121,000 human treatments to date in critical illnesses and in cardiac surgery. Our purification technologies are based on biocompatible, highly porous polymer beads that can actively remove toxic substances from blood and other bodily fluids by pore capture and surface adsorption. . The technology is protected by 16 issued U.S. patents and multiple international patents, with applications pending both in the U.S. and internationally. We have numerous product candidates under development based upon this unique blood purification technology, including HemoDefend, ContrastSorb, DrugSorb, and others.\\nIn March 2011, CytoSorb was “CE Marked” in the EU as an extracorporeal cytokine filter indicated for use in clinical situations where cytokines are elevated, allowing for commercial marketing. The CE Mark demonstrates that a conformity assessment has been carried out and the product complies with the Medical Devices Directive. The goal of CytoSorb is to prevent or treat organ failure by reducing cytokine storm and the potentially deadly systemic inflammatory response syndrome (“SIRS”) in diseases such as sepsis, trauma, burn injury, acute respiratory distress syndrome, pancreatitis, liver failure, and many others. Organ failure is the leading cause of death in the ICU, and remains a major unmet medical need, with little more than supportive care therapy (e.g., mechanical ventilation, dialysis, vasopressors, fluid support, etc.) as treatment options. By potentially preventing or treating organ failure, CytoSorb may improve clinical outcome, including survival, while reducing the need for costly ICU treatment, thereby potentially saving significant healthcare costs.\\nOur CE Mark enables CytoSorb to be sold throughout the European Union and member states of the European Economic Area. In addition, many countries outside the EU accept the CE Mark for medical devices, but may also require registration with or without additional clinical studies. The broad indication for which CytoSorb is CE marked allows it to be used “on-label” in diseases where cytokines are elevated including, but not limited to, critical illnesses such as those mentioned above, autoimmune disease flares, cancer cachexia, and many other conditions where cytokine-induced inflammation plays a detrimental role.\\nCytokines are small proteins that normally stimulate and regulate the immune response. However, in certain diseases, particularly life-threatening conditions commonly seen in the ICU, such as sepsis and infection, trauma, acute respiratory distress syndrome (“ARDS”), severe burn injury, liver failure, and acute pancreatitis, cytokines are often produced in vast excess - a condition known as cytokine storm. Left unchecked, this cytokine storm can lead to severe maladaptive SIRS that can then cause cell death, multiple organ dysfunction syndrome, and multiple organ failure. Failure of vital organs such as the heart, lungs, and kidneys, accounts for nearly half of all deaths in the ICU, despite the wide availability of supportive care therapies, or “life support”, such as dialysis, mechanical ventilation, extracorporeal membrane oxygenation, and vasopressors. By replacing the function of failed organs, these supportive care therapies can initially help to keep patients alive, but do not help patients recover faster, and in many cases can increase the risk of dangerous complications. Unlike these supportive care therapies, the goal of the CytoSorb cytokine filter is to proactively prevent or treat organ failure by reducing cytokine storm and reducing the maladaptive SIRS response. In doing so, CytoSorb targets\\nthe reduction in the severity of patient illness and the need for intensive care, while potentially improving clinical outcome and saving healthcare costs.\\nAs part of the CE Mark approval process, we completed our randomized, controlled, European Sepsis Trial amongst 14 trial sites in Germany in 2011, with enrollment of 100 patients with sepsis and respiratory failure. The trial established that CytoSorb was sufficiently safe in this critically-ill population, and that it was able to broadly reduce key cytokines in the blood of these patients.\\nIn addition to CE Marking, we also achieved ISO 13485:2003 Full Quality Systems certification, an internationally recognized quality standard designed to ensure that medical device manufacturers have the necessary comprehensive management systems in place to safely design, develop, manufacture and distribute medical devices in the EU. We manufacture CytoSorb at our manufacturing facilities in New Jersey for commercial sales abroad and for additional clinical studies, the expansion of which we officially completed in June 2018. Upon expanding our facility we quadrupled our manufacturing capacity and completed an audit upgrade from an ISO 13485:2003 certification to an ISO 13485:2016 certification.\\nIn late June 2012, following the establishment of our European subsidiary, CytoSorbents Europe GmbH, a wholly-owned operating subsidiary of CytoSorbents Corporation, we began the commercial launch of CytoSorb in Germany with the hiring of Dr. Christian Steiner as Vice President of Sales and Marketing and three additional sales representatives who joined us and completed their sales training during the third quarter of 2012. The fourth quarter of 2012 represented the first quarter of direct sales with the full sales team in place. During this period, we expanded our direct sales efforts to include both Austria and Switzerland.\\nFiscal year 2013 represented the first full year of CytoSorb commercialization. We focused our direct sales efforts in Germany, Austria and Switzerland with four sales representatives. The focus of the team was to encourage acceptance and usage by key opinion leaders (“KOLs”) throughout these countries. We believe our relationships with KOLs are essential to drive adoption and recurrent usage of CytoSorb, facilitate purchases by hospital administration, arrange reimbursement, and generate data for papers and presentations. As of the end of 2020, we had hundreds of KOLs in our commercialized territories worldwide in critical care, cardiac surgery, and blood purification, who were either using CytoSorb or supporting its use in clinical practice or clinical trials.\\nIn March 2016, we established CytoSorbents Switzerland GmbH, a wholly-owned subsidiary of CytoSorbents Europe GmbH, to conduct marketing and direct sales in Switzerland. This subsidiary began operations during the second quarter of 2016. In 2017, we further expanded our direct sales efforts into Belgium and Luxembourg.\\nIn May 2018, the approved uses of CytoSorb in the E.U. were expanded to include the removal of bilirubin in liver disease, and the removal of myoglobin in trauma.\\nOn March 5, 2019, the Company announced the expansion of direct sales of CytoSorb for all applications to Poland and the Netherlands, and critical care applications to Sweden, Denmark and Norway. As part of this effort, the Company established CytoSorbents Poland Sp. z.o.o., a wholly-owned subsidiary of CytoSorbents Europe GmbH.\\nIn the third quarter of 2019, we established CytoSorbents UK Limited, a wholly-owned subsidiary of CytoSorbents Medical, Inc., to manage our clinical trial activities in the United Kingdom.\\nIn August 2019, we announced that CytoSorb had received renewal of its European Union CE Mark through May 2024 and ISO 13485:2016 Full Quality Assurance System certification of its manufacturing facility through September 2022.\\nIn January 2020, we received CE-Mark label expansion approving the use of CytoSorb to remove the anti-platelet agent, ticagrelor, in cardiac patients during surgery requiring cardiopulmonary bypass.\\nIn April 2020, the Company announced that the FDA granted EUA of CytoSorb for use in critically-ill patients infected with COVID-19. Under the EUA, the Company can make CytoSorb available, through commercial sales, to all hospitals in the United States for use in patients, 18 years of age or older, with confirmed COVID-19 infection who are admitted to the intensive care unit (ICU) with confirmed or imminent respiratory failure who have early acute lung injury or acute respiratory distress syndrome (ARDS), severe disease, or life-threatening illness resulting in respiratory failure, septic shock, and/or multiple organ dysfunction or failure. The CytoSorb device has neither been cleared nor approved for the indication to treat patients with COVID-19 infection. The EUA will be effective until a declaration is made that the circumstances justifying the EUA have terminated or until revoked by the FDA.\\nIn April 2020, the Company also announced that the FDA had granted Breakthrough Designation to CytoSorb for the removal of ticagrelor in a cardiopulmonary bypass circuit during emergent and urgent cardiothoracic surgery. The Breakthrough Devices\\nProgram provides for more effective treatment of life-threatening or irreversibly debilitating disease or conditions, in this case the need to reverse the effects of ticagrelor in emergent or urgent cardiac surgery that can otherwise cause a high risk of serious or life-threatening bleeding. Through Breakthrough Designation, the FDA intends to work with CytoSorbents to expedite the development, assessment, and regulatory review of CytoSorb for the removal of ticagrelor, while maintaining statutory standards of regulatory approval (e.g., 510(k), de novo 510(k) or premarket approval) consistent with the FDA’s mission to protect and promote public health.\\nIn May 2020, we received CE-Mark label expansion approving the use of CytoSorb for the removal of rivaroxaban, a widely-used Factor Xa inhibitor and novel oral anticoagulant, during cardiothoracic surgery requiring cardiopulmonary bypass. With this announcement, and the E.U. approval earlier this year to remove ticagrelor, for the same indication, CytoSorb is providing cardiac surgeons and perfusionists an easy-to-use and rapid new treatment option to help reduce the risk of serious and potentially fatal perioperative bleeding complications caused by these two drugs, in separate categories of blood thinners.\\nIn addition, we now have more than 50 investigator-initiated studies and additional Company sponsored trials that are currently planned, enrolling or completed in Europe and elsewhere outside of the United States. We believe that these trials, which are conducted and supported by what we believe to be well-known university hospitals and KOLs, are the equivalent of Phase 3 and Phase 4 clinical studies. We believe they will provide invaluable information regarding the success of the device in the treatment of sepsis, cardio-pulmonary bypass surgery, liver failure, and many other indications, and if successful, may be integral in helping to drive additional usage and adoption of CytoSorb.\\nAs of March 1, 2021, our European commercialization team included 93 people.\\nWe have complemented our direct sales efforts with sales to distributors and/or strategic corporate partners. For more information regarding our distributors and strategic partners, refer to the Sales and Marketing section in item 1 of this report.\\nWe continuously evaluate other potential distributor and strategic partner networks in other countries where we are approved to market the device.\\nOverall, we have established either direct sales or distribution (via distributors or strategic partners) of CytoSorb in 67 countries worldwide. Registration of CytoSorb is typically required in each of these countries prior to active commercialization. With CE Mark approval, this can be typically achieved within several months in EU countries. Outside of the EU, the process is more variable and can take several months to more than a year due to different requirements for documentation and clinical data. Variability in the timing of registration affects the initiation of active commercialization in these countries, which affects the timing of expected CytoSorb sales. We actively support all of our distributors and strategic partners in the product registration process. We cannot generally predict the timing of these registrations, and there can be no guarantee that we will ultimately achieve registration in countries where we have established distribution. For example, in August 2014 we announced exclusive distribution of CytoSorb in Taiwan with Hemoscien Corporation. However, in March 2015, due to the complexity we encountered with Taiwanese product registration, we elected to terminate our agreement with Hemoscien. Outside of the EU, CytoSorb has distribution in Turkey, India, Sri Lanka, Australia, New Zealand, Russia, Serbia, Norway, Vietnam, Malaysia, Hong Kong, Chile, Panama, Costa Rica, Colombia, Brazil, Mexico, Argentina, Perú, Guatemala, Ecuador, Bolivia, the Dominican Republic, El Salvador, Iceland, Israel, UAE, Iran, Saudi Arabia and other Middle Eastern countries, and South Korea. We cannot guarantee that we will generate meaningful sales in the countries where we have established registration, due to other factors such as market adoption and reimbursement. For example, in December 2019, we discontinued our distributor relationship with Dr. Reddy’s in South Africa due to lack of market adoption. We continuously evaluate other potential distributor and strategic partner networks in other countries that accept CE Mark approval.\\nIn February 2020, we announced an agreement with China Medical System Holdings Limited (“CMS”), a well-established, innovation-driven specialty pharma with a focus on sales and marketing in China and Asia, to bring CytoSorb to mainland China to treat critically-ill patients with COVID-19 (fka Wuhan or 2019-nCoV) coronavirus infection. Under the terms of the agreement, CytoSorbents and CMS agreed to partner together to earn regulatory clearance to import CytoSorb into China under the “fast-track” review process established by the National Medical Products Administration of the People’s Republic of China (NMPA) to respond to the 2019 novel coronavirus (COVID-19) pandemic. CytoSorbents donated the initial CytoSorb devices and provided product, training, and support to CMS to introduce CytoSorb initially into four hospitals in the Wuhan, China area. The therapy was used and will be evaluated in severe COVID-19 coronavirus patients with a systemic inflammatory response being treated with either continuous renal replacement therapy (CRRT) or extracorporeal membrane oxygenation (ECMO). The use of CytoSorb for the treatment of patients with severe COVID-19 coronavirus infection is considered exploratory in nature in China, and is currently not yet approved for commercial purposes in mainland China.\\nIn addition to our direct and distributor commercial channels, we have a number of strategic partners to market and distribute CytoSorb. These partners include Biocon Biologics Limited, Fresenius Medical Care AG, B. Braun Avitum AG, Aferetica s.r.l., and Terumo Cardiovascular Group. For detailed information regarding these partnerships, see the section entitled “Commercial and Research Partners” in item 1 of this report.\\nThe market focus for CytoSorb is the prevention or treatment of organ failure in life-threatening conditions, including commonly seen illnesses in the ICU such as infection and sepsis, trauma, burn injury, ARDS, and others. Severe sepsis and septic shock, a potentially life-threatening systemic inflammatory response to a serious infection, accounts for approximately 10% to 20% of all ICU admissions, and is responsible for an estimated one in every five deaths worldwide. Sepsis is one of the largest target markets for CytoSorb. Sepsis is a major unmet medical need with no approved products in the U.S. or Europe to treat it. As with other critical care illnesses, multiple organ failure is the primary cause of death in sepsis. When used with standard of care therapy, that includes antibiotics, the goal of CytoSorb in sepsis is to reduce excessive levels of cytokines and other inflammatory toxins, to help reduce the SIRS response and either prevent or treat organ failure.\\nIn addition to the sepsis indication, we intend to conduct or support additional clinical studies in sepsis, cardiac surgery, and other critical care diseases where CytoSorb could be used, such as ARDS, trauma, severe burn injury, acute pancreatitis, and in other acute conditions that may benefit by the reduction of cytokines in the bloodstream. Some examples include the prevention of post-operative complications of cardiac surgery (cardiopulmonary bypass surgery) and damage to organs donated for transplant prior to organ harvest. We intend to generate additional clinical data to expand the scope of clinical experience for marketing purposes, to increase the number of treated patients, and to support potential future publications and regulatory submissions.\\nIn 2014, we completed a single arm, dose ranging trial in Germany amongst several clinical trial sites to evaluate the safety and efficacy of CytoSorb when used 24 hours per day for seven days, each day with a new device. These additional dosing data were used to support the label expansion to increase treatment time from 6 hours, the initial approval, to 24 hours of treatment. This study also provided additional treatment options for CytoSorb, helped to support the positive clinical data from our first European Sepsis Trial, and helped to shape the trial protocol for a pivotal sepsis study.\\nIn addition to the dosing study, we plan to use data generated and published in the more than 50 investigator-initiated studies and trials sponsored by us currently planned, enrolling or completed in Europe and abroad. Approximately 20 of these studies are currently enrolling. These trials, which are funded and supported by well-known university hospitals and KOLs, are the equivalent of Phase 2 clinical studies. They will provide invaluable information regarding the success of the device in the treatment of sepsis, cardio-pulmonary bypass surgery, trauma, and many other indications, and if successful, will be integral in helping to drive additional usage and adoption of CytoSorb.\\nIn addition to sepsis and other critical care applications, cardiac surgery is an important application for CytoSorb in the European market. There are approximately one million cardiac surgery procedures performed annually in the U.S. and EU combined including, for example, coronary artery bypass graft surgery, valve replacement surgery, heart and lung transplant, congenital heart defect repair, aortic reconstruction, and left ventricular assist device (“LVAD”) implantation. Cardiac surgery can result in inflammation and the production of high levels of inflammatory cytokines, as activation of complement, and cause hemolysis, leading to the release of toxic plasma free hemoglobin. These can lead to post-operative complications such as respiratory failure, circulatory failure, and acute kidney injury. CytoSorb has a unique competitive advantage as the only cytokine and free hemoglobin removal technology that can be used during the operative procedure and can be easily installed in a bypass circuit in a heart-lung machine without the need for an additional pump. Direct cytokine and hemoglobin removal with CytoSorb enables it to replace the former market for leukoreduction filters in cardiac surgery that attempt to indirectly reduce cytokines by capturing cytokine-producing leukocytes - an inefficient and suboptimal approach.\\nThe Company is currently conducting the following clinical trials:\\nThe Company is also providing an update on the following investigator initiated clinical trial:\\nGermany\\nREMOVE\\nInfective Endocarditis\\nFor further detailed information regarding our clinical trial strategy, see the section entitled “Clinical Studies” of this Item 1 of this Report.\\nEven though we have obtained CE Mark approval for CytoSorb, no guarantee or assurance can be given that our CytoSorb product will work as intended or that we will be able to obtain FDA approval to sell CytoSorb in the U.S. or approval in any other country or jurisdiction. Because of the limited studies we have conducted, we are subject to substantial risk that our technology will have little or no effect on the treatment of any indications that we have targeted.\\nWe have been successful in obtaining technology development contracts from agencies in the U.S. Department of Defense, including the Defense Advanced Research Projects Agency (“DARPA”), the U.S. Army, the U.S. Air Force, as well as the National Institutes of Health. See the section entitled “Government Research Grants” of this Item 1 of this Report for information regarding the specific grants.\\nCorporate History\\nWe were originally organized as a Delaware limited liability company in August 1997 as Advanced Renal Technologies, LLC. We changed our name to RenalTech International, LLC in November 1998, and to MedaSorb Technologies, LLC in October 2003. In December 2005, MedaSorb Technologies, LLC converted from a limited liability company to a corporation, called MedaSorb Technologies, Inc. CytoSorbents Corporation was incorporated in Nevada on April 25, 2002 as Gilder Enterprises, Inc., and was originally engaged in the business of installing and operating computer networks that provided high-speed access to the Internet. On June 30, 2006, we disposed of our original business, and pursuant to an Agreement and Plan of Merger, acquired all of the stock of MedaSorb Technologies, Inc., in a merger, and the business of MedaSorb Technologies, Inc. became our business. Following the merger, in July 2006, we changed our name to MedaSorb Technologies Corporation. In November 2008, we changed the name of our operating subsidiary from MedaSorb Technologies, Inc. to CytoSorbents, Inc. In May 2010, we finalized the name change of MedaSorb Technologies Corporation to CytoSorbents Corporation. On October 28, 2014, we changed the name of our operating subsidiary from CytoSorbents, Inc. to CytoSorbents Medical, Inc.\\nOn December 3, 2014, we effected a twenty-five-for-one (25:1) reverse split of our common stock. As a result of this reverse stock split, shares of our common stock outstanding were reduced by approximately 96%. Immediately after the reverse stock split, pursuant to an Agreement and Plan of Merger dated December 3, 2014, we changed our state of incorporation from the State of Nevada to the State of Delaware, whereby we merged with and into our wholly-owned Delaware subsidiary. At the effective time of the merger, (i) we merged with and into our Delaware subsidiary, (ii) our separate corporate existence in Nevada ceased to exist, (iii) the Delaware subsidiary became the surviving corporation, (iv) the certificate of incorporation, as amended and restated, and the bylaws of the Delaware subsidiary became our certificate of incorporation and bylaws, and (v) each share of our common stock outstanding immediately prior to the effective time was converted into one fully-paid and non-assessable share of our common stock as a Delaware corporation. The reverse stock split, the merger and the Agreement and Plan of Merger were approved by our Board of Directors and stockholders representing a majority of our then-outstanding common stock. All references to “us”, “we”, or the Company, on or after December 3, 2014, refer to CytoSorbents Corporation, a Delaware corporation.\\nOur executive offices are located at 7 Deer Park Drive, Suite K, Monmouth Junction, New Jersey 08852, and our telephone number is (732) 329-8885. Our website address is http://www.cytosorbents.com. We have included our website address as an inactive textual reference only. We make available free of charge through our website our Annual Reports on Form 10-K, our Quarterly Reports on Form 10-Q, our Current Reports on Form 8-K and amendments to those reports filed or furnished pursuant to Section 13(a) or 15(d) of the Exchange Act as soon as reasonably practicable after we electronically file such material, or furnish it to the SEC. We also similarly make available, free of charge on our website, the reports filed with the SEC by our executive officers, directors and 10% stockholders pursuant to Section 16 under the Exchange Act as soon as reasonably practicable after copies of those filings are provided to us by those persons. We are not including the information contained at http://www.cytosorbents.com, or at any other website address, as part of, or incorporating it by reference into, this Annual Report on Form 10-K.\\nWe have been engaged in research and development since our inception and have raised approximately $214 million from investors. These proceeds have been used to fund the development of multiple product applications and to conduct clinical studies, to establish in-house manufacturing capacity to meet commercial and clinical testing needs, expand our intellectual property through additional patents, and to develop extensive proprietary know-how with regard to our products.\\nWe have raised funds through various means including convertible note offerings, equity transactions, and term loans. Our most significant financing transactions are discussed below.\\nJuly 24, 2020 Offering\\nOn July 24, 2020, the Company closed an underwritten public offering of 6,052,631 shares of its common stock at a public offering price of $9.50 per share (the \"Offering\"). The Company completed the Offering pursuant to the terms of an Underwriting Agreement, dated as of July 21, 2020, by and among the Company and Cowen and Company, LLC and SVB Leerink LLC, as representatives of the several underwriters named therein. The Company received gross proceeds of approximately $57.5 million from the Offering. After deducting the underwriting discounts and commissions and fees and expenses payable by the Company in connection with the Offering, the Company received net proceeds of approximately $53.8 million.\\nShelf Registration\\nOn July 26, 2018, the Company filed a registration statement on Form S-3 with the SEC (as amended, the “2018 Shelf”). The 2018 Shelf, which was declared effective on August 7, 2018, enables the Company to offer and sell, in one or more offerings, any combination of common stock, preferred stock, senior or subordinated debt securities, warrants and units, up to a total dollar amount of $150 million.\\nOpen Market Sale Agreement with Jefferies LLC and B. Riley FBR, Inc.\\nOn July 9, 2019, the Company entered into an Open Market Sale Agreement (the “New Sale Agreement”) with Jefferies LLC and B. Riley FBR, Inc. (each an “Agent” and, together, the “Agents”), pursuant to which the Company may sell, from time to time, at its option, shares of the Company’s common stock having an aggregate offering price of up to $25,000,000 through the Agents, as the Company’s sales agents. All shares of the Company’s common stock offered and sold, or to be offered and sold under the New Sale Agreement were or will be issued and sold pursuant to the Company’s 2018 Shelf by methods deemed to be an “at the market offering” as defined in Rule 415(a)(4) promulgated under the Securities Act of 1933, as amended, in block transactions or if specified by the Company, in privately negotiated transactions.\\nOn April 20, 2020, the Company and the Agents entered into an amendment to the New Sale Agreement (the “Amendment”) to provide for an increase in the aggregate offering amount under the New Sales Agreement, such that as of April 20, 2020, the Company may offer and sell Shares having an additional aggregate offering price of up to $50 million under the New Sale Agreement, as amended by the Amendment (the “Amended Sale Agreement”).\\nSubject to the terms of the Amended Sales Agreement, the Agents are required to use their commercially reasonable efforts consistent with their normal sales and trading practices to sell the shares of the Company’s common stock from time to time, based upon the Company’s instructions (including any price, time or size limits or other customary parameters or conditions the Company may impose). The Company is required to pay the Agents a commission of up to 3.0% of the gross proceeds from the sale of the shares of the Company’s common stock sold thereunder, if any. The Company has also agreed to provide the Agents with customary indemnification rights. The offering of the shares of the Company’s common stock under the Amended Sales Agreement will terminate upon the earliest of (a) the sale of the maximum number or amount of the shares of the Company’s stock permitted to be sold under the Amended Sale Agreement and (b) the termination of the Amended Sale Agreement by the parties thereto. During the year ended December 31, 2019, the Company sold 191,244 shares pursuant to the Amended Sale Agreement, at an average selling price of $4.11 per share, generating net proceeds of approximately $762,000. During the year ended December 31, 2020, the Company sold 4,110,625 shares pursuant to the Amended Sale Agreement, at an average selling price of $6.64 per share, generating net proceeds of approximately $26,476,000. In the aggregate, the Company has sold 4,301,869 shares pursuant to the Amended Sale Agreement, at an average selling price of $6.53 per share, generating net proceeds of approximately $27,238,000. In addition, during the year ended December 31, 2020, the Company paid approximately $49,000 in expenses related to the Amended Sale Agreement.\\nResearch and Development\\nWe have been engaged in research and development since inception. Since 2012, we have been awarded an aggregate of approximately $28.4 million in grants, contracts, and other non-dilutive funding from DARPA ($3.8M over 5 years), the U.S. Army ($100K Phase I SBIR; $50K Phase I option, $803K Phase II SBIR, $443K Phase II enhancement, $1.1M 2nd Phase II), the U.S. Air Force $3.0M Rapid Innovation Fund, the Congressionally Directed Medical Research Program Office, (“CDMRP”, $718K), the National Heart, Lung and Blood Institute and USSOCOM ($203K Phase I SBIR; $1.5M Phase II SBIR; $3.0M Bridge SBIR), the Joint Program Executive Office - Chemical and Biological Defense, (JPEO-CBD), ($150K Phase I and Phase I option, $1.0M Phase II), the\\nU.S. Army Peritoneal dialysis/mesh packing for hyperkalemia ($150K Phase I SBIR, $1.0M Phase II), Universal Plasma ($150K Phase I and 1.0M Phase II STTR; $2.9M Defense Health Agency, US Army and CDMRP Phase III STTR; $4.4M US Army and CDMRP Rapid Innovation Fund; and a $1.1M US Army contract), New Jersey Technology Business Tax Certificate Program for research related expenses ($5.9M), and others to further develop our technologies for sepsis, trauma and burn injury, and blood transfusions, respectively. Some payments are based on achieving certain technology milestones.\\nTechnology, Products and Applications\\nFor approximately the past half-century, the field of blood purification has been focused on hemodialysis, a mature, well-accepted medical technique primarily used to sustain the lives of patients with permanent or temporary loss of kidney function. It is widely understood by the medical community that dialysis has inherent limitations in that its ability to remove toxic substances from blood drops precipitously as the size of toxins increases. Our hemocompatible adsorbent technology helps to address this shortcoming by removing toxins and toxic compounds largely untouched by dialysis technology.\\nOur polymer adsorbent technology can remove drugs, bioactive lipids, inflammatory mediators such as cytokines, free hemoglobin, toxins, and immunoglobulin from blood and physiologic fluids depending on the polymer construct. It is believed that the technology may have many applications in the treatment of common, chronic and acute healthcare conditions including, but not limited to, the adjunctive treatment and/or prevention of sepsis; the treatment of other critical care illnesses such as severe burn injury, trauma, acute respiratory distress syndrome and pancreatitis; the prevention of post-operative complications of cardiopulmonary bypass surgery; the treatment of cancer cachexia; the treatment of cytokine release syndrome in cancer immunotherapy, the prevention of damage to organs donated by brain-dead donors prior to organ harvest; the prevention of transfusion reactions caused by contaminants in transfused blood products; the prevention of contrast induced nephropathy, the treatment of drug overdose, and the treatment of chronic kidney failure. These applications vary by cause and complexity as well as by severity but share a common characteristic, i.e., high concentrations of inflammatory mediators and toxins in the circulating blood.\\nOur flagship product, CytoSorb, animal-targeted VetResQ, ECOS-300CY, and other product candidates under development, including CytoSorb XL, BetaSorb, ContrastSorb, DrugSorb, HemoDefend-RBC, HemoDefend-BGA, K+ontrol,and others consist of a cartridge containing adsorbent, porous polymer beads, although the polymers used in these devices are physically different. The cartridges incorporate industry standard connectors at either end of the device, which connect directly to the extracorporeal circuit (bloodlines) in series with a dialyzer as a standalone device. The extra-corporeal circuit consists of plastic blood tubing, our blood filtration cartridges containing adsorbent polymer beads, pressure monitoring gauges, and a blood pump to maintain blood flow. The patient’s blood is accessed through a catheter inserted into his or her veins. The catheter is connected to the extra-corporeal circuit and the blood pump draws blood from the patient, pumps it through the cartridge and returns it back to the patient in a closed loop system. All of these devices are expected to be compatible with standard blood pumps or hemodialysis machines used commonly in hospitals and will therefore not require hospitals to purchase additional expensive equipment, and will require minimal training.\\nThe polymer beads designed for the HemoDefend platform are intended to be used in multiple configurations, including a point-of-transfusion in-line filter between the blood bag and the patient, as well as a patent-pending “Beads in a Bag” configuration, where the beads are placed directly into a blood storage bag.\\nMarkets\\nWe are a critical care focused immunotherapy company. Immunotherapy is the ability to control the immune response to fight disease. Critical care medicine includes the treatment of patients with serious or life-threatening conditions who require comprehensive care in the ICU, with highly-skilled physicians and nurses and advanced technologies to support critical organ function to keep patients alive. Examples of such conditions include severe sepsis and septic shock, severe burn injury, trauma, acute respiratory distress syndrome, acute liver disease, and severe acute pancreatitis. In the U.S., an estimated $110 billion or 0.7% of the U.S. gross domestic product is spent annually on critical care medicine. In larger hospitals, critical care treatment accounts for up to 20% of a hospital’s overall budget and often results in financial losses for the hospital.\\nIn many critical care illnesses, the mortality is often higher than 30%. A major cause of death is multiple organ failure, where vital organs such as the lungs, kidneys, heart and liver are damaged and no longer function properly. These patients are kept alive with supportive care therapy, or “life support”, such as mechanical ventilation, dialysis and vasopressor treatment, that is designed to keep the patient from dying while using careful patient management to tip the balance towards gradual recovery over time. Unfortunately, most supportive care therapies only help to keep patients alive by supporting organ function but do not help reverse the underlying causes of organ failure and do not help patients recover more quickly. Because of this, the treatment course is often poorly defined and highly variable, leading to lengthy ICU stays, a higher risk of adverse outcomes from hospital acquired infections, medical errors, and\\nother factors, as well as exorbitant costs. There is an urgent need for more effective “active” therapies that can help to reverse or prevent organ failure. Our main product, CytoSorb, is a unique cytokine filter designed to try to address this void, by reducing “cytokine storm” and working to reduce the subsequent deadly inflammation that can lead to organ failure and death. In May 2018, the approved indications for use of CytoSorb in the EU were expanded to include the removal of bilirubin in liver disease, and the removal of myoglobin in trauma. In 2020, the Company received CE-Mark label expansions for CytoSorb to remove the anti-platelet agent ticagrelor and the direct oral anticoagulant rivaroxaban in patients undergoing cardiac surgery on cardiopulmonary bypass.\\nIn addition to critical care, CytoSorb is used in many applications related to cardiac surgery. Intra-operatively, CytoSorb is either used to help stabilize patients with serious conditions such as infective endocarditis, or to prevent post-operative complications such as acute kidney injury, vasoplegia, respiratory failure, infection, and others. Post-operatively, CytoSorb is used in the intensive care unit to treat the post-operative systemic inflammatory response syndrome (post-op SIRS), sepsis, and other complications.\\nTogether the total addressable market for these numerous critical care and cardiac surgery applications with CytoSorb is estimated to be in excess of $20 billion worldwide.\\nSepsis\\nSepsis is characterized by a systemic inflammatory response triggered by a severe infection. It is commonly seen in the ICU, accounting for approximately 10% to 20% of all ICU admissions. However, there are currently no approved products that are available to treat sepsis in the U.S. or EU. A 2020 study published in The Lancet estimated that there were 49 million new cases of sepsis globally, killing 11 million people every year. The researchers estimate that 1 in every 5 deaths worldwide is due to sepsis. Data released by the Healthcare Cost and Utilization Project (H-CUP) identified approximately 1.6 million cases of sepsis each year in the U.S. According to the CDC, the incidence of serious infection and sepsis has doubled in the U.S. in the past 10 years. The main driver of sepsis incidence is the aging demographic, specifically patients who are older than age 65 who are more prone to infection and now account for two-thirds of patients hospitalized for sepsis and the majority of sepsis deaths. Other factors contributing to the increase in sepsis incidence include the spread of antibiotic resistant bacteria like methicillin-resistant Staphylococcus aureus (“MRSA”), an increase in co-morbid conditions like HIV, cancer, obesity, and diabetes that increases the risk of infection, an increasing use of implantable devices like artificial hips and knees that are prone to colonization by bacteria, and the appearance of new highly virulent or contagious strains of common pathogens such as H3N2 or H1N1 influenza, COVID-19 coronavirus, and others.\\nThere are generally three categories of sepsis, including mild to moderate sepsis, severe sepsis and septic shock. Mild to moderate sepsis typically occurs with an infection that is responsive to antibiotics or antiviral medication. An example is a patient with self-limiting influenza or a treatable community acquired pneumonia. Mortality is generally very low. Severe sepsis is sepsis with evidence of organ dysfunction. An example is a patient who develops respiratory failure due to a severe pneumonia and requires mechanical ventilation in the ICU. Severe sepsis has a mortality rate of approximately 20% to 25% despite the use of antibiotics and the highest level of available care. Septic shock, or severe sepsis with low blood pressure that is not responsive to fluid resuscitation, is the most serious form of sepsis with an expected mortality in excess of 40% to 50%, and up to 80-100% if it is refractory to vasopressors and other therapies.\\nIn sepsis, there are two major problems: the infection and the body’s immune response to the infection. Antibiotics are the main therapy used to treat the triggering infection, and although antibiotic resistance is growing, the infection is often eventually controlled. However, it is the body’s immune response to this infection that frequently leads to the most devastating damage. In recognition of this, in 2016 the 3rd International Consensus Definition Task Force re-defined sepsis as “life-threatening organ dysfunction due to a dysregulated host response to infection.” The body’s immune system normally produces large amounts of inflammatory mediators called cytokines to help stimulate and regulate the immune response during an infection. In severe infection, however, many people suffer from a massive, unregulated overproduction of cytokines, often termed “cytokine storm” that can kill cells and damage organs, leading to multiple organ dysfunction syndrome and multiple organ failure, and in many cases death. Until recently, there have been no available therapies in the U.S. or EU that can control the aberrant immune response and cytokine storm. Our CytoSorb device is a first-in-class, clinically-proven broad-spectrum extracorporeal cytokine adsorber currently approved for sale in the E.U. The goal of CytoSorb is to prevent or treat organ failure by reducing cytokine storm and controlling a “run-away” immune response, while antibiotics work to control the actual infection. CytoSorb has been evaluated in the randomized, controlled European Sepsis Trial in 100 patients in Germany with predominantly septic shock and acute respiratory distress syndrome or acute lung injury. The therapy was safe in more than 300 human treatments and generally well-tolerated. CytoSorb demonstrated the ability to reduce a broad range of cytokines from the blood of critically-ill patients. In a post-hoc analysis, this was associated with improvements in clinical outcome in two high-risk patient populations - those with very high cytokine levels and patients 65 years of age and older. We have completed a follow-up dosing study at several clinical trial sites in Germany, supporting the safety of continuous treatment, exchanging a new device daily for up to 7 days.\\nThe only treatment that had been approved to treat sepsis in the U.S. or EU was Xigris from Eli Lilly. Because of concerns of cost, limited efficacy, and potentially dangerous side effects including the increased risk of fatal bleeding events such as intracranial bleeding for those at risk, and also because of problems with reimbursement, worldwide sales of Xigris decreased from $160M in 2009 to $104M in 2010. In October 2011, following its PROWESS SHOCK trial that demonstrated no benefit in mortality in septic shock patients, Lilly voluntarily withdrew Xigris from all markets worldwide, and is no longer available as a treatment.\\nDevelopment of many experimental therapies has been discontinued, including Eritoran from Eisai, CytoFab from BTG/Astra Zeneca, Talactoferrin from Agennix, tranexemic acid from Leading Biosciences, and others.\\nFor more information regarding our competitor’s clinical trials, see the section entitled “Competition” in Item 1 of this report.\\nSevere sepsis and septic shock patients are among the most expensive patients to treat in a hospital. Because of this, we believe that cost savings to hospitals and/or clinical efficacy, rather than the cost of treatment itself, will be the determining factor in the adoption of CytoSorb in the treatment of sepsis. CytoSorb is approved in the EU and is being sold directly in Germany, Austria, Switzerland, Belgium, Luxembourg, Poland, Norway, Denmark, Sweden, and the Netherlands with our own direct sales force. In December 2016, we announced the achievement of a permanent, dedicated reimbursement procedure code for CytoSorb therapy in Germany, providing for specific and enhanced reimbursement in the largest medical device market in Europe. We have established strategic partnerships with Fresenius Medical Care, the world’s largest dialysis company, for exclusive distribution of CytoSorb for critical care applications in France, Finland, the Czech Republic, Mexico, and Korea, and Terumo Cardiovascular, the largest cardiac surgery disposables company, for exclusive distribution of the CytoSorb Cardiopulmonary Bypass Kit in France, Denmark, Sweden, Norway, Finland, and Iceland. We are also partnered with Biocon Biologics Limited, India’s largest biopharmaceutical company, for exclusive distribution of CytoSorb in India, Sri Lanka, and other select emerging markets. In March 2021, we announced a strategic partnership with B. Braun Avitum AG, and the launch of a global co-marketing agreement to promote the use of CytoSorb with B Braun’s latest OMNI® continuous blood purification platform and OMNIset® Plus bloodline set (set version 3.0 or higher). We have ongoing discussions with potential corporate partners and independent distributors to market CytoSorb in other select EU countries and in other countries outside the EU that accept CE Mark approval. We have established direct sales or distribution of CytoSorb in 67 countries worldwide.\\nWe estimate that the market potential in Europe for our products is larger than that in the U.S. For example, in the U.S. there are an estimated 1.6 million cases of sepsis, while the European Sepsis Alliance estimates 3.4 million individuals in Europe become septic each year. In Germany alone, according to the Center of Sepsis Control and Care, there are approximately 175,000 cases of severe sepsis each year. Germany is the largest medical device market in Europe and the third largest in the world.\\nSepsis patients are treated in the ICU for 12 to 18 days on average and for a total of 20 to 25 days in the hospital. A typical severe sepsis or septic shock patient in the U.S. costs approximately $45,000 to $60,000 to treat without using CytoSorb. CytoSorb therapy for sepsis typically costs in the range of $1,000 to $5,000, depending on the number of treatments. The goal of therapy is to not only improve clinical outcomes, but to also reduce the severity of illness and reduce the need for costly ICU care (estimated at approximately $4,300 per day in the ICU in the U.S.). The cost of CytoSorb therapy represents a fraction of what is currently spent on the treatment of patients with sepsis and would be cost-effective if it decreased ICU stay by one to two days. Based upon this price point, the total addressable market for CytoSorb for the treatment of sepsis in the U.S. and EU is approximately $6 billion to $8 billion.\\nCardiac Surgery\\nThere are approximately 500,000 cardiac surgery procedures performed on cardiopulmonary bypass annually in the U.S., another 500,000 in the EU, and approximately a total of 1.5 million procedures worldwide. These include relatively common procedures including coronary artery bypass graft surgery, valve replacement surgery, heart and lung transplantation, aortic reconstruction, congenital heart defect repair, and LVAD placements for the treatment of heart failure. Cardiac surgery can result in inflammation and the production of high levels of inflammatory cytokines, activation of complement, as well as hemolysis, causing the release of free hemoglobin. These can lead to post-operative complications including infection, pulmonary, renal, and neurological dysfunction. Complications lead to longer ICU recovery times and hospital stays, increased morbidity and mortality, and higher costs. An average coronary artery bypass graft procedure already costs approximately $36,000 in the U.S. without complications. According to the National Foundation for Transplants, a heart and lung transplant plus first year expenses total $1.2 million in the U.S. Valve replacement surgery for infective endocarditis is poorly reimbursed and may cost up to $150,000-$250,000 in the U.S. The use of CytoSorb to reduce cytokines and other inflammatory mediators during and after the surgical procedure may prevent or mitigate these post-operative complications. During the procedure, the CytoSorb adsorber can be easily incorporated in the heart-lung machine circuit without the need for a separate pump, a unique competitive advantage over other technologies. After the surgery, CytoSorb can continue to be used similarly to dialysis on patients that develop a severe post-operative inflammatory response with hemodynamic instability. Modified ultrafiltration is sometimes used after termination of cardiopulmonary bypass in cardiac surgery to remove excess fluid and inflammatory\\nsubstances, but has had mixed benefit. The peri-procedural total addressable market for CytoSorb in the U.S. and EU in cardiothoracic surgery procedures is estimated to be $500 million to $1 billion.\\nRemoval of Antithrombotic Drugs in Cardiac Patients During Surgery Requiring Cardiopulmonary Bypass\\nThe role of antithrombotics, a category that includes both antiplatelet and anticoagulant drugs in cardiovascular medicine is constantly growing. Antiplatelet drugs are routinely used in patients with atherosclerotic cardiovascular disease such coronary disease, vascular disease or stroke. In the acute management of these patients especially when they need interventional procedures such as stent placement therapy is escalated using two antiplatelet drugs (dual antiplatelet therapy - DAPT). Ticagrelor (Astra Zeneca - Brilinta(R), Brilique(R)) is considered best in class and is one of the most commonly used anti-platelet drugs to reduce the risk of cardiac death, heart attacks, and strokes in patients with either a history of a heart attack, or those actively undergoing percutaneous coronary intervention (PCI) with stent placement for acute coronary syndrome or heart attack. On the other hand, patients with atrial fibrillation or venous thrombosis require chronic anticoagulation. A new category of drugs called Direct Oral Anticoagulants (DOAC) is now the new standard of care with tens of millions of patients relying on them for lifelong protection. The two leaders in the category,Apixaban (Bristol Myers Squibb - Eliquis(R)) and rivaroxaban (Janssen and Bayer - Xarelto(R)) aare estimated to reach 40 billion USD in sales by 2026.\\nThere is a clear and large unmet medical when patients on these antithrombotic agents need to undergo surgery due to the very high risk of bleeding. Specifically, in patients on these drugs requiring urgent or emergent cardiac surgery the risk of major fatal/life-threatening bleeding has been reported to be as high as 65%. This scenario is most common in patients presenting with an acute coronary syndrome (ACS). In the US alone there are approximately 1.1 million ACS hospital admissions annually. CytoSorb is able to very efficiently remove ticagrelor and DOACs from blood. The use of CytoSorb during emergency coronary artery bypass surgery (CABG) in patients on ticagrelor or rivaroxaban significantly reduced post-operative bleeding complications in a landmark observational study and had projected cost savings of approximately $5,000 per patient, including the cost of the device. Every year there are approximately 400,000 CABG procedures performed in the US and 250,000 CABG procedures in the EU with nearly 100,000 in Germany alone.\\nAcute Respiratory Distress Syndrome\\nAcute lung injury (“ALI”) and acute respiratory distress syndrome (ARDS) are two of the most serious conditions on the continuum of respiratory failure when both lungs are compromised by inflammation and fluid infiltration, severely compromising their ability to both oxygenate the blood and rid the blood of carbon dioxide produced by the body. There are an estimated 165,000 cases of ARDS in the U.S. each year, with even more cases in the EU. During the COVID-19 pandemic in 2020-2021, ALI and ARDS were responsible for more than 500,000 deaths in the U.S. alone. Patients with ALI and ARDS typically require mechanical ventilation, and sometimes extracorporeal membrane oxygenation (ECMO) therapy, to help achieve adequate oxygenation of the blood. Patients on mechanical ventilation are at high risk of ongoing ventilator-induced lung injury, oxygen toxicity, barotrauma, ventilator-acquired pneumonias, and other hospital acquired infections, and outcome is significantly dependent on the presence of other organ dysfunction as well as co-morbid conditions such as pre-existing lung disease (e.g., emphysema or chronic obstructive pulmonary disease) and age. Because of this, mortality has been high (16-33%) even with modern medicine and ventilation techniques. ALI and ARDS can be precipitated by a number of conditions including pneumonia and other infections, burn and smoke inhalation injury, aspiration, reperfusion injury and shock. Cytokine injury plays a major role in the vascular compromise and cell-mediated damage to the lung through tight junction disruption of respiratory endothelium, leading to capillary leak syndrome, and other factors. Reduction of cytokine levels may either prevent or mitigate lung injury, enabling patients to wean from mechanical ventilation faster, potentially reducing numerous sequelae such as infection, pneumothoraces, and respiratory muscle deconditioning, and allow faster ICU discharge, thereby potentially saving costs. CytoSorb treatment of patients with either ALI or ARDS in the setting of sepsis was the subject of our European Sepsis Trial where in a post-hoc analysis in patients with very high cytokine levels, we observed faster ventilator weaning in CytoSorb treated patients that showed a statistical trend towards benefit. Future, prospectively defined, larger studies are required to confirm these findings. Although a number of therapies have been tried such as nitric oxide, surfactant therapy, and others, only corticosteroids, such a dexamethasone or methylprednisolone, have demonstrated mortality benefit in patients with ARDS. For example, in critically ill COVID-19 patients on mechanical ventilation, the RECOVERY study demonstrated use of once daily dexamethasone led to a reduction in mortality from 41.4% control to 29.3% treatment. However, techniques to improve ventilation and reduce ongoing lung injury are being used. For example, low tidal volume ventilation has been demonstrated to improve mortality (31.0% as compared to 39.8% control) in this patient population in the ARDSNet Trial. Prone positioning, or placing a patient chest-side down, in severe ARDS patients in order to redistribute gravity-dependent pulmonary edema and allow ventilation of collapsed or atelectatic alveoli, is also used, following studies that suggest benefit including the PROSEVA trial (16% vs 32.8% in the control). However, even with these interventions, we believe mortality is still unacceptably high. The total addressable market for CytoSorb to treat ARDS and ALI in the EU is estimated to be between $500 million to $1.25 billion, and approximately $2 billion for the U.S and EU combined.\\nSevere Burn Injury\\nIn the U.S., there are approximately 2.4 million burn injuries per year, with 650,000 treated by medical professionals and approximately 75,000 requiring hospitalization. Aggressive modern management of burn injury, including debridement, skin grafts, anti-microbial dressings and mechanical ventilation for smoke and chemical inhalation injury has led to significant improvements in survival of burn injury to approximately 95% on average at leading burns centers. However, there remains a need for better therapies to reduce the mortality in those patients with large burns and inhalation injury as well as to reduce complications of burn injury and hospital length of stay for all patients. According to National Burn Repository Data, the average hospital stay for burn patients is directly correlated with the percent total body surface area (“TBSA”) burned. Every 1% increase of TBSA burned equates to approximately 1 additional day in the hospital. A single patient with more than 30% TBSA burned who survives, is hospitalized for an average of 30 days and costs approximately $200,000 to treat. Major causes of death following severe burn and smoke inhalation injury are multiple organ failure (hemodynamic shock, respiratory failure, acute renal failure) and sepsis, particularly in patients with greater than 30% TBSA burns. Burns and inhalation injury lead to severe systemic and localized lung inflammation, loss of fluid, and cytokine overproduction. This “cytokine storm” causes numerous problems, including: hypovolemic shock and inadequate oxygen and blood flow to critical organs, ARDS preventing adequate oxygenation of blood, capillary leakage resulting in tissue edema and intravascular depletion, hypermetabolism leading to massive protein degradation and catabolism and is also associated with increased risk of infection, impaired healing, severe weakness and delayed recovery, immune dysfunction causing a higher risk of secondary infections (wound infections, pneumonia) and sepsis, and direct apoptosis and cell-mediated killing of cells, leading to organ damage. Up to a third of severe hospitalized burn patients develop multiple organ failure and sepsis that can often lead to complicated, extended hospital courses, or death. Broad reduction of cytokine storm has not been previously feasible and represents a novel approach to limiting or reversing organ failure, potentially enabling more rapid mechanical ventilation weaning, prevention of shock, reversal of the hypermetabolic state encouraging faster healing and patient recovery, reducing hospital costs, and potentially improving survival. The total addressable market in the EU for CytoSorb to address burn and smoke inhalation injury is estimated at $150 million to $350 million and up to $600 million for the U.S. and EU combined.\\nTrauma\\nAccording to the National Center for Health Statistics, in the U.S., there are more than 31 million visits to hospital emergency rooms, with 1.9 million hospitalizations, and 167,000 deaths every year due to injury. The leading causes of injury are trauma from motor vehicle accidents, being struck by an object or other person, and falls. Trauma is a well-known trigger of the immune response and a surge in cytokine production or cytokine storm. In trauma, cytokine storm contributes to the systemic inflammatory response syndrome triggering a cascade of events that cause cell death, organ damage, organ failure and often death. Cytokine storm exacerbates physical trauma in many ways. For instance, trauma can cause hypovolemic shock due to blood loss, while cytokine storm causes capillary leak and intravascular volume loss, and triggers nitric oxide production that causes cardiac depression and peripheral dilation. Shock can lead to a lack of oxygenated blood flow to vital organs, causing organ injury. Severe systemic inflammation and cytokine storm can lead to ALI and ARDS as is often seen in ischemia and reperfusion injury following severe bleeding injuries. Penetrating wound injury from bullets, shrapnel and knives, can lead to infection and sepsis, another significant cause of organ failure in trauma. Complicating matters is the breakdown of damaged skeletal muscle, or rhabdomyolysis, from blunt trauma that can lead to a massive release of myoglobin into the blood that can crystallize in the kidneys, leading to acute kidney injury and renal failure. Renal failure in trauma is associated with a significant increase in expected mortality. Cytokine and myoglobin reduction by CytoSorb and related technologies may have benefit in trauma, potentially improving clinical outcome. In May 2018, the approved indications for use of CytoSorb in the EU were expanded to include the removal of myoglobin in trauma. The total addressable market for CytoSorb for the treatment of trauma is estimated to be $1.5 billion to $2.0 billion in the U.S. and the EU.\\nTrauma patients on antithrombotic drugs represent an especially challenging cohort since any necessary surgery would be associated with very high bleeding risk. The ability of CytoSorb to efficiently remove some of the most popular antithrombotic drugs may represent an additional mode of benefit to improve clinical outcomes in trauma patients.\\nAcute Liver Disease\\nChronic liver disease afflicts an estimated 850 million people worldwide, or 11% of the world population, due to the prevalence of viral hepatitis infection, alcohol abuse, and non-alcoholic steatohepatitis (NASH or “fatty liver”). Chronic liver disease is blamed for nearly one million deaths a year, with another one million dying of hepatic cancer and acute hepatitis. In the U.S., liver disease is the second leading cause of death from digestive disease, and the 10th leading cause of death amongst men. Many patients with advanced chronic liver disease will develop an acute exacerbation or decompensation (“acute-on-chronic”) of their disease, with associated inflammation and cytokine elevation, often requiring hospitalization. Also, many patients will present with acute hepatitis triggered by viral infection or alcohol. A range of symptoms, depending on the severity of illness include jaundice (high bilirubin), variceal hemorrhage, cognitive dysfunction and hepatic encephalopathy, ascites, coagulopathy, renal failure, liver failure, and others. The extracorporeal blood purification of liver toxins such as bilirubin has been used to help treat patients and is often called “liver dialysis”. Current liver dialysis therapies include MARS (Molecular Adsorbent Recirculation System; Baxter), Prometheus (Fresenius), SPAD (single pass albumin dialysis), and others. However, none of these therapies can remove cytokines, key elements in acute-on-chronic exacerbations and cases of acute hepatitis. CytoSorb represents a potentially superior liver dialysis therapy, as it can remove both liver toxins such as bilirubin and bile salts, as well as cytokines. In May 2018, the approved indications for use of CytoSorb in the E.U. were expanded to include the removal of bilirubin in liver disease. The total addressable market for CytoSorb for the treatment of acute-on-chronic liver disease, acute hepatitis, and acute liver failure is estimated in excess of $15 billion worldwide.\\nSevere Acute Pancreatitis\\nAcute pancreatitis is the inflammation of the pancreas that results in the local release of digestive enzymes and chemicals that cause severe inflammation, necrosis and hemorrhage of the pancreas and local tissues. Approximately 210,000 people in the U.S. are hospitalized each year with acute pancreatitis with roughly 20% requiring ICU care. It is caused most frequently by a blockage of the pancreatic duct or biliary duct with gallstones, cancer, hyperlipidemia, or from excessive alcohol use. Severe acute pancreatitis is characterized by severe pain, inflammation, and edema in the abdominal cavity, as well as progressive systemic inflammation, generalized edema, and multiple organ failure that is correlated with high levels of cytokines and digestive enzymes in the blood. Little can be done to treat severe acute pancreatitis today, except for pancreatic duct decompression with endoscopic techniques, supportive care therapy, pain control, enteral tube feeding, and fluid support. ICU stay is frequently measured in weeks and although overall ICU mortality is approximately 10%, patients with multiple organ failure have a much higher risk of death. CytoSorb may potentially benefit overall outcomes in episodes of acute pancreatitis by removing a diverse set of toxins and cytokines from blood. The total addressable market for CytoSorb for the treatment of severe acute pancreatitis in the U.S. and EU is estimated to be between $400 million to $600 million.\\nCancer Cachexia and Cancer Immunotherapy\\nCancer cachexia is a progressive wasting syndrome characterized by rapid weight loss, anorexia, and physical debilitation that significantly contributes to death in many cancer patients. Cancer cachexia is a systemic inflammatory condition, driven by excessive pro-inflammatory cytokines and other factors, that cripples the patient’s physical and immunologic reserve to fight cancer. Despite afflicting millions of patients worldwide each year, there are no effective approved treatments for cancer cachexia, with only symptomatic treatments available. CytoSorb blood purification may stop or reverse cancer cachexia through broad reduction of cytokines and other inflammatory mediators, when treated over time. For example, CytoSorb efficiently removes TNF-alpha (originally called “cachectin” or “cachexin” when first isolated in cancer cachexia patients) and other major pro-inflammatory cytokines including IL-1, IL-6, and gamma interferon that can cause cachexia. This broad immunotherapy approach may lead to improved clinical outcomes while reducing patient suffering.\\nCytoSorb may also represent a rescue or salvage therapy in activated CAR T-cell cancer immunotherapy, where cytokine release syndrome (i.e. CRS or cytokine storm) is common, and can lead to organ failure and death in certain patients. In the CRS literature, researchers have drawn parallels to both macrophage activating syndrome and secondary hemophagocytic lymphohistiocytosis (HLH) which produce a similar clinical picture and cytokine storm profile. CytoSorb has been used successfully in many cases of secondary HLH. In March 2017, the pioneer of CAR T-cell immunotherapy, Dr. Carl June at University of Pennsylvania, joined our scientific advisory board. In 2017, both Kymriah from University of Pennsylvania and Novartis, and Yescarta from Kite Pharma and Gilead Sciences, received FDA approval for the treatment of certain hematologic cancers. In early 2020, the first two case reports of CRS successfully treated with the adjunctive use of CytoSorb were published.\\nThe total addressable market for CytoSorb for the treatment of cancer cachexia and cancer in the U.S. and EU is estimated to be in excess of $4 billion.\\nOrgan Transplant and Brain-Dead Organ Donors\\nThere are in excess of 6,000 brain dead organ donors each year in the United States; worldwide, the number of these organ donors is estimated to be at least double the U.S. brain dead organ donor population. There is a severe shortage of donor organs. Currently, there are more than 100,000 individuals on transplant waiting lists in the United States. Cytokine storm is common in these organ donors, resulting in reduced viability of potential donor organs. The potential use of CytoSorb hemoperfusion to control cytokine storm in brain dead organ donors could increase the number of viable organs harvested from the donor pool and improve the survival of transplanted organs. A proof-of-concept pilot study using our technology in human brain dead donors has been published. In addition, CytoSorb treatment in a porcine animal model of brain death demonstrated a reduction in cytokines as well as a preservation of cardiac function compared to untreated controls.\\nIn October 2020, CytoSorbents announced the EU approval of the ECOS-300CY cartridge for the removal of inflammatory mediators during ex vivo organ perfusion, with the goal of either preserving organ function in healthy organs, or rehabilitating dysfunctional organs that would otherwise have been discarded. We believe the ECOS-300CY cartridge has the potential to expand the organ donor pool. According to Eurotransplant, there were approximately 6,400 transplants from deceased donors and roughly 14,000 patients on waiting list for organs in Europe last year. In the United States, UNOS cites 39,035 organ transplants in 2020, with approximately 108,000 patients on the waiting list. This represents a US and European total addressable market for the ECOS-300CY device of approximately $400-600 million.\\nBlood Transfusions\\nThe HemoDefend platform is a development-stage technology designed to be a practical, low cost, and effective way to safeguard the quality and safety of the blood supply. In the U.S. alone, 15 million packed red blood cell (“pRBC”) transfusions and another 15 million transfusions of other blood products (e.g., platelet, plasma, and cryoprecipitate) are administered each year with an average of 10% of all U.S. hospital admissions requiring a blood transfusion. The sheer volume of transfusions, not just in the U.S., but worldwide, complicates an already difficult task of maintaining a safe and reliable blood supply. Trauma, invasive operative procedures, critical care illnesses, supportive care in cancer, military usage, and inherited blood disorders are just some of the drivers of the use of transfused blood. In war, hemorrhage from trauma is a leading cause of preventable death, accounting for an estimated 30% to 40% of all fatalities. For example, in Operation Iraqi Freedom, due to a high rate of penetrating wound injuries, up to 8% of admissions required massive transfusions, defined as 10 units of blood or more in the first 24 hours. There is a clear need for a stable and safe source of blood products. However, blood shortages are common and exacerbated by the finite lifespan of blood. According to the Red Cross, pRBC units have a refrigerated life span of 42 days. However, many medical experts believe there is an increased risk of infection and transfusion reactions once stored blood ages beyond two weeks. Transfusion-related acute lung injury is the leading cause of non-hemolytic transfusion-related morbidity and mortality, with an incidence of 1 in 2,000-5,000 transfusions and a mortality rate of up to 10%. Fatal cases of transfusion-related acute lung injury have been most closely related to anti-HLA or anti-granulocyte antibodies found in a donor’s transfused blood. Other early transfusion reactions such as transfusion-associated dyspnea, fever and allergic reactions occur in 3% to 5% of all transfusions and can vary in severity depending on the patient’s condition. These are caused by cytokines, bioactive lipids, free hemoglobin, toxins, foreign antigens, and a number of other inflammatory mediators that accumulate in transfused blood products during storage. Leukoreduction can remove the majority of white cells that can produce new cytokines but cannot eliminate those cytokines already in blood, and cannot otherwise remove other causative agents such as free hemoglobin and antibodies. Automated washing of pRBC is effective but is impractical due to the time, cost, and logistics of washing each unit of blood. The HemoDefend platform is a potentially superior alternative to purify blood transfusion products to these methods. The total addressable market for HemoDefend is more than $500 million for pRBCs alone. CytoSorbents has also received grant and contract funding to develop the HemoDefend platform to enable both universal plasma and fresh whole blood transfusions through the reduction of anti-A and anti-B blood group antibodies. Today, plasma and whole blood products must be carefully blood-type matched to prevent potentially fatal hemolytic transfusion reactions in the recipient, caused by the accidental administration of mismatched blood products. The reduction of anti-A and anti-B antibodies could potentially reduce or eliminate this risk, allowing for a broader range of available donors and simplifying the transfusion process. According to the American Red Cross, nearly 10,000 units of plasma are needed daily in the United States, or more than 3.5 million units a year. The World Health Organization (WHO) reports that plasma is transfused at a rate of 2.2 - 18.9 units per 1,000 population (median 7.7 units) globally. In westernized countries alone, with a population of 1.5 billion, there are approximately 12 million units of plasma administered each year. The total addressable market for HemoDefend-BGA in transfusion medicine in westernize countries alone is an estimated $400 million to $600 million, and represents a fraction of the global market.\\nRadiocontrast Removal\\nContrastSorb is a development-stage blood purification technology that is being optimized for the removal of IV contrast from blood in order to prevent CIN. Contrast-induced nephropathy is the acute loss of renal function within the first 48 hours following IV contrast administration. IV contrast is widely administered to patients undergoing CT scans, to enhance the images and make it easier to identify anatomic structures. IV contrast is also administered during invasive and interventional cardiovascular procedures in the brain, heart, limbs, and other parts of the body to diagnose and treat atherosclerosis (narrowing of blood vessels due to cholesterol deposits), vascular injury, aneurysms, etc. For example, an estimated 10 million coronary angiograms are performed worldwide each year to diagnose and treat coronary artery disease by placing coronary stents, performing balloon angioplasty, or atherectomy (removal of plaque in arteries). Overall, there are an estimated 80 million doses of IV contrast administered worldwide each year, split between approximately 65 million contrast-enhanced CT scans, 10 million coronary angiograms, and 5 million conventional angiograms. There are an estimated 30 million doses administered each year in the U.S. alone. The reported risk of CIN in patients undergoing contrast enhanced CT scans has been reported to be 2% to 13%. For coronary intervention, the risk has been estimated to be as high as 20% to 30% in high risk patients with pre-existing renal insufficiency, long-term diabetes, hypertension, congestive heart failure, and older age. The use of low osmolar IV contrast, hydration of patients pre-procedure, orally administration of N-acetylcysteine, and other agents to prevent CIN have demonstrated modest benefit in some clinical studies, but in many cases, the results across studies have been equivocal and inconsistent. In high risk patients, the direct removal of IV contrast from the blood with ContrastSorb to prevent CIN represents a potentially more effective alternative. The worldwide market opportunity for ContrastSorb in this high risk group is approximately $1 billion to $2 billion.\\nDrug Removal\\nDrugSorb is a development-stage blood purification technology that is capable of removing a wide variety of drugs and chemicals from blood, as a potential treatment for drug overdose, drug toxicity, toxic chemical exposure, use in high-dose regional chemotherapy, and other applications. It has demonstrated extremely high single pass removal efficiency of a number of different drugs that exceeds the extraction capability of hemodialysis or other filtration technologies. It is similar in action to activated charcoal hemoperfusion cartridges that have been available for many years, but has the advantage of having inherent biocompatibility and hemocompatibility without coatings, and can be easily customized for specific agents.\\nChronic Kidney Failure\\nThe National Kidney Foundation estimates that more than 20 million Americans have chronic kidney disease. Left untreated, chronic kidney disease can ultimately lead to chronic kidney failure, which requires a kidney transplant or chronic dialysis (generally three times per week) to sustain life. There are approximately 500,000 patients in the U.S. currently receiving chronic dialysis and more than 3.0 million worldwide. Approximately 66% of patients with chronic kidney disease are treated with hemodialysis. One of the problems with standard high-flux dialysis is the limited ability to remove certain mid-molecular weight toxins such as β2 -microglobulin. Over time, β2 -microglobulin can accumulate and cause amyloidosis in joints and elsewhere in the musculoskeletal system, leading to pain and disability. Our BetaSorb device has been designed to remove these mid-molecular weight toxins when used in conjunction with standard dialysis. Standard dialysis care typically involves three sessions per week, averaging approximately 150 sessions per year.\\nProducts\\nThe polymer adsorbent technology used in our products can remove middle molecular weight toxins, such as cytokines, from blood and physiologic fluids. All of the potential applications described below (i.e., the adjunctive treatment and/or prevention of sepsis; the adjunctive treatment and/or prevention of other critical care conditions such as acute respiratory distress syndrome, burn injury, trauma and pancreatitis; the prevention of damage to organs donated by brain-dead donors prior to organ harvest; the prevention of post-operative complications of cardiopulmonary bypass surgery; the prevention of kidney injury from IV contrast; and the treatment of chronic kidney failure) share in common high concentrations of toxins in the circulating blood. However, because of the limited studies we have conducted to date, we are subject to substantial risk that our technology will have little or no effect on the treatment of any of these indications. In 2011, we completed our European Sepsis Trial of our CytoSorb device. The study was a randomized, open label, controlled clinical study in 14 sites in Germany of 100 critically ill patients with predominantly septic shock and respiratory failure. The trial successfully demonstrated the ability of CytoSorb to reduce levels of key cytokines from whole blood in treated patients, and that treatment was safe in these critically-ill patients with multiple organ failure. We completed the CytoSorb technical file review with our notified body and CytoSorb subsequently received EU regulatory approval under the CE Mark as an extracorporeal cytokine filter indicated for use in any clinical situation where cytokines are elevated. Given sufficient and timely financial resources, we intend to continue to commercialize in Europe and conduct additional clinical studies of our products. However, there can be no assurance that we will ever obtain regulatory approval for any other device, or that the CytoSorb device will be able to generate significant sales.\\nWe manufacture the CytoSorb device at our facility located in Monmouth Junction, New Jersey. We purchase our raw materials from multiple vendors located primarily in the United States. We believe that our risk of an interruption in the supply of our raw materials is minimal due to the use of multiple vendors and the availability of alternate vendors. We do not have contractual minimum finished goods inventory requirements, however our practice is to maintain a minimum inventory level sufficient to provide a supply of products for the next three months.\\nThe CytoSorb Device (Critical Care)\\nAPPLICATION: Adjunctive Therapy in the Treatment of Sepsis\\nSepsis is a potentially life-threatening disease defined as “life-threatening organ dysfunction caused by a dysregulated host response to an infection”. Sepsis is mediated by high levels of inflammatory mediators such as cytokines, which are released into the bloodstream as part of the body’s immune response to severe infection or injury. Excessive concentrations of these mediators cause severe inflammation and damage healthy tissues, which can lead to organ dysfunction and failure. Organ failure is the leading cause of death in the ICU. Sepsis is very expensive to treat and has a high mortality rate.\\nPotential Benefits: To the extent our adsorbent blood purification technology is able to prevent or reduce the accumulation of cytokines, toxins, or other inflammatory mediators in the circulating blood, we believe our products may be able to prevent or mitigate severe inflammation, organ dysfunction and failure in sepsis patients. Therapeutic goals as an adjunctive therapy include improved clinical outcome, reduced ICU and total hospitalization time, and reduced hospital costs.\\nBackground and Rationale: We believe that the effective treatment of sepsis is the most valuable potential application for our technology. Severe sepsis (sepsis with organ dysfunction) and septic shock (severe sepsis with persistent hypotension despite fluid resuscitation) carries mortality rates of between 20% and 80%. Death can occur within hours or days, depending on many variables, including cause, severity, patient age and co-morbidities. There are approximately 1.6 million new cases of sepsis in the U.S. each year; and based on a recent 2020 The Lancet study, the worldwide incidence is estimated to be 49 million cases annually, accounting for 1 in every 5 deaths globally. The incidence of sepsis is also rising due to:\\n●an aging population;\\n●increased incidence of antibiotic resistance;\\n●increase in co-morbid conditions like cancer and diabetes; and\\n●increased use of indwelling medical devices that are susceptible to infection.\\nIn the U.S. alone, treatment of sepsis costs nearly $20 billion annually. According to the CDC, sepsis is a top ten cause of death in the U.S. The incidence of sepsis is believed to be under-reported as the primary infection (i.e., pneumonia, pyelonephritis, etc.) is often cited as the cause of death.\\nAn effective treatment for sepsis has been elusive. Pharmaceutical companies have been trying to develop drug therapies to treat the condition. With the exception of Xigris® from Eli Lilly, no other products have been approved in either the U.S. or Europe for the treatment of sepsis. In 2011, after completing a follow up study required by the FDA, it was subsequently determined that Xigris® did not have a statistically significant mortality benefit, and Eli Lilly withdrew Xigris® from all markets worldwide.\\nMany medical professionals believe that blood purification for the treatment of sepsis holds tremendous promise. Studies using dialysis and hemofiltration technology have been encouraging, but have only had limited benefit to sepsis patients. The reason for this appears to be rooted in a primary limitation of dialysis technology itself: the inability of standard dialysis to effectively and efficiently remove significant quantities of larger toxins such as cytokines from circulating blood. CytoSorb has demonstrated the ability to safely reduce key cytokines in the blood of septic patients with multiple organ failure in our European Sepsis Trial.\\nThe ability of CytoSorb to interact safely with blood (hemocompatibility) has been demonstrated through ISO 10993 testing, which includes testing for hemocompatibility, biocompatibility, cytotoxicity, genotoxicity, acute sensitivity and complement activation. CytoSorb use has been considered safe and well-tolerated in more than 121,000 human treatments to date.\\nCytoSorb has been designed to achieve broad-spectrum removal of both pro- and anti-inflammatory cytokines, preventing or reducing the accumulation of high concentrations in the bloodstream. It also removes a wide range of inflammatory mediators such as activated complement, bacterial toxins, myoglobin, free hemoglobin, bilirubin, and many others. This approach is intended to modulate the immune response without causing damage to the immune system. For this reason, researchers have referred to the approach reflected in our technology as “immunomodulatory” therapy.\\nProjected Timeline: In 2011, the CytoSorb filter received EU regulatory approval under the CE Mark as an extracorporeal cytokine filter to be used in clinical situations where cytokines are elevated. Our manufacturing facility has also achieved ISO 13485:2003 Full Quality Systems certification, an internationally recognized quality standard designed to ensure that medical device manufacturers have the necessary comprehensive management systems in place to safely design, develop, manufacture and distribute medical devices in the EU. We are currently manufacturing our CytoSorb device for commercial sale in the EU. We are currently selling CytoSorb in Germany, Austria, Switzerland, Belgium, Luxembourg, Poland, Norway, Sweden, Denmark, and the Netherlands with a direct sales force. Based on its CE Mark approval, CytoSorb can also be sold throughout all 27 countries of the EU, the United Kingdom and countries outside the EU that will accept European regulatory approval with registration. Overall, we have established either direct sales or distribution (via distributors or strategic partners) of CytoSorb in 67 countries worldwide. Registration of CytoSorb is typically required in each of these countries prior to active commercialization. With CE Mark approval, this can be typically achieved within several months in EU countries. Outside of the EU, the process is more variable and can take months to more than a year due to different requirements for documentation and clinical data. Variability in the timing of registration affects the initiation of active commercialization in these countries, which affects the timing of expected CytoSorb sales. We actively support all of our distributors and strategic partners in the product registration process. Outside of the EU, CytoSorb has distribution in Turkey, India, Sri Lanka, Australia, New Zealand, Russia, Serbia, Norway, Vietnam, Malaysia, Hong Kong, Chile, Panama, Costa Rica, Colombia, Brazil, Mexico, Argentina, Perú, Guatemala, Ecuador, Bolivia, the Dominican Republic, El Salvador, Iceland, Israel, UAE, Iran, Saudi Arabia and other Middle Eastern countries, and South Korea. We cannot generally predict the timing of these registrations, and there can be no guarantee that we will ultimately achieve registration in countries where we have established distribution. We also cannot guarantee that we will generate meaningful sales in the countries where we have established registration, due to other factors such as market adoption and reimbursement. We are currently actively evaluating other potential distributor and strategic partner networks in other major countries that accept CE Mark approval. With sufficient resources and continued positive clinical data, assuming availability of adequate and timely funding, and continued positive results from our clinical studies, we intend to continue our commercialization plans for our product worldwide as well as to pursue U.S. clinical trials to seek FDA regulatory approval for CytoSorb in the U.S. by 2022.\\nAPPLICATION: Adjunctive Therapy in Other Critical Care Applications\\nPotential Benefits: Cytokine-mediated organ damage and immune suppression can increase the risk of death and infection in patients with commonly seen critical care illnesses such as acute respiratory distress syndrome, severe burn injury, trauma and pancreatitis. By reducing both pro- and anti-inflammatory cytokines, CytoSorb has the potential to reduce the systemic inflammatory response and:\\n·\\nprevent or mitigate multiple organ dysfunction syndrome (“MODS”) and/or multiple organ failure (“MOF”);\\n·\\nprevent or reduce secondary infections;\\n·\\nreduce the need for expensive life-sparing supportive care therapies such as mechanical ventilation; and\\n·\\nreduce the need for ICU care, freeing expensive critical care resources, and reducing hospital costs and costs to the healthcare system.\\nBackground and Rationale: A shared feature of many life-threatening conditions seen in the ICU is severe inflammation (either sepsis or systemic inflammatory response syndrome) due to an over-reactive immune system and high levels of cytokines that can cause or contribute to organ dysfunction, organ failure and patient death. Examples of such conditions include severe burn injury, trauma, acute respiratory distress syndrome and severe acute pancreatitis. MODS and MOF are common causes of death in these illnesses and mortality is directly correlated with the number of organs involved. There are currently few active therapies to prevent or treat MODS or MOF. If CytoSorb can reduce direct or indirect cytokine injury of organs, it may mitigate MODS or MOF, improve overall patient outcome and reduce costs of treatment. In addition, secondary infection, such as ventilator-acquired pneumonia, urinary tract infections, or catheter-related line infections, are another major cause of morbidity and mortality in all patients treated in the ICU that increase with longer ICU stay. Prolonged illness, malnutrition, age, multiple interventional procedures, and exposure to antibiotic resistant pathogens are just some of the many risk factors for functional immune suppression and infection. In sepsis and SIRS, the overexpression of pro-inflammatory cytokines can also cause a depletion of immune effector cells through apoptosis and other means, and anti-inflammatory cytokines can cause profound immune suppression, both major risk factors for infection.\\nProjected Timeline: The EU CE Mark approval for CytoSorb as an extracorporeal cytokine filter and its broad approved indication to be used in any clinical situation where cytokines are elevated, allows it to be used “on label” in critical care applications such as acute respiratory distress syndrome, severe burn injury, trauma, liver failure, and pancreatitis, and in other conditions where cytokine storm, sepsis and/or SIRS plays a prominent role in disease pathology. In addition, the expanded indications for use label now includes reduction of bilirubin and reduction of myoglobin, further strengthens the on-label use of the technology for the treatment of liver disease, and severe trauma, respectively. Our goal is to stimulate investigator-initiated clinical studies with our device for these applications. Currently, we have more than 50 investigator initiated or company-sponsored studies being planned, enrolling, or completed. We have been moving forward in parallel with a program to further understand the potential benefit of CytoSorb hemoperfusion in these conditions through additional investigational animal studies and potential human pilot studies in the U.S. funded either directly by us, through grants, or through third-parties. Commencement of these and other formal studies is contingent upon adequate funding and, in the case of U.S. human studies, FDA IDE approval of the respective human trial protocols.\\nAPPLICATION: Prevention and treatment of post-operative complications of cardiopulmonary bypass surgery\\nPotential Benefits: If CytoSorb is able to prevent or reduce high levels of cytokines, free hemoglobin, and other inflammatory mediators from accumulating in the bloodstream during and following cardiac surgery, we anticipate that post-operative complications of cardiopulmonary bypass surgery may be able to be prevented or mitigated. In addition, CytoSorb can remove certain anti-thrombotics such as ticagrelor and rivaroxaban during cardiopulmonary bypass in patients requiring urgent or emergent surgery. The primary goals for this application are to:\\n·\\nreduce ventilator and oxygen therapy requirements;\\n·\\nreduce post-operative complications such as ARDS, acute kidney injury, post-perfusion syndrome, and the SIRS;\\n·\\nreduce length of stay in hospital ICUs;\\n·\\nreduce the total cost of patient care;and\\n·\\nreduce the risk of post-operative bleeding complications such as need for blood and platelet transfusions, rethoracotomy, and death\\nBackground and Rationale: Due to the highly invasive nature of cardiopulmonary bypass surgery, high levels of cytokines are produced by the body, triggering severe inflammation. In addition, hemolysis of red blood cells frequently occurs, resulting in the release of free hemoglobin into the bloodstream. These inflammatory mediators can lead to post-operative complications. CytoSorb is the only cytokine reduction technology approved in the EU that can be used intraoperatively in a bypass circuit in a heart-lung machine during cardiopulmonary bypass without the need for another machine. If our products are able to prevent or reduce the accumulation of cytokines or free hemoglobin in a patient’s blood stream, we may be able to prevent or mitigate post-operative complications caused by an excessive or protracted inflammatory response to the surgery. Intra-operative use of CytoSorb on high-risk cardiac surgery patients, where the risk of post-operative complications is the highest, is expected to be the main initial target market. The use of CytoSorb in the post-operative period to treat post-operative SIRS is another application of the technology.\\nCytoSorb was recently approved to remove the anti-platelet agent, ticagrelor, during cardiac surgery involving cardiopulmonary bypass via label expansion of its CE Mark. Ticagrelor (Brilinta®, Astra Zeneca) is a widely-used anti-platelet agent used to decrease cardiovascular risk and risk of stroke in patients with a known history of heart disease or heart attack. It is also widely used during dual-anti platelet therapy in patients with acute coronary syndrome undergoing percutaneous coronary intervention and stent placement. However, when patients on ticagrelor require emergent or urgent cardiac surgery, up to 65% of patients will have severe or massive peri-operative bleeding complications that contributes to a high risk of death and major costs to the healthcare system. CytoSorb has already demonstrated the ability to remove ticagrelor rapidly and efficiently from human blood in vitro. Meanwhile, a retrospective case series reported by clinicians at Asklepios Klinik St. Georg in Hamburg, Germany on the investigational use of CytoSorb to reverse the effects of ticagrelor during emergency cardiac surgery demonstrated a greatly reduced risk of bleeding complications and the need for repeat surgery to explore the source of bleeding, with extrapolations showing projected cost savings of £3,982, or approximately $5,000 USD, per patient in a U.K. based study.\\nProjected Timeline: Cardiac surgeons, cardiac perfusionists, and cardiothoracic ICU intensivists in Germany, Austria, and other countries have now used CytoSorb intra-operatively and post-operatively in more than 40,000 treatments in cardiac surgery patients. This application is also the focus of number of planned and enrolling company-sponsored and investigator-initiated studies in the United States and Europe.\\nCytoSorb is the subject of a pivotal, 400-patient randomized controlled trial in the United States called the REFRESH 2-AKI trial. Two CytoSorb cartridges are being used intraoperatively to reduce activated complement, free hemoglobin, and other inflammatory toxins that are generated during valve replacement surgery as well as aortic reconstruction with hypothermic cardiac\\narrest, with the goal of reducing the risk of acute kidney injury. Acute kidney injury following cardiac surgery is associated with an increased risk of death in the first 5 years after surgery. The trial has enrolled more than 150 patients to date. The Company has been undertaking multiple activities in preparation to resume the study, which is estimated to take place in the first half of 2021. If the study is successful, we plan to submit a PMA application to the FDA in 2023 for U.S. regulatory approval.\\nThe 250-patient randomized controlled REMOVE infective endocarditis trial was completed in January 2020. The COVID-19 pandemic has caused delays in data monitoring and data analysis. Topline data are expected to be reported in the first half of 2021 with full data presentation at a major international conference also in 2021.\\nWe are currently conducting the 30-patient, single arm trial in the United Kingdom called the TISORB trial, obtaining more country-specific data to support the use of CytoSorb to remove ticagrelor in emergent or urgent cardiac surgery to reduce perioperatively bleeding complications. Due to the COVID-19 pandemic, the execution of the TISORB trial has been greatly impacted and ongoing national restrictions in the UK on the conduct of non-COVID clinical studies add further uncertainty to TISORB.\\nFor further detailed information regarding our clinical trial strategy, see the section entitled \"Clinical Studies\" of this Item 1 of this Report.\\nAPPLICATION : Maintaining or improving the quality of solid organs harvested from donors for organ transplant\\nPotential Benefits:\\nECOS-300CY: Solid organ transplant is very costly, and the success of the transplant is heavily dependent upon the health and quality of the harvested organs. ECOS-300CY was designed to maintain or improve the quality of these organs prior to transplant in an ex vivo perfusion system, and may have the benefit of improving outcomes in organ transplant and also increasing the availability of organs by rehabilitating organs that would have otherwise been discarded.\\nCytoSorb : By preventing or reducing high-levels of cytokines from accumulating in the bloodstream of brain-dead organ donors, we believe CytoSorb may be able to mitigate organ dysfunction and failure, which results from severe inflammation following brain-death. The primary goals for this application are:\\n·\\nimproving the viability of organs which can be harvested from brain-dead organ donors, and\\n·\\nincreasing the likelihood of organ survival following transplant.\\nBackground and Rationale: When brain death occurs, the body responds by generating large quantities of inflammatory cytokines. This process is similar to the systemic inflammatory response syndrome and sepsis. A high percentage of donated organs are never transplanted due to this response, which damages healthy organs and prevents transplant. In addition, inflammation in the donor may damage organs that are harvested and reduce the probability of graft survival following transplant. CytoSorb treatment in a porcine animal model of brain death demonstrated a reduction in cytokines as well as a preservation of cardiac function compared to untreated controls.\\nThere is a shortage of donated organs worldwide, with approximately 100,000 people currently on the waiting list for organ transplants in the U.S. alone. Because there are an insufficient number of organs donated to satisfy demand, it is vital to maximize the number of viable organs donated, and optimize the probability of organ survival following transplant.\\nProjected Timeline: ECOS-300CY: The ECOS-300CY was approved in the E.U. for the removal of inflammatory mediators during ex vivo organ perfusion under CE Mark designation in 2020. CytoSorbents announced a partnership with Aferetica srl to provide the ECOS-300CY cartridge under the exclusive trade name, PerSorb™, that is compatible with Aferetica’s PerLife™ ex vivo organ perfusion system, recently approved in the E.U. as well.\\nCytoSorb for brain dead organ donors: Studies have been conducted under a $1 million grant from the Health Resources and Services Administration (“HRSA”), an agency of the U.S. Department of Health and Human Services. Researchers at the University of Pittsburgh Medical Center and the University of Texas, Houston Medical Center have completed the observational and dosing phases of the project. The results were published in Critical Care Medicine, January 2008. The next phase of this study, the treatment phase, would involve viable donors treated with the CytoSorb device. In this phase of the project, viable donors will be treated and the survival and function of organs in transplant recipients will be tracked and measured. The treatment phase would be contingent upon further discussion with the FDA and HRSA regarding study design, as well as obtaining additional funding.\\nThe VetResQ Device (Animal Health Critical Care)\\nAPPLICATION: Adjunctive Therapy in the Treatment of Sepsis, Pancreatitis and Other Critical Illnesses in Animals\\nPotential Benefits and Rationale: In January 2017, the VetResQ device became commercially available for the United States veterinary market. VetResQ is a broad spectrum blood purification adsorber based upon similar underlying technology to CytoSorb and has been configured in 3 sizes (50, 150 and 300mL sized cartridges) to accommodate treatment of small, medium, and large animals such as cats, dogs, and high-value animals such as foals and horses. VetResQ is compatible with standard hemodialysis, continuous renal replacement therapy (“CRRT”), and hemoperfusion blood pumps. Like CytoSorb, VetResQ is designed to help treat (via hemoadsorption of cytokines, bacterial toxins and other inflammatory mediators) deadly inflammation and toxic injury in animals with critical illnesses such as septic shock, toxic shock syndrome, toxin-mediated diseases, pancreatitis, trauma, liver failure, drug intoxication, and lung injury. Critical illness in animals is similar to that in humans. Based upon cumulative studies, VetResQ is capable of reducing a broad range of excessive inflammatory mediators and toxins that could otherwise cause direct tissue injury or serious systemic inflammation that can rapidly lead to instability, organ failure, and death. VetResQ is available in the U.S. only for veterinary animal usage and is not for human use.\\nProjected Timeline: VetResQ is available for commercial purchase for animal health applications in the United States. The FDA was notified of the launch in 2016 and we have provided the FDA with the related instructions for use and a marketing brochure.\\nThe CytoSorb-XL Device (Critical Care)\\nAPPLICATION: Adjunctive Therapy in the Treatment of Sepsis and other critical illnesses\\nPotential Benefits and Rationale: The CytoSorb-XL device is a next-generation porous polymer under advanced development and targets the same markets as CytoSorb. Through novel patent-pending chemistry, CytoSorb-XL adds the ability to reduce Gram negative bacterial endotoxin (lipopolysaccharide) to broad spectrum cytokine, exotoxin, and other inflammatory mediator removal. CytoSorb-XL removed comparable amounts of endotoxin when compared in vitro against the leading standalone endotoxin filter, Toraymyxin (Toray, Japan). This could potentially increase the effectiveness of CytoSorb in sepsis and septic shock caused by Gram negative bacteria.\\nProjected Timeline: CytoSorb-XL is in advanced pre-clinical development as a potential next generation polymer to CytoSorb. It is expected to follow a similar path to E.U. approval as CytoSorb, expected within 4-5 years.\\nThe HemoDefend Blood Purification Technology Platform (Acute and Critical Care)\\nAPPLICATION: Reduction of contaminants in the blood supply that can cause transfusion reactions or disease when administering blood and blood products to patients.\\nPotential Benefits: The HemoDefend RBC blood purification technology platform is designed to reduce contaminants in the blood supply that can cause transfusion reactions or disease. It is a development stage technology that is not yet approved in any markets, but is comprised of our highly advanced, biocompatible, polymer bead technology. If this technology is successfully developed and then incorporated into a regulatory approved product, it could have a number of important benefits, including:\\n·\\nreduce the risk of transfusion reactions and improve patient outcome;\\n·\\nimprove the quality, or extend the shelf life of stored blood products;\\n·\\nimprove the availability of blood and reduce blood shortages by reducing the limitations of donors to donate blood; and\\n·\\nallow easier processing of blood.\\nBackground and Rationale: The HemoDefend technology platform was built upon our successes in designing and manufacturing porous polymer beads that can remove cytokines. We have expanded the technology to be able to remove substances as small as drugs and bioactive lipids, to proteins as large as antibodies from blood that can cause transfusion reactions and disease. Although the frequency of these reactions are relatively low (approximately 3% to 5%), the sheer number of blood transfusions is so large, that the number of transfusion reactions, ranging from mild to life-threatening, is substantial, ranging from several hundreds of thousands to millions of reactions each year. In critically-ill patients, the risk of transfusion reactions is significantly higher than in the general population and can increase the risk of death because their underlying illnesses have depleted protective mechanisms and have primed their bodies to respond more vigorously to transfusion-associated insults.\\nA number of retrospective studies have also suggested that administration of older blood leads to increased adverse events and even increased mortality, compared with blood recently harvested. Biological studies have demonstrated the accumulation of erythrocyte storage lesions that compromise the function and structural integrity of packed red blood cells and have also demonstrated the accumulation of substances during blood storage that can lead to transfusion reactions. Three adult, prospective, randomized, controlled studies, RECESS (completed), ABLE (completed), and TRANSFUSE (completed) were designed to evaluate the morbidity and mortality in cardiovascular surgery patients (RECESS) and critically ill patients (ABLE and TRANSFUSE), treated with either “new or fresh” or “older” blood. The RECESS Trial was a randomized, controlled trial in a total of 1,098 evaluable patients undergoing complex cardiac surgery given fresh blood (≤10 days old) as compared to older blood (≥21 days old). The overall conclusion was that the age of blood had no statistically significant impact on the progression to organ dysfunction (as measured by the multiple organ dysfunction syndrome score) or death. However, a statistically significant increase in hepatobiliary-related serious adverse events (5% fresh vs 9% older, p=0.02) was related to hyperbilirubinemia, possibly caused by hemolysis and release of free hemoglobin in old blood. The serious adverse event rate in both new and old blood groups was approximately 50%, which is considered high for this group of patients. There are many details and subgroup analyses that were not discussed, particularly an analysis of those patients receiving more units of blood than average, as the risk of adverse events is cumulative. The ABLE Trial was a randomized, controlled trial in 2,430 critically-ill patients receiving either fresh (≤ 7 days) or standard issue blood. There was no difference in 90-day mortality between the two groups. The TRANSFUSE Trial was a large scale RCT in Australia evaluating the impact of age of leukodepleted pRBCs (short-term storage: 11.8 days mean, N=2,457, mean 4.1 units transfused; long-term storage: 22.4 days mean, N=2,462, m) on 90-day mortality in critically-ill patients. There was no significant difference in 90-day mortality (24.8% mortality short-term storage vs 24.1% long-term storage) though there were statistically more febrile non-hemolytic transfusion reactions (n=123; 5% short-term storage vs n=88; 3.6% long-term storage). Also, patients who had short-term storage blood with APACHE III > 21.5% (median risk), demonstrated higher mortality (37.7% vs 34% long-term storage , p=0.05). The outcomes of these trials do not alter the current pressing need for better solutions to purify transfused blood products in order to reduce transfusion-related adverse events and improve clinical outcome, but suggest that age of blood is not the critical factor.\\nProjected Timeline: The HemoDefend platform is a development stage product based on our advanced polymer technology. The base polymer is ISO 10993 biocompatible, meeting standards for biocompatibility, hemocompatibility, cytotoxicity, genotoxicity, acute sensitivity and complement activation. HemoDefend has demonstrated the in vitro removal of many different substances from blood such as antibodies, free hemoglobin, cytokines and bioactive lipids. We have also prototyped a number of different implementations of the HemoDefend technology, including the “Beads in a Bag” blood treatment blood storage bag, and standard in-line blood filters. The technology has been supported by the NHLBI, a division of the National Institute of Health, under a Phase I SBIR, an awarded $1.5M Phase II SBIR contract (funded by NHLBI and U.S. Special Operations Command (USSOCOM)), and more recently under a $3M multi-year Phase IIB bridge contract funded by NHLBI. As a result of delays caused by the COVID-19 pandemic, we expect to complete bench testing required for U.S. approval in 2021, and advance the in-line filter to human testing in the first half of 2022.\\nAPPLICATION: Removal of anti-A and anti-B blood group antibodies from fresh whole blood and plasma\\nPotential Benefits: The HemoDefend-BGA blood purification technology platform is designed to reduce anti-A and anti-B antibodies in plasma and whole blood. The goal is to either enable the production of universal plasma, or enable fresh warm whole blood transfusions. If this technology is successfully developed and then incorporated into a regulatory approved product, it could have a number of important benefits, including:\\n·\\nreduce the risk of transfusion reactions and improve patient outcome;\\n·\\neliminate the need to blood-type plasma, improving its availability\\n·\\nenable the use of low titer whole blood, ideal for trauma resuscitation; and\\n·\\neasier processing of blood products.\\nBackground and Rationale: Plasma is the straw-colored, cell-free portion of whole blood. It contains a wide range of important substances such as electrolytes, hormones, proteins such as albumin, clotting factors, and antibodies. The transfusion of plasma, or plasma-derived products, is used widely to help save the lives of trauma and bleeding victims, septic and other critically-ill patients, and patients with life-threatening blood coagulation and autoimmune disorders. In 2008, more than 4.5 million units of plasma were transfused in the United States alone. With the exception of the relatively uncommon Type AB, or “universal” plasma, most plasma contains blood-type specific antibodies and must be cross-matched with the intended recipient ahead of time or risk serious transfusion reactions. By reducing these blood-type specific antibodies, the goal is to create a cost-effective, reliable, and expanded source of “universal” plasma that can be administered immediately, without blood-typing, in a wide range of emergent and non-emergent situations.\\nProjected Timeline: The HemoDefend-BGA platform is a development stage product based on our advanced blood purification technology. Prototype filtration devices have been evaluated by a government agency, resulting in excellent depletion of both anti-A and anti-B antibodies. Work is continuing to advance these prototypes to commercial-ready devices. This work has received cumulatively approximately $9.6 million in Phase I and II Small Business Technology Transfer (STTR) funding by the U.S. Army Medical Research Acquisition Activity (USAMRAA), U.S. Army Medical Research and Materiel Command (USAMRMC), Defense Health Agency, and CDMRP.\\nK+ontrol (Acute and Critical Care)\\nAPPLICATION: Treatment of severe hyperkalemia that can occur in patients with life-threatening conditions such as trauma, burn injury, kidney failure, tumor lysis syndrome, and those with no access to dialysis\\nPotential Benefits: K+ontrol was developed to rapidly treat severe hyperkalemia by reducing potassium in the blood. Although hemodialysis remains the definitive treatment for severe hyperkalemia, K+ontrol represents a simpler, and more flexible alternative. The primary goals for this application are to:\\n·\\nEnable the rapid treatment of deadly hyperkalemia without the need for hemodialysis\\n·\\nPrevent potentially fatal cardiac arrhythmias following severe injury\\n·\\nImprove survival in victims in remote areas and during prolonged field care in combat\\nBackground and Rationale: Potassium is an important electrolyte in the body that is present inside cells at high concentrations, with the amount in blood tightly regulated. Following injury to cells by, for example, trauma, burn injury, ischemia, or cytotoxic drugs, such cells will continuously leak high levels of potassium into the blood, resulting in hyperkalemia. The kidneys normally excrete excess potassium from the blood, but when compromised, as in critically-ill patients suffering from kidney failure or in chronic dialysis patients with end-stage kidney disease, the levels of blood potassium can rapidly rise unabated. When the potassium level in the blood exceeds a concentration of 6.0 mmol/L (normal 3.6 - 5.2 mmol/L), the risk of heart arrhythmias and sudden cardiac death increases significantly. Orally administered potassium sorbents such as Kayexalate® (Sanofi-Aventis) and Veltassa® (Relypsa) are only recommended for the non-emergent lowering of mild to moderate hyperkalemia, while the use of insulin and glucose to drive potassium into cells in severe hyperkalemia is only a temporary strategy. Dialysis has been the definitive treatment of severe hyperkalemia, but requires a large dialysis machine, electricity, bags of dialysate, a skilled technician, and prolonged treatment times that are not practical in certain situations such as in remote locations, during prolonged field care in combat, in areas that lack modern medical facilities, or in situations where the numbers of victims outstrip available dialysis equipment and supplies. Because of this, there is a major need for simple, but effective ways to rapidly treat severe hyperkalemia.\\nHyperkalemia is a common problem and has been reported to occur in 1.7-5.2% of hospitalized patients in a number of studies. It has also been recognized as a serious complication of combat injury since World War II, when hyperkalemia and acute kidney injury was associated with a mortality rate of 90%, and was a leading cause of post-traumatic death in the Korean War, until the advent of dialysis therapy. In the wars in Iraq and Afghanistan, an estimated 5.8% of all combat casualties developed hyperkalemia within 48 hours of injury. Even in non-crush traumatic injury, severe hyperkalemia (>6 mmol/L) occurred in approximately 20% of patients. Hyperkalemia was also observed in approximately 16% of victims of natural disasters such as earthquakes, where crush injury is common.\\nProjected Timeline: K+ontrol has demonstrated the ability to reduce potassium in several animal models of hyperkalemia and is currently being optimized with funding support from the U.S. Army and Defense Health Agency under under a Phase I and Phase II SBIR contract for a total of $1.15 million and a $3 million Rapid Innovation Fund (RIF) award from the U.S. Air Force Materiel Command. We plan to move forward with clinical development of this product, pending the successful outcome of these animal studies.\\nContrastSorb (Radiology and Interventional Radiology)\\nAPPLICATION: Removal of IV contrast in blood administered during CT imaging, an angiogram, or during a vascular interventional radiology procedure, in order to reduce the risk of contrast-induced nephropathy.\\nPotential Benefits: IV contrast can lead to CIN, in susceptible patients. Risk factors include chronic kidney disease and renal insufficiency caused by age, diabetes, congestive heart failure, long-standing hypertension, and others co-morbid illnesses. CIN can lead to increased risk of patient morbidity and mortality. Removal of IV contrast by ContrastSorb may:\\n·\\nreduce the risk of acute kidney injury\\n·\\nimprove the safety of these procedures and reduce the risk of morbidity and mortality\\nBackground and Rationale: Contrast-induced nephropathy is the acute loss of renal function within the first 48 hours following IV contrast administration. IV contrast is widely administered to patients undergoing CT scans, to enhance the images and make it easier to identify anatomic structures. IV contrast is also administered during vascular interventional radiology procedures and angiography of blood vessels in the brain, heart, limbs, and other parts of the body to diagnose and treat atherosclerosis (narrowing of blood vessels due to cholesterol deposits), vascular injury, aneurysms, etc. The reported risk of CIN undergoing contrast enhanced CT scans has been reported to be 2% to 13%. For coronary intervention, the risk has been estimated to be as high as 20% to 30% in high risk patients with pre-existing renal insufficiency, and other risk factors. The use of low osmolar IV contrast, hydration of patients pre-procedure, orally administration of N-acetylcysteine, and other agents to prevent CIN have demonstrated modest benefit in some clinical studies, but in many cases, the results across studies have been equivocal and inconsistent. In high risk patients, the direct removal of IV contrast from the blood with ContrastSorb to prevent CIN represents a potentially more effective alternative.\\nProjected Timeline: ContrastSorb has demonstrated the high efficiency single pass removal of IV contrast and is in the process of optimization. The underlying polymer is made of the same ISO 10993 biocompatible polymer as CytoSorb, but with different structural characteristics. The ContrastSorb device is a hemoperfusion device similar in construction to CytoSorb and BetaSorb. Assuming successful optimization of the ContrastSorb polymer, safety and efficacy of IV contrast removal will need to be established in human clinical studies. We seek to out-license this technology to a potential strategic partner.\\nThe BetaSorb Device (Chronic Care)\\nAPPLICATION: Prevention and treatment of health complications caused by the accumulation of metabolic toxins in patients with chronic renal failure\\nPotential Benefits: If BetaSorb is able to prevent or reduce high levels of metabolic waste products from accumulating in the blood and tissues of long-term dialysis patients, we anticipate that certain health complications characteristic to these patients can be prevented or mitigated. The primary goals for this application are to:\\n·\\nimprove and maintain the general health of dialysis patients;\\n·\\nreduce disability and improve the quality of life of these patients\\n·\\nreduce the total cost of patient care; and\\n·\\nincrease life expectancy.\\nBackground and Rationale: Our BetaSorb device is intended for use on patients suffering from chronic kidney failure who rely on long-term dialysis therapy to sustain life. Due to the widely recognized inability of dialysis to remove larger proteins from blood, metabolic waste products, such as beta2-microglobulin, accumulate to toxic levels and are deposited in the joints and tissues of patients. Specific toxins known to accumulate in these patients have been linked to their severe health complications, increased healthcare costs, and reduced quality of life.\\nResearchers also believe that the accumulation of toxins may play an important role in the significantly reduced life expectancy experienced by dialysis patients. In the U.S., the average life expectancy of a dialysis patient is five years. Industry research has identified links between many of these toxins and poor patient outcomes. If our BetaSorb device is able to routinely remove these toxins during dialysis and prevent or reduce their accumulation, we expect our BetaSorb device to maintain or improve patient health in the long-term. We believe that by reducing the incidence of health complications, the annual cost of patient care will be reduced and life expectancy increased.\\nThe poor health experienced by beta2-microglobulin patients is illustrated by the fact that in the U.S. alone, more than $33 billion is spent annually caring for this patient population according to the United States Renal Data System, at a cost of approximately $88,000 per patient annually.\\nProjected Timeline: We have collected a significant amount of empirical data for the development of this application. As the developer of this technology, we had to undertake extensive research, as no comparable technology was available for reference purposes. We have completed four human pilot studies, including a clinical pilot of six patients in California for up to 24 weeks in which our BetaSorb device removed the targeted toxin, beta2-microglobulin, as expected. In total, we have sponsored clinical studies utilizing our BetaSorb device on 20 patients involving approximately 345 total treatments. Each study was conducted by a clinic or hospital personnel with us providing technical assistance as requested.\\nAs discussed above, due to practical and economic considerations, we are focusing our efforts and resources on commercializing our CytoSorb device for critical care and cardiac surgery applications. Following commercial introduction of the CytoSorb device, and with sufficient additional resources, we may continue development of the BetaSorb resin and may conduct additional clinical studies using the BetaSorb device in the treatment of end stage renal disease patients.\\nCommercial and Research Partners\\nBiocon Biologics Limited\\nIn September 2013, we entered into a distribution agreement with Biocon Biologics Limited, (“Biocon”), India’s largest biopharmaceuticals company, under which Biocon was granted exclusive commercialization rights to the CytoSorb therapy in India and select emerging markets, initially focused on sepsis. Biocon committed to annual minimum purchases to maintain exclusivity. In October 2014, the Biocon partnership was expanded to include all critical care applications and cardiac surgery. In addition, Biocon committed to higher annual minimum purchases of CytoSorb to maintain distribution exclusivity and committed to conduct and publish results from multiple investigator initiated studies and patient case studies. Under the terms of the expanded partnership, the term of the distribution agreement was extended to December 2022. On May 27, 2020, Biocon announced that CytoSorb has received approval from the Drugs Controller General of India to treat COVID-19 patients in certain instances.\\nFresenius Medical Care AG\\nIn December 2014, we entered into a multi-country strategic partnership with Fresenius Medical Care AG & Co KGaA (together with its affiliates, as appropriate, “Fresenius”) to commercialize the CytoSorb therapy. Under the agreement reflecting the terms of the partnership, Fresenius was granted exclusive rights to distribute CytoSorb for critical care applications in France, Poland, Sweden, Denmark, Norway, and Finland. The partnership allows Fresenius to offer an innovative and easy way to use blood purification therapy for removing cytokines in patients that are treated in the ICU. To promote the success of CytoSorb, Fresenius agreed to also engage in the ongoing clinical development of the product. This includes the support and publication of a number of small case series and patient case reports as well as the potential for future larger, clinical collaborations. In May 2016, Fresenius launched the product in the six countries for which it was granted exclusive distribution rights. In January 2017, the Fresenius partnership was expanded pursuant to a revised three-year agreement. The terms of the revised agreement extended Fresenius’ exclusive distributorship of CytoSorb for all critical care applications in their existing territories through 2019 and include guaranteed minimum quarterly orders and payments, evaluable every one and a half years.\\nAt the same time, we entered into a comprehensive co-marketing agreement with Fresenius. Under the terms of the co-marketing agreement, CytoSorbents and Fresenius agreed to jointly market CytoSorb to Fresenius’ critical care customer base in all countries where CytoSorb is being actively commercialized. CytoSorb continues to be sold by our direct sales force or through our international network of distributors and partners, while Fresenius sells all ancillary products to their customers. Fresenius further provides written endorsements of CytoSorb for use with their multiFiltrate and multiFiltratePRO acute care dialysis machines that can be used by us and our distribution partners to promote CytoSorb worldwide. Training and preparation for this co-marketing program began in five initial countries in 2017 and is continuing, with implementation of the co-marketing program in additional countries planned for the future.\\nIn December 2018, the Fresenius agreement signed in December 2014 was amended, to grant Fresenius exclusive distribution rights for the Czech Republic and Finland and all critical care medicine and ICU applications on dialysis or ECMO machines for France. In addition, in 2019, Poland, Sweden, Denmark, and Norway were transitioned into the co-marketing program. Finally, the guaranteed minimum quarterly purchases and payments requirements were removed for 2019.\\nIn addition, also in December 2018, we entered into agreements to expand the partnership with Fresenius into South Korea and Mexico. Under the terms of these agreements, Fresenius has exclusive rights to distribute CytoSorb for acute care and other hospital applications in South Korea and Mexico. Commercial sales of CytoSorb are underway in both countries after securing market registration clearance from the South Korean and Mexican health authorities in 2021 and 2020, respectively. These multi-year agreements include an initial stocking order and are subject to annual minimum purchases of CytoSorb to maintain exclusivity. These agreements, which commenced on January 1, 2019, have an initial term of three years and will automatically renew for an additional two years unless terminated by either party.\\nAferetica s.r.l.\\nIn 2015, we entered into a distribution agreement with Aferetica s.r.l., a distributor based in Bologna, Italy that specializes in the sale of certain medical products and devices, specifically extracorporeal therapies, in the critical care, cardiac surgery and liver disease markets (“Aferetica”). Under the terms of the agreement, we granted Aferetica the exclusive right to distribute CytoSorb in Italy, San Marino and the Vatican for application in CRRT (Continuous Renal Replacement Therapies), dialysis and hemoperfusion machine run treatments, as described in the agreement. In connection with the grant of distribution rights, Aferetica agreed to certain minimum purchase and inventory requirements. Aferetica further agreed not to market or sell products competitive with CytoSorb in Italy, San Marino and the Vatican. The agreement was renewed through 2023.\\nIn addition, in September 2017, we announced a partnership with Aferetica to provide dedicated, branded sorbent cartridges for use with Aferetica’s proprietary PerLife™ ex-vivo organ perfusion system, with the goal of rehabilitating or preserving the function solid organs destined for eventual transplant. In July 2018, Aferetica and CytoSorbents debuted the PerLife™ system for organ preservation at the 27th International Congress of the Transplantation Society. In the fourth quarter of 2020, Aferetica announced CE Mark registration of the PerLife system. At the same time, CytoSorbents announced CE Mark approval of the ECOS-300CY cartridge for the removal of inflammatory mediators during ex vivo perfusion, which has been designated, PerSorb™, a trade name exclusive to the PerLife system.\\nTerumo Cardiovascular Group\\nIn September 2016, we entered into a multi-country strategic partnership with Terumo Cardiovascular Group (“Terumo”) to commercialize CytoSorb for cardiac surgery applications. Under the terms of the agreement, Terumo has exclusive rights to distribute the CytoSorb CPB procedure pack for intra-operative use during cardiac surgery in France, Sweden, Denmark, N\\nIn August 2020, we announced an initial collaboration with Terumo to exclusively sell CytoSorb to hospitals in ten U.S. COVID-19 hotspot states including Alabama, Arizona, California, Georgia, Louisiana, Mississippi, New Mexico, Oregon, Texas, and Washington. CytoSorb previously received Emergency Use Authorization (EUA) by the U.S. Food and Drug Administration (FDA) for use in adult, critically-ill COVID-19 patients with imminent or confirmed respiratory failure. Under the initial terms of the agreement, Terumo will ensure hospitals in the defined hot-spot states have access to the CytoSorb therapy for use in critically-ill COVID-19 patients that meet strict criteria under CytoSorbents\\' EUA. CytoSorbents will provide all primary clinical and technical training, customer support, and product fulfillment.\\nB. Braun Avitum AG\\nIn March 2021, we announced announce the launch of a global co-marketing agreement with B. Braun Avitum AG to promote the use of CytoSorb® with B. Braun’s latest OMNI® continuous blood purification platform and OMNIset® Plus bloodline set (set version 3.0 or higher). The CytoSorb® adsorber is used in critical care for the extracorporeal removal of cytokines and inflammatory mediators from the bloodstream and can be operated with the B. Braun OMNI® acute dialysis machine. B. Braun will supply the market with the OMNI® and OMNIset® Plus while CytoSorbents and its network of direct sales, strategic partners, and distributors will continue to supply the market with CytoSorb®. CytoSorb® is CE Mark certified and distributed in 67 countries worldwide. This global co-marketing agreement applies to the countries where both products are registered (US market is specifically excluded). Financial terms of this agreement have not been disclosed.\\nB. Braun is one of the world\\'s leading manufacturers of medical devices and pharmaceutical products and services. With 64,000 employees in 64 countries, B. Braun develops high quality product systems and services for users around the world. In 2019, the Group generated sales of €7.5 billion.\\nUniversity of Pittsburgh Medical Center\\nTwo government research grants by the National Institutes of Health (“NIH”) and the U.S. Department of Health and Human Services were awarded to investigators at the University of Pittsburgh to explore the use of adsorbent polymers in the treatment of sepsis and organ transplant preservation. Under “Sub Award Agreements” with the University of Pittsburgh, we developed polymers for use in these studies.\\nA grant of $1 million was awarded to the University of Pittsburgh Medical Center in 2003. The project sought to improve the quantity and viability of organs donated for transplant by using CytoSorb to detoxify the donor’s blood. The observational and dosing phases of the study, involving 30 viable donors and eight non-viable donors, respectively, have been completed. The next phase of this\\nstudy, the treatment phase, was planned to involve viable donors. However, we are not currently focusing our efforts on the commercialization of CytoSorb for application in organ donors.\\nIn September 2005, the University of Pittsburgh Medical Center was awarded a grant of approximately $7 million from NIH entitled “Systems Engineering of a Pheresis Intervention for Sepsis (SEPsIS)” to study the use of adsorbent polymer technology in the treatment of severe sepsis. The study, which lasted for a total of five years, commenced in September 2005. Under a SubAward Agreement, we worked with researchers at the University of Pittsburgh - Critical Care Medicine Department. We believe that the only polymers used in this study were polymers we have developed specifically for use in the study, which are similar to the polymers used in our devices. Under the SubAward Agreement, for our efforts in support of the grant during 2006 through 2010, we received approximately $402,000.\\nDr. John Kellum, a member of the UPMC faculty since 1994, was the Chairman of our Sepsis Advisory Board. On March 1, 2021, Dr. Kellum became the Chief Medical Officer for Toronto, Canada-based Spectral Medical, Inc. Concurrent with his appointment at Spectral, Dr. Kellum formally resigned from our Advisory Board.\\nAdvisory Boards\\nFrom time to time our management meets with scientific advisors who sit on our Scientific Advisory Boards (“SAB”). We have 3 SABs that include a Basic Science and Technology SAB, a Critical Care Medicine SAB, and a Cardiac Surgery SAB. Each SAB comprises of approximately five scientists with deep expertise in their respective fields. We compensate all our SAB members according to fair market value and reimburse them for their travel expenses when attending meetings in person.\\nRoyalty Agreement\\nIn August 2003, in order to induce Guillermina Vega Montiel, a principal member of RenalTech International, LLC at the time, to make a $4 million investment in RenalTech International, LLC, Ms. Montiel was granted a perpetual royalty (the “Royalty”) equal to three percent of all gross revenues received by us from sales of CytoSorb in the applications of sepsis, cardiopulmonary bypass surgery, organ donor, chemotherapy and inflammation control. In addition, for her investment, Ms. Montiel received 1,230,770 membership units of RenalTech International, LLC. Such membership units ultimately were converted into and became 7,420 shares of our common stock following our June 30, 2006 merger. In February 2017, all rights, title and interest to the Royalty was assigned to The Robert Shipley Living Trust. For the year ended December 31, 2020 we have recorded royalty costs of approximately $1,172,000.\\nLicense Agreement\\nIn 2003, Purolite filed a lawsuit against us asserting, among other things, co-ownership and co-inventorship of certain of our patents. On September 1, 2006, the United States District Court for the Eastern District of Pennsylvania approved a Stipulated Order and Settlement Agreement under which we and Purolite agreed to the settlement of the action. The Settlement Agreement provides us with the exclusive right to use our patented technology and proprietary know how relating to adsorbent polymers for a period of 18 years. In particular, the Settlement Agreement relates to several of our issued patents and several of our pending patent applications covering our biocompatible polymeric resins, our methods of producing these polymers, and the methods of using the polymers to remove impurities from physiological fluids, such as blood.\\nUnder the terms of the Settlement Agreement, we have agreed to pay Purolite royalties of 2.5% to 5% on the sale of those of our products, if and when those products are sold commercially, that are used in direct contact with blood or, in certain cases, in direct contact with a physiological fluid other than blood. The royalty payments provided for under the Settlement Agreement would apply to our currently envisioned CytoSorb, VetResQ, and BetaSorb products. For the year ended December 31, 2020 per the terms of the license agreement we have recorded royalty costs of approximately $1,954,000.\\nFollowing the expiration of the 18-year term of the Settlement Agreement, the patents and patent applications that are the subject of the Settlement Agreement should have expired under current patent laws, and the technology claimed in them will be available to the public. However, we have additional issued patents separate from those in this Settlement Agreement, and patents pending worldwide that may extend patent protection of our core technology. We will also continue to exclusively own any confidential and proprietary know how.\\nProduct Payment & Reimbursement\\nCytoSorb\\nGermany\\nEffective January 1, 2017, we achieved a dedicated reimbursement code in Germany that provides for specific and enhanced reimbursement for our CytoSorb device. We believe in most cases that this dedicated reimbursement code provides our customers in Germany with reimbursement that not only covers the cost of the device, but the procedural costs as well. Reimbursement can also be covered by the standard “diagnosis related group” (“DRG”) acute care reimbursement. Under this system, hospitals would purchase CytoSorb and subtract the cost from a pre-determined lump-sum payment made by the payor to the hospital based on the patient’s diagnosis.\\nSwitzerland\\nIn 2019, CytoSorb was assigned two specific procedure codes from the Swiss Federal Statistical Office, a division of the Federal Department of Home Affairs in Switzerland. With cost data related to use of the CytoSorb device, a prerequisite for receiving reimbursement from the Swiss DRG system, we expect to receive a response soon regarding reimbursement levels.\\nEurope (excluding Germany and Switzerland)\\nPayment for our CytoSorb device for the removal of cytokines in patients with life-threatening illnesses is country dependent in Europe. We are pursuing reimbursement of CytoSorb in other major territories, with our partners, such as France, England, Italy and Spain, representing the other four economic leaders in Europe. There can be no assurances that reimbursement will be granted. Additional clinical data may be required to establish reimbursement.\\nUnited States\\nCritical care applications such as those targeted by our CytoSorb device involve a high mortality rate and extended hospitalization, coupled with extremely expensive ICU time. In view of these high costs and high mortality rates, we believe acceptance of our proprietary technology by critical care practitioners and hospital administrators will primarily depend on safety and efficacy factors rather than solely based on cost.\\nCytoSorb is not yet approved in the U.S. but has received FDA Emergency Use Authorization in April 2020 for use in adult critically ill COVID-19 patients with imminent or confirmed respiratory failure. There is currently no specific reimbursement for CytoSorb in the U.S. Payment for our CytoSorb device in the U.S. for this application falls under the DRG prospective repayment system, which is currently the predominant inpatient hospital reimbursement methodology in the U.S. Under this system, hospital reimbursement is generally based upon pre-determined amounts payable for specific diagnoses (e.g. septic shock with respiratory failure), regardless of the number of services provided during the patient’s stay. If CytoSorb can improve outcomes and reduce the costs of ICU treatment and hospital length of stay, it could potentially save hospitals a significant amount of money.\\nIn January 2021, the Centers for the Centers for Medicare & Medicaid Services (CMS) announced the Medicare Coverage of Innovative Technology pathway that will provide national Medicare coverage as early as the same day as FDA market authorization for Breakthrough medical devices, where coverage would last 4 years. This program may be applicable to CytoSorb, if it can achieve U.S. approval for the removal of ticagrelor during emergent or urgent cardiothoracic surgery, which was granted FDA Breakthrough Designation in April 2020.\\nCompetition\\nGeneral\\nWe believe that our products represent a unique approach to disease states and health complications associated with the presence of larger toxins (often referred to as middle molecular weight toxins) in the bloodstream, including sepsis, acute respiratory distress syndrome, trauma, severe burn injury, pancreatitis, post-operative complications of cardiac surgery, damage to organs donated for transplant prior to organ harvest, renal disease and drug intoxication. Researchers have explored the potential of using existing membrane-based dialysis technology to treat patients suffering from sepsis. These techniques are unable to effectively remove the middle molecular weight toxins. We have demonstrated the ability of CytoSorb to reduce key cytokines in the blood of human patients with\\npredominantly septic shock and acute respiratory distress syndrome. In a post-hoc subgroup analysis of our European Sepsis Trial, we have also demonstrated statistically significant improvements in mortality in patients at high risk of death, including patients with either very high cytokine levels or patients older than age 65, both of which have a high predicted mortality. Larger studies are needed to confirm these preliminary data.\\nThe CytoSorb, VetResQ, CytoSorb XL, DrugSorb, ContrastSorb, and BetaSorb devices consist of a cartridge containing adsorbent polymer beads. The cartridge incorporates industry standard connectors at either end of the device which connect directly to an extra-corporeal circuit (bloodlines) on a standalone basis. The extra-corporeal circuit consists of plastic tubing through which the blood flows, our cartridge containing our adsorbent polymer beads, pressure monitoring gauges, and a blood pump to maintain blood flow. The patient’s blood is accessed through a catheter inserted into his or her veins. The catheter is connected to the extra-corporeal circuit and the blood pump draws blood from the patient, pumps it through the cartridge and returns it back to the patient in a closed loop system. As blood passes over the polymer beads in the cartridge, toxins are adsorbed from the blood, without removing any fluids from the blood or the need for replacement fluid or dialysate.\\nThere are three common forms of blood purification, including hemodialysis, hemofiltration, and hemoperfusion. All modes are generally supported by standard hemodialysis machines. All take blood out of the body to remove toxins and unwanted substances from blood, and utilize extracorporeal circuits and blood pumps. Dialysis and hemofiltration remove substances from blood by diffusion and ultrafiltration, respectively, through a semi-permeable membrane, allowing the passage of certain sized molecules across the membrane, but preventing the passage of other, larger molecules. Hemoperfusion utilizes solid or porous sorbents to remove substances based on pore capture and surface adsorption, not filtration.\\nCytoSorb is a hemoperfusion cartridge, using an adsorbent of specified pore size, which controls the size of the molecules which can pass into the adsorbent and vastly increases the area available for surface adsorption. As blood flows over our polymer adsorbent, middle molecules such as cytokines flow into the polymer adsorbent and are adsorbed. Our devices do not use semipermeable membranes or dialysate. In addition, our devices do not remove fluids from the blood like hemodialysis or hemofiltration. Accordingly, we believe that our technology has significant advantages as compared to traditional dialysis techniques, including ease of use.\\nOur HemoDefend platform is a development-stage technology utilizing a mixture of proprietary porous polymer beads that target the removal of contaminants that can cause transfusion reactions or cause disease in patients receiving transfused blood products. The HemoDefend beads can be used in multiple configurations, including the common in-line filter between the blood bag and the patient as well as a unique, patent-pending “Beads in a Bag” treatment configuration, where the beads are placed directly into a blood storage bag.\\nSepsis\\nResearchers have explored the potential of using existing membrane-based dialysis technologies to treat patients suffering from sepsis. These techniques are unable to effectively remove middle molecular weight toxins, which leading researchers have shown to cause and complicate sepsis. The same experts believe that a blood purification technique that efficiently removes, or significantly reduces, the circulating concentrations of such toxins might represent a successful therapeutic option. CytoSorb has demonstrated the ability to remove middle molecular weight toxins, such as cytokines, from circulating blood in a statistically significant manner.\\nMedical research during the past two decades has focused on drug interventions aimed at chemically blocking or suppressing the function of one or two inflammatory agents. In hindsight, some researchers now believe this approach has little chance of significantly improving patient outcomes because of the complex pathways and multiple chemical factors at play. Clinical studies of these drug therapies have been largely unsuccessful. An Eli Lilly drug, Xigris®, cleared by the FDA in November 2001, is the first and only drug to be approved for the treatment of severe sepsis. Clinical studies demonstrated that use of Xigris® resulted in an average absolute 6% reduction in 28-day mortality, and an absolute 13% reduction in 28-day mortality in the most severe sepsis patients. The drug remains controversial and is considered expensive when compared to the percentage of patients who benefit. In 2011, after completing a follow up study required by the FDA, it was subsequently determined that Xigris® did not have a statistically significant mortality benefit, and in October 2011, Eli Lilly withdrew Xigris® from all markets worldwide.\\nDevelopment of many experimental therapies has been discontinued, including Eritoran from Eisai, CytoFab from BTG/Astra Zeneca, Talactoferrin from Agennix, tranexemic acid from Leading Biosciences, selective cytapheresis from CytoPheryx, and others.\\nThere have been many large scale clinical trials in sepsis. The primary outcomes of these studies have generally included:\\n●number of days alive without CV, renal, or pulmonary organ support\\n●number of days free of treatment with vasopressors\\n●28-day survival and all-cause mortality\\n●60-day hospital mortality\\n●reduction rate of IL-6 serum concentration\\n●change in biomarkers indicative of endothelial activation and damage\\n●change in microvascular perfusion\\n●hemodynamic effects\\n●immune reconstitution of lymphocytopenic sepsis patients\\n●immunomodulatory effect (IL6/IL10 ratio)\\n●lymphocyte counts and percentage\\n●post-operative sepsis\\n●reduction in Sequential Organ Failure Assessment score (SOFA)\\nCOVID-19 disrupted many clinical studies in 2020. Notable active Phase III trials in sepsis include the following:\\nInitiated in November 2012, the 800 patient Phase III randomized controlled SCARLET study began for Recomodulin (ART 123, Artisan/Asahi Kasei), a recombinant human thrombomodulin, for the treatment of septic patients with coagulopathy. In 2019, the results of the study were published in JAMA, demonstrating no benefit in 28-day all-cause mortality. The 800 patient Phase III SCARLET-2 randomized, controlled trial, evaluating Recomodulin in patients with sepsis and coagulopathy, was scheduled to begin in July 2019, but was withdrawn to be amended following the results of the SCARLET trial. The status of the trial is unknown.\\nAnother study is being conducted by Atox Bio, a development stage company in clinical studies with peptide therapeutics that are designed to prevent superactivation of the immune response by certain toxins such as toxic shock syndrome toxin. It is currently focused on necrotizing soft tissue infections. The investigational peptide, AB103 or Reltecimod, binds to the CD28 co-stimulatory receptor to attempt to restore the host’s appropriate immune response to severe infections and was evaluated in the ACCUTE Trial, a Phase III randomized controlled trial in 60 investigative sites in the U.S in 290 patients with necrotizing soft tissue infections. The primary endpoint of the study was based on a modified Intent-to-treat (mITT) analysis of a primary composite endpoint that was defined as: alive at day 28, ≤ 3 debridements, no amputation beyond first operation, and day 14 mSOFA ≤ 1 with ≥ 3 point reduction (organ dysfunction resolution). A prespecified, per protocol (PP) analysis excluded 17 patients with major protocol violations before unblinding. There was no difference in 28-day mortality of 15% in each group, and the study did not reach significant improvement in the primary endpoint in the pre-defined mITT population. However, in the PP analysis that excluded 17 patients, the company claims clinical composite endpoint success of 54.3% treatment vs 40.3% control. In December 2020, Atox Bio announced that they had filed an NDA under the FDA Accelerated Approval Program with a PDUFA date of September 30, 2021.In January 2021, the company announced the termination of its Phase 3 REAKT (Reltecimod Efficacy for Acute Kidney Injury Trial) trial in patients with abdominal sepsis induced AKI due to slow enrollment.\\nSpectral Medical, Inc. collaborated with Toray on the EUPHRATES trial, combining an endotoxin assay with extracorporeal endotoxin removal by Toraymyxin, a polymyxin-B immobilized polystyrene fiber cartridge. The study began in June 2010 and completed enrollment in June 2016. Endotoxemia is a result of Gram negative sepsis, which only accounts for 45% of cases of sepsis. It is a potent stimulator of cytokine storm. However, all anti-endotoxin strategies have failed pivotal studies to date, believed to be the result of intervening too late in the sepsis cascade. The original trial was designed as a randomized control trial in 360 patients with septic shock and high endotoxin levels (≥ 0.60 EAA units) as confirmed by Spectral’s Endotoxin Activity Assay (“EAA”). In a second interim analysis finalized in April 2014, following the enrollment of 184 patients with 28-day follow-up, the DSMB recommended that the trial continue. However, the expected trial size was increased to 650 patients and the exclusion criteria was modified to only accept sicker patients with a multiple organ dysfunction syndrome score greater than 9. In September 2015, Spectral reported that the composite mortality in the new subgroup had risen to ~50%, from ~30% previously. New statistical analysis on patients in the new subgroup, and comparable patients in a European treatment registry, led to a sample size recalculation of 446 evaluable patients. Spectral announced in June 2016 that they had completed enrollment for the EUPHRATES trial. In October 2016, Spectral announced top-line results that the trial did not meet the main goal of absolute reduction in 28-day all-cause mortality, but reiterated safety of treatment and potential benefit in the sickest group of patients (multiple organ dysfunction score > 9). A secondary analysis of the sub-population of patients with septic shock and high circulating endotoxin activity also failed to demonstrate a beneficial effect of Toraymyxin on 28-day mortality in sepsis, however, an exploratory post-hoc analysis of the suggested trends toward improvements in changes in mean arterial pressure and ventilator-free days. In February 2019, Spectral announced an amendment of the original EUPHRATES trial to enroll an additional 150 septic shock patients under the TIGRIS expansion, in patients with a MODS score > 9 and an EAA level between 0.60 and 0.90, and will analyze the combined data from these two trials using a Bayesian statistical approach. Based on the 179 patients from the EUPHRATES trial, treated patients had a mortality of 38% (N=90) compared to 48% mortality in the control (N=89), but not statistically significant. The TIGRIS study will be in US sites only, randomized (2:1), open label trial, with an additional 150 new patients (100 treated, 50 control) to be added. Projected completion of trial enrollment at 10 sites is 18 months (projected June 2022).\\nEnlivex has developed an investigational cell-based therapy called Allocetra that is an infusion of donor mononuclear cells that have been chemically induced to be apoptotic. Once infused, the patient’s macrophages and dendritic cells phagocytose these apoptotic cells which purportedly then causes them to reduce inflammatory signals that results in immune modulation. Enlivex recently reported on the use of its therapy, as a single or double dose in a single arm Phase IB study in 10 patients who presented with sepsis to the emergency room with a SOFA score > 2 above baseline. The severity of illness in this patient population was low, with a mean APACHE II score of 12.9 (range 8-21) and a predicted mortality of approximately 15%, and a mean SOFA score of 3.4 (range 2-6), with a predicted mortality of less than 10%. There were no deaths reported in the study. Results were compared with a poorly matched and significantly sicker control population who were admitted to the intensive care unit or intermediate care unit.\\nIn 2017, a single center, retrospective, non-randomized, unblinded before-after clinical study evaluating the effect of hydrocortisone, intravenous Vitamin C, and thiamine in a total of 94 patients with severe sepsis and septic shock was published suggesting a significant decrease in hospital mortality of 8.5% (4 of 47 treated) versus mortality of 40.4% (19 of 47 control), p<0.001. Mechanistically, Vitamin C is an antioxidant that scavenges free oxygen radicals, and plays a role in preserving endothelial function and microcirculatory flow. Thiamine is a co-factor of pyruvate dehydrogenase that is a key step in the conversion of lactate to pyruvate to acetyl-CoA, then to the Krebs cycle, leading to a consumption of lactate. Steroids are anti-inflammatory. Vitamin C or steroids alone have not demonstrated a significant benefit in patients with severe sepsis and septic shock in large scale clinical trials. Observational studies in septic patients have demonstrated a deficiency in Vitamin C and thiamine. Critics of this study cite weaknesses in the study design, and confounders such as the significantly higher incidence of renal replacement therapy in the control arm (33% vs 10% treatment, p=0.02), that is an independent and significant risk factor for mortality in sepsis. Many compare it to another well-known single center trial in 2001 in 263 patients that suggested a significant reduction in hospital mortality (30.5%, N=130 treatment versus 46.5%, N=133 control) due to early goal directed therapy (EGDT), which protocolized resuscitation, oxygenation, and hemodynamic targets in the emergency room for patients with severe sepsis or septic shock prior to being admitted to the ICU. Three subsequent large scale randomized controlled trials failed to demonstrate any benefit. Regardless, the results of the Vitamin C, thiamine and steroid single center trial have spawned a number of randomized controlled clinical trials evaluating this therapeutic strategy, including VICTAS, VITAMINS, ACTS, and others. The largest of these studies is VICTAS, a 2,000 patient U.S. multi-center randomized controlled trial that started in August 2018 comparing intravenous Vitamin C, thiamine, and hydrocortisone for 4 days or until ICU discharge versus placebo and standard of care in patients with suspected or confirmed infection and either respiratory dysfunction requiring mechanical support or shock of less than 24 hours from enrollment. The primary outcome is vasopressor and ventilator-free days at 30 days. The trial was terminated early at 501 patients due to a withdrawal of funding from the study, with results published in JAMA in February 2021. Ventilator- and vasopressor-free days showed no significant improvement with a median of 25 days (IQR, 0-29 days) in the intervention group and 26 days (IQR, 0-28 days) in the placebo group, with a median difference of −1 day (95% CI, −4 to 2 days; P\\u2009=\\u2009.85). Thirty-day mortality was 22% in the intervention group and 24% in the placebo group and was not statistically significant. The ACTS trial is a 200 patient U.S. multicenter study that started in February 2018 comparing 4 days of treatment with intravenous Vitamin C (1500 mg/d), thiamine (100 mg/d), and hydrocortisone (50 mg every 6 hours) versus saline placebo in patients having suspected or confirmed infection, requiring vasopressors. The primary endpoint is change in SOFA score in 72 hours. Results from this study were published in JAMA in August 2020. Change in the SOFA score was 4.7 in the intervention group vs 4.1 in the placebo group over 72 hours, a difference that was not statistically significant. The VITAMINS RCT began in Australia and New Zealand in November 2017, comparing the effect of Vitamin C (6g/d), thiamine (400 mg/d) and hydrocortisone (50mg every 6 hours) versus hydrocortisone (50mg every 6 hours) alone, in 216 patients with septic shock and a blood lactate > 2 mmol/L, with a primary endpoint of time alive and free of vasopressors at day 7 after randomization. The results of the VITAMINS trial were published in JAMA in January 2020, concluding that treatment with vitamin C, hydrocortisone, and thiamine, compared with intravenous hydrocortisone alone, did not significantly improve the duration of time alive and free of vasopressor administration over 7 days, and does not lead to a more rapid resolution of septic shock compared with intravenous hydrocortisone alone. Ninety-day mortality was 28.6% in the treatment group, and 24.5% in the control group. The authors of these studies do not recommend the routine use of the combination of Vitamin C, corticosteroids, and thiamine in septic shock patients.\\nUsing a medical device to treat sepsis remains a relatively novel treatment approach. Toray Industries currently markets an endotoxin removal cartridge called Toraymyxin™ for the treatment of sepsis in Europe, Japan, and 16 other countries, but is not yet approved in the United States. To date, it has been used in more than 100,000 treatments since 1994. Toraymyxin does not directly reduce cytokines. Spectral Medical Inc. has obtained exclusive development and commercial rights in the U.S. for Toraymyxin, with plans to combine the use of its endotoxin activity assay to create a theranostic product. Spectral collaborated with Toray on the EUPHRATES trial, combining an endotoxin assay with extracorporeal endotoxin removal by Toraymyxin, a polymyxin-B immobilized polystyrene fiber cartridge. As noted above, the EUPHRATES trial failed to demonstrate its primary endpoint. Spectral is now pursuing an amendment to the EUPHRATES trial, called TIGRIS. There have been now several large scale studies failing to demonstrate a benefit of Toraymyxin on 28-day mortality in sepsis. Toraymyxin represents a competitive, although potentially complementary, therapeutic approach to CytoSorb.\\nIn September 2017, Baxter re-launched oXiris in the E.U., a hollow-fiber acrylonitrile and methalylsulfonate (AN69) membrane hemofilter coated with polyethyleneimine (PEI) that was originally launched by Gambro in 2009 for use in hemodialysis as a strategy to treat acute kidney injury and gram negative septic shock while reducing endotoxin. The filter itself has not changed. However, Baxter has expanded the label to now include reduction of cytokines based on a set of in vitro experiments evaluating cytokine reduction from recirculating plasma over two hours. In December 2018, Baxter began a 40 patient randomized, controlled trial, called ECRO, evaluating the effect of endotoxin and cytokine (IL-6) removal during continuous hemofiltration with oXiris in patients with septic shock due to peritonitis, as compared to a standard polysulfone filter. The estimated study completion date is March 2022. In 2020, oXiris received FDA Emergency Use Authorization for use in adult critically ill COVID-19 patients in imminent or confirmed respiratory failure. In October 2020, results from 4 hospitals on 37 patients from its OxirisNet Registry in the journal, Critical Care.\\nMortality was 66.6% in patients receiving oXiris treatment after 14 days from admission, and a mortality of 47.4% mortality when used earlier. In addition, Baxter also launched the Theranova mid-molecular weight cutoff or high retention onset (HRO) hemodialysis membrane to improve the efficiency of hemodialysis, claiming improved mid-molecular weight substance removal. Neither oXiris nor Theranova are approved in the U.S.\\nEach of the following technologies claims to remove inflammatory mediators such as cytokines, or to treat sepsis, and represents a potential competitive alternative to CytoSorb. However, to our knowledge, none of these technologies are approved in the U.S. and none are approved in the European Union to reduce cytokines.\\nToray markets its Hemofeel CH1.0 polymethylmethacrylate membrane (“PMMA”) in Japan and it has been used in several non-controlled, or historically controlled, clinical or case studies treating patients with sepsis, acute respiratory distress syndrome and pancreatitis. We are not aware of any prospective, randomized controlled studies using this PMMA hemofilter in patients with sepsis. Without such studies, it is difficult to assess the true impact of this technology in these conditions. Gambro AB launched its Prismaflex eXeed system in August 2009 and introduced the SepteX high molecular weight cutoff hemodialyzer in Europe, intended to treat patients with acute renal failure and the removal of inflammatory mediators from blood. Gambro also launched the oXiris dialyzer, based upon the AN69 CRRT membrane, to bind endotoxin. To our knowledge, neither are specifically approved for the treatment of sepsis. Fresenius had launched a high molecular weight cut off filter in response to SepteX called the Ultraflux EMiC2. To our knowledge, there has been a lack of published data on the treatment of sepsis with these devices. Bellco S.R.L, acquired by Medtronic in February 2016, also sells the CPFA (coupled plasma filtration and adsorption) system in Europe. This uses a sorbent cartridge to remove cytokines from plasma. However, because the sorbent cannot treat blood directly, it requires the cost and complexity of an additional plasma separator to treat blood. In April 2018, Medtronic issued a field safety notice informing all users of CPFA that the COMPACT-2 study using CPFA in septic shock patients was terminated early due to observed higher mortality rates in septic shock patients receiving CPFA therapy compared to patients receiving standard care. The CPFA system is similar to the I.M.P.A.C.T. System that was commercialized outside of the U.S. by Hemolife Medical Inc. that requires a three-cartridge system and a proprietary blood pump. In 2018, Hemolife Medical filed for Chapter 11 bankruptcy. We believe that CytoSorb, which can treat whole blood directly, and which works with standard hemodialysis pumps already found in hospitals worldwide, has significant competitive advantages compared to these multi-cartridge sorbent systems.\\nKaneka Corporation currently markets Lixelle™, a modified porous cellulosic bead, for the removal of beta2-microglobulin during hemodialysis in Japan. Lixelle has been used in several small human pilot studies including a 5 patient pilot study in 2002 and a 4 patient pilot study in 2009. Though these studies correlate Lixelle use with cytokine reduction, they are not randomized, controlled studies and so do not control for natural cytokine clearance. To our knowledge, no large, randomized, controlled trials have been conducted with Lixelle as a treatment for sepsis. Kaneka obtained U.S. humanitarian device exemption for Lixelle in March 2015, but is restricted to treating amyloidosis in chronic dialysis patients. Kaneka has since developed a modified cellulosic resin called CTR that can also remove cytokines from experimental pre-clinical systems. In 2009, CTR was used in an 18-patient randomized, controlled trial in patients with septic shock with undisclosed improvements in APACHE II scores and IL-6 and IL-8. To our knowledge, Kaneka has not conducted or published any other study using CTR to treat human sepsis patients since then. To our knowledge, none of the following technologies are approved in the U.S. and none are approved for cytokine reduction or as a therapy to treat sepsis in the EU. Jafron Biomedical is an integrated dialysis public company in China selling dialysis machines and hemodialysis and hemoperfusion cartridges containing a neutral microporous adsorption resin to purify blood of toxins in liver failure, critical illness, poisoning, and autoimmune diseases. Jafron is currently recruiting a 144 patient efficacy and safety study in China using its CA330 cartridge to reduce IL-6 in septic patients. The estimated study completion date is October 2020. Foshan Biosun Medical Technology Co, Ltd, and Baihe Medical Technology Co, market hemoperfusion cartridges under the BioSky brand name, including the MG series claiming cytokine reduction, and the DX series for bilirubin reduction. Ube Industries, Ltd was currently developing an adsorbent resin called CF-X for the removal of cytokines. To our knowledge, Ube has not published any study using CF-X to treat human sepsis patients. CytoPherx Inc., had developed an extracorporeal system based on selective cytapheresis, or the inactivation or removal of activated leukocytes. It was enrolling a 344 patient pivotal trial that began in August 2011 and was expected to be completed by December 2014 in patients with acute kidney injury with or without severe sepsis, on continuous renal replacement therapy with the goal of reducing mortality. This system does not remove cytokines directly, but attempts to reduce the numbers of activated white blood cells that can produce cytokines or cause cell-mediated injury. The company appears to no longer be in business. ExThera Medical Corporation is a privately held company that has developed its Seraph™ (Selective Removal by Apheresis) platform that consists of heparin coated, solid polyurethane beads. Heparin has the ability to bind some, but not all viruses, bacteria, toxins and cytokines. In in vitro studies using 1 mL of human septic blood, there was no statistically different change in IL-6 or Interferon-gamma compared to control, but effected a ~50% reduction in TNF-alpha. This inability to remove a broad range of cytokines will likely limit its efficacy as a treatment in sepsis. It has repositioned Seraph™ as a pathogen removal technology, and has completed a 15 patient CE Mark registration trial in Germany evaluating the safety and efficacy of bacterial removal from blood. It received EU CE-Mark approval in July 2019, and established distribution in Germany, Italy and Benelux. In addition, in 2013, it partnered with BioBridge Global to apply its technology to pathogen reduction in transfused\\nblood products. In 2020, Seraph received FDA Emergency Use Authorization for use in adult critically ill COVID-19 patients to reduce pathogens and inflammatory mediators from the bloodstream. Seraph was recently designated by FDA for inclusion into the Expedited Access Pathway (EAP) Program for the specific application of removing drug resistant pathogens from whole blood. We believe our CytoSorb cartridge has significant competitive, technological, and/or economic advantages over systems by these other companies.\\nAcute Respiratory Distress Syndrome\\nTreatment of ARDS is predominantly supportive care using supplemental oxygen, careful fluid management, multiple modes of ventilation incorporating the concepts of low tidal volume, prone ventilation, and extracorporeal membrane oxygenation (“ECMO”). Although a number of therapies have been tried such as nitric oxide, surfactant therapy, and others, only corticosteroids, such a dexamethasone or methylprednisolone, have demonstrated mortality benefit in patients with ARDS. For example, in critically ill COVID-19 patients on mechanical ventilation, the RECOVERY study demonstrated use of once daily dexamethasone led to a reduction in mortality from 41.4% control to 29.3% treatment.\\nSee “Markets: Acute Respiratory Distress Syndrome” above for a more detailed discussion.\\nSevere Burn Injury\\nModern management of severe burn injury patients involves a combination of therapies. From a burn standpoint, patients undergo active escharotomy and debridement of burns, the use of skin grafts and substitutes, anti-microbial dressings and negative pressure dressings. Tight fluid control, nutrition, prevention of hypothermia and infection are also priorities. Smoke and chemical inhalation injury in burn victims is also common and increasing as a cause of death in severe burn injury. Carbon monoxide and cyanide poisoning is also an issue. Supplemental oxygen, mechanical ventilation, and ECMO are often required and are the mainstay of supportive care treatment. Recently continuous renal replacement therapy has been used to treat patients with acute kidney injury with an improvement in survival compared to a historical control cohort. We believe CytoSorb therapy may yield improved results. We are not aware of any specific products approved to directly address inhalational lung injury or multiple organ failure in severe burn injury.\\nTrauma\\nTrauma management initially involves respiratory, hemodynamic and physical stabilization of the patient. However, in the days to weeks that ensue, the focus shifts to preventing or treating organ failure and preventing or treating infection. We are not aware of any specific therapies to prevent or treat multiple organ dysfunction or multiple organ failure in trauma. Rhabdomyolysis, or the breakdown of muscle fibers due to crush injury or other means, occurs in trauma and can lead to acute kidney injury or renal failure. Aggressive hydration, urine alkalinization, and forced diuresis are the main therapies to prevent renal injury. Continuous hemodiafiltration with super-high-flux membranes has demonstrated modest myoglobin clearance but was associated with albumin loss. In general, however, most extracorporeal therapies are not well-suited to remove myoglobin. CytoSorb reduces myoglobin, and other polymers under development, reduces myoglobin, some without significant losses of albumin.\\nSevere Acute Pancreatitis\\nTreatment of severe acute pancreatitis is predominantly supportive care focused on aggressive hydration, enteral nutrition and pain control. Mechanical ventilation, hemodialysis and vasopressor use is common in cases of multiple organ failure. In cases where cholelithiasis or other obstruction is the underlying cause of the pancreatitis, endoscopic retrograde cholangiopancreatography and/or stent placement can be used to relieve the obstruction. Antibiotics are often instituted to prevent or treat infection. Surgery is sometimes indicated to remove or drain necrotic or infected portions of the pancreas. To our knowledge, there are no other specific treatments approved to treat severe acute pancreatitis or multiple organ failure that is caused by systemic inflammation in this disease.\\nCardiopulmonary Bypass Surgery\\nThere is currently a pre-existing market for the use of leukocyte reduction filters sold by Pall Corporation, Terumo Medical Corporation and others in the cardiopulmonary bypass circuit. The purpose of these devices is to reduce cytokine-producing white blood cells from blood. They do not remove cytokines, free hemoglobin, or activated complement directly and are not considered by many to be an effective solution for the reduction of these substances. Other than blood compatible sorbent technologies, we are not aware of any practical competitive approaches for removing cytokines, free hemoglobin, activated complement, and a broad range of other inflammatory mediators in patients undergoing cardiopulmonary bypass during cardiac surgery. To our knowledge, CytoSorb is the leading cytokine reduction therapy capable of being placed directly into a bypass circuit in the heart-lung machine and used during cardiopulmonary bypass without the need for another pump. Modified ultrafiltration is sometimes used after termination of\\ncardiopulmonary bypass in cardiac surgery to remove excess fluid and inflammatory substances, but has had mixed benefit. Cell saver machines that collect and wash pericardial shed blood is one potential alternative, but is typically done in batches and not a real-time filter during surgery. Alternative therapies such as “off-pump” surgeries are available but “post-bypass” syndrome and cytokine production still remain a problem in this less invasive, but more technically challenging procedure. If successful, CytoSorb is expected to be useful in both on-pump and off-pump procedures. CytoSorb is also being used with a dialysis machine to treat the development of a post-cardiac surgery systemic inflammatory response syndrome, a deadly complication of open-heart surgery that if left untreated, can lead to multiple organ dysfunction syndrome, multiple organ failure, and potentially death.\\nRadiocontrast Removal\\nContrastSorb has demonstrated the rapid, high efficiency single pass removal of IV contrast. The use of low osmolar IV contrast, oral administration of N-acetylcysteine, and other agents to prevent CIN have demonstrated modest benefit in some clinical studies, but in many cases, the results across studies have been equivocal and inconsistent. Hydration of high risk patients pre-procedure is standard of care but has limited efficacy. PLC Medical Systems, Inc., now Renalguard Solutions, received CE Mark approval for its RenalGuard system in 2007. RenalGuard encourages excretion of IV contrast and a reduction of CIN, by administering IV hydration that matches urine output in patients receiving a loop diuretic. Hemodialysis can remove IV contrast, but is relatively slow (46% at 1 hour, 65% at 2 hours, and 75% at 3 hours) in chronic renal failure patients who lack normal renal clearance. In high risk patients, the rapid and direct removal of IV contrast from the blood with ContrastSorb to prevent CIN represents a potentially more effective alternative.\\nDrug Intoxication\\nTreatment of patients suffering from drug overdose often involves a number of pharmacological treatments and mechanical interventions to detoxify and stabilize the patient. Mechanical interventions include procedures such as gastric lavage, activated charcoal, whole bowel irrigation and extracorporeal blood purification. Each method has its own limitations, many of which are associated with the timing of administration following overdose. Blood purification with high flux dialyzers or with activated charcoal cartridges by Gambro, Fresenius, Nephros and others are typically efficient at removing hydrophilic drugs that are not protein bound. However, they are inefficient at removing drugs that have a large volume of distribution, or drugs that are hydrophobic or lipophilic. Many drugs of overdose fall into this category. The administration of lipid emulsions, such as Intralipid, have been used with some success to create a depot for lipophilic drugs. Resin based hemoperfusion devices have been used to remove lipophilic drugs that are protein bound, but have historically had issues of biocompatibility. DrugSorb is a highly biocompatible resin-based hemoperfusion device that can remove a wide range of drugs of overdose in vitro very rapidly, with high single pass removal.\\nChronic Dialysis\\nAlthough standard dialysis treatment effectively removes urea and creatinine from the blood stream (which are normally filtered by functioning kidneys), standard dialysis has not been effective in removing beta2 -microglobulin toxins from the blood of patients suffering from chronic kidney failure. High flux dialyzers by Gambro, Fresenius, Nephros and others are capable of removing some beta2-microglobulin. However, we believe our technology would significantly improve clearance of this and other toxins. Kaneka markets Lixelle™, a cellulosic resin, outside the US to remove beta2-microglobulin in dialysis patients. In March 2015, Lixelle received Humanitarian Device Exemption (“HDE”) approval in the U.S. for the treatment of beta-amyloidosis and removal of beta2-microglobulin , a complication of chronic dialysis. HDE approval applies to the treatment of diseases with an incidence of less than 8,000 cases a year in the U.S. annually. Other than blood compatible sorbents, we know of no other device, medication or therapy considered directly competitive with our technology.\\nUse for Organ Transplant in Ex Vivo Organ Perfusion Systems or in the Treatment of Organ Dysfunction in Brain-Dead Organ Donors\\nWe are not aware of any directly competitive products to address the application of our technology for the mitigation of organ dysfunction and failure resulting from severe inflammation following brain-death, or in the removal of inflammatory mediators during ex vivo organ perfusion\\nRemoval of Anti-thrombotics such as Ticagrelor in Cardiac Patients During Surgery Requiring Cardiopulmonary Bypass\\nThere are more than $20 billion in annual worldwide sales of anti-thrombotic drugs such as the P2Y12 platelet inhibitors (e.g. clopidogrel, ticagrelor, prasugrel), and the Direct Oral AntiCoagulants (DOAC) comprising of direct thrombin inhibitors (dabigatran), and Factor Xa inhibitors (e.g. apixaban, rivaroxaban, edoxaban). These are generally used to reduce thromboembolic events in a wide range of applications, including dual anti-platelet therapy in percutaneous coronary intervention and stent placement, myocardial\\ninfarction, stroke, peripheral artery disease, atrial fibrillation, deep vein thrombosis, pulmonary embolus, and others. For example, ticagrelor (Brilinta®, Astra Zeneca) is a widely-used anti-platelet agent used to decrease cardiovascular risk in patients with acute coronary syndromes or a past history of heart attack. It is also widely used during as part of the dual-anti platelet therapy regimen in patients undergoing percutaneous coronary intervention and stent placement. However, when patients on ticagrelor require emergent or urgent cardiac surgery, up to 65% of patients will have severe or massive peri-operative bleeding complications that contributes to a high risk of morbidity and death and major costs to the healthcare system.\\nTo our knowledge, CytoSorb is the only therapy approved for the removal of ticagrelor and rivaroxaban (Xarelto®, Janssen, Bayer) in the E.U. during cardiopulmonary bypass in urgent or emergent cardiopulmonary bypass. The only recommended alternative is to wait for 3-5 days to allow natural drug elimination and washout prior to surgery.\\nCytoSorb has already demonstrated the ability to remove ticagrelor rapidly and efficiently from human blood in vitro. Meanwhile, a retrospective case series reported by clinicians at Asklepios Klinik St. Georg in Hamburg, Germany on the investigational use of CytoSorb to reverse the effects of ticagrelor and the Factor Xa inhibitor, rivaroxaban, during emergency cardiac surgery demonstrated a greatly reduced risk of bleeding complications and the need for repeat surgery to explore the source of bleeding. Extrapolations of the clinical benefits showed projected cost savings of £3,982, or approximately $5,000 USD, per patient in a U.K. based economics study. CytoSorb recently received E.U. CE Mark label expansion to remove ticagrelor and rivaroxaban during cardiac surgery involving cardiopulmonary bypass via label expansion of its CE Mark. We are currently executing the Safe and Timely Antithrombotic Removal (STAR) international registry collecting real world evidence in this application and two multicenter prospective single arm studies (TISORB in the UK and CYTATION in Germany) to obtain more country-specific data to support the use of CytoSorb for reduction of peri-operative bleeding complications in urgent or emergent cardiac surgery.\\nThe use of platelet transfusions, Kcentra® (CSL Behring; four factor prothrombin complex concentrate; reversal for warfarin anticoagulation), Andexxa® (recombinant Factor Xa; AstraZeneca; reversal for rivaroxaban and apixaban), Praxbind® (idarucizumab, Boeringer Ingelheim; reversal agent for dabigatran) and other interventions have either not demonstrated consistent benefit, or are not used because of potential safety concerns, in the reversal of antithrombotics in the setting of cardiopulmonary bypass.\\nPhaseBio, a clinical-stage biopharmaceutical company, has licensed an intravenously administered monoclonal antibody fragment with high affinity for ticagrelor called bentracimab (PB2452) from Medimmune, a division of AstraZeneca. The company paid AstraZeneca $100,000 upfront, with $68 million in potential future milestones. AstraZeneca owns approximately 5% of PhaseBio’s stock. PB2452 is a novel reversal agent for the antiplatelet drug ticagrelor, which was developed for the treatment of patients on ticagrelor who are experiencing a major bleeding event or those who require urgent surgery. The FDA granted Breakthrough Therapy designation for PB2452 in April 2019. PhaseBio is seeking US FDA approval of PB2452 in the United States through an accelerated approval process.\\nPhaseBio is currently conducting a U.S. Phase 2b clinical study evaluating the safety and efficacy of PB2452 in approximately 200 healthy volunteers aged 50-80. Patients receive loading with dual anti-platelet therapy consisting of aspirin and ticagrelor in the U.S. and will be evaluated on the ability of PB2452 to reverse platelet dysfunction. PhaseBio has also launched its REVERSE-IT (Rapid and SustainEd ReVERSal of TicagrElor - Intervention Trial) study, a Phase 3, multi-center, open-label, prospective single-arm trial designed to study reversal of the antiplatelet effects of ticagrelor with bentracimab in patients who present with uncontrolled major or life-threatening bleeding or who require urgent surgery or invasive procedure. Approximately 200 adult patients, within 72 hours of ticagrelor intake who require urgent reversal of the antiplatelet effects of ticagrelor, are being targeted to be enrolled from centers in the U.S. and worldwide. Patients with reported use of ticagrelor within the prior 3 days who require urgent ticagrelor reversal are eligible for enrollment. Patients receive an intravenous (IV) infusion comprised of an initial IV bolus of 6 grams (g) infused over 10 minutes for rapid reversal, followed immediately by a 6g IV loading infusion over 4 hours and then a 6 g IV maintenance infusion over 12 hours. In January 2021, PhaseBio expanded the study to include patients in the European Union. The primary endpoints of the study are: 1) reversal of platelet inhibition 2) Major life-threatening bleeding and 3) achievement of hemostasis in urgent surgery or invasive procedures. Bentracimab is not yet approved in any market.\\nBased on feedback from the FDA, PhaseBio intends to submit a Biologics License Application, or BLA, for potential accelerated approval based on an interim analysis of the first approximately 100 patients treated in their Phase 3 trial, with approximately 50 subjects from each of the major bleeding and surgical populations. To support full approval for patients with major bleeding or requiring urgent surgery, the FDA recommended enrollment of 200 total patients in the Phase 3 trial. For post-approval commitments, the FDA recommended the completion of the remaining portions of the Phase 3 trial and the establishment of post-approval registry.\\nMeanwhile, Andexxa is a Factor Xa analog that competes for binding to Factor Xa inhibitors. Due to the short duration of action, pro-thrombotic effect, and very high cost, it is not indicated to reduce the risk of perioperative bleeding in cardiac surgery.\\nCytoSorb has demonstrated very efficient removal of all the major drugs of the DOAC category in clinical use today including rivaroxaban (Xarelto®; Bayer, Janssen), apixaban (Eliquis®, Bristol-Myers Squibb), edoxaban (Savaysa®, Daiichi-Sankyo) and dabigatran (Pradaxa®, Boehringer Ingelheim).\\nWe believe that CytoSorb represents a more cost-effective, readily available, and easy to implement solution for ticagrelor or DOAC reversal in cardiac surgery than these biologic alternatives.\\nHemoDefend Purification Technology Platform for Transfused Blood Products\\nThere are only a few directly competitive approved products to address the removal of substances from blood and blood products that can cause transfusion reactions. Leukoreduction (Haemonetics, Terumo-BCT, Hemerus Corporation, others) is widely used in transfusion medicine and can remove the majority of white cells that can produce new cytokines but cannot eliminate those cytokines already in blood, and cannot otherwise remove other causative agents. Automated washing of pRBC is very effective at cleansing contaminants from blood, but is impractical due to the time, cost, materials, and logistics of washing each unit of blood and is not widely used. Blood filters that utilize affinity technologies are in development to remove certain substances such as antibodies from blood, but have other issues, such as cost and concern about the stability or leachability of the affinity technology. The HemoDefend platform represents a potentially superior alternative to these methods, as it can provide comprehensive removal of a wide variety of contaminants that can trigger transfusion reactions without washing blood, requires no additional equipment, energy source, or manipulation, and can be incorporated directly into the blood storage bag or used as an in-line blood filter.\\nClinical Studies\\nOur first clinical studies were conducted in patients with chronic renal failure. The health of these patients is challenged by high levels of toxins circulating in their blood but, unlike sepsis patients, they are not at imminent risk of death. The toxins involved in chronic renal failure are generally different from those involved in sepsis, eroding health gradually over time. The treatment of patients with chronic renal failure is a significant target market for us, although not the current focus of our efforts and resources. Our clinical studies and product development work in this application functioned to obtain safety and instrument data without the need to put the patient at additional risk (e.g. placing a new temporary dialysis catheter), with direct benefit to the development of the critical care applications on which we are now focusing our efforts.\\nWe are focusing our research efforts on critical care and cardiac surgery applications of our technology.\\nCritical Care\\nIn 2011, the CytoSorb adsorber received EU regulatory approval under the CE Mark as an extracorporeal cytokine filter to be used in clinical situations where cytokines are elevated. As part of the CE Mark process, in 2011 we completed our randomized, controlled, European Sepsis Trial amongst 14 trial sites in Germany, with enrollment of 100 patients with sepsis and respiratory failure. The trial established that CytoSorb was sufficiently safe in this critically-ill population to support the CE mark and published in PLOS ONE. In the European Sepsis Trial, the treatment was well-tolerated with no serious device related adverse events reported. The trial also demonstrated the ability of CytoSorb to reduce cytokines such as IL-6 from the blood of septic patients. The trial was not powered to demonstrate significant reduction in other clinical endpoints such as mortality.\\nIn September 2019, a new publication entitled, \"Hemoadsorption with CytoSorb showed a decreased observed versus expected 28-day all-cause mortality in ICU patients with septic shock: a propensity-score-weighted retrospective study,\" in the journal Critical Care. In this study, clinical researchers at Maasstad Hospital and at Erasmus University Medical Center in Rotterdam, Netherlands conducted a retrospective evaluation of 116 patients with septic shock, who required vasopressors to increase their blood pressure, and renal replacement therapy (RRT) due to kidney failure. Of these, 49 patients received standard of care therapy, and 67 were treated with standard of care plus CytoSorb. Both groups were compared by stabilized Inverse Probability of Treatment Weights (sIPTW) to overcome baseline differences in the type of sepsis, age, comorbidities, surgery vs no surgery, Sequential Organ Failure Assessment (SOFA) score, use of the vasopressor noradrenaline, and lactate levels. Patients treated with standard of care and CytoSorb had a statistically significant reduction in 28-day all-cause mortality compared to standard of care alone (53% vs 72% control, p<0.04), based on the sIPTW analysis. In addition, observed 28-day all-cause mortality in the CytoSorb treatment group was significantly lower than the predicted mortality (48% observed vs 75% predicted, p<0.001), based on SOFA score.\\nIn April 2020 the FDA granted Emergency Use Authorization for CytoSorb use in critically ill COVID-19 patients treated in the ICU. The company is conducting the CytoSorb Therapy in COVID-19 (CTC) Registry to systematically capture high fidelity data from U.S.institutions using CytoSorb under the EUA.\\nCardiac Surgery\\nIn February 2015, the U.S. Food and Drug Administration (the “FDA”) approved our Investigational Device Exemption (“IDE”) application to commence a planned U.S. cardiac surgery feasibility study called REFRESH I (REduction of FREe Hemoglobin) amongst 20 patients and three U.S. clinical sites. The FDA subsequently approved an amendment to the protocol, expanding the study to a 40-patient randomized controlled study (20 treatment, 20 control) in eight clinical centers. REFRESH I represented the first part of a larger clinical trial strategy intended to support the approval of CytoSorb in the U.S. for intra-operative use during cardiac surgery.\\nThe REFRESH I study was designed to evaluate the safety and feasibility of two CytoSorb devices used intra-operatively with a heart-lung machine to reduce plasma free hemoglobin (pfHb) and cytokines in patients undergoing complex cardiac surgery. The study was not powered to measure effect on clinical outcomes. The length, complexity and invasiveness of these procedures cause hemolysis and inflammation, leading to high levels of plasma free hemoglobin, cytokines, activated complement, and other substances. These inflammatory mediators are correlated with the incidence of serious post-operative complications such as kidney injury, renal failure and other organ dysfunction. The goal of CytoSorb is to actively remove these inflammatory and toxic substances as they are being generated during the surgery and reduce complications. Enrollment was completed with 46 patients. A total of 38 patients were evaluable for pfHb and completed all aspects of the study.\\nThe primary safety and efficacy endpoints of the study were the assessment of serious device related adverse events and the change in plasma free hemoglobin levels, respectively. On October 5, 2016, we announced positive top-line safety data. In addition, following a detailed review of all reported adverse events in a total of 46 enrolled patients, the independent Data Safety Monitoring Board (“DSMB”) found no serious device related adverse events with the CytoSorb device, achieving the primary safety endpoint of the study. In addition, the therapy was well-tolerated and technically feasible, implementing easily into the cardiopulmonary bypass circuit without the need for an additional external blood pump. The REFRESH I study represented the first randomized controlled study demonstrating the safety of intra-operative CytoSorb use in patients undergoing high risk cardiac operations.\\nInvestigators of the REFRESH I study submitted an abstract with data, including free hemoglobin data, from the REFRESH I study which was selected for a podium presentation at the American Association of Thoracic Surgery conference on May 1, 2017. On May 5, 2017, we announced additional REFRESH I data, including data from the study on the reduction of pfHb and activated complement, and in May 2019, the manuscript of the REFRESH I study was electronically published in the journal, Seminars in Thoracic and Cardiovascular Surgery.\\nIn December 2017, the FDA approved our IDE application for our REFRESH 2-AKI study, permitting us to conduct this pivotal study designed to provide the key safety and efficacy data needed to support United States regulatory approval for CytoSorb in cardiac surgery, which we plan to pursue via the premarket approval (PMA) pathway. The REFRESH 2-AKI study is a randomized, controlled, multi-center, clinical study designed to evaluate intraoperative use of two CytoSorb devices as a therapy to reduce the incidence and severity of AKI, as measured by Kidney Disease Improving Global Outcomes (KDIGO) criteria, following complex cardiac surgery. Postoperative AKI following cardiac surgery is common and is associated with higher mortality, and is a risk factor for developing chronic kidney disease requiring hemodialysis in the future. The study will enroll up to 400 patients at increased risk of cardiovascular surgery-associated AKI, undergoing elective, non-emergent open-heart surgery for either valve replacement, or aortic reconstruction with hypothermic cardiac arrest. In April 2018, we announced the first patient enrollment into the pivotal U.S. REFRESH 2-AKI study. Based on the recommendations of key clinical advisors, a protocol amendment was submitted to the FDA on July 19, 2018 to improve operational aspects of the patient screening process and expand the inclusion criteria. It was the preference of clinical trial sites to defer enrollment until the amendment was approved by the FDA, announced in September 2018. On November 25, 2019 the Company announced a pause in enrollment for the REFRESH 2-AKI study. The study’s Data Monitoring Committee (the “DMC”) recommended this pause following a blinded, interim, milestone review of clinical study data. The DMC requested that additional clinical data and data analysis, not pre-specified in the current version of the protocol, be provided by Company. In addition, the Company appointed NAMSA as the new contract research organization (“CRO”) for the study to improve the monitoring of patient safety endpoints. As of November 25, 2019, the study had enrolled 153 patients at 25 initiated sites. Since then, the Company and its new CRO have completed a comprehensive program to re-monitor existing data, collect new data, and analyze the safety data from the 153 patients included in the trial to date. These data were reviewed by the DMC resulting in a favorable opinion on safety, dated July 24, 2020, and the recommendation to resume the trial with only minor modifications. The Company has been undertaking multiple activities in preparation to resume the study, which is planned in the first half of 2021, notwithstanding potential COVID-19 related delays. If the study is successful, we plan to submit a PMA application to the FDA in 2023 for U.S. regulatory approval.\\nIn October 2019, CytoSorbents initiated TISORB (Ticagrelor CytoSorb Hemoadsorption), a Company-sponsored, multi-center single arm study in the United Kingdom to prospectively evaluate the removal of ticagrelor during cardiopulmonary bypass in patients on ticagrelor undergoing emergent cardiothoracic surgery. A protocol amendment was submitted to expand the population of eligible patients to now include patients requiring urgent cardiac surgery. These changes were approved by the UK Medicines and Healthcare products Regulatory Agency (MHRA) at the end of February 2020. In December 2020, CytoSorbents initiated CYTATION (CytoSorb Ticagrelor Hemoadsorption), a Company-sponsored multicenter study in Germany to prospectively evaluate the removal of ticagrelor during cardiopulmonary bypass in patients on ticagrelor undergoing emergent cardiothoracic surgery. Ticagrelor (Brilinta®, Astra Zeneca) is a potent platelet inhibitor and antithrombotic therapy and recognized as a standard of care to reduce the risk of heart attacks and strokes in patients with acute coronary syndromes. Unfortunately, given the absence of an approved treatment to reverse the antithrombotic effects of ticagrelor, treated patients who require urgent or emergent cardiothoracic surgery may either proceed at high risk for severe perioperative bleeding (as high as 65% higher risk due to ticagrelor) or stay waiting for days in the hospital until the ticagrelor antithrombotic effect washes out. Neither option is optimal since patients proceeding to surgery are at great risk for serious or even fatal bleeding and patients waiting for washout are at risk of a thrombotic complications such as a stroke or heart attack, while delaying surgery and increasing hospitalization costs. The benefits of CytoSorb in this setting are both clinical and economic. In the publication in 2019 by Hassan et al, outcomes of 55 patients requiring emergent cardiac surgery while on ticagrelor or rivaroxaban therapy were evaluated according to the use of CytoSorb. Antithrombotic (either ticagrelor or rivaroxaban) removal with CytoSorb was associated with significant reductions in operative time, need for red blood cell and platelet transfusions and re-operations to control bleeding and those clinical benefits resulted in shorter length of ICU stay. These significant clinical benefits are expected to also result in substantial economic benefits. This was demonstrated in the publication by Javanbakht et al. in 2019, that projected an average cost savings of £3,982 per patient (approximately $5,000 USD per patient), including the cost of the CytoSorb adsorber. The primary objective in both TISORB and CYTATION studies is the change in platelet reactivity and ticagrelor blood concentration before and after cardiopulmonary bypass for patients undergoing CytoSorb hemoadsorption removal of ticagrelor from their blood. Due to the COVID-19 pandemic, the execution of the TISORB trial has been greatly impacted and ongoing national restrictions in the UK on the conduct of non-COVID clinical studies add further uncertainty to TISORB. In Germany, however, clinical research is continuing and we therefore expect that the CYTATION study will not be equally impacted.\\nIn January 2020, CytoSorb received European Union CE Mark label expansion to include the removal of ticagrelor during cardiopulmonary bypass in patients undergoing cardiothoracic surgery. In May 2020, CytoSorb also received European Union CE Mark label expansion to include rivaroxaban removal for the same indication. We have recently announced the Safe and Timely Antithrombotic Removal (STAR) development program that will comprise of a number of clinical projects relating to the antithrombotic removal application. The first study is the STAR Registry scheduled to commence enrollment in 2021 that will capture real world use of CytoSorb for this indication. The registry will initially launch in Europe with the intent of expanding to the United States and the rest of the territories where CytoSorb is available in the future. We anticipate that additional clinical studies, including randomized clinical trials on antithrombotic removal will be conducted as part of the STAR program.\\nUpdate on the REMOVE Investigator Initiated Study\\nThe German government, via the German Federal Ministry of Education and Research, is funding a 250 patient, multi-center randomized, controlled study (“REMOVE”) using CytoSorb during valve replacement open heart surgery in patients with infective endocarditis. The study enrolled its first patient in January 2018. An interim analysis of the first 50 patients has been completed. On February 4, 2019, Prof. Dr. med. Frank Brunkhorst, Director of the Center for Clinical Studies at Jena University Hospital, who is providing management and oversight to the REMOVE study, and Prof. Dr. med. Torsten Doenst, Director of the Clinic for Cardiac and Thoracic Surgery at the University of Jena, provided the following joint statement, “The Scientific Advisory Board (SAB) of the Center of Sepsis Control and Care (CSCC) and the Data Safety Monitoring Board (DSMB) of the REMOVE study recommended continuation of the study, based upon results of a pre-specified interim analysis that analyzed cytokine and vasoactive mediator levels as an indicator of the mechanistic mode of action of the device in 28 CytoSorb-treated patients and 22 control patients. There were no device-associated adverse events in the CytoSorb group.” The study completed enrollment in 2020 but the COVID-19 pandemic has caused delays in data monitoring and data analysis. Topline data are expected to be reported in the first half of 2021 with full data presentation at a major international conference also in 2021.\\nCOVID-19 Business Update\\nCOVID-19 patients develop life-threatening complications such as ARDS, shock (i.e. a potentially fatal drop in blood pressure), kidney failure, acute cardiac injury and secondary bacterial infections. The underlying cause for these complications is often a cytokine storm that results in a massive, systemic inflammatory response, leading to the damage of vital organs such as the lungs, heart, and kidneys, and ultimately multiple organ failure and death in many cases. CytoSorb has been used in more than 121,000 treatments as an approved treatment of cytokine storm in the European Union and is distributed in 67 countries around the world, where it has helped physicians control severe inflammation while helping to reverse shock and improve lung and other organ function.\\nThe use of CytoSorb in patients infected with COVID-19 in Italy, China, Germany and France began in March 2020. CytoSorb has now been used in approximately 5,000 COVID-19 patients to help treat cytokine storm and the related life-threatening complications in more than 30 countries. Based upon initial data and reports from physicians treating these complications, CytoSorb use has generally been associated with a marked reduction in cytokine storm and inflammation, improved lung function, weaning from mechanical ventilation, decannulation from extracorporeal membrane oxygenation (ECMO), and a reversal of shock. CytoSorb has been specifically recommended in the Italy Brescia Renal COVID Task Force Guidelines to treat patients with severe COVID-19 infection and Stage 3 renal failure on continuous renal replacement therapy. CytoSorb has also been recommended in the National Treatment Guidelines from Panama for Adult COVID-19 Patients if patients have either refractory shock, or have severe or refractory respiratory failure requiring either high ventilator support or extracorporeal membrane oxygenation. CytoSorb has now received approval from the Drugs Controller General of India to treat COVID-19 patients in certain instances. CytoSorb has also received approval to treat patients with COVID-19 from the Israel Ministry of Health (AMAR). In January 2021, Health Canada granted Medical Device Authorization for the importation, sale, and emergency use of CytoSorb in hospitalized COVID-19 patients.\\nThe use of CytoSorb has not been approved in the U.S. by FDA. However, under certain circumstances, investigational medical devices that have not yet been FDA-approved may be made available for emergency use in the U.S. under the FDA’s Expanded Access Program (“EAP”). On April 13, 2020, we announced that the FDA, in a different program than the EAP, granted Emergency Use Authorization (EUA) of CytoSorb for use in U.S. COVID-19 patients. Under the EUA, CytoSorbents can make CytoSorb available, through commercial sales, to all hospitals in the U.S. for use in patients, 18 years of age or older, with confirmed COVID-19 infection who are admitted to the intensive care unit with confirmed or imminent respiratory failure and who have early acute lung injury or ARDS, severe disease, or life-threatening illness resulting in respiratory failure, septic shock, and/or multiple organ dysfunction or failure. The CytoSorb device has been authorized by FDA under an EUA. It has neither been cleared nor approved for the indication to treat patients with COVID-19 Infection. The EUA will be effective until a declaration is made that the circumstances justifying the EUA have terminated or until revoked by the FDA.\\nThe CTC (CytoSorb Therapy in COVID-19) Registry has been launched and is actively enrolling patients with the intent of systematically capturing usage patterns and outcomes associated with the use of CytoSorb under the EUA at U.S. institutions. The CTC Registry may be expanded to outside-U.S. territories based on the evolution of the pandemic in an effort to better characterize best practice patterns and clinical outcomes.\\nTo meet the growing demand for CytoSorb worldwide, our manufacturing team continues to make any adjustments required to the production schedule to meet the increased sales demand.\\nGovernment Research Grants\\nWe have historically been successful in obtaining technology development contracts from governmental agencies such as the National Institutes of Health and the U.S. Department of Defense, including the Defense Advanced Research Projects Agency (“DARPA”), the U.S. Army, U.S. Special Operations Command (“USSOCOM”), the U.S. Air Force, Air Force Material Command (“USAF/AFMC”) and others. Currently, we have ongoing projects funded, in part, by the U.S. Army Medical Research Acquisition Activity (“USAMRAA”), the NHLBI, and the USAF/AFMC.\\nIn August 2012, we were awarded a $3.8 million, five-year contract by DARPA for our “Dialysis-Like Therapeutics” (“DLT”) program to treat sepsis. DARPA has been instrumental in funding many of the major technological and medical advances since its inception in 1958, including development of the Internet, development of GPS, and robotic surgery. The DLT program in sepsis sought to develop a therapeutic blood purification device that was capable of identifying the cause of sepsis (e.g., cytokines, toxins, pathogens, activated cells) and remove these substances in an intelligent, automated, and efficient manner. Our contract was for advanced technology development of our hemocompatible porous polymer technologies to remove cytokines and a number of pathogen and biowarfare toxins from blood. We have completed our work under the contract with DARPA and SSC Pacific under Contract No. N66001-12-C-4199, that provided for maximum funding of approximately $3,825,000. We received approximately $3,825,000 in funding under this contract and no funding remains. Our performance under this contract has been completed.\\nIn September 2012, we were awarded a Phase II SBIR contract by the U.S. Army Medical Research and Material Command to evaluate our technology for the treatment of trauma and burn injury in large animal models. In 2013, we finalized the Phase II SBIR contract which provided for a maximum funding of approximately $803,000 with the granting agency. This work is supported by the U.S. Army Medical Research and Material Command under an amendment to Contract W81XWH-12-C-0038. In June 2016, this contract was further amended to increase the maximum funding by $443,000 to approximately $1,246,000. We received approximately $1,246,000 in funding under this contract and no funding remains. Our performance under this contract has been completed.\\nIn September 2013, the National Heart Lung and Blood Institute (“NHLBI”) awarded us a Phase I Small Business Innovation Research (“SBIR”) contract, (number HHSN-268201-300044C), valued at $203,351, to further advance our HemoDefend blood purification technology for pRBC transfusions. The University of Dartmouth collaborated with us as a subcontractor on the project, entitled “Elimination of blood contaminants from pRBCs using HemoDefend hemocompatible porous polymer beads.” The overall goal of this program was to reduce the risk of potential side effects of blood transfusions, and help to extend the useful life of pRBCs. Our performance under this contract has been completed.\\nIn October 2015, we were awarded a Phase II SBIR contract by the NHLBI and USSOCOM to help advance our HemoDefend blood purification technology towards commercialization for the purification of pRBC transfusions. The contract, entitled “pRBCs Contaminant Removal with Porous Polymer Beads”, (contract number HHSN-268201-600006C), provided for maximum funding of approximately $1,524,000 over a two-year period. We received approximately $1,524,000 under this contract and no funding remains. Our performance under this contract has been completed.\\nIn March 2016, we were awarded a Phase I SBIR contract for a development program entitled “Mycotoxin Adsorption with Hemocompatible Porous Polymer Beads.” The purpose of this contract was to develop effective blood purification countermeasures for weaponized mycotoxins that can be easily disseminated in water, food and air. This work was funded by the U.S. Joint Program Executive Office for Chemical and Biological Defense, or JPEO-CBD, under contract number W911QY-16-P-0048 and provided for maximum funding of $150,000. We received approximately $150,000 and no funding remains under this contract. Our performance under this contract has been completed.\\nIn June 2016, we were awarded a Phase I Small Business Technology Transfer (“STTR”) contract for its development program entitled “Use of Highly Porous Polymer Beads to Remove Anti-A and Anti-B antibodies from Plasma for Transfusion”. The purpose of this contract was to develop our HemoDefend blood purification technology to potentially enable universal plasma. This work was funded by the USAMRAA under contract W81XWH-16-C-0025 and provided for maximum funding of $150,000. We received approximately $150,000 and no funding remains under this contract. Our performance under this contract has been completed.\\nIn July 2016, we were awarded a Phase I SBIR contract for its development program entitled “Investigation of a sorbent-based potassium adsorber for the treatment of hyperkalemia induced by traumatic injury and acute kidney injury in austere conditions”. The objective of this Phase I project was to develop two novel and distinct treatment options for life-threatening hyperkalemia. This work was funded by the U.S. Army Medical Research Acquisition Activity (“USAMRAA”) under contract W81XWH-16-C-0080 and provided for maximum funding of approximately $150,000. We received approximately $150,000 and no funding remains under this contract. Our performance under this contract has been completed.\\nIn January 2017, we were awarded a Phase II SBIR contract to continue development of CytoSorb for fungal mycotoxin blood purification. This program focused on demonstrating the ability of CytoSorb to adsorb mycotoxins in vivo and improve survival in animals. This contract, W911QY-17-C-0007, provided for maximum funding of $999,996 over two years. This program was funded by the Joint Program Executive Office - Chemical and Biological Defense (“CBD”) SBIR program. We received approximately $999,996 in funding under this contract and no further funding remains under this contract. Our performance under this contract has been completed.\\nIn May 2017, we were awarded a Phase II STTR contract entitled “Use of Highly Porous Polymer Beads to Remove Anti-A and Anti-B Antibiotics from Plasma Transfusion”. The purpose of this contract is to continue development of our HemoDefend blood purification technology to potentially enable universal plasma. We collaborate with researchers at Penn State University on this project. This contract provides for maximum funding of $999,070 over two years. This work is being funded by the USAMRAA under contract number W81XWH-17-C-0053. We received approximately $999,070 and no further funding remaining under this contract. Our performance under this contract has been completed.\\nIn May 2017, the Company was awarded a Congressionally Directed Medical Research Program (“CDMRP”) Phase I contract to improve delayed evacuation and prolonged field care for severe burn injury via novel hemoadsorptive and hydration therapies. This work is being funded by the USAMRAA under contract number W81WH-17-2-0013. This contract provides for maximum funding of $719,000 over four years. As of December 31, 2020, we received approximately $659,000 and have approximately $60,000 remaining under this contract.\\nIn September 2017, the Company was awarded a Phase II SBIR contract for its development program entitled “Investigation of a sorbent-based potassium adsorber for the treatment of hyperkalemia induced by traumatic injury and acute kidney injury”. The purpose of this contract is to continue development of two novel and distinct treatment options for life-threatening hyperkalemia. This work is being funded by the USAMRAA under contract W81XWH-17-C-0142 and provides for maximum funding of approximately $999,871. As of December 31, 2020, we received approximately $999,871 and no further funding remains under this contract.\\nIn August 2018, the Company was awarded a Phase IIB Bridge SBIR contract by the NHLBI to facilitate and accelerate the commercialization of our HemoDefend blood purification technology for the purification of pRBC transfusions. The contract, entitled “pRBCs Contaminant Removal with Hemocompatible Porous Polymer Beads” (award number 2R44HL141928-03), provides for maximum funding of approximately $2,971,000 over a three-year period. As of December 31, 2020, we received approximately $1,646,000 in funding under this contract and have approximately $1,325,000 remaining under this contract. Under the terms of this contract, we must make a matching contribution equal to the funds awarded thereunder.\\nIn September 2019, the Company was awarded a Rapid Innovation Fund contract by the USAF/AFMC to develop a simple, easy-to-use renal support system to treat severe hyperkalemia. The contract, entitled “K+ontrol Renal Support System for Reduction of Hyperkalemia” (award number FA8650-19-C-6065), provides for maximum funding of approximately $2,960,000 over a two-year period. As of December 31, 2020, we received approximately $814,000 funding under this contract and have approximately $2,146,000 remaining under this contract.\\nIn June 2020, the Company was awarded a two-year Defense Health Agency Small Business Technology transfer (STTR) Phase III contract to advance its HemoDefend-BGA plasma and whole blood adsorber to human clinical trials. (award number W81XWH20C0050), provides for maximum funding of approximately $2,897,000 over a two-year period. As of December 31, 2020, we received approximately $724,000 funding under this contract and have approximately $2,173,000 remaining under this contract.\\nIn July 2020, the Company was awarded by the Assistant Secretary of Defense for Health Affairs, endorsed by the Department of Defense office of the Congressionally Directed Medical Research Programs (CDMRP), a three-year contract as part of a Peer Reviewed Medical Research Program Technology/ Therapeutic Development Award to complete preclinical development of the HemoDefend™-BGA plasma and whole blood adsorber, (award number W81XWH2010712), provides for maximum funding of approximately $4,422,000 over a three-year period. As of December 31, 2020, we received approximately $81,000 funding under this contract and have approximately $4,341,000 remaining under this contract.\\nIn October 2020, the Company was awarded a two-year SBIR Sequential Phase II contract by the U.S. Army Medical Research Acquisition Activity (USAMRAA), to optimize development of the HemoDefend-BGA™ adsorber. (award number W81XWH20C0087), provides for maximum funding of approximately $1,100,000 over a two-year period. As of December 31, 2020, we received approximately $133,000 funding under this contract and have approximately $967,000 remaining under this contract.\\nOur business could be adversely impacted by automatic cuts in Federal spending. The American Taxpayer Relief Act (“ATRA”) of 2012, referred to generally as the fiscal cliff deal, that went into effect on March 1, 2013, enacted automatic spending cuts\\nof nearly $1 trillion over the next 10 years (commonly known as sequestration) that were included under the Budget Control Act of 2011. Sequestration may delay payments under the SBIR grant agreements, although no material delays have occurred to date. The short term and long-term economic impact of the sequestration will not be known until the actual spending cuts are implemented and the economic impact of the changes in the budget and taxes are known. It will take an extended number of years to understand the impact of any changes brought about from the sequester.\\nThe COVID-19 pandemic also has slowed progress on executing and invoicing for our funded grant and contract programs. This was due to social distancing and remote working requirements in our laboratories and at the facilities of our collaborators. Given the uncertain nature of COVID-19, we cannot predict the future impact of the pandemic on our research and development efforts and on our revenue recognition for total revenue.\\nThese grants represent a substantial research cost savings to us and we believe demonstrate the strong interest of the medical and scientific communities in our technology. We are also exploring potential eligibility in several other government-sponsored grant programs which could, if approved, represent a future source of non-dilutive funds for our research programs.\\nRegulation\\nThe medical devices that we manufacture are subject to regulation by numerous regulatory bodies, including the FDA and comparable international regulatory agencies. These agencies require manufacturers of medical devices to comply with applicable laws and regulations governing the development, testing, manufacturing, labeling, marketing and distribution of medical devices. Devices are generally subject to varying levels of regulatory control, the most comprehensive of which requires that a clinical evaluation be conducted before a device receives approval for commercial distribution.\\nIn the EU, medical devices that we manufacture are required to comply with the Medical Devices Directive 93/42/EC (“MDD”) and obtain CE Mark certification in order to market medical devices. The CE Mark certification, granted following approval from an independent notified body, is an EU-wide international symbol evidencing adherence to quality assurance standards and compliance with the MDD or other applicable European Medical Devices Directives. Distributors of medical devices may also be required to comply with other foreign regulations, such as Ministry of Health Labor and Welfare approval in Japan. The time required to obtain these foreign approvals to market our products may be longer or shorter than that required in the U.S., and requirements for those approvals may differ from those required by the FDA. In Europe, our devices are classified as Class IIb, and conform to the MDD. As of May 27, 2021, devices that have not received CE Mark renewal under the MDD or where existing device or processes are substantially amended, certification would be required in accordance with the new European Union Medical Device Regulation (“MDR”). However, devices already certified under the MDD can continue to use the CE Mark under the MDD until the expiry of those MDD CE certificates and in August of 2019, we announced that CytoSorb received renewal of its E.U. CE Mark through May 2024.\\nIn March 2011, we successfully completed our technical file review with our notified body, and received approval to apply the CE Mark to the CytoSorb device as an extracorporeal cytokine filter. We also achieved ISO 13485:2003 Full Quality Systems certification, an internationally recognized quality standard designed to ensure that medical device manufacturers have the necessary comprehensive management systems in place to safely design, develop, manufacture and distribute medical devices in the EU. In February 2015, we extended the coverage of our ISO 13485 Certificate with the inclusion of Canadian Quality Systems requirements. This additional level of certification will allow us to apply for product approvals in Canada in the future.\\nIn June 2016, we successfully completed an ISO 13485:2003 annual surveillance audit maintaining our good standing with our notified body. In September 2016, we were granted a two-year renewal for the CytoSorb CE Mark. In June 2018, we received clearance from our notified body to begin production in our new manufacturing facility. In July 2018, we successfully completed an audit upgrade from an ISO 13485:2003 certification to an ISO 13485:2016 certification, which is valid through September 2022.\\nIn the EU, as in other geographies, there are limits to the claims we are allowed to make, associated with the use of our devices. Specifically, any claims that we make should be included in our Clinical Evaluation Report, which is part of the conformity assessment process conducted by the Notified Body. If our claims exceed the assessed claims, either regarding performance or intended uses, we may be subject to regulatory actions, which could include customer notifications or even product or literature (i.e. labeling) recalls.\\nIn the U.S., specific permission from FDA to distribute a new device is usually required (that is, other than in the case of very low risk devices), and we expect that some form of marketing authorization will be necessary for our devices. Marketing authorization is generally sought and obtained in one of three ways. The first process requires that a pre-market notification (510(k) Submission) be made to the FDA to demonstrate that the device is as safe and effective as, or “substantially equivalent” to, a legally marketed device that is not subject to pre-market approval (“PMA”). A legally marketed device is a device that (i) was legally marketed prior to May 28,\\n1976, (ii) has been reclassified from Class III to Class II or I, or (iii) has been found to be substantially equivalent to another legally marketed device following a 510(k) Submission. The legally marketed device to which equivalence is drawn is known as the “predicate” device. Applicants must submit descriptive data and, when necessary, performance data to establish that the device is substantially equivalent to a predicate device. In some instances, data from human clinical studies must also be submitted in support of a 510(k) Submission. If so, these data must be collected in a manner that conforms with specific requirements in accordance with federal regulations including the Investigational Device Exemption (IDE) and human subject protections or “Good Clinical Practice” regulations. After the 510(k) application is submitted, the applicant cannot market the device unless FDA issues “510(k) clearance” deeming the device substantially equivalent. The FDA’s 510(k) review process usually takes from three to six months, but may take longer. The FDA may require additional information, including clinical data, to make a determination regarding substantial equivalence. After an applicant has obtained clearance, the changes to existing devices covered by a 510(k) Changes to the device which do not significantly affect safety or effectiveness can generally be made without additional 510(k) Submissions, but evaluation of whether a new 510(k) is needed is a complex regulatory issue, and changes must be evaluated on an ongoing basis to determine whether a proposed change triggers the need for a new 510(k), or even PMA .The 510(k) clearance pathway is not available for all devices: whether it is a suitable path to market depends on several factors, including regulatory classifications, the intended use of the device, and technical and risk-related issues for the device. Should a suitable predicate device not be available, the second pathway is the de novo request pathway. The de novo pathway is available for novel device technologies, including novel device changes, that have not been previously classified by FDA and for which there is no suitable predicate device. To obtain marketing authorization via the de novo pathway, the applicant must show that the subject device can be reclassified as Class I or Class II. The de novo request pathway typically requires additional testing data, which may include clinical data.\\nThe third, more rigorous, process requires that an application for PMA be made to the FDA to demonstrate that the device is safe and effective for its intended use as manufactured. This approval process applies to most Class III devices. A PMA submission is the most burdensome FDA premarket submission type for devices and includes data regarding design, materials, bench and animal testing, and human clinical data for the medical device. Again, clinical trials are subject to extensive FDA regulation.\\nFollowing completion of clinical trials, an applicant will submit a PMA with the required data. Within 45 days after a PMA is received by the FDA, the agency will notify the applicant whether the application has been “filed” (a threshold determination that the application is sufficiently complete to begin an in-depth review), then a substantive review period begins on the date of filing. Although the stated regulatory timeframe for the FDA’s review of PMAs is 180 days, FDA does not meet this goal for all applications; review often takes at least one year and may take significantly longer. During this review period, the FDA may request additional information or clarification of information already provided. Also during the review period, an advisory panel of experts from outside the FDA may be convened to review and evaluate the application and provide recommendations to the FDA. In addition, the FDA will conduct a pre-approval inspection of the manufacturing facilities to evaluate compliance with the FDA’s Quality System Regulation (“OSR”), which requires manufacturers to implement and follow design, testing, control, documentation and other quality assurance and good manufacturing practice procedures.\\nFollowing review of a PMA, the FDA will authorize commercial distribution if it determines there is reasonable assurance that the medical device is safe and effective for its intended purpose. This determination is based on the benefit outweighing the risk for the population intended to be treated with the device. Alternatively, the agency may issue an “approvable letter” or “not approvable letter” identifying deficiencies of varying degrees, or issue an order denying approval. The PMA process is much more detailed, time-consuming, and expensive than the 510(k) process. Also, FDA may impose a variety of conditions on the approval of a PMA.\\nIn the U.S., we believe that our potential devices, if we were to pursue marketing authorization, would likely fall under the classification for “Sorbent Hemoperfusion Systems” (21 C.F.R. § 876.5870). This category of device is Class II (subject to a 510(k) and special controls) when the device is intended for the treatment of poisoning and drug overdose, and Class III (subject to PMA) when the device is intended for the treatment of sepsis, hepatic coma and metabolic disturbances or other life-threatening illnesses.\\nBoth before and after a device for the U.S. market is commercially released, we would have ongoing responsibilities under FDA regulations. The FDA reviews design and manufacturing practices, labeling and record keeping, complaint handling, and manufacturers’ required reports of adverse events and device malfunctions and other information to identify potential problems with marketed medical devices. We would also be subject to periodic inspection by the FDA for compliance with the FDA’s QSR requirements, as mentioned above. In addition, the FDA and other U.S. regulatory bodies (including the Federal Trade Commission, the Office of the Inspector General of the Department of Health and Human Services, the Department of Justice (DOJ), and various state Attorneys General) monitor the manner in which we promote and advertise our products. Although physicians are permitted to use their medical judgment to employ medical devices for indications other than those cleared or approved by the FDA, we are prohibited from promoting products for such “off-label” uses, and can only market our products for cleared or approved uses. If the FDA were to conclude that we are not in compliance with applicable laws or regulations, or that any of our medical devices are ineffective or pose an\\nunreasonable health risk, the FDA could require us to notify health professionals and others that the devices present unreasonable risks of substantial harm to the public health; order a recall, repair, replacement, or refund of such devices, detain or seize adulterated or misbranded medical devices; or ban such medical devices. The FDA may also impose operating restrictions, enjoin and/or restrain certain conduct resulting in violations of applicable law pertaining to medical devices, including a hold on approving new devices until issues are resolved to its satisfaction, and work with the DOJ to assess civil or criminal penalties against our officers, employees, or us. Conduct giving rise to civil or criminal penalties may also form the basis for private civil litigation by third-party payers or other persons allegedly harmed by our conduct.\\nOn April 10, 2020 the FDA granted CytoSorbents Emergency Use Authorization of CytoSorb for the treatment of COVID-19. Per the FDA, \"The Emergency Use Authorization (EUA) authority allows FDA to help strengthen the nation\\'s public health protections against chemical, biological, radiological, and nuclear (CBRN) threats by facilitating the availability and use of medical countermeasures needed during public health emergencies. Under Section 564 of the Federal Food, Drug, and Cosmetic Act (the \"Act\"), the FDA commissioner may allow unapproved medical products or unapproved uses of approved medical products to be used in an emergency to diagnose, treat, or prevent serious or life-threatening disease or conditions caused by CBRN threat agents when there are no adequate, approved, and available alternatives.\"\\nEUA is an approval limited in scope and, subject to FDA discretion regarding duration of the approval. The FDA can at its discretion cancel the EUA approval when there is no longer a threat to public health.\\nThe delivery of our devices in the U.S. market would be subject to regulation by the U.S. Department of Health and Human Services and comparable state agencies responsible for reimbursement and regulation of health care items and services. U.S. laws and regulations are imposed primarily in connection with the Medicare and Medicaid programs, as well as the government’s interest in regulating the quality and cost of health care.\\nFederal health care laws apply when we or customers submit claims for items or services that are reimbursed under Medicare, Medicaid, or other federally-funded health care programs. The principal federal laws include: (1) the False Claims Act which prohibits the submission of false or otherwise improper claims for payment to a federally-funded health care program; (2) the Anti-Kickback Statute which prohibits offers to pay or receive remuneration of any kind for the purpose of inducing or rewarding referrals of items or services reimbursable by a Federal health care program; (3) the Stark law which prohibits physicians from referring Medicare or Medicaid patients to a provider that bills these programs for the provision of certain designated health services if the physician (or a member of the physician’s immediate family) has a financial relationship with that provider; and (4) health care fraud statutes that prohibit false statements and improper claims to any third-party payer. There are often similar state false claims, anti-kickback, and anti-self referral and insurance laws that apply to state-funded Medicaid and other health care programs and private third-party payers and some state laws apply regardless of payor (i.e., even in self-pay scenarios). These and other laws (including, for example, the Physician Payment Sunshine Act and state transparency and compliance laws) will become increasingly important as we progress toward commercialization in the U.S. In addition, the U.S. Foreign Corrupt Practices Act can be used to prosecute companies in the U.S. for arrangements with physicians, or other parties outside the U.S. if the physician or party is a government official of another country and the arrangement violates the law of that country.\\nThe laws applicable to us are subject to change, and subject to evolving interpretations. If a governmental authority were to conclude that we are not in compliance with applicable laws and regulations, we and our officers and employees could be subject to severe criminal and civil penalties including substantial fines and damages, and exclusion from participation as a supplier of product to beneficiaries covered by Medicare or Medicaid.\\nThe process of obtaining clearance or approval to market products is costly and time-consuming in virtually all of the major markets in which we expect to sell products and may delay the marketing and sale of our products. Countries around the world have recently adopted more stringent regulatory requirements, which are expected to add to the delays and uncertainties associated with new product releases, as well as the clinical and regulatory costs of supporting those releases. No assurance can be given that any of our other medical devices will be approved on a timely basis, if at all, or that our CytoSorb® device will be approved for CE Mark labeling under the MDR in other potential medical applications or that it will be approved for cytokine filtration in markets not covered by the CE Mark on a timely basis, or at all. In addition, regulations regarding the development, manufacture and sale of medical devices are subject to future change. We cannot predict what impact, if any, those changes might have on our business. Failure to comply with regulatory requirements could have a material adverse effect on our business, financial condition and results of operations.\\nPertaining to our VetResQ™ device (offered for veterinary use only), in the U.S., the FDA does not require submission of a 510(k), PMA, or any other pre-market review application for devices used in veterinary medicine. Device manufacturers who exclusively manufacture or distribute veterinary devices are not required to register their establishments and list veterinary devices and are exempt\\nfrom post-marketing reporting. FDA does have regulatory oversight over veterinary devices and can take appropriate regulatory action if a veterinary device is misbranded or adulterated. It is the responsibility of the manufacturer and/or distributor of these articles to assure that these animal devices are safe, effective, and properly labeled.\\nExported devices are subject to the regulatory requirements of each country to which the device is exported. Some countries do not have medical device regulations, but in most foreign countries medical devices are regulated. Frequently, device companies may choose to seek and obtain regulatory approval of a device in a foreign country prior to application in the U.S., as we have done, given the differing regulatory requirements. However, this does not ensure approval of a device in the U.S.\\nSales and Marketing\\nIn 2012, we established our European subsidiary, CytoSorbents Europe GmbH, a wholly-owned subsidiary of CytoSorbents Corporation. Following the completion of a controlled market release in late June 2012, CytoSorb was formally launched in Germany with reimbursement established at more than $500 per cartridge. We recruited Dr. Christian Steiner, MD as our Vice President of Sales and Marketing and hired three additional sales representatives. The fourth quarter of 2012 was the first full quarter of direct CytoSorb sales with our sales force in place. We began expansion into Austria, where reimbursement for CytoSorb is now available, and Switzerland. In March 2016, we established CytoSorbents Switzerland GmbH, a wholly-owned subsidiary of CytoSorbents Europe GmbH, to conduct marketing and direct sales in Switzerland. This subsidiary began operations during the second quarter of 2016. In 2017 we began direct sales in Belgium and Luxembourg. On March 5, 2019, the Company announced the expansion of direct sales of CytoSorb for all applications to Poland and the Netherlands, and critical care applications to Sweden, Denmark and Norway. As part of this effort, the Company established CytoSorbents Poland Sp. z.o.o., a wholly-owned subsidiary of CytoSorbents Europe GmbH. From the beginning of the controlled market release in the fourth quarter of 2011 through December 31, 2020, we achieved cumulative sales of CytoSorb of approximately $111,792,000. During this time period, the CytoSorb device represented substantially all of our product sales. At the end of 2020, we had hundreds of KOLs worldwide who are either using CytoSorb or supporting its use in clinical practice and/or in clinical studies.\\nWe are approved to sell CytoSorb in all 27 countries in the EU, including Germany, Italy, France and Spain as well as the United Kingdom, and currently have either direct sales or distributors or strategic partnerships in 67 countries worldwide. We plan to expand to other countries in the EU, and with registration, other countries outside the EU that will accept CE Mark approval with a mixed direct and independent distributor strategy, that can be augmented through strategic partnerships.\\nOverall, we have established either direct sales or distribution (via distributors or strategic partners) of CytoSorb in 67 countries worldwide. Registration of CytoSorb is typically required in each of these countries prior to active commercialization, in a process that can take several months to more than a year to achieve. Variability in the timing of registration affects the initiation of active commercialization in these countries, which affects the timing of expected CytoSorb sales. We cannot generally predict the timing of these registrations, and there can be no guarantee that we will ultimately achieve registration in countries where we have established distribution. For example, in August 2014 we announced exclusive distribution of CytoSorb in Taiwan with Hemoscien Corporation. However, in March 2015, due to the complexity we encountered with Taiwanese product registration, we elected to terminate our agreement with Hemoscien. Outside of the EU, CytoSorb has distribution in Turkey, India, Sri Lanka, Australia, New Zealand, Russia, Serbia, Norway, Vietnam, Malaysia, Hong Kong, Chile, Panama, Costa Rica, Colombia, Brazil, Mexico, Argentina, Perú, Guatemala, Ecuador, Bolivia, the Dominican Republic, El Salvador, Iceland, Israel, UAE, Iran, Saudi Arabia and other Middle Eastern countries, and South Korea. We cannot guarantee that we will generate meaningful sales in the countries where we have established registration, due to other factors such as market adoption and reimbursement. For example, in December 2019, we discontinued our distributor relationship with Dr. Reddy’s in South Africa due to lack of market adoption. We continuously evaluate other potential distributor and strategic partner networks in other countries that accept CE Mark approval.\\nIn addition to our direct and distributor commercial channels, we have a number of strategic partners to market and distribute CytoSorb. These partners include Biocon Biologics Limited, Fresenius Medical Care AG, Aferetica s.r.l. and Terumo Cardiovascular Group. In March 2021, we added B. Braun Avitum AG as a global co-marketing partner. For detailed information regarding these partnerships, see the section entitled “Commercial and Research Partners” in item 1 of this report.\\nA significant portion of our revenues are from product sales in Germany. Substantially all of our grant and other income are from grant agencies in the United States.\\nDuring the years ended 2020, 2019 and 2018, no agency, distributor or direct customer represented more than 10 percent of the Company’s total revenue.\\nOrders received for product from both direct customers and distributors are fulfilled upon receipt. Accordingly, we have no significant sales backlog.\\nIntellectual Property and Patent Litigation\\nThe medical device market in which we primarily participate is in large part technology driven. As a result, intellectual property rights, particularly patents and trade secrets, play a significant role in product development and differentiation. However, intellectual property litigation to defend or create market advantage is inherently complex, unpredictable and is expensive to pursue. Litigation often is not ultimately resolved until an appeal process is completed and appellate courts frequently overturn lower court patent decisions.\\nMoreover, competing parties frequently file multiple suits to leverage patent portfolios across product lines, technologies and geographies and to balance risk and exposure between the parties. In some cases, several competitors are parties in the same proceeding, or in a series of related proceedings, or litigate multiple features of a single class of devices. These forces frequently drive settlement not only of individual cases, but also of a series of pending and potentially related and unrelated cases. In addition, although monetary and injunctive relief is typically sought, remedies are generally not determined until the conclusion of the proceedings, and are frequently modified on appeal. Accordingly, the outcomes of individual cases are difficult to time, predict or quantify and are often dependent upon the outcomes of other cases in other forums, both domestic and international.\\nWe rely on a combination of patents, trademarks, trade secrets and non-disclosure agreements to protect our intellectual property. As of February 28, 2021, our patent portfolio includes 16 issued United States patents as well as multiple issued foreign patents and pending patent applications both in the U.S. and internationally, directed to various compositions and methods of use related to our blood purification technologies, which are expected to expire between 2021 and 2035, absent any patent term extensions. Management believes that any near-term expiring patents will not have a significant impact on our ongoing business. The following table provides a brief description of our patents that have been issued in the U.S.:\\nThere can be no assurance that pending patent applications will result in issued patents, that patents issued to us will not be challenged or circumvented by competitors, or that such patents will be found to be valid or sufficiently broad to protect our technology or to provide us with a competitive advantage. Certain of these patents also have foreign counterparts.\\nWe also rely on non-disclosure and non-competition agreements with employees, consultants and other parties to protect, in part, trade secrets and other proprietary technology. There can be no assurance that these agreements will not be breached, that we will have adequate remedies for any breach, that others will not independently develop equivalent proprietary information or that third parties will not otherwise gain access to our trade secrets and proprietary knowledge.\\nWe may find it necessary to initiate litigation to enforce our patent rights, to protect our trade secrets or know-how and to determine the scope and validity of the proprietary rights of others. Patent litigation can be costly and time-consuming, and there can be no assurance that our litigation expenses will not be significant in the future or that the outcome of litigation will be favorable to us.\\nAccordingly, we may seek to settle some or all of our pending litigation described below. Settlement may include cross-licensing of the patents which are the subject of the litigation as well as our other intellectual property and may involve monetary payments to or from third parties.\\nWe currently hold multiple trademarks including CytoSorb®, ECOS-300CY®, VetResQ®, HemoDefend™, BetaSorb™,DrugSorb™, and K+ontrolTM. We have spent considerable resources registering the trademark and building brand awareness and equity of the CytoSorb® tradename, which has been used in commerce since 2006. We expect to maintain and defend our various trademarks to the fullest extent possible.\\nEnvironmental Matters\\nWe believe that there are no compliance issues associated with applicable environmental laws and regulations that would have a material adverse effect on us or our business. We incur waste removal costs in connection with both our solid and liquid wastes which are byproducts of our manufacturing process. We utilize the services of various qualified contractors to dispose of these waste products. These waste removal costs amounted to approximately $324,000 for the year ended December 31, 2020.\\nEmployees\\nAs of March 1, 2021, we had 195 full-time and part-time employees. We also utilize consultants and temporary service providers who are not our employees, as necessary. None of our employees are represented by a labor union or are subject to collective-bargaining agreements and we believe we have good relationships with our employees.\\nItem 1A.', 'dataset': '', 'label': 'fin', 'answers': ['CytoSorb has received FDA Breakthrough Designation for the removal of ticagrelor in a cardiopulmonary bypass circuit during emergent and urgent cardiothoracic surgery, and Emergency Use Authorization (EUA) for use in critically-ill patients with COVID-19 infection.'], '_id': '027d0b5dfc645b2719dd10bbad5002c2', 'length': 50755}\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What regulatory milestones has CytoSorb achieved in the United States?\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0][\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CytoSorb has received FDA Breakthrough Designation for the removal of ticagrelor in a cardiopulmonary bypass circuit during emergent and urgent cardiothoracic surgery, and Emergency Use Authorization (EUA) for use in critically-ill patients with COVID-19 infection.']\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0][\"answers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "IT Revolution Press, LLC\n",
      "\n",
      "25 NW 23rd Pl, Suite 6314\n",
      "\n",
      "Portland, OR 97210\n",
      "\n",
      "Copyright © 2016 by Gene Kim, Jez Humble, Patrick Debois, and John Willis\n",
      "\n",
      "All rights reserved, for information about permission to reproduce selections from this book, write to Permissions, IT Revolution Press, LLC, 25 NW 23rd Pl, Suite 6314, Portland, OR 97210\n",
      "\n",
      "First Edition\n",
      "\n",
      "Printed in the United States of America\n",
      "\n",
      "10 9 8 7 6 5 4 3 2 1\n",
      "\n",
      "Cover design by Strauber Design Studio\n",
      "\n",
      "Cover illustration by eboy\n",
      "\n",
      "Book design by Mammoth Collective\n",
      "\n",
      "Ebook design by Digital Bindery\n",
      "\n",
      "Print ISBN: 978-1942788003\n",
      "\n",
      "Ebook–EPUB ISBN: 978-1-942788-07-2\n",
      "\n",
      "Ebook–Kindle ISBN: 978-1-942788-08-9\n",
      "\n",
      "Library of Congress Control Number: 2016951904\n",
      "\n",
      "Publisher's note to readers: Many of the ideas, quotations, and paraphrases attributed to different thinkers and industry leaders herein are excerpted from informal conversations, correspondence, interviews, conference roundtables, and other forms of oral communication that took place over the last six years during the development and writing of this book. Although the authors and publisher have made every effort to ensure that the information in this book was correct at press time, the authors and publisher do not assume and hereby disclaim any liability to any party for any loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result from negligence, accident, or any other cause.\n",
      "\n",
      "The author of the 18F case study on page 325 has dedicated the work to the public domain by waiving all of his or her rights to the work worldwide under copyright law, including all related and neighboring rights, to the extent allowed by law. You can copy, modify, distribute, and perform case study 18F, even for commercial purposes, all without asking permission.\n",
      "\n",
      "For information about special discounts for bulk purchases or for information on booking authors for an event, please visit ITRevolution.com.\n",
      "\n",
      "THE DEVOPS HANDBOOK\n",
      "\n",
      "## TABLE OF CONTENTS\n",
      "\n",
      "  1. Preface\n",
      "  2. Foreword\n",
      "  3. [Imagine a World Where Dev and Ops Become DevOps:  \n",
      "_An Introduction to The DevOps Handbook_](DOHB-0-FM-introduction.xhtml)\n",
      "  4. PART I—THE THREE WAYS \n",
      "  5. Part I Introduction\n",
      "    1.  **1** Agile, Continuous Delivery, and the Three Ways\n",
      "    2.  **2** The First Way: _The Principles of Flow_\n",
      "    3.  **3** The Second Way: _The Principles of Feedback_\n",
      "    4.  **4** The Third Way: _The Principles of Continual Learning and Experimentation_\n",
      "  6. PART II—WHERE TO START\n",
      "  7. Part II Introduction\n",
      "    1.  **5** Selecting Which Value Stream to Start With\n",
      "    2.  **6** Understanding the Work in Our Value Stream, Making it Visible, and Expanding it Across the Organization\n",
      "    3.  **7** How to Design Our Organization and Architecture with Conway's Law in Mind\n",
      "    4.  **8** How to Get Great Outcomes by Integrating Operations into the Daily Work of Development \n",
      "  8. PART III—THE FIRST WAY: _THE TECHNICAL PRACTICES OF FLOW_\n",
      "  9. Part III Introduction\n",
      "    1.  **9** Create the Foundations of Our Deployment Pipeline\n",
      "    2. **10** Enable Fast and Reliable Automated Testing\n",
      "    3. **11** Enable and Practice Continuous Integration\n",
      "    4. **12** Automate and Enable Low-Risk Releases\n",
      "    5. **13** Architect for Low-Risk Releases\n",
      "  10. PART IV—THE SECOND WAY: _THE TECHNICAL PRACTICES OF FEEDBACK_\n",
      "  11. Part IV Introduction \n",
      "    1. **14** Create Telemetry to Enable Seeing and Solving Problems\n",
      "    2. **15** Analyze Telemetry to Better Anticipate Problems and Achieve Goals\n",
      "    3. **16** Enable Feedback So Development and Operations Can Safely Deploy Code\n",
      "    4. **17** Integrate Hypothesis-Driven Development and A/B Testing into Our Daily Work\n",
      "    5. **18** Create Review and Coordination Processes to Increase Quality of Our Current Work\n",
      "  12. PART V—THE THIRD WAY: _THE TECHNICAL PRACTICES OF CONTINUAL LEARNING AND EXPERIMENTATION_\n",
      "  13. Part V Introduction\n",
      "    1. **19** Enable and Inject Learning into Daily Work\n",
      "    2. **20** Convert Local Discoveries into Global Improvements\n",
      "    3. **21** Reserve Time to Create Organizational Learning and Improvement\n",
      "  14. PART VI—THE TECHNOLOGICAL PRACTICES OF INTEGRATING INFORMATION SECURITY, CHANGE MANAGEMENT, AND COMPLIANCE \n",
      "  15. Part VI Introduction\n",
      "    1. **22** Information Security as Everyone's Job, Every Day\n",
      "    2. **23** Protecting the Deployment Pipeline and Integrating into Change Management and Other Security and Compliance Controls\n",
      "    3. Conclusion to the DevOps Handbook:\n",
      "    4. _A Call to Action_\n",
      "  16. Additional Material\n",
      "    1. Appendices\n",
      "    2. Additional Resources\n",
      "    3. Endnotes\n",
      "    4. Index\n",
      "    5. Acknowledgments\n",
      "    6. Author Biographies\n",
      "\n",
      "## Landmarks\n",
      "\n",
      "  1. Cover\n",
      "  2. Contents\n",
      "  3. Begin Reading\n",
      "\n",
      "  1. i\n",
      "  2. iv\n",
      "  3. ix\n",
      "  4. x\n",
      "  5. xi\n",
      "  6. xii\n",
      "  7. xiii\n",
      "  8. xiv\n",
      "  9. xv\n",
      "  10. xvi\n",
      "  11. xvii\n",
      "  12. xviii\n",
      "  13. xix\n",
      "  14. xxii\n",
      "  15. xxiii\n",
      "  16. xxiv\n",
      "  17. xxv\n",
      "  18. xxvi\n",
      "  19. xxvii\n",
      "  20. xxviii\n",
      "  21. xxix\n",
      "  22. xxx\n",
      "  23. xxxi\n",
      "  24. xxxii\n",
      "  25. xxxiii\n",
      "  26. xxxiv\n",
      "  27. xxxv\n",
      "  28. \n",
      "  29. \n",
      "  30. \n",
      "  31. \n",
      "  32. \n",
      "  33. \n",
      "  34. \n",
      "  35. \n",
      "  36. \n",
      "  37. \n",
      "  38. \n",
      "  39. \n",
      "  40. \n",
      "  41. \n",
      "  42. \n",
      "  43. \n",
      "  44. \n",
      "  45. \n",
      "  46. \n",
      "  47. \n",
      "  48. \n",
      "  49. \n",
      "  50. \n",
      "  51. \n",
      "  52. \n",
      "  53. \n",
      "  54. \n",
      "  55. \n",
      "  56. \n",
      "  57. \n",
      "  58. \n",
      "  59. \n",
      "  60. \n",
      "  61. \n",
      "  62. \n",
      "  63. \n",
      "  64. \n",
      "  65. \n",
      "  66. \n",
      "  67. \n",
      "  68. \n",
      "  69. \n",
      "  70. \n",
      "  71. \n",
      "  72. \n",
      "  73. \n",
      "  74. \n",
      "  75. \n",
      "  76. \n",
      "  77. \n",
      "  78. \n",
      "  79. \n",
      "  80. \n",
      "  81. \n",
      "  82. \n",
      "  83. \n",
      "  84. \n",
      "  85. \n",
      "  86. \n",
      "  87. \n",
      "  88. \n",
      "  89. \n",
      "  90. \n",
      "  91. \n",
      "  92. \n",
      "  93. \n",
      "  94. \n",
      "  95. \n",
      "  96. \n",
      "  97. \n",
      "  98. \n",
      "  99. \n",
      "  100. \n",
      "  101. \n",
      "  102. \n",
      "  103. \n",
      "  104. \n",
      "  105. \n",
      "  106. \n",
      "  107. \n",
      "  108. \n",
      "  109. \n",
      "  110. \n",
      "  111. \n",
      "  112. \n",
      "  113. \n",
      "  114. \n",
      "  115. \n",
      "  116. \n",
      "  117. \n",
      "  118. \n",
      "  119. \n",
      "  120. \n",
      "  121. \n",
      "  122. \n",
      "  123. \n",
      "  124. \n",
      "  125. \n",
      "  126. \n",
      "  127. \n",
      "  128. \n",
      "  129. \n",
      "  130. \n",
      "  131. \n",
      "  132. \n",
      "  133. \n",
      "  134. \n",
      "  135. \n",
      "  136. \n",
      "  137. \n",
      "  138. \n",
      "  139. \n",
      "  140. \n",
      "  141. \n",
      "  142. \n",
      "  143. \n",
      "  144. \n",
      "  145. \n",
      "  146. \n",
      "  147. \n",
      "  148. \n",
      "  149. \n",
      "  150. \n",
      "  151. \n",
      "  152. \n",
      "  153. \n",
      "  154. \n",
      "  155. \n",
      "  156. \n",
      "  157. \n",
      "  158. \n",
      "  159. \n",
      "  160. \n",
      "  161. \n",
      "  162. \n",
      "  163. \n",
      "  164. \n",
      "  165. \n",
      "  166. \n",
      "  167. \n",
      "  168. \n",
      "  169. \n",
      "  170. \n",
      "  171. \n",
      "  172. \n",
      "  173. \n",
      "  174. \n",
      "  175. \n",
      "  176. \n",
      "  177. \n",
      "  178. \n",
      "  179. \n",
      "  180. \n",
      "  181. \n",
      "  182. \n",
      "  183. \n",
      "  184. \n",
      "  185. \n",
      "  186. \n",
      "  187. \n",
      "  188. \n",
      "  189. \n",
      "  190. \n",
      "  191. \n",
      "  192. \n",
      "  193. \n",
      "  194. \n",
      "  195. \n",
      "  196. \n",
      "  197. \n",
      "  198. \n",
      "  199. \n",
      "  200. \n",
      "  201. \n",
      "  202. \n",
      "  203. \n",
      "  204. \n",
      "  205. \n",
      "  206. \n",
      "  207. \n",
      "  208. \n",
      "  209. \n",
      "  210. \n",
      "  211. \n",
      "  212. \n",
      "  213. \n",
      "  214. \n",
      "  215. \n",
      "  216. \n",
      "  217. \n",
      "  218. \n",
      "  219. \n",
      "  220. \n",
      "  221. \n",
      "  222. \n",
      "  223. \n",
      "  224. \n",
      "  225. \n",
      "  226. \n",
      "  227. \n",
      "  228. \n",
      "  229. \n",
      "  230. \n",
      "  231. \n",
      "  232. \n",
      "  233. \n",
      "  234. \n",
      "  235. \n",
      "  236. \n",
      "  237. \n",
      "  238. \n",
      "  239. \n",
      "  240. \n",
      "  241. \n",
      "  242. \n",
      "  243. \n",
      "  244. \n",
      "  245. \n",
      "  246. \n",
      "  247. \n",
      "  248. \n",
      "  249. \n",
      "  250. \n",
      "  251. \n",
      "  252. \n",
      "  253. \n",
      "  254. \n",
      "  255. \n",
      "  256. \n",
      "  257. \n",
      "  258. \n",
      "  259. \n",
      "  260. \n",
      "  261. \n",
      "  262. \n",
      "  263. \n",
      "  264. \n",
      "  265. \n",
      "  266. \n",
      "  267. \n",
      "  268. \n",
      "  269. \n",
      "  270. \n",
      "  271. \n",
      "  272. \n",
      "  273. \n",
      "  274. \n",
      "  275. \n",
      "  276. \n",
      "  277. \n",
      "  278. \n",
      "  279. \n",
      "  280. \n",
      "  281. \n",
      "  282. \n",
      "  283. \n",
      "  284. \n",
      "  285. \n",
      "  286. \n",
      "  287. \n",
      "  288. \n",
      "  289. \n",
      "  290. \n",
      "  291. \n",
      "  292. \n",
      "  293. \n",
      "  294. \n",
      "  295. \n",
      "  296. \n",
      "  297. \n",
      "  298. \n",
      "  299. \n",
      "  300. \n",
      "  301. \n",
      "  302. \n",
      "  303. \n",
      "  304. \n",
      "  305. \n",
      "  306. \n",
      "  307. \n",
      "  308. \n",
      "  309. \n",
      "  310. \n",
      "  311. \n",
      "  312. \n",
      "  313. \n",
      "  314. \n",
      "  315. \n",
      "  316. \n",
      "  317. \n",
      "  318. \n",
      "  319. \n",
      "  320. \n",
      "  321. \n",
      "  322. \n",
      "  323. \n",
      "  324. \n",
      "  325. \n",
      "  326. \n",
      "  327. \n",
      "  328. \n",
      "  329. \n",
      "  330. \n",
      "  331. \n",
      "  332. \n",
      "  333. \n",
      "  334. \n",
      "  335. \n",
      "  336. \n",
      "  337. \n",
      "  338. \n",
      "  339. \n",
      "  340. \n",
      "  341. \n",
      "  342. \n",
      "  343. \n",
      "  344. \n",
      "  345. \n",
      "  346. \n",
      "  347. \n",
      "  348. \n",
      "  349. \n",
      "  350. \n",
      "  351. \n",
      "  352. \n",
      "  353. \n",
      "  354. \n",
      "  355. \n",
      "  356. \n",
      "  357. \n",
      "  358. \n",
      "  359. \n",
      "  360. \n",
      "  361. \n",
      "  362. \n",
      "  363. \n",
      "  364. \n",
      "  365. \n",
      "  366. \n",
      "  367. \n",
      "  368. \n",
      "  369. \n",
      "  370. \n",
      "  371. \n",
      "  372. \n",
      "  373. \n",
      "  374. \n",
      "  375. \n",
      "  376. \n",
      "  377. \n",
      "  378. \n",
      "  379. \n",
      "  380. \n",
      "  381. \n",
      "  382. \n",
      "  383. \n",
      "  384. \n",
      "  385. \n",
      "  386. \n",
      "  387. \n",
      "  388. \n",
      "  389. \n",
      "  390. \n",
      "  391. \n",
      "  392. \n",
      "  393. \n",
      "  394. \n",
      "  395. \n",
      "  396. \n",
      "  397. \n",
      "  398. \n",
      "  399. \n",
      "  400. \n",
      "  401. \n",
      "  402. \n",
      "  403. \n",
      "  404. \n",
      "  405. \n",
      "  406. \n",
      "  407. \n",
      "  408. \n",
      "  409. \n",
      "  410. \n",
      "  411. \n",
      "  412. \n",
      "  413. \n",
      "  414. \n",
      "  415. \n",
      "  416. \n",
      "  417. \n",
      "  418. \n",
      "  419. \n",
      "  420. \n",
      "  421. \n",
      "  422. \n",
      "  423. \n",
      "  424. \n",
      "  425. \n",
      "  426. \n",
      "  427. \n",
      "  428. \n",
      "  429. \n",
      "  430. \n",
      "  431. \n",
      "  432. \n",
      "  433. \n",
      "  434. \n",
      "  435. \n",
      "  436. \n",
      "  437. \n",
      "  438. \n",
      "  439. \n",
      "  440. \n",
      "  441. \n",
      "  442. \n",
      "  443. \n",
      "  444. \n",
      "  445. \n",
      "  446. \n",
      "  447. \n",
      "  448. \n",
      "  449. \n",
      "  450. \n",
      "  451. \n",
      "  452. \n",
      "  453.\n",
      "\n",
      "# Preface\n",
      "\n",
      "## Aha!\n",
      "\n",
      "The journey to complete _The DevOps Handbook_ has been a long one—it started with weekly working Skype calls between the co-authors in February of 2011, with the vision of creating a prescriptive guide that would serve as a companion to the as-yet unfinished book _The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_.\n",
      "\n",
      "More than five years later, with over two thousand hours of work, _The DevOps Handbook_ is finally here. Completing this book has been an extremely long process, although one that has been highly rewarding and full of incredible learning, with a scope that is much broader than we originally envisioned. Throughout the project, all the co-authors shared a belief that DevOps is genuinely important, formed in a personal \"aha\" moment much earlier in each of our professional careers, which I suspect many of our readers will resonate with.\n",
      "\n",
      "### Gene Kim\n",
      "\n",
      "I've had the privilege of studying high-performing technology organizations since 1999, and one of the earliest findings was that boundary-spanning between the different functional groups of IT Operations, Information Security, and Development was critical to success. But I still remember the first time I saw the magnitude of the downward spiral that would result when these functions worked toward opposing goals.\n",
      "\n",
      "It was 2006, and I had the opportunity to spend a week with the group who managed the outsourced IT Operations of a large airline reservation service. They described the downstream consequences of their large, annual software releases: each release would cause immense chaos and disruption for the outsourcer, as well as customers; there would be SLA (service level agreement) penalties, because of the customer-impacting outages; there would be layoffs of the most talented and experienced staff, because of the resulting profit shortfalls; there would be much unplanned work and firefighting so that the remaining staff couldn't work on the ever-growing service request backlogs coming from customers; the contract would be held together by the heroics of middle management; and everyone felt that the contract would be doomed to be put out for re-bid in three years.\n",
      "\n",
      "The sense of hopelessness and futility that resulted created for me the beginnings of a moral crusade. Development seemed to always be viewed as strategic, but IT Operations was viewed as tactical, often delegated away or outsourced entirely, only to return in five years in worse shape than it was first handed over.\n",
      "\n",
      "For many years, many of us knew that there must be a better way. I remember seeing the talks coming out of the 2009 Velocity Conference, describing amazing outcomes enabled by architecture, technical practices, and cultural norms that we now know as DevOps. I was so excited, because it clearly pointed to the better way that we had all been searching for. And helping spread that word was one of my personal motivations to co-author _The Phoenix Project_. You can imagine how incredibly rewarding it was to see the broader community react to that book, describing how it helped them achieve their own \"aha\" moments.\n",
      "\n",
      "### Jez Humble\n",
      "\n",
      "My DevOps \"aha\" moment was at a start-up in 2000—my first job after graduating. For some time, I was one of two technical staff. I did everything: networking, programming, support, systems administration. We deployed software to production by FTP directly from our workstations.\n",
      "\n",
      "Then in 2004 I got a job at ThoughtWorks, a consultancy where my first gig was working on a project involving about seventy people. I was on a team of eight engineers whose full-time job was to deploy our software into a production-like environment. In the beginning, it was really stressful. But over a few months we went from manual deployments that took two weeks to an automated deployment that took one hour, where we could roll forward and back in milliseconds using the blue-green deployment pattern during normal business hours.\n",
      "\n",
      "That project inspired a lot of the ideas in both the _Continuous Delivery_ (Addison-Wesley, 2000) book and this one. A lot of what drives me and others working in this space is the knowledge that, whatever your constraints, we can always do better, and the desire to help people on their journey.\n",
      "\n",
      "### Patrick Debois\n",
      "\n",
      "For me, it was a collection of moments. In 2007 I was working on a data center migration project with some Agile teams. I was jealous that they had such high productivity—able to get so much done in so little time.\n",
      "\n",
      "For my next assignment, I started experimenting with Kanban in Operations and saw how the dynamic of the team changed. Later, at the Agile Toronto 2008 conference I presented my IEEE paper on this, but I felt it didn't resonate widely in the Agile community. We started an Agile system administration group, but I overlooked the human side of things.\n",
      "\n",
      "After seeing the 2009 Velocity Conference presentation \"10 Deploys per Day\" by John Allspaw and Paul Hammond, I was convinced others were thinking in a similar way. So I decided to organize the first DevOpsDays, accidently coining the term DevOps.\n",
      "\n",
      "The energy at the event was unique and contagious. When people started to thank me because it changed their life for the better, I understood the impact. I haven't stopped promoting DevOps since.\n",
      "\n",
      "### John Willis\n",
      "\n",
      "In 2008, I had just sold a consulting business that focused on large-scale, legacy IT operations practices around configuration management and monitoring (Tivoli) when I first met Luke Kanies (the founder of Puppet Labs). Luke was giving a presentation on Puppet at an O'Reilly open source conference on configuration management (CM).\n",
      "\n",
      "At first I was just hanging out at the back of the room killing time and thinking, \"What could this twenty-year-old tell me about configuration management?\" After all, I had literally been working my entire life at some of the largest enterprises in the world, helping them architect CM and other operations management solutions. However, about five minutes into his session, I moved up to the first row and realized everything I had been doing for the last twenty years was wrong. Luke was describing what I now call second generation CM.\n",
      "\n",
      "After his session I had an opportunity to sit down and have coffee with him. I was totally sold on what we now call infrastructure as code. However, while we met for coffee, Luke started going even further, explaining his ideas. He started telling me he believed that operations was going to have to start behaving like software developers. They were going to have to keep their configurations in source control and adopt CI/CD delivery patterns for their workflow. Being the old IT Operations person at the time, I think I replied to him with something like, \"That idea is going to sink like Led Zeppelin with Ops folk.\" (I was clearly wrong.)\n",
      "\n",
      "Then about a year later in 2009 at another O'Reilly conference, Velocity, I saw Andrew Clay Shafer give a presentation on Agile Infrastructure. In his presentation, Andrew showed this iconic picture of a wall between developers and operations with a metaphorical depiction of work being thrown over the wall. He coined this \"the wall of confusion.\" The ideas he expressed in that presentation codified what Luke was trying to tell me a year earlier. That was the light bulb for me. Later that year, I was the only American invited to the original DevOpsDays in Ghent. By the time that event was over, this thing we call DevOps was clearly in my blood.\n",
      "\n",
      "Clearly, the co-authors of this book all came to a similar epiphany, even if they came there from very different directions. But there is now an overwhelming weight of evidence that the problems described above happen almost everywhere, and that the solutions associated with DevOps are nearly universally applicable.\n",
      "\n",
      "The goal of writing this book is to describe how to replicate the DevOps transformations we've been a part of or have observed, as well as dispel many of the myths of why DevOps won't work in certain situations. Below are some of the most common myths we hear about DevOps.\n",
      "\n",
      "**Myth** — _DevOps is Only for Startups:_ While DevOps practices have been pioneered by the web-scale, Internet \"unicorn\" companies such as Google, Amazon, Netflix, and Etsy, each of these organizations has, at some point in their history, risked going out of business because of the problems associated with more traditional \"horse\" organizations: highly dangerous code releases that were prone to catastrophic failure, inability to release features fast enough to beat the competition, compliance concerns, an inability to scale, high levels of distrust between Development and Operations, and so forth.\n",
      "\n",
      "However, each of these organizations was able to transform their architecture, technical practices, and culture to create the amazing outcomes that we associate with DevOps. As Dr. Branden Williams, an information security executive, quipped, \"Let there be no more talk of DevOps unicorns or horses but only thoroughbreds and horses heading to the glue factory.\"\n",
      "\n",
      "**Myth** — _DevOps Replaces Agile:_ DevOps principles and practices are compatible with Agile, with many observing that DevOps is a logical continuation of the Agile journey that started in 2001. Agile often serves as an effective enabler of DevOps, because of its focus on small teams continually delivering high quality code to customers.\n",
      "\n",
      "Many DevOps practices emerge if we continue to manage our work beyond the goal of \"potentially shippable code\" at the end of each iteration, extending it to having our code always in a deployable state, with developers checking into trunk daily, and that we demonstrate our features in production-like environments.\n",
      "\n",
      "**Myth** — _DevOps is incompatible with ITIL:_ Many view DevOps as a backlash to ITIL or ITSM (IT Service Management), which was originally published in 1989. ITIL has broadly influenced multiple generations of Ops practitioners, including one of the co-authors, and is an ever-evolving library of practices intended to codify the processes and practices that underpin world-class IT Operations, spanning service strategy, design, and support.\n",
      "\n",
      "DevOps practices can be made compatible with ITIL process. However, to support the shorter lead times and higher deployment frequencies associated with DevOps, many areas of the ITIL processes become fully automated, solving many problems associated with the configuration and release management processes (e.g., keeping the configuration management database and definitive software libraries up to date). And because DevOps requires fast detection and recovery when service incidents occur, the ITIL disciplines of service design, incident, and problem management remain as relevant as ever.\n",
      "\n",
      "**Myth** — _DevOps is Incompatible with Information Security and Compliance:_ The absence of traditional controls (e.g., segregation of duty, change approval processes, manual security reviews at the end of the project) may dismay information security and compliance professionals.\n",
      "\n",
      "However, that doesn't mean that DevOps organizations don't have effective controls. Instead of security and compliance activities only being performed at the end of the project, controls are integrated into every stage of daily work in the software development life cycle, resulting in better quality, security, and compliance outcomes.\n",
      "\n",
      "**Myth** — _DevOps Means Eliminating IT Operations, or \"NoOps\":_ Many misinterpret DevOps as the complete elimination of the IT Operations function. However, this is rarely the case. While the nature of IT Operations work may change, it remains as important as ever. IT Operations collaborates far earlier in the software life cycle with Development, who continues to work with IT Operations long after the code has been deployed into production.\n",
      "\n",
      "Instead of IT Operations doing manual work that comes from work tickets, it enables developer productivity through APIs and self-serviced platforms that create environments, test and deploy code, monitor and display production telemetry, and so forth. By doing this, IT Operations become more like Development (as do QA and Infosec), engaged in product development, where the product is the platform that developers use to safely, quickly, and securely test, deploy, and run their IT services in production.\n",
      "\n",
      "**Myth** — _DevOps is Just \"Infrastructure as Code\" or Automation:_ While many of the DevOps patterns shown in this book require automation, DevOps also requires cultural norms and an architecture that allows for the shared goals to be achieved throughout the IT value stream. This goes far beyond just automation. As Christopher Little, a technology executive and one of the earliest chroniclers of DevOps, wrote, \"DevOps isn't about automation, just as astronomy isn't about telescopes.\"\n",
      "\n",
      "**Myth** — _DevOps is Only for Open Source Software:_ Although many DevOps success stories take place in organizations using software such as the LAMP stack (Linux, Apache, MySQL, PHP), achieving DevOps outcomes is independent of the technology being used. Successes have been achieved with applications written in Microsoft.NET, COBOL, and mainframe assembly code, as well as with SAP and even embedded systems (e.g., HP LaserJet firmware).\n",
      "\n",
      "## SPREADING THE AHA! MOMENT\n",
      "\n",
      "Each of the authors has been inspired by the amazing innovations happening in the DevOps community and the outcomes they are creating: they are creating safe systems of work, and enabling small teams to quickly and independently develop and validate code that can be safely deployed to customers. Given our belief that DevOps is a manifestation of creating dynamic, learning organizations that continually reinforce high-trust cultural norms, it is inevitable that these organizations will continue to innovate and win in the marketplace.\n",
      "\n",
      "It is our sincere hope that _The DevOps Handbook_ will serve as a valuable resource for many people in different ways: a guide for planning and executing DevOps transformations, a set of case studies to research and learn from, a chronicle of the history of DevOps, a means to create a coalition that spans Product Owners, Architecture, Development, QA, IT Operations, and Information Security to achieve common goals, a way to get the highest levels of leadership support for DevOps initiatives, as well as a moral imperative to change the way we manage technology organizations to enable better effectiveness and efficiency, as well as enabling a happier and more humane work environment, helping everyone become lifelong learners—this not only helps everyone achieve their highest goals as human beings, but also helps their organizations win.\n",
      "\n",
      "# Foreword\n",
      "\n",
      "In the past, many fields of engineering have experienced a sort of notable evolution, continually \"leveling-up\" its understanding of its own work. While there are university curriculums and professional support organizations situated within specific disciplines of engineering (civil, mechanical, electrical, nuclear, etc.), the fact is, modern society needs all forms of engineering to recognize the benefits of and work in a multidisciplinary way.\n",
      "\n",
      "Think about the design of a high-performance vehicle. Where does the work of a mechanical engineer end and the work of an electrical engineer begin? Where (and how, and when) should someone with domain knowledge of aerodynamics (who certainly would have well-formed opinions on the shape, size, and placement of windows) collaborate with an expert in passenger ergonomics? What about the chemical influences of fuel mixture and oil on the materials of the engine and transmission over the lifetime of the vehicle? There are other questions we can ask about the design of an automobile, but the end result is the same: success in modern technical endeavors absolutely requires multiple perspectives and expertise to collaborate.\n",
      "\n",
      "In order for a field or discipline to progress and mature, it needs to reach a point where it can thoughtfully reflect on its origins, seek out a diverse set of perspectives on those reflections, and place that synthesis into a context that is useful for how the community pictures the future.\n",
      "\n",
      "This book represents such a synthesis and should be seen as a seminal collection of perspectives on the (I will argue, still emerging and quickly evolving) field of software engineering and operations.\n",
      "\n",
      "No matter what industry you are in, or what product or service your organization provides, this way of thinking is paramount and necessary for survival for every business and technology leader.\n",
      "\n",
      "—John Allspaw, CTO, Etsy  \n",
      " _Brooklyn, NY, August 2016_\n",
      "\n",
      "# Imagine a World Where Dev and Ops Become DevOps\n",
      "\n",
      "## An Introduction to The DevOps Handbook\n",
      "\n",
      "Imagine a world where product owners, Development, QA, IT Operations, and Infosec work together, not only to help each other, but also to ensure that the overall organization succeeds. By working toward a common goal, they enable the fast flow of planned work into production (e.g., performing tens, hundreds, or even thousands of code deploys per day), while achieving world-class stability, reliability, availability, and security.\n",
      "\n",
      "In this world, cross-functional teams rigorously test their hypotheses of which features will most delight users and advance the organizational goals. They care not just about implementing user features, but also actively ensure their work flows smoothly and frequently through the entire value stream without causing chaos and disruption to IT Operations or any other internal or external customer.\n",
      "\n",
      "Simultaneously, QA, IT Operations, and Infosec are always working on ways to reduce friction for the team, creating the work systems that enable developers to be more productive and get better outcomes. By adding the expertise of QA, IT Operations, and Infosec into delivery teams and automated self-service tools and platforms, teams are able to use that expertise in their daily work without being dependent on other teams.\n",
      "\n",
      "This enables organizations to create a safe system of work, where small teams are able to quickly and independently develop, test, and deploy code and value quickly, safely, securely, and reliably to customers. This allows organizations to maximize developer productivity, enable organizational learning, create high employee satisfaction, and win in the marketplace.\n",
      "\n",
      "These are the outcomes that result from DevOps. For most of us, this is not the world we live in. More often than not, the system we work in is broken, resulting in extremely poor outcomes that fall well short of our true potential. In our world, Development and IT Operations are adversaries; testing and Infosec activities happen only at the end of a project, too late to correct any problems found; and almost any critical activity requires too much manual effort and too many handoffs, leaving us to always be waiting. Not only does this contribute to extremely long lead times to get anything done, but the quality of our work, especially production deployments, is also problematic and chaotic, resulting in negative impacts to our customers and our business.\n",
      "\n",
      "As a result, we fall far short of our goals, and the whole organization is dissatisfied with the performance of IT, resulting in budget reductions and frustrated, unhappy employees who feel powerless to change the process and its outcomes.† The solution? We need to change how we work; DevOps shows us the best way forward.\n",
      "\n",
      "To better understand the potential of the DevOps revolution, let us look at the Manufacturing Revolution of the 1980s. By adopting Lean principles and practices, manufacturing organizations dramatically improved plant productivity, customer lead times, product quality, and customer satisfaction, enabling them to win in the marketplace.\n",
      "\n",
      "Before the revolution, average manufacturing plant order lead times were six weeks, with fewer than 70% of orders being shipped on time. By 2005, with the widespread implementation of Lean practices, average product lead times had dropped to less than three weeks, and more than 95% of orders were being shipped on time. Organizations that did not implement Lean practices lost market share, and many went out of business entirely.\n",
      "\n",
      "Similarly, the bar has been raised for delivering technology products and services—what was good enough in previous decades is not good enough now. For each of the last four decades, the cost and time required to develop and deploy strategic business capabilities and features has dropped by orders of magnitude. During the 1970s and 1980s, most new features required one to five years to develop and deploy, often costing tens of millions of dollars.\n",
      "\n",
      "By the 2000's, because of advances in technology and the adoption of Agile principles and practices, the time required to develop new functionality had dropped to weeks or months, but deploying into production would still require weeks or months, often with catastrophic outcomes.\n",
      "\n",
      "And by 2010, with the introduction of DevOps and the neverending commoditization of hardware, software, and now the cloud, features (and even entire startup companies) could be created in weeks, quickly being deployed into production in just hours or minutes—for these organizations, deployment finally became routine and low risk. These organizations are able to perform experiments to test business ideas, discovering which ideas create the most value for customers and the organization as a whole, which are then further developed into features that can be rapidly and safely deployed into production.\n",
      "\n",
      "**Table 1.** The ever accelerating trend toward faster, cheaper, low-risk delivery of software\n",
      "\n",
      "(Source: Adrian Cockcroft, \"Velocity and Volume (or Speed Wins),\" presentation at FlowCon, San Francisco, CA, November 2013.)\n",
      "\n",
      "Today, organizations adopting DevOps principles and practices often deploy changes hundreds or even thousands of times per day. In an age where competitive advantage requires fast time to market and relentless experimentation, organizations that are unable to replicate these outcomes are destined to lose in the marketplace to more nimble competitors and could potentially go out of business entirely, much like the manufacturing organizations that did not adopt Lean principles.\n",
      "\n",
      "These days, regardless of what industry we are competing in, the way we acquire customers and deliver value to them is dependent on the technology value stream. Put even more succinctly, as Jeffrey Immelt, CEO of General Electric, stated, \"Every industry and company that is not bringing software to the core of their business will be disrupted.\" Or as Jeffrey Snover, Technical Fellow at Microsoft, said, \"In previous economic eras, businesses created value by moving atoms. Now they create value by moving bits.\"\n",
      "\n",
      "It's difficult to overstate the enormity of this problem—it affects every organization, independent of the industry we operate in, the size of our organization, whether we are profit or non-profit. Now more than ever, how technology work is managed and performed predicts whether our organizations will win in the marketplace, or even survive. In many cases, we will need to adopt principles and practices that look very different from those that have successfully guided us over the past decades. See Appendix 1.\n",
      "\n",
      "Now that we have established the urgency of the problem that DevOps solves, let us take some time to explore in more detail the symptomatology of the problem, why it occurs, and why, without dramatic intervention, the problem worsens over time.\n",
      "\n",
      "## THE PROBLEM: SOMETHING IN YOUR ORGANIZATION MUST NEED IMPROVEMENT (OR YOU WOULDN'T BE READING THIS BOOK)\n",
      "\n",
      "Most organizations are not able to deploy production changes in minutes or hours, instead requiring weeks or months. Nor are they able to deploy hundreds or thousands of changes into production per day; instead, they struggle to deploy monthly or even quarterly. Nor are production deployments routine, instead involving outages and chronic firefighting and heroics.\n",
      "\n",
      "In an age where competitive advantage requires fast time to market, high service levels, and relentless experimentation, these organizations are at a significant competitive disadvantage. This is in large part due to their inability to resolve a core, chronic conflict within their technology organization.\n",
      "\n",
      "### THE CORE, CHRONIC CONFLICT\n",
      "\n",
      "In almost every IT organization, there is an inherent conflict between Development and IT Operations which creates a downward spiral, resulting in ever-slower time to market for new products and features, reduced quality, increased outages, and, worst of all, an ever-increasing amount of technical debt.\n",
      "\n",
      "The term \"technical debt\" was first coined by Ward Cunningham. Analogous to financial debt, technical debt describes how decisions we make lead to problems that get increasingly more difficult to fix over time, continually reducing our available options in the future—even when taken on judiciously, we still incur interest.\n",
      "\n",
      "One factor that contributes to this is the often competing goals of Development and IT Operations. IT organizations are responsible for many things. Among them are the two following goals, which must be pursued simultaneously:\n",
      "\n",
      "  * Respond to the rapidly changing competitive landscape\n",
      "  * Provide stable, reliable, and secure service to the customer\n",
      "\n",
      "Frequently, Development will take responsibility for responding to changes in the market, deploying features and changes into production as quickly as possible. IT Operations will take responsibility for providing customers with IT service that is stable, reliable, and secure, making it difficult or even impossible for anyone to introduce production changes that could jeopardize production. Configured this way, Development and IT Operations have diametrically opposed goals and incentives.\n",
      "\n",
      "Dr. Eliyahu M. Goldratt, one of the founders of the manufacturing management movement, called these types of configuration \"the core, chronic conflict\"—when organizational measurements and incentives across different silos prevent the achievement of global, organizational goals.‡\n",
      "\n",
      "This conflict creates a downward spiral so powerful it prevents the achievement of desired business outcomes, both inside and outside the IT organization. These chronic conflicts often put technology workers into situations that lead to poor software and service quality, and bad customer outcomes, as well as a daily need for workarounds, firefighting, and heroics, whether in Product Management, Development, QA, IT Operations, or Information Security. See Appendix 2.\n",
      "\n",
      "### DOWNWARD SPIRAL IN THREE ACTS\n",
      "\n",
      "The downward spiral in IT has three acts that are likely familiar to most IT practitioners.\n",
      "\n",
      "The first act begins in IT Operations, where our goal is to keep applications and infrastructure running so that our organization can deliver value to customers. In our daily work, many of our problems are due to applications and infrastructure that are complex, poorly documented, and incredibly fragile. This is the technical debt and daily workarounds that we live with constantly, always promising that we'll fix the mess when we have a little more time. But that time never comes.\n",
      "\n",
      "Alarmingly, our most fragile artifacts support either our most important revenue-generating systems or our most critical projects. In other words, the systems most prone to failure are also our most important and are at the epicenter of our most urgent changes. When these changes fail, they jeopardize our most important organizational promises, such as availability to customers, revenue goals, security of customer data, accurate financial reporting, and so forth.\n",
      "\n",
      "The second act begins when somebody has to compensate for the latest broken promise—it could be a product manager promising a bigger, bolder feature to dazzle customers with or a business executive setting an even larger revenue target. Then, oblivious to what technology can or can't do, or what factors led to missing our earlier commitment, they commit the technology organization to deliver upon this new promise.\n",
      "\n",
      "As a result, Development is tasked with another urgent project that inevitably requires solving new technical challenges and cutting corners to meet the promised release date, further adding to our technical debt—made, of course, with the promise that we'll fix any resulting problems when we have a little more time.\n",
      "\n",
      "This sets the stage for the third and final act, where everything becomes just a little more difficult, bit by bit—everybody gets a little busier, work takes a little more time, communications become a little slower, and work queues get a little longer. Our work becomes more tightly-coupled, smaller actions cause bigger failures, and we become more fearful and less tolerant of making changes. Work requires more communication, coordination, and approvals; teams must wait just a little longer for their dependent work to get done; and our quality keeps getting worse. The wheels begin grinding slower and require more effort to keep turning. See Appendix 3.\n",
      "\n",
      "Although it's difficult to see in the moment, the downward spiral is obvious when one takes a step back. We notice that production code deployments are taking ever-longer to complete, moving from minutes to hours to days to weeks. And worse, the deployment outcomes have become even more problematic, that resulting in an ever-increasing number of customer-impacting outages that require more heroics and firefighting in Operations, further depriving them of their ability to pay down technical debt.\n",
      "\n",
      "As a result, our product delivery cycles continue to move slower and slower, fewer projects are undertaken, and those that are, are less ambitious. Furthermore, the feedback on everyone's work becomes slower and weaker, especially the feedback signals from our customers. And, regardless of what we try, things seem to get worse—we are no longer able to respond quickly to our changing competitive landscape, nor are we able to provide stable, reliable service to our customers. As a result, we ultimately lose in the marketplace.\n",
      "\n",
      "Time and time again, we learn that when IT fails, the entire organization fails. As Steven J. Spear noted in his book _The High-Velocity Edge_ , whether the damages \"unfold slowly like a wasting disease\" or rapidly \"like a fiery crash...the destruction can be just as complete.\"\n",
      "\n",
      "### WHY DOES THIS DOWNWARD SPIRAL HAPPEN EVERYWHERE?\n",
      "\n",
      "For over a decade, the authors of this book have observed this destructive spiral occur in countless organizations of all types and sizes. We understand better than ever why this downward spiral occurs and why it requires DevOps principles to mitigate. First, as described earlier, every IT organization has two opposing goals, and second, every company is a technology company, whether they know it or not.\n",
      "\n",
      "As Christopher Little, a software executive and one of the earliest chroniclers of DevOps, said, \"Every company is a technology company, regardless of what business they think they're in. A bank is just an IT company with a banking license.\"§\n",
      "\n",
      "To convince ourselves that this is the case, consider that the vast majority of capital projects have some reliance upon IT. As the saying goes, \"It is virtually impossible to make any business decision that doesn't result in at least one IT change.\"\n",
      "\n",
      "In the business and finance context, projects are critical because they serve as the primary mechanism for change inside organizations. Projects are typically what management needs to approve, budget for, and be held accountable for; therefore, they are the mechanism that achieve the goals and aspirations of the organization, whether it is to grow or even shrink.¶\n",
      "\n",
      "Projects are typically funded through capital spending (i.e., factories, equipment, and major projects, and expenditures are capitalized when payback is expected to take years), of which 50% is now technology related. This is even true in \"low tech\" industry verticals with the lowest historical spending on technology, such as energy, metal, resource extraction, automotive, and construction. In other words, business leaders are far more reliant upon the effective management of IT in order to achieve their goals than they think.**\n",
      "\n",
      "### THE COSTS: HUMAN AND ECONOMIC\n",
      "\n",
      "When people are trapped in this downward spiral for years, especially those who are downstream of Development, they often feel stuck in a system that pre-ordains failure and leaves them powerless to change the outcomes. This powerlessness is often followed by burnout, with the associated feelings of fatigue, cynicism, and even hopelessness and despair.\n",
      "\n",
      "Many psychologists assert that creating systems that cause feelings of powerlessness is one of the most damaging things we can do to fellow human beings—we deprive other people of their ability to control their own outcomes and even create a culture where people are afraid to do the right thing because of fear of punishment, failure, or jeopardizing their livelihood. This can create the conditions of _learned helplessness_ , where people become unwilling or unable to act in a way that avoids the same problem in the future.\n",
      "\n",
      "For our employees, it means long hours, working on weekends, and a decreased quality of life, not just for the employee, but for everyone who depends on them, including family and friends. It is not surprising that when this occurs, we lose our best people (except for those that feel like they can't leave, because of a sense of duty or obligation).\n",
      "\n",
      "In addition to the human suffering that comes with the current way of working, the opportunity cost of the value that we could be creating is staggering—the authors believe that we are missing out on approximately $2.6 trillion of value creation per year, which is, at the time of this writing, equivalent to the annual economic output of France, the sixth-largest economy in the world.\n",
      "\n",
      "Consider the following calculation—both IDC and Gartner estimated that in 2011, approximately 5% of the worldwide gross domestic product($3.1 trillion) was spent on IT (hardware, services, and telecom). If we estimate that 50% of that $3.1 trillion was spent on operating costs and maintaining existing systems, and that one-third of that 50% was spent on urgent and unplanned work or rework, approximately $520 billion was wasted.\n",
      "\n",
      "If adopting DevOps could enable us, through better management and increased operational excellence, to halve that waste and redeploy that human potential into something that's five times the value (a modest proposal), we could create $2.6 trillion of value per year.\n",
      "\n",
      "## THE ETHICS OF DEVOPS: THERE IS A BETTER WAY\n",
      "\n",
      "In the previous sections, we described the problems and the negative consequences of the status quo due to the core, chronic conflict, from the inability to achieve organizational goals, to the damage we inflict on fellow human beings. By solving these problems, DevOps astonishingly enables us to simultaneously improve organizational performance, achieve the goals of all the various functional technology roles (e.g., Development, QA, IT Operations, Infosec), and improve the human condition.\n",
      "\n",
      "This exciting and rare combination may explain why DevOps has generated so much excitement and enthusiasm in so many in such a short time, including technology leaders, engineers, and much of the software ecosystem we reside in.\n",
      "\n",
      "### BREAKING THE DOWNWARD SPIRAL WITH DEVOPS\n",
      "\n",
      "Ideally, small teams of developers independently implement their features, validate their correctness in production-like environments, and have their code deployed into production quickly, safely and securely. Code deployments are routine and predictable. Instead of starting deployments at midnight on Friday and spending all weekend working to complete them, deployments occur throughout the business day when everyone is already in the office and without our customers even noticing—except when they see new features and bug fixes that delight them. And, by deploying code in the middle of the workday, for the first time in decades IT Operations is working during normal business hours like everyone else.\n",
      "\n",
      "By creating fast feedback loops at every step of the process, everyone can immediately see the effects of their actions. Whenever changes are committed into version control, fast automated tests are run in production-like environments, giving continual assurance that the code and environments operate as designed and are always in a secure and deployable state.\n",
      "\n",
      "Automated testing helps developers discover their mistakes quickly (usually within minutes), which enables faster fixes as well as genuine learning—learning that is impossible when mistakes are discovered six months later during integration testing, when memories and the link between cause and effect have long faded. Instead of accruing technical debt, problems are fixed as they are found, mobilizing the entire organization if needed, because global goals outweigh local goals.\n",
      "\n",
      "Pervasive production telemetry in both our code and production environments ensure that problems are detected and corrected quickly, confirming that everything is working as intended and customers are getting value from the software we create.\n",
      "\n",
      "In this scenario, everyone feels productive—the architecture allows small teams to work safely and architecturally decoupled from the work of other teams who use self-service platforms that leverage the collective experience of Operations and Information Security. Instead of everyone waiting all the time, with large amounts of late, urgent rework, teams work independently and productively in small batches, quickly and frequently delivering new value to customers.\n",
      "\n",
      "Even high-profile product and feature releases become routine by using dark launch techniques. Long before the launch date, we put all the required code for the feature into production, invisible to everyone except internal employees and small cohorts of real users, allowing us to test and evolve the feature until it achieves the desired business goal.\n",
      "\n",
      "And, instead of firefighting for days or weeks to make the new functionality work, we merely change a feature toggle or configuration setting. This small change makes the new feature visible to ever-larger segments of customers, automatically rolling back if something goes wrong. As a result, our releases are controlled, predictable, reversible, and low stress.\n",
      "\n",
      "It's not just feature releases that are calmer—all sorts of problems are being found and fixed early, when they are smaller, cheaper, and easier to correct. With every fix, we also generate organizational learnings, enabling us to prevent the problem from recurring and enabling us to detect and correct similar problems faster in the future.\n",
      "\n",
      "Furthermore, everyone is constantly learning, fostering a hypothesis-driven culture where the scientific method is used to ensure nothing is taken for granted—we do nothing without measuring and treating product development and process improvement as experiments.\n",
      "\n",
      "Because we value everyone's time, we don't spend years building features that our customers don't want, deploying code that doesn't work, or fixing something that isn't actually the cause of our problem.\n",
      "\n",
      "Because we care about achieving goals, we create long-term teams that are responsible for meeting them. Instead of project teams where developers are reassigned and shuffled around after each release, never receiving feedback on their work, we keep teams intact so they can keep iterating and improving, using those learnings to better achieve their goals. This is equally true for the product teams who are solving problems for our external customers, as well as our internal platform teams who are helping other teams be more productive, safe, and secure.\n",
      "\n",
      "Instead of a culture of fear, we have a high-trust, collaborative culture, where people are rewarded for taking risks. They are able to fearlessly talk about problems as opposed to hiding them or putting them on the backburner—after all, we must see problems in order to solve them.\n",
      "\n",
      "And, because everyone fully owns the quality of their work, everyone builds automated testing into their daily work and uses peer reviews to gain confidence that problems are addressed long before they can impact a customer. These processes mitigate risk, as opposed to approvals from distant authorities, allowing us to deliver value quickly, reliably, and securely—even proving to skeptical auditors that we have an effective system of internal controls.\n",
      "\n",
      "And when something does go wrong, we conduct _blameless post-mortems_ , not to punish anyone, but to better understand what caused the accident and how to prevent it. This ritual reinforces our culture of learning. We also hold internal technology conferences to elevate our skills and ensure that everyone is always teaching and learning.\n",
      "\n",
      "Because we care about quality, we even inject faults into our production environment so we can learn how our system fails in a planned manner. We conduct planned exercises to practice large-scale failures, randomly kill processes and compute servers in production, and inject network latencies and other nefarious acts to ensure we grow ever more resilient. By doing this, we enable better resilience, as well as organizational learning and improvement.\n",
      "\n",
      "In this world, everyone has ownership in their work, regardless of their role in the technology organization They have confidence that their work matters and is meaningfully contributing to organizational goals, proven by their low-stress work environment and their organization's success in the marketplace. Their proof is that the organization is indeed winning in the marketplace.\n",
      "\n",
      "###  THE BUSINESS VALUE OF DEVOPS\n",
      "\n",
      "We have decisive evidence of the business value of DevOps. From 2013 through 2016, as part of Puppet Labs' _State Of DevOps Report,_ to which authors Jez Humble and Gene Kim contributed, we collected data from over twenty-five thousand technology professionals, with the goal of better understanding the health and habits of organizations at all stages of DevOps adoption.\n",
      "\n",
      "The first surprise this data revealed was how much high-performing organizations using DevOps practices were outperforming their non–high performing peers in the following areas:\n",
      "\n",
      "  * Throughput metrics\n",
      "  * Code and change deployments (thirty times more frequent)\n",
      "  * Code and change deployment lead time (two hundred times faster)\n",
      "  * Reliability metrics\n",
      "  * Production deployments (sixty times higher change success rate)\n",
      "  * Mean time to restore service (168 times faster)\n",
      "  * Organizational performance metrics\n",
      "  * Productivity, market share, and profitability goals (two times more likely to exceed)\n",
      "  * Market capitalization growth (50% higher over three years)\n",
      "\n",
      "In other words, high performers were both more agile and more reliable, providing empirical evidence that DevOps enables us to break the core, chronic conflict. High performers deployed code thirty times more frequently, and the time required to go from \"code committed\" to \"successfully running in production\" was two hundred times faster—high performers had lead times measured in minutes or hours, while low performers had lead times measured in weeks, months, or even quarters.\n",
      "\n",
      "Furthermore, high performers were twice as likely to exceed profitability, market share, and productivity goals. And, for those organizations that provided a stock ticker symbol, we found that high performers had 50% higher market capitalization growth over three years. They also had higher employee job satisfaction, lower rates of employee burnout, and their employees were 2.2 times more likely to recommend their organization to friends as a great place to work.†† High performers also had better information security outcomes. By integrating security objectives into all stages of the development and operations processes, they spent 50% less time remediating security issues.\n",
      "\n",
      "### DEVOPS HELPS SCALE DEVELOPER PRODUCTIVITY\n",
      "\n",
      "When we increase the number of developers, individual developer productivity often significantly decreases due to communication, integration, and testing overhead. This is highlighted in the famous book by Frederick Brook _, The Mythical Man-Month_ , where he explains that when projects are late, adding more developers not only decreases individual developer productivity but also decreases overall productivity.\n",
      "\n",
      "On the other hand, DevOps shows us that when we have the right architecture, the right technical practices, and the right cultural norms, small teams of developers are able to quickly, safely, and independently develop, integrate, test, and deploy changes into production. As Randy Shoup, formerly a director of engineering at Google, observed, large organizations using DevOps \"have thousands of developers, but their architecture and practices enable small teams to still be incredibly productive, as if they were a startup.\"\n",
      "\n",
      "The _2015 State of DevOps Report_ examined not only \"deploys per day\" but also \"deploys per day per developer.\" We hypothesized that high performers would be able to scale their number of deployments as team sizes grew.\n",
      "\n",
      "  **Figure 1.** Deployments/day vs. number of developers (Source: Puppet Labs, _2015 State Of DevOps Report_.)‡‡\n",
      "\n",
      "Indeed, this is what we found. Figure 1 shows that in low performers, deploys per day per developer go down as team size increases, stays constant for medium performers, and increases linearly for high performers.\n",
      "\n",
      "In other words, organizations adopting DevOps are able to linearly increase the number of deploys per day as they increase their number of developers, just as Google, Amazon, and Netflix have done.§§\n",
      "\n",
      "### THE UNIVERSALITY OF THE SOLUTION\n",
      "\n",
      "One of the most influential books in the Lean manufacturing movement is _The Goal: A Process of Ongoing Improvement_ written by Dr. Eliyahu M. Goldratt in 1984. It influenced an entire generation of professional plant managers around the world. It was a novel about a plant manager who had to fix his cost and product due date issues in ninety days, otherwise his plant would be shut down.\n",
      "\n",
      "Later in his career, Dr. Goldratt described the letters he received in response to _The Goal._ These letters would typically read, \"You have obviously been hiding in our factory, because you've described my life [as a plant manager] exactly...\" Most importantly, these letters showed people were able to replicate the breakthroughs in performance that were described in the book in their own work environments.\n",
      "\n",
      "_The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_ , written by Gene Kim, Kevin Behr, and George Spafford in 2013, was closely modeled after _The Goal_. It is a novel that follows an IT leader who faces all the typical problems that are endemic in IT organizations: an over-budget, behind-schedule project that must get to market in order for the company to survive. He experiences catastrophic deployments; problems with availability, security, and compliance; and so forth. Ultimately, he and his team use DevOps principles and practices to overcome those challenges, helping their organization win in the marketplace. In addition, the novel shows how DevOps practices improved the workplace environment for the team, creating lower stress and higher satisfaction because of greater practitioner involvement throughout the process.\n",
      "\n",
      "As with _The Goal_ , there is tremendous evidence of the universality of the problems and solutions described in _The Phoenix Project._ Consider some of the statements found in the Amazon reviews: \"I find myself relating to the characters in _The Phoenix Project_...I've probably met most of them over the course of my career,\" \"If you have ever worked in any aspect of IT, DevOps, or Infosec you will definitely be able to relate to this book,\" or \"There's not a character in _The Phoenix Project_ that I don't identify with myself or someone I know in real life... not to mention the problems faced and overcome by those characters.\"\n",
      "\n",
      "In the remainder of this book, we will describe how to replicate the transformation described in _The Phoenix Project_ , as well provide many case studies of how other organizations have used DevOps principles and practices to replicate those outcomes.\n",
      "\n",
      "## THE DEVOPS HANDBOOK: AN ESSENTIAL GUIDE\n",
      "\n",
      "The purpose of the _DevOps Handbook_ is to give you the theory, principles, and practices you need to successfully start your DevOps initiative and achieve your desired outcomes. This guidance is based on decades of sound management theory, study of high-performing technology organizations, work we have done helping organizations transform, and research that validates the effectiveness of the prescribed DevOps practices. As well as interviews with relevant subject matter experts and analyses of nearly one hundred case studies presented at the DevOps Enterprise Summit.\n",
      "\n",
      "Broken into six parts, this book covers DevOps theories and principles using the Three Ways, a specific view of the underpinning theory originally introduced in _The Phoenix Project_. _The DevOps Handbook_ is for everyone who performs or influences work in the technology value stream (which typically includes Product Management, Development, QA, IT Operations, and Information Security), as well as for business and marketing leadership, where most technology initiatives originate.\n",
      "\n",
      "The reader is not expected to have extensive knowledge of any of these domains, or of DevOps, Agile, ITIL, Lean, or process improvement. Each of these topics is introduced and explained in the book as it becomes necessary.\n",
      "\n",
      "Our intent is to create a working knowledge of the critical concepts in each of these domains, both to serve as a primer and to introduce the language necessary to help practitioners work with all their peers across the entire IT value stream, and to frame shared goals.\n",
      "\n",
      "This book will be of value to business leaders and stakeholders who are increasingly reliant upon the technology organization for the achievement of their goals.\n",
      "\n",
      "Furthermore, this book is intended for readers whose organizations might not be experiencing all the problems described in the book (e.g., long deployment lead times or painful deployments). Even readers in this fortunate position will benefit from understanding DevOps principles, especially those relating to shared goals, feedback, and continual learning.\n",
      "\n",
      "In Part I, we present a brief history of DevOps and introduce the underpinning theory and key themes from relevant bodies of knowledge that span over decades. We then present the high level principles of the Three Ways: Flow, Feedback, and Continual Learning and Experimentaion.\n",
      "\n",
      "Part II describes how and where to start, and presents concepts such as value streams, organizational design principles and patterns, organizational adoption patterns, and case studies.\n",
      "\n",
      "Part III describes how to accelerate Flow by building the foundations of our deployment pipeline: enabling fast and effective automated testing, continuous integration, continuous delivery, and architecting for low-risk releases.\n",
      "\n",
      "Part IV discusses how to accelerate and amplify Feedback by creating effective production telemetry to see and solve problems, better anticipate problems and achieve goals, enable feedback so that Dev and Ops can safely deploy changes, integrate A/B testing into our daily work, and create review and coordination processes to increase the quality of our work.\n",
      "\n",
      "Part V describes how we accelerate Continual Learning by establishing a just culture, converting local discoveries into global improvements, and properly reserving time to create organizational learning and improvements.\n",
      "\n",
      "Finally, in Part VI we describe how to properly integrate security and compliance into our daily work, by integrating preventative security controls into shared source code repositories and services, integrating security into our deployment pipeline, enhancing telemetry to better enable detection and recovery, protecting the deployment pipeline, and achieving change management objectives.\n",
      "\n",
      "By codifying these practices, we hope to accelerate the adoption of DevOps practices, increase the success of DevOps initiatives, and lower the activation energy required for DevOps transformations.\n",
      "\n",
      "* * *\n",
      "\n",
      "† This is just a small sample of the problems found in typical IT organizations.\n",
      "\n",
      "‡ In the manufacturing realm, a similar core, chronic conflict existed: the need to simultaneously ensure on-time shipments to customers and control costs. How this core, chronic conflict was broken is described in Appendix 2.\n",
      "\n",
      "§ In 2013, the European bank HSBC employed more software developers than Google.\n",
      "\n",
      "¶ For now, let us suspend the discussion of whether software should be funded as a \"project\" or a \"product.\" This is discussed later in the book.\n",
      "\n",
      "** For instance, Dr. Vernon Richardson and his colleagues published this astonishing finding. They studied the 10-K SEC filings of 184 public corporations and divided them into three groups: A) firms with material weaknesses with IT-related deficiencies, B) firms with material weaknesses with no IT-related deficiencies, and C) \"clean firms\" with no material weaknesses. Firms in Group A saw eight times higher CEO turnover than Group C, and there was four times higher CFO turnover in Group A than in Group C. Clearly, IT may matter far more than we typically think.\n",
      "\n",
      "†† As measured by employee Net Promoter Score (eNPS). This is a significant finding, as research has shown that \"companies with highly engaged workers grew revenues two and a half times as much as those with low engagement levels. And [publicly traded] stocks of companies with a high-trust work environment outperformed market indexes by a factor of three from 1997 through 2011.\"\n",
      "\n",
      "‡‡ Only organizations that are deploying at least once per day are shown.\n",
      "\n",
      "§§ Another more extreme example is Amazon. In 2011, Amazon was performing approximately seven thousand deploys per day. By 2015, they were performing 130,000 deploys per day.\n",
      "\n",
      "# Part I\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In Part I of _The DevOps Handbook_ , we will explore how the convergence of several important movements in management and technology set the stage for the DevOps movement. We describe value streams, how DevOps is the result of applying Lean principles to the technology value stream, and the Three Ways: Flow, Feedback, and Continual Learning and Experimentation.\n",
      "\n",
      "Primary focuses within these chapters include:\n",
      "\n",
      "  * The principles of Flow, which accelerate the delivery of work from Development to Operations to our customers\n",
      "  * The principles of Feedback, which enable us to create ever safer systems of work\n",
      "  * The principles of Continual Learning and Experimentation foster a high-trust culture and a scientific approach to organizational improvement risk-taking as part of our daily work\n",
      "\n",
      "## A BRIEF HISTORY\n",
      "\n",
      "DevOps and its resulting technical, architectural, and cultural practices represent a convergence of many philosophical and management movements. While many organizations have developed these principles independently, understanding that DevOps resulted from a broad stroke of movements, a phenomenon described by John Willis (one of the co-authors of this book) as the \"convergence of DevOps,\" shows an amazing progression of thinking and improbable connections. There are decades of lessons learned from manufacturing, high reliability organization, high-trust management models, and others that have brought us to the DevOps practices we know today.\n",
      "\n",
      "DevOps is the outcome of applying the most trusted principles from the domain of physical manufacturing and leadership to the IT value stream. DevOps relies on bodies of knowledge from Lean, Theory of Constraints, the Toyota Production System, resilience engineering, learning organizations, safety culture, human factors, and many others. Other valuable contexts that DevOps draws from include high-trust management cultures, servant leadership, and organizational change management. The result is world-class quality, reliability, stability, and security at ever lower cost and effort; and accelerated flow and reliability throughout the technology value stream, including Product Management, Development, QA, IT Operations, and Infosec.\n",
      "\n",
      "While the foundation of DevOps can be seen as being derived from Lean, the Theory of Constraints, and the Toyota Kata movement, many also view DevOps as the logical continuation of the Agile software journey that began in 2001.\n",
      "\n",
      "### THE LEAN MOVEMENT\n",
      "\n",
      "Techniques such as Value Stream Mapping, Kanban Boards, and Total Productive Maintenance were codified for the Toyota Production System in the 1980s. In 1997, the Lean Enterprise Institute started researching applications of Lean to other value streams, such as the service industry and healthcare.\n",
      "\n",
      "Two of Lean's major tenets include the deeply held belief that _manufacturing lead time_ required to convert raw materials into finished goods was the best predictor of quality, customer satisfaction, and employee happiness, and that one of the best predictors of short lead times was small batch sizes of work.\n",
      "\n",
      "Lean principles focus on how to create value for the customer through systems thinking by creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual.\n",
      "\n",
      "### THE AGILE MANIFESTO\n",
      "\n",
      "The Agile Manifesto was created in 2001 by seventeen of the leading thinkers in software development. They wanted to create a lightweight set of values and principles against heavyweight software development processes such as waterfall development, and methodologies such as the Rational Unified Process.\n",
      "\n",
      "One key principle was to \"deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale,\" emphasizing the desire for small batch sizes, incremental releases instead of large, waterfall releases. Other principles emphasized the need for small, self-motivated teams, working in a high-trust management model.\n",
      "\n",
      "Agile is credited for dramatically increasing the productivity of many development organizations. And interestingly, many of the key moments in DevOps history also occurred within the Agile community or at Agile conferences, as described below.\n",
      "\n",
      "### AGILE INFRASTRUCTURE AND VELOCITY MOVEMENT\n",
      "\n",
      "At the 2008 Agile conference in Toronto, Canada, Patrick Debois and Andrew Schafer held a \"birds of a feather\" session on applying Agile principles to infrastructure as opposed to application code. Although they were the only people who showed up, they rapidly gained a following of like-minded thinkers, including co-author John Willis.\n",
      "\n",
      "Later, at the 2009 Velocity conference, John Allspaw and Paul Hammond gave the seminal \"10 Deploys per Day: Dev and Ops Cooperation at Flickr\" presentation, where they described how they created shared goals between Dev and Ops and used continuous integration practices to make deployment part of everyone's daily work. According to first hand accounts, everyone attending the presentation immediately knew they were in the presence of something profound and of historic significance.\n",
      "\n",
      "Patrick Debois was not there, but was so excited by Allspaw and Hammond's idea that he created the first DevOpsDays in Ghent, Belgium, (where he lived) in 2009. There the term \"DevOps\" was coined.\n",
      "\n",
      "### THE CONTINUOUS DELIVERY MOVEMENT\n",
      "\n",
      "Building upon the development discipline of continuous build, test, and integration, Jez Humble and David Farley extended the concept to _continuous delivery_ , which defined the role of a \"deployment pipeline\" to ensure that code and infrastructure are always in a deployable state, and that all code checked in to trunk can be safely deployed into production. This idea was first presented at the 2006 Agile conference, and was also independently developed in 2009 by Tim Fitz in a blog post on his website titled \"Continuous Deployment.\"¶¶\n",
      "\n",
      "### TOYOTA KATA\n",
      "\n",
      "In 2009, Mike Rother wrote _Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results_ , which framed his twenty-year journey to understand and codify the Toyota Production System. He had been one of the graduate students who flew with GM executives to visit Toyota plants and helped develop the Lean toolkit, but he was puzzled when none of the companies adopting these practices replicated the level of performance observed at the Toyota plants.\n",
      "\n",
      "He concluded that the Lean community missed the most important practice of all, which he called the _improvement kata_. He explains that every organization has work routines, and the improvement kata requires creating structure for the daily, habitual practice of improvement work, because daily practice is what improves outcomes. The constant cycle of establishing desired future states, setting weekly target outcomes, and the continual improvement of daily work is what guided improvement at Toyota.\n",
      "\n",
      "The above describes the history of DevOps and relevant movements that it draws upon. Throughout the rest of Part I, we look at value streams, how Lean principles can be applied to the technology value stream, and the Three Ways of Flow, Feedback, and Continual Learning and Experimentation.\n",
      "\n",
      "* * *\n",
      "\n",
      "¶¶ DevOps also extends and builds upon the practices of _infrastructure as code_ , which was pioneered by Dr. Mark Burgess, Luke Kanies, and Adam Jacob. In infrastructure as code, the work of Operations is automated and treated like application code, so that modern development practices can be applied to the entire development stream. This further enabled fast deployment flow, including continuous integration (pioneered by Grady Booch and integrated as one of the key 12 practices of Extreme Programming), continuous delivery (pioneered by Jez Humble and David Farley), and continuous deployment (pioneered by Etsy, Wealthfront, and Eric Ries's work at IMVU).\n",
      "\n",
      "# 1Agile, Continuous Delivery, and the Three Ways\n",
      "\n",
      "In this chapter, an introduction to the underpinning theory of Lean Manufacturing is presented, as well as the Three Ways, the principles from which all of the observed DevOps behaviors can be derived.\n",
      "\n",
      "Our focus here is primarily on theory and principles, describing many decades of lessons learned from manufacturing, high-reliability organizations, high-trust management models, and others, from which DevOps practices have been derived. The resulting concrete principles and patterns, and their practical application to the technology value stream, are presented in the remaining chapters of the book.\n",
      "\n",
      "## THE MANUFACTURING VALUE STREAM\n",
      "\n",
      "One of the fundamental concepts in Lean is the _value stream_. We will define it first in the context of manufacturing and then extrapolate how it applies to DevOps and the technology value stream.\n",
      "\n",
      "Karen Martin and Mike Osterling define value stream in their book _Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation_ as \"the sequence of activities an organization undertakes to deliver upon a customer request,\" or \"the sequence of activities required to design, produce, and deliver a good or service to a customer, including the dual flows of information and material.\"\n",
      "\n",
      "In manufacturing operations, the value stream is often easy to see and observe: it starts when a customer order is received and the raw materials are released onto the plant floor. To enable fast and predictable lead times in any value stream, there is usually a relentless focus on creating a smooth and even flow of work, using techniques such as small batch sizes, reducing work in process (WIP), preventing rework to ensure we don't pass defects to downstream work centers, and constantly optimizing our system toward our global goals.\n",
      "\n",
      "## THE TECHNOLOGY VALUE STREAM\n",
      "\n",
      "The same principles and patterns that enable the fast flow of work in physical processes are equally applicable to technology work (and, for that matter, for all knowledge work). In DevOps, we typically define our technology value stream as the process required to convert a business hypothesis into a technology-enabled service that delivers value to the customer.\n",
      "\n",
      "The input to our process is the formulation of a business objective, concept, idea, or hypothesis, and starts when we accept the work in Development, adding it to our committed backlog of work.\n",
      "\n",
      "From there, Development teams that follow a typical Agile or iterative process will likely transform that idea into user stories and some sort of feature specification, which is then implemented in code into the application or service being built. The code is then checked in to the version control repository, where each change is integrated and tested with the rest of the software system.\n",
      "\n",
      "Because value is created only when our services are running in production, we must ensure that we are not only delivering fast flow, but that our deployments can also be performed without causing chaos and disruptions such as service outages, service impairments, or security or compliance failures.\n",
      "\n",
      "### FOCUS ON DEPLOYMENT LEAD TIME\n",
      "\n",
      "For the remainder of this book, our attention will be on deployment lead time, a subset of the value stream described above. This value stream begins when any engineer*** in our value stream (which includes Development, QA, IT Operations, and Infosec) checks a change into version control and ends when that change is successfully running in production, providing value to the customer and generating useful feedback and telemetry.\n",
      "\n",
      "The first phase of work that includes Design and Development is akin to Lean Product Development and is highly variable and highly uncertain, often requiring high degrees of creativity and work that may never be performed again, resulting in high variability of process times. In contrast, the second phase of work, which includes Testing and Operations, is akin to Lean Manufacturing. It requires creativity and expertise, and strives to be predictable and mechanistic, with the goal of achieving work outputs with minimized variability (e.g., short and predictable lead times, near zero defects).\n",
      "\n",
      "Instead of large batches of work being processed sequentially through the design/development value stream and then through the test/operations value stream (such as when we have a large batch waterfall process or long-lived feature branches), our goal is to have testing and operations happening simultaneously with design/development, enabling fast flow and high quality. This method succeeds when we work in small batches and build quality into every part of our value stream.†††\n",
      "\n",
      "#### Defining Lead Time vs. Processing Time\n",
      "\n",
      "In the Lean community, lead time is one of two measures commonly used to measure performance in value streams, with the other being processing time (sometimes known as touch time or task time).‡‡‡\n",
      "\n",
      "Whereas the lead time clock starts when the request is made and ends when it is fulfilled, the process time clock starts only when we begin work on the customer request—specifically, it omits the time that the work is in queue, waiting to be processed (figure 2).\n",
      "\n",
      "  **Figure 2.** Lead time vs. process time of a deployment operation\n",
      "\n",
      "Because lead time is what the customer experiences, we typically focus our process improvement attention there instead of on process time. However, the proportion of process time to lead time serves as an important measure of efficiency—achieving fast flow and short lead times almost always requires reducing the time our work is waiting in queues.\n",
      "\n",
      "#### The Common Scenario: Deployment Lead Times Requiring Months\n",
      "\n",
      "In business as usual, we often find ourselves in situations where our deployment lead times require months. This is especially common in large, complex organizations that are working with tightly-coupled, monolithic applications, often with scarce integration test environments, long test and production environment lead times, high reliance on manual testing, and multiple required approval processes.When this occurs, our value stream may look like figure 3:\n",
      "\n",
      "  **Figure 3:** A technology value stream with a deployment lead time of three months (Source: Damon Edwards, \"DevOps Kaizen,\" 2015.)\n",
      "\n",
      "When we have long deployment lead times, heroics are required at almost every stage of the value stream. We may discover that nothing works at the end of the project when we merge all the development team's changes together, resulting in code that no longer builds correctly or passes any of our tests. Fixing each problem requires days or weeks of investigation to determine who broke the code and how it can be fixed, and still results in poor customer outcomes.\n",
      "\n",
      "#### Our DevOps Ideal: Deployment Lead Times of Minutes\n",
      "\n",
      "In the DevOps ideal, developers receive fast, constant feedback on their work, which enables them to quickly and independently implement, integrate, and validate their code, and have the code deployed into the production environment (either by deploying the code themselves or by others).\n",
      "\n",
      "We achieve this by continually checking small code changes into our version control repository, performing automated and exploratory testing against it, and deploying it into production. This enables us to have a high degree of confidence that our changes will operate as designed in production and that any problems can be quickly detected and corrected.\n",
      "\n",
      "This is most easily achieved when we have architecture that is modular, well encapsulated, and loosely-coupled so that small teams are able to work with high degrees of autonomy, with failures being small and contained, and without causing global disruptions.\n",
      "\n",
      "In this scenario, our deployment lead time is measured in minutes, or, in the worst case, hours. Our resulting value stream map should look something like figure 4:\n",
      "\n",
      "  **Figure 4:** A technology value stream with a lead time of minutes\n",
      "\n",
      "### OBSERVING \"%C/A\" AS A MEASURE OF REWORK\n",
      "\n",
      "In addition to lead times and process times, the third key metric in the technology value stream is percent complete and accurate ( _%C/A_ ). This metric reflects the quality of the output of each step in our value stream. Karen Martin and Mike Osterling state that \"the %C/A can be obtained by asking downstream customers what percentage of the time they receive work that is 'usable as is,' meaning that they can do their work without having to correct the information that was provided, add missing information that should have been supplied, or clarify information that should have and could have been clearer.\"\n",
      "\n",
      "## THE THREE WAYS: THE PRINCIPLES UNDERPINNING DEVOPS\n",
      "\n",
      "_The Phoenix Project_ presents the Three Ways as the set of underpinning principles from which all the observed DevOps behaviors and patterns are derived (figure 5).\n",
      "\n",
      "The First Way enables fast left-to-right flow of work from Development to Operations to the customer. In order to maximize flow, we need to make work visible, reduce our batch sizes and intervals of work, build in quality by preventing defects from being passed to downstream work centers, and constantly optimize for the global goals.\n",
      "\n",
      "  **Figure 5:** The Three Ways (Source: Gene Kim, \"The Three Ways: The Principles Underpinning DevOps,\" IT Revolution Press blog, accessed August 9, 2016, <http://itrevolution.com/the-three-ways-principles-underpinning-devops/>.)\n",
      "\n",
      "By speeding up flow through the technology value stream, we reduce the lead time required to fulfill internal or customer requests, especially the time required to deploy code into the production environment. By doing this, we increase the quality of work as well as our throughput, and boost our ability to out-experiment the competition.\n",
      "\n",
      "The resulting practices include continuous build, integration, test, and deployment processes; creating environments on demand; limiting work in process (WIP); and building systems and organizations that are safe to change.\n",
      "\n",
      "The Second Way enables the fast and constant flow of feedback from right to left at all stages of our value stream. It requires that we amplify feedback to prevent problems from happening again, or enable faster detection and recovery. By doing this, we create quality at the source and generate or embed knowledge where it is needed—this allows us to create ever-safer systems of work where problems are found and fixed long before a catastrophic failure occurs.\n",
      "\n",
      "By seeing problems as they occur and swarming them until effective countermeasures are in place, we continually shorten and amplify our feedback loops, a core tenet of virtually all modern process improvement methodologies. This maximizes the opportunities for our organization to learn and improve.\n",
      "\n",
      "The Third Way enables the creation of a generative, high-trust culture that supports a dynamic, disciplined, and scientific approach to experimentation and risk-taking, facilitating the creation of organizational learning, both from our successes and failures. Furthermore, by continually shortening and amplifying our feedback loops, we create ever-safer systems of work and are better able to take risks and perform experiments that help us learn faster than our competition and win in the marketplace.\n",
      "\n",
      "As part of the Third Way, we also design our system of work so that we can multiply the effects of new knowledge, transforming local discoveries into global improvements. Regardless of where someone performs work, they do so with the cumulative and collective experience of everyone in the organization.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we described the concepts of value streams, lead time as one of the key measures of the effectiveness for both manufacturing and technology value streams, and the high-level concepts behind each of the Three Ways, the principles that underpin DevOps.\n",
      "\n",
      "In the following chapters, the principles for each of the Three Ways are described in greater detail. The first of these principles is Flow, which is focused on how we create the fast flow of work in any value stream, whether it's in manufacturing or technology work. The practices that enable fast flow are described in Part III.\n",
      "\n",
      "* * *\n",
      "\n",
      "*** Going forward, _engineer_ refers to anyone working in our value stream, not just developers.\n",
      "\n",
      "††† In fact, with techniques such as test-driven development, testing occurs even before the first line of code is written.\n",
      "\n",
      "‡‡‡ In this book, the term _process time_ will be favored for the same reason Karen Martin and Mike Osterling cite: \"To minimize confusion, we avoid using the term cycle time as it has several definitions synonymous with processing time and pace or frequency of output, to name a few.\"\n",
      "\n",
      "# 2The First Way:  \n",
      " _The Principles of Flow_\n",
      "\n",
      "In the technology value stream, work typically flows from Development to Operations, the functional areas between our business and our customers. The First Way requires the fast and smooth flow of work from Development to Operations, to deliver value to customers quickly. We optimize for this global goal instead of local goals, such as Development feature completion rates, test find/fix ratios, or Ops availability measures.\n",
      "\n",
      "We increase flow by making work visible, by reducing batch sizes and intervals of work, and by building quality in, preventing defects from being passed to downstream work centers. By speeding up the flow through the technology value stream, we reduce the lead time required to fulfill internal and external customer requests, further increasing the quality of our work while making us more agile and able to out-experiment the competition.\n",
      "\n",
      "Our goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services. Clues on how we do this in the technology value stream can be gleaned from how the Lean principles were applied to the manufacturing value stream.\n",
      "\n",
      "## MAKE OUR WORK VISIBLE\n",
      "\n",
      "A significant difference between technology and manufacturing value streams is that our work is invisible. Unlike physical processes, in the technology value stream we cannot easily see where flow is being impeded or when work is piling up in front of constrained work centers. Transferring work between work centers is usually highly visible and slow because inventory must be physically moved.\n",
      "\n",
      "However, in technology work the move can be done with a click of a button, such as by re-assigning a work ticket to another team. Because it is so easy, work can bounce between teams endlessly due to incomplete information, or work can be passed onto downstream work centers with problems that remain completely invisible until we are late delivering what we promised to the customer or our application fails in the production environment.\n",
      "\n",
      "To help us see where work is flowing well and where work is queued or stalled, we need to make our work as visible as possible. One of the best methods of doing this is using visual work boards, such as kanban boards or sprint planning boards, where we can represent work on physical or electronic cards. Work originates on the left (often being pulled from a backlog), is pulled from work center to work center (represented in columns), and finishes when it reaches the right side of the board, usually in a column labeled \"done\" or \"in production.\"\n",
      "\n",
      "  **Figure 6:** An example kanban board, spanning Requirements, Dev, Test, Staging, and In Production (Source: David J. Andersen and Dominica DeGrandis, _Kanban for ITOps_ , training materials for workshop, 2012.)\n",
      "\n",
      "Not only does our work become visible, we can also manage our work so that it flows from left to right as quickly as possible. Furthermore, we can measure lead time from when a card is placed on the board to when it is moved into the \"Done\" column.\n",
      "\n",
      "Ideally, our kanban board will span the entire value stream, defining work as completed only when it reaches the right side of the board (figure 6). Work is not done when Development completes the implementation of a feature—rather, it is only done when our application is running successfully in production, delivering value to the customer.\n",
      "\n",
      "By putting all work for each work center in queues and making it visible, all stakeholders can more easily prioritize work in the context of global goals. Doing this enables each work center to single-task on the highest priority work until it is completed, increasing throughput.\n",
      "\n",
      "## LIMIT WORK IN PROCESS (WIP)\n",
      "\n",
      "In manufacturing, daily work is typically dictated by a production schedule that is generated regularly (e.g., daily, weekly), establishing which jobs must be run based on customer orders, order due dates, parts available, and so forth.\n",
      "\n",
      "In technology, our work is usually far more dynamic—this is especially the case in shared services, where teams must satisfy the demands of many different stakeholders. As a result, daily work becomes dominated by the priority _du jour_ , often with requests for urgent work coming in through every communication mechanism possible, including ticketing systems, outage calls, emails, phone calls, chat rooms, and management escalations.\n",
      "\n",
      "Disruptions in manufacturing are also highly visible and costly, often requiring breaking the current job and scrapping any incomplete work in process to start the new job. This high level of effort discourages frequent disruptions.\n",
      "\n",
      "However, interrupting technology workers is easy, because the consequences are invisible to almost everyone, even though the negative impact to productivity may be far greater than in manufacturing. For instance, an engineer assigned to multiple projects must switch between tasks, incurring all the costs of having to re-establish context, as well as cognitive rules and goals.\n",
      "\n",
      "Studies have shown that the time to complete even simple tasks, such as sorting geometric shapes, significantly degrades when multitasking. Of course, because our work in the technology value stream is far more cognitively complex than sorting geometric shapes, the effects of multitasking on process time is much worse.\n",
      "\n",
      "We can limit multitasking when we use a kanban board to manage our work, such as by codifying and enforcing WIP (work in progress) limits for each column or work center that puts an upper limit on the number of cards that can be in a column.\n",
      "\n",
      "For example, we may set a WIP limit of three cards for testing. When there are already three cards in the test lane, no new cards can be added to the lane unless a card is completed or removed from the \"in work\" column and put back into queue (i.e., putting the card back to the column to the left). Nothing can can be worked on until it is represented first in a work card, reinforcing that all work must be made visible.\n",
      "\n",
      "Dominica DeGrandis, one of the leading experts on using kanbans in DevOps value streams, notes that \"controlling queue size [WIP] is an extremely powerful management tool, as it is one of the few leading indicators of lead time—with most work items, we don't know how long it will take until it's actually completed.\"\n",
      "\n",
      "Limiting WIP also makes it easier to see problems that prevent the completion of work.† For instance, when we limit WIP, we find that we may have nothing to do because we are waiting on someone else. Although it may be tempting to start new work (i.e., \"It's better to be doing something than nothing\"), a far better action would be to find out what is causing the delay and help fix that problem. Bad multitasking often occurs when people are assigned to multiple projects, resulting in many prioritization problems.\n",
      "\n",
      "In other words, as David J. Andersen, author of _Kanban: Successful Evolutionary Change for Your Technology Business_ , quipped, \"Stop starting. Start finishing.\"\n",
      "\n",
      "## REDUCE BATCH SIZES\n",
      "\n",
      "Another key component to creating smooth and fast flow is performing work in small batch sizes. Prior to the Lean manufacturing revolution, it was common practice to manufacture in large batch sizes (or lot sizes), especially for operations where job setup or switching between jobs was time-consuming or costly. For example, producing large car body panels requires setting large and heavy dies onto metal stamping machines, a process that could take days. When changeover cost is so expensive, we would often stamp as many panels at a time as possible, creating large batches in order to reduce the number of changeovers.\n",
      "\n",
      "However, large batch sizes result in skyrocketing levels of WIP and high levels of variability in flow that cascade through the entire manufacturing plant. The result is long lead times and poor quality—if a problem is found in one body panel, the entire batch has to be scrapped.\n",
      "\n",
      "One of the key lessons in Lean is that in order to shrink lead times and increase quality, we must strive to continually shrink batch sizes. The theoretical lower limit for batch size is _single-piece flow_ , where each operation is performed one unit at a time.‡\n",
      "\n",
      "The dramatic differences between large and small batch sizes can be seen in the simple newsletter mailing simulation described in _Lean Thinking: Banish Waste and Create Wealth in Your Corporation_ by James P. Womack and Daniel T. Jones.\n",
      "\n",
      "Suppose in our own example we have ten brochures to send and mailing each brochure requires four steps: fold the paper, insert the paper into the envelope, seal the envelope, and stamp the envelope.\n",
      "\n",
      "The large batch strategy (i.e., \"mass production\") would be to sequentially perform one operation on each of the ten brochures. In other words, we would first fold all ten sheets of paper, then insert each of them into envelopes, then seal all ten envelopes, and then stamp them.\n",
      "\n",
      "On the other hand, in the small batch strategy (i.e., \"single-piece flow\"), all the steps required to complete each brochure are performed sequentially before starting on the next brochure. In other words, we fold one sheet of paper, insert it into the envelope, seal it, and stamp it—only then do we start the process over with the next sheet of paper.\n",
      "\n",
      "The difference between using large and small batch sizes is dramatic (see figure 7). Suppose each of the four operations takes ten seconds for each of the ten envelopes. With the large batch size strategy, the first completed and stamped envelope is produced only after 310 seconds.\n",
      "\n",
      "Worse, suppose we discover during the envelope sealing operation that we made an error in the first step of folding—in this case, the earliest we would discover the error is at two hundred seconds, and we have to refold and reinsert all ten brochures in our batch again.\n",
      "\n",
      "  **Figure 7:** Simulation of \"envelope game\" (fold, insert, seal, and stamp the envelope)  \n",
      "(Source: Stefan Luyten, \"Single Piece Flow: Why mass production isn't the most efficient way of doing 'stuff',\" Medium.com, August 8, 2014, https://medium.com/@stefanluyten/single-piece-flow-5d2c2bec845b#.9o7sn74ns.)\n",
      "\n",
      "In contrast, in the small batch strategy the first completed stamped envelope is produced in only forty seconds, eight times faster than the large batch strategy. And, if we made an error in the first step, we only have to redo the one brochure in our batch. Small batch sizes result in less WIP, faster lead times, faster detection of errors, and less rework.\n",
      "\n",
      "The negative outcomes associated with large batch sizes are just as relevant to the technology value stream as in manufacturing. Consider when we have an annual schedule for software releases, where an entire year's worth of code that Development has worked on is released to production deployment.\n",
      "\n",
      "Like in manufacturing, this large batch release creates sudden, high levels of WIP and massive disruptions to all downstream work centers, resulting in poor flow and poor quality outcomes. This validates our common experience that the larger the change going into production, the more difficult the production errors are to diagnose and fix, and the longer they take to remediate.\n",
      "\n",
      "In a post on _Startup Lessons Learned_ , Eric Ries states, \"The batch size is the unit at which work-products move between stages in a development [or DevOps] process. For software, the easiest batch to see is code. Every time an engineer checks in code, they are batching up a certain amount of work. There are many techniques for controlling these batches, ranging from the tiny batches needed for continuous deployment to more traditional branch-based development, where all of the code from multiple developers working for weeks or months is batched up and integrated together.\"\n",
      "\n",
      "The equivalent to single piece flow in the technology value stream is realized with continuous deployment, where each change committed to version control is integrated, tested, and deployed into production. The practices that enable this are described in Part IV.\n",
      "\n",
      "## REDUCE THE NUMBER OF HANDOFFS\n",
      "\n",
      "In the technology value stream, whenever we have long deployment lead times measured in months, it is often because there are hundreds (or even thousands) of operations required to move our code from version control into the production environment. To transmit code through the value stream requires multiple departments to work on a variety of tasks, including functional testing, integration testing, environment creation, server administration, storage administration, networking, load balancing, and information security.\n",
      "\n",
      "Each time the work passes from team to team, we require all sorts of communication: requesting, specifying, signaling, coordinating, and often prioritizing, scheduling, deconflicting, testing, and verifying. This may require using different ticketing or project management systems; writing technical specification documents; communicating via meetings, emails, or phone calls; and using file system shares, FTP servers, and Wiki pages.\n",
      "\n",
      "Each of these steps is a potential queue where work will wait when we rely on resources that are shared between different value streams (e.g., centralized operations). The lead times for these requests are often so long that there is constant escalation to have work performed within the needed timelines.\n",
      "\n",
      "Even under the best circumstances, some knowledge is inevitably lost with each handoff. With enough handoffs, the work can completely lose the context of the problem being solved or the organizational goal being supported. For instance, a server administrator may see a newly created ticket requesting that user accounts be created, without knowing what application or service it's for, why it needs to be created, what all the dependencies are, or whether it's actually recurring work.\n",
      "\n",
      "To mitigate these types of problems, we strive to reduce the number of handoffs, either by automating significant portions of the work or by reorg-anizing teams so they can deliver value to the customer themselves, instead of having to be constantly dependent on others. As a result, we increase flow by reducing the amount of time that our work spends waiting in queue, as well as the amount of non–value-added time. See Appendix 4.\n",
      "\n",
      "## CONTINUALLY IDENTIFY AND ELEVATE OUR CONSTRAINTS\n",
      "\n",
      "To reduce lead times and increase throughput, we need to continually identify our system's constraints and improve its work capacity. In _Beyond the Goal_ , Dr. Goldratt states, \"In any value stream, there is always a direction of flow, and there is always one and only constraint; any improvement not made at that constraint is an illusion.\" If we improve a work center that is positioned before the constraint, work will merely pile up at the bottleneck even faster, waiting for work to be performed by the bottlenecked work center.\n",
      "\n",
      "On the other hand, if we improve a work center positioned _after_ the bottleneck, it remains starved, waiting for work to clear the bottleneck. As a solution, Dr. Goldratt defined the \"five focusing steps\":\n",
      "\n",
      "  * Identify the system's constraint.\n",
      "  * Decide how to exploit the system's constraint.\n",
      "  * Subordinate everything else to the above decisions.\n",
      "  * Elevate the system's constraint.\n",
      "  * If in the previous steps a constraint has been broken, go back to step one, but do not allow inertia to cause a system constraint.\n",
      "\n",
      "In typical DevOps transformations, as we progress from deployment lead times measured in months or quarters to lead times measured in minutes, the constraint usually follows this progression:\n",
      "\n",
      "  * **Environment creation:** We cannot achieve deployments on-demand if we always have to wait weeks or months for production or test environments. The countermeasure is to create environments that are on demand and completely self-serviced, so that they are always available when we need them.\n",
      "  * **Code deployment:** We cannot achieve deployments on demand if each of our production code deployments take weeks or months to perform (i.e., each deployment requires 1,300 manual, error-prone steps, involving up to three hundred engineers). The countermeasure is to automate our deployments as much as possible, with the goal of being completely automated so they can be done self-service by any developer.\n",
      "  * **Test setup and run:** We cannot achieve deployments on demand if every code deployment requires two weeks to set up our test environments and data sets, and another four weeks to manually execute all our regression tests. The countermeasure is to automate our tests so we can execute deployments safely and to parallelize them so the test rate can keep up with our code development rate.\n",
      "  * **Overly tight architecture:** We cannot achieve deployments on demand if overly tight architecture means that every time we want to make a code change we have to send our engineers to scores of committee meetings in order to get permission to make our changes. Our countermeasure is to create more loosely-coupled architecture so that changes can be made safely and with more autonomy, increasing developer productivity.\n",
      "\n",
      "After all these constraints have been broken, our constraint will likely be Development or the product owners. Because our goal is to enable small teams of developers to independently develop, test, and deploy value to customers quickly and reliably, this is where we want our constraint to be. High performers, regardless of whether an engineer is in Development, QA, Ops, or Infosec, state that their goal is to help maximize developer productivity.\n",
      "\n",
      "When the constraint is here, we are limited only by the number of good business hypotheses we create and our ability to develop the code necessary to test these hypotheses with real customers.\n",
      "\n",
      "The progression of constraints listed above are generalizations of typical transformations—techniques to identify the constraint in actual value streams, such as through value stream mapping and measurements, are described later in this book.\n",
      "\n",
      "## ELIMINATE HARDSHIPS AND WASTE IN THE VALUE STREAM\n",
      "\n",
      "Shigeo Shingo, one of the pioneers of the Toyota Production System, believed that waste constituted the largest threat to business viability—the commonly used definition in Lean is \"the use of any material or resource beyond what the customer requires and is willing to pay for.\" He defined seven major types of manufacturing waste: inventory, overproduction, extra processing, transportation, waiting, motion, and defects.\n",
      "\n",
      "More modern interpretations of Lean have noted that \"eliminating waste\" can have a demeaning and dehumanizing context; instead, the goal is reframed to reduce hardship and drudgery in our daily work through continual learning in order to achieve the organization's goals. For the remainder of this book, the term _waste_ will imply this more modern definition, as it more closely matches the DevOps ideals and desired outcomes.\n",
      "\n",
      "In the book _Implementing Lean Software Development: From Concept to Cash_ , Mary and Tom Poppendieck describe waste and hardship in the software development stream as anything that causes delay for the customer, such as activities that can be bypassed without affecting the result.\n",
      "\n",
      "The following categories of waste and hardship come from _Implementing Lean Software Development_ unless otherwise noted:\n",
      "\n",
      "  * **Partially done work:** This includes any work in the value stream that has not been completed (e.g., requirement documents or change orders not yet reviewed) and work that is sitting in queue (e.g., waiting for QA review or server admin ticket). Partially done work becomes obsolete and loses value as time progresses.\n",
      "  * **Extra processes:** Any additional work that is being performed in a process that does not add value to the customer. This may include documentation not used in a downstream work center, or reviews or approvals that do not add value to the output. Extra processes add effort and increase lead times.\n",
      "  * **Extra features:** Features built into the service that are not needed by the organization or the customer (e.g., \"gold plating\"). Extra features add complexity and effort to testing and managing functionality.\n",
      "  * **Task switching:** When people are assigned to multiple projects and value streams, requiring them to context switch and manage dependencies between work, adding additional effort and time into the value stream.\n",
      "  * **Waiting:** Any delays between work requiring resources to wait until they can complete the current work. Delays increase cycle time and prevent the customer from getting value.\n",
      "  * **Motion:** The amount of effort to move information or materials from one work center to another. Motion waste can be created when people who need to communicate frequently are not colocated. Handoffs also create motion waste and often require additional communication to resolve ambiguities. \n",
      "  * **Defects:** Incorrect, missing, or unclear information, materials, or products create waste, as effort is needed to resolve these issues. The longer the time between defect creation and defect detection, the more difficult it is to resolve the defect.\n",
      "  * **Nonstandard or manual work:** Reliance on nonstandard or manual work from others, such as using non-rebuilding servers, test environments, and configurations. Ideally, any dependencies on Operations should be automated, self-serviced, and available on demand.\n",
      "  * **Heroics:** In order for an organization to achieve goals, individuals and teams are put in a position where they must perform unreasonable acts, which may even become a part of their daily work (e.g., nightly 2:00 a.m. problems in production, creating hundreds of work tickets as part of every software release).§\n",
      "\n",
      "Our goal is to make these wastes and hardships—anywhere heroics become necessary—visible, and to systematically do what is needed to alleviate or eliminate these burdens and hardships to achieve our goal of fast flow.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Improving flow through the technology value stream is essential to achieving DevOps outcomes. We do this by making work visible, limiting WIP, reducing batch sizes and the number of handoffs, continually identifying and evaluating our constraints, and eliminating hardships in our daily work.\n",
      "\n",
      "The specific practices that enable fast flow in the DevOps value stream are presented in Part IV. In the next chapter, we present The Second Way: The Principles of Feedback.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Taiichi Ohno compared enforcing WIP limits to draining water from the river of inventory in order to reveal all the problems that obstruct fast flow.\n",
      "\n",
      "‡ Also known as \"batch size of one\" or \"1x1 flow,\" terms that refer to batch size and a WIP limit of one.\n",
      "\n",
      "§ Although heroics is not included in the Poppendieck categories of waste, it is included here because of how often it occurs, especially in Operation shared services.\n",
      "\n",
      "# 3The Second Way:  \n",
      " _The Principles of Feedback_\n",
      "\n",
      "While the First Way describes the principles that enable the fast flow of work from left to right, the Second Way describes the principles that enable the reciprocal fast and constant feedback from right to left at all stages of the value stream. Our goal is to create an ever safer and more resilient system of work.\n",
      "\n",
      "This is especially important when working in complex systems, when the earliest opportunity to detect and correct errors is typically when a catastrophic event is underway, such as a manufacturing worker being hurt on the job or a nuclear reactor meltdown in progress.\n",
      "\n",
      "In technology, our work happens almost entirely within complex systems with a high risk of catastrophic consequences. As in manufacturing, we often discover problems only when large failures are underway, such as a massive production outage or a security breach resulting in the theft of customer data.\n",
      "\n",
      "We make our system of work safer by creating fast, frequent, high quality information flow throughout our value stream and our organization, which includes feedback and feedforward loops. This allows us to detect and remediate problems while they are smaller, cheaper, and easier to fix; avert problems before they cause catastrophe; and create organizational learning that we integrate into future work. When failures and accidents occur, we treat them as opportunities for learning, as opposed to a cause for punishment and blame. To achieve all of the above, let us first explore the nature of complex systems and how they can be made safer.\n",
      "\n",
      "## WORKING SAFELY WITHIN COMPLEX SYSTEMS\n",
      "\n",
      "One of the defining characteristics of a complex system is that it defies any single person's ability to see the system as a whole and understand how all the pieces fit together. Complex systems typically have a high degree of interconnectedness of tightly-coupled components, and system-level behavior cannot be explained merely in terms of the behavior of the system components.\n",
      "\n",
      "Dr. Charles Perrow studied the Three Mile Island crisis and observed that it was impossible for anyone to understand how the reactor would behave in all circumstances and how it might fail. When a problem was underway in one component, it was difficult to isolate from the other components, quickly flowing through the paths of least resistance in unpredictable ways.\n",
      "\n",
      "Dr. Sidney Dekker, who also codified some of the key elements of safety culture, observed another characteristic of complex systems: doing the same thing twice will not predictably or necessarily lead to the same result. It is this characteristic that makes static checklists and best practices, while valuable, insufficient to prevent catastrophes from occurring. See Appendix 5.\n",
      "\n",
      "Therefore, because failure is inherent and inevitable in complex systems, we must design a safe system of work, whether in manufacturing or technology, where we can perform work without fear, confident that any errors will be detected quickly, long before they cause catastrophic outcomes, such as worker injury, product defects, or negative customer impact.\n",
      "\n",
      "After he decoded the causal mechanism behind the Toyota Product System as part of his doctoral thesis at Harvard Business School, Dr. Steven Spear stated that designing perfectly safe systems is likely beyond our abilities, but we can make it safer to work in complex systems when the four following conditions are met:†\n",
      "\n",
      "  * Complex work is managed so that problems in design and operations are revealed\n",
      "  * Problems are swarmed and solved, resulting in quick construction of new knowledge\n",
      "  * New local knowledge is exploited globally throughout the organization\n",
      "  * Leaders create other leaders who continually grow these types of capabilities\n",
      "\n",
      "Each of these capabilities are required to work safely in a complex system. In the next sections, the first two capabilities and their importance are described, as well as how they have been created in other domains and what practices enable them in the technology value stream. (The third and fourth capabilities are described in chapter 4.)\n",
      "\n",
      "## SEE PROBLEMS AS THEY OCCUR\n",
      "\n",
      "In a safe system of work, we must constantly test our design and operating assumptions. Our goal is to increase information flow in our system from as many areas as possible, sooner, faster, cheaper, and with as much clarity between cause and effect as possible. The more assumptions we can invalidate, the faster we can find and fix problems, increasing our resilience, agility, and ability to learn and innovate.\n",
      "\n",
      "We do this by creating feedback and feedforward loops into our system of work. Dr. Peter Senge in his book _The Fifth Discipline: The Art & Practice of the Learning Organization_ described feedback loops as a critical part of learning organizations and systems thinking. Feedback and feedforward loops cause components within a system to reinforce or counteract each other.\n",
      "\n",
      "In manufacturing, the absence of effective feedback often contribute to major quality and safety problems. In one well-documented case at the General Motors Fremont manufacturing plant, there were no effective procedures in place to detect problems during the assembly process, nor were there explicit procedures on what to do when problems were found. As a result, there were instances of engines being put in backward, cars missing steering wheels or tires, and cars even having to be towed off the assembly line because they wouldn't start.\n",
      "\n",
      "In contrast, in high-performing manufacturing operations there is fast, frequent, and high quality information flow throughout the entire value stream—every work operation is measured and monitored, and any defects or significant deviations are quickly found and acted upon. These are the foundation of what enables quality, safety, and continual learning and improvement.\n",
      "\n",
      "In the technology value stream, we often get poor outcomes because of the absence of fast feedback. For instance, in a waterfall software project, we may develop code for an entire year and get no feedback on quality until we begin the testing phase—or worse, when we release our software to customers. When feedback is this delayed and infrequent, it is too slow to enable us to prevent undesirable outcomes.\n",
      "\n",
      "In contrast, our goal is to create fast feedback and fastforward loops wherever work is performed, at all stages of the technology value stream, encompassing Product Management, Development, QA, Infosec, and Operations. This includes the creation of automated build, integration, and test processes, so that we can immediately detect when a change has been introduced that takes us out of a correctly functioning and deployable state.\n",
      "\n",
      "We also create pervasive telemetry so we can see how all our system components are operating in the production environment, so that we can quickly detect when they are not operating as expected. Telemetry also allows us to measure whether we are achieving our intended goals and, ideally, is radiated to the entire value stream so we can see how our actions affect other portions of the system as a whole.\n",
      "\n",
      "Feedback loops not only enable quick detection and recovery of problems, but they also inform us on how to prevent these problems from occurring again in the future. Doing this increases the quality and safety of our system of work, and creates organizational learning.\n",
      "\n",
      "As Elisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of _Explore It!: Reduce Risk and Increase Confidence with Exploratory Testing_ , said, \"When I headed up quality engineering, I described my job as 'creating feedback cycles.' Feedback is critical because it is what allows us to steer. We must constantly validate between customer needs, our intentions and our implementations. Testing is merely one sort of feedback.\"\n",
      "\n",
      "## SWARM AND SOLVE PROBLEMS TO BUILD NEW KNOWLEDGE\n",
      "\n",
      "Obviously, it is not sufficient to merely detect when the unexpected occurs. When problems occur, we must swarm them, mobilizing whoever is required to solve the problem.\n",
      "\n",
      "According to Dr. Spear, the goal of swarming is to contain problems before they have a chance to spread, and to diagnose and treat the problem so that it cannot recur. \"In doing so,\" he says, \"they build ever-deeper knowledge about how to manage the systems for doing our work, converting inevitable up-front ignorance into knowledge.\"\n",
      "\n",
      "The paragon of this principle is the Toyota _Andon cord_. In da Toyota manufacturing plant, above every work center is a cord that every worker and manager is trained to pull when something goes wrong; for example, when a part is defective, when a required part is not available, or even when work takes longer than documented.‡\n",
      "\n",
      "When the Andon cord is pulled, the team leader is alerted and immediately works to resolve the problem. If the problem cannot be resolved within a specified time (e.g., fifty-five seconds), the production line is halted so that the entire organization can be mobilized to assist with problem resolution until a successful countermeasure has been developed.\n",
      "\n",
      "Instead of working around the problem or scheduling a fix \"when we have more time,\" we swarm to fix it immediately—this is nearly the opposite of the behavior at the GM Fremont plant described earlier. Swarming is necessary for the following reasons:\n",
      "\n",
      "  * It prevents the problem from progressing downstream, where the cost and effort to repair it increases exponentially and technical debt is allowed to accumulate.\n",
      "  * It prevents the work center from starting new work, which will likely introduce new errors into the system.\n",
      "  * If the problem is not addressed, the work center could potentially have the same problem in the next operation (e.g., fifty-five seconds later), requiring more fixes and work. See Appendix 6.\n",
      "\n",
      "This practice of swarming seems contrary to common management practice, as we are deliberately allowing a local problem to disrupt operations globally. However, swarming enables learning. It prevents the loss of critical information due to fading memories or changing circumstances. This is especially critical in complex systems, where many problems occur because of some unexpected, idiosyncratic interaction of people, processes, products, places, and circumstances—as time passes, it becomes impossible to reconstruct exactly what was going on when the problem occurred.\n",
      "\n",
      "As Dr. Spear notes, swarming is part of the \"disciplined cycle of real-time problem recognition, diagnosis,...and treatment (countermeasures or corrective measures in manufacturing vernacular). It [is] the discipline of the Shewhart cycle—plan, do, check, act—popularized by W. Edwards Deming, but accelerated to warp speed.\"\n",
      "\n",
      "It is only through the swarming of ever smaller problems discovered ever earlier in the life cycle that we can deflect problems before a catastrophe occurs. In other words, when the nuclear reactor melts down, it is already too late to avert worst outcomes.\n",
      "\n",
      "To enable fast feedback in the technology value stream, we must create the equivalent of an Andon cord and the related swarming response. This requires that we also create the culture that makes it safe, and even encouraged, to pull the Andon cord when something goes wrong, whether it is when a production incident occurs or when errors occur earlier in the value stream, such as when someone introduces a change that breaks our continuous build or test processes.\n",
      "\n",
      "When conditions trigger an Andon cord pull, we swarm to solve the problem and prevent the introduction of new work until the issue has been resolved.§ This provides fast feedback for everyone in the value stream (especially the person who caused the system to fail), enables us to quickly isolate and diagnose the problem, and prevents further complicating factors that can obscure cause and effect.\n",
      "\n",
      "Preventing the introduction of new work enables continuous integration and deployment, which is single-piece flow in the technology value stream. All changes that pass our continuous build and integration tests are deployed into production, and any changes that cause any tests to fail trigger our Andon cord and are swarmed until resolved.\n",
      "\n",
      "## KEEP PUSHING QUALITY CLOSER TO THE SOURCE\n",
      "\n",
      "We may inadvertently perpetuate unsafe systems of work due to the way we respond to accidents and incidents. In complex systems, adding more inspection steps and approval processes actually increases the likelihood of future failures. The effectiveness of approval processes decreases as we push decision-making further away from where the work is performed. Doing so not only lowers the quality of decisions but also increases our cycle time, thus decreasing the strength of the feedback between cause and effect, and reducing our ability to learn from successes and failures.¶\n",
      "\n",
      "This can be seen even in smaller and less complex systems. When top-down, bureaucratic command and control systems become ineffective, it is usually because the variance between \"who should do something\" and \"who is actually doing something\" is too large, due to insufficient clarity and timeliness.\n",
      "\n",
      "Examples of ineffective quality controls include:\n",
      "\n",
      "  * Requiring another team to complete tedious, error-prone, and manual tasks that could be easily automated and run as needed by the team who needs the work performed\n",
      "  * Requiring approvals from busy people who are distant from the work, forcing them to make decisions without an adequate knowledge of the work or the potential implications, or to merely rubber stamp their approvals\n",
      "  * Creating large volumes of documentation of questionable detail which become obsolete shortly after they are written\n",
      "  * Pushing large batches of work to teams and special committees for approval and processing and then waiting for responses\n",
      "\n",
      "Instead, we need everyone in our value stream to find and fix problems in their area of control as part of our daily work. By doing this, we push quality and safety responsibilities and decision-making to where the work is performed, instead of relying on approvals from distant executives.\n",
      "\n",
      "We use peer reviews of our proposed changes to gain whatever assurance is needed that our changes will operate as designed. We automate as much of the quality checking typically performed by a QA or Information Security department as possible. Instead of developers needing to request or schedule a test to be run, these tests can be performed on demand, enabling developers to quickly test their own code and even deploy those changes into production themselves.\n",
      "\n",
      "By doing this, we truly make quality everyone's responsibility as opposed to it being the sole responsibility of a separate department. Information security is not just Information Security's job, just as availability isn't merely the job of Operations.\n",
      "\n",
      "Having developers share responsibility for the quality of the systems they build not only improves outcomes but also accelerates learning. This is especially important for developers as they are typically the team that is furthest removed from the customer.  Gary Gruver observes, \"It's impossible for a developer to learn anything when someone yells at them for something they broke six months ago—that's why we need to provide feedback to everyone as quickly as possible, in minutes, not months.\"\n",
      "\n",
      "## ENABLE OPTIMIZING FOR DOWNSTREAM WORK CENTERS\n",
      "\n",
      "In the 1980s, Designing for Manufacturability principles sought to design parts and processes so that finished goods could be created with the lowest cost, highest quality, and fastest flow. Examples include designing parts that are wildly asymmetrical to prevent them from being put on backwards, and designing screw fasteners so that they are impossible to over-tighten.\n",
      "\n",
      "This was a departure from how design was typically done, which focused on the external customers but overlooked internal stakeholders, such as the people performing the manufacturing.\n",
      "\n",
      "Lean defines two types of customers that we must design for: the external customer (who most likely pays for the service we are delivering) and the internal customer (who receives and processes the work immediately after us). According to Lean, our most important customer is our next step downstream. Optimizing our work for them requires that we have empathy for their problems in order to better identify the design problems that prevent fast and smooth flow.\n",
      "\n",
      "In the technology value stream, we optimize for downstream work centers by designing for operations, where operational non-functional requirements (e.g., architecture, performance, stability, testability, configurability, and security) are prioritized as highly as user features.\n",
      "\n",
      "By doing this, we create quality at the source, likely resulting in a set of codified non-functional requirements that we can proactively integrate into every service we build.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Creating fast feedback is critical to achieving quality, reliability, and safety in the technology value stream. We do this by seeing problems as they occur, swarming and solving problems to build new knowledge, pushing quality closer to the source, and continually optimizing for downstream work centers.\n",
      "\n",
      "The specific practices that enable fast flow in the DevOps value stream are presented in Part IV. In the next chapter, we present the Third Way: The Principles of Feedback\n",
      "\n",
      "* * *\n",
      "\n",
      "† Dr. Spear extended his work to explain the long-lasting successes of other organizations, such as the Toyota supplier network, Alcoa, and the US Navy's Nuclear Power Propulsion Program.\n",
      "\n",
      "‡ In some of its plants, Toyota has moved to using an Andon button.\n",
      "\n",
      "§ Astonishingly, when the number of Andon cord pulls drop, plant managers will actually decrease the tolerances to get an increase in the number of Andon cord pulls in order to continue to enable more learnings and improvements and to detect ever-weaker failure signals.\n",
      "\n",
      "¶ In the 1700s, the British government engaged in a spectacular example of top-down, bureaucratic command and control, which proved remarkably ineffective. At the time, Georgia was still a colony, and despite the fact that the British government was three thousand miles away and lacked firsthand knowledge of local land chemistry, rockiness, topography, accessibility to water, and other conditions, it tried to plan Georgia's entire agricultural economy. The results of the attempt were dismal and left Georgia with the lowest levels of prosperity and population in the thirteen colonies.\n",
      "\n",
      "# 4The Third Way:  \n",
      " _The Principles of Continual Learning and Experimentation_\n",
      "\n",
      "While the First Way addresses work flow from left to right and the Second Way addresses the reciprocal fast and constant feedback from right to left, the Third Way focuses on creating a culture of continual learning and experimentation. These are the principles that enable constant creation of individual knowledge, which is then turned into team and organizational knowledge.\n",
      "\n",
      "In manufacturing operations with systemic quality and safety problems, work is typically rigidly defined and enforced. For instance, in the GM Fremont plant described in the previous chapter, workers had little ability to integrate improvements and learnings into their daily work, with suggestions for improvement \"apt to meet a brick wall of indifference.\"\n",
      "\n",
      "In these environments, there is also often a culture of fear and low trust, where workers who make mistakes are punished, and those who make suggestions or point out problems are viewed as whistle-blowers and troublemakers. When this occurs, leadership is actively suppressing, even punishing, learning and improvement, perpetuating quality and safety problems.\n",
      "\n",
      "In contrast, high-performing manufacturing operations require and actively promote learning—instead of work being rigidly defined, the system of work is dynamic, with line workers performing experiments in their daily work to generate new improvements, enabled by rigorous standardization of work procedures and documentation of the results.\n",
      "\n",
      "In the technology value stream, our goal is to create a high-trust culture, reinforcing that we are all lifelong learners who must take risks in our daily work. By applying a scientific approach to both process improvement and product development, we learn from our successes and failures, identifying which ideas don't work and reinforcing those that do. Moreover, any local learnings are rapidly turned into global improvements, so that new techniques and practices can be used by the entire organization.\n",
      "\n",
      "We reserve time for the improvement of daily work and to further accelerate and ensure learning. We consistently introduce stress into our systems to force continual improvement. We even simulate and inject failures in our production services under controlled conditions to increase our resilience.\n",
      "\n",
      "By creating this continual and dynamic system of learning, we enable teams to rapidly and automatically adapt to an ever-changing environment, which ultimately helps us win in the marketplace.\n",
      "\n",
      "## ENABLING ORGANIZATIONAL LEARNING AND A SAFETY CULTURE\n",
      "\n",
      "When we work within a complex system, by definition it is impossible for us to perfectly predict all the outcomes for any action we take. This is what contributes to unexpected, or even catastrophic, outcomes and accidents in our daily work, even when we take precautions and work carefully.\n",
      "\n",
      "When these accidents affect our customers, we seek to understand why it happened. The root cause is often deemed to be human error, and the all too common management response is to \"name, blame, and shame\" the person who caused the problem.† And, either subtly or explicitly, management hints that the person guilty of committing the error will be punished. They then create more processes and approvals to prevent the error from happening again.\n",
      "\n",
      "Dr. Sidney Dekker, who codified some of the key elements of safety culture and coined the term _just culture_ , wrote, \"Responses to incidents and accidents that are seen as unjust can impede safety investigations, promote fear rather than mindfulness in people who do safety-critical work, make organizations more bureaucratic rather than more careful, and cultivate professional secrecy, evasion, and self-protection.\"\n",
      "\n",
      "These issues are especially problematic in the technology value stream—our work is almost always performed within a complex system, and how management chooses to react to failures and accidents leads to a culture of fear, which then makes it unlikely that problems and failure signals are ever reported. The result is that problems remain hidden until a catastrophe occurs.\n",
      "\n",
      "Dr. Ron Westrum was one of the first to observe the importance of organizational culture on safety and performance. He observed that in healthcare organizations, the presence of \"generative\" cultures was one of the top predictors of patient safety. Dr. Westrum defined three types of culture:\n",
      "\n",
      "  * Pathological organizations are characterized by large amounts of fear and threat. People often hoard information, withhold it for political reasons, or distort it to make themselves look better. Failure is often hidden.\n",
      "  * Bureaucratic organizations are characterized by rules and processes, often to help individual departments maintain their \"turf.\" Failure is processed through a system of judgment, resulting in either punishment or justice and mercy.\n",
      "  * Generative organizations are characterized by actively seeking and sharing information to better enable the organization to achieve its mission. Responsibilities are shared throughout the value stream, and failure results in reflection and genuine inquiry **.**\n",
      "\n",
      "  **Figure 8:** The Westrum organizational typology model: how organizations process information (Source: Ron Westrum, \"A typology of organisation culture,\" BMJ Quality & Safety 13, no. 2 (2004), doi:10.1136/qshc.2003.009522.)\n",
      "\n",
      "Just as Dr. Westrum found in healthcare organizations, a high-trust, generative culture also predicted IT and organizational performance in technology value streams.\n",
      "\n",
      "In the technology value stream, we establish the foundations of a generative culture by striving to create a safe system of work. When accidents and failures occur, instead of looking for human error, we look for how we can redesign the system to prevent the accident from happening again.\n",
      "\n",
      "For instance, we may conduct a blameless post-mortem after every incident to gain the best understanding of how the accident occurred and agree upon what the best countermeasures are to improve the system, ideally preventing the problem from occurring again and enabling faster detection and recovery.\n",
      "\n",
      "By doing this, we create organizational learning. As Bethany Macri, an engineer at Etsy who led the creation of the Morgue tool to help with recording of post-mortems, stated, \"By removing blame, you remove fear; by removing fear, you enable honesty; and honesty enables prevention.\"\n",
      "\n",
      "Dr. Spear observes that the result of removing blame and putting organizational learning in its place is that \"organizations become ever more self-diagnosing and self-improving, skilled at detecting problems [and] solving them.\"\n",
      "\n",
      "Many of these attributes were also described by Dr. Senge __as attributes of learning organizations. In _The Fifth Discipline,_ he wrote that these characteristics help customers, ensure quality, create competitive advantage and an energized and committed workforce, and uncover the truth.\n",
      "\n",
      "## INSTITUTIONALIZE THE IMPROVEMENT OF DAILY WORK\n",
      "\n",
      "Teams are often not able or not willing to improve the processes they operate within. The result is not only that they continue to suffer from their current problems, but their suffering also grows worse over time. Mike Rother observed in _Toyota Kata_ that in the absence of improvements, processes don't stay the same—due to chaos and entropy, processes actually degrade over time.\n",
      "\n",
      "In the technology value stream, when we avoid fixing our problems, relying on daily workarounds, our problems and technical debt accumulates until all we are doing is performing workarounds, trying to avoid disaster, with no cycles leftover for doing productive work. This is why Mike Orzen, author of _Lean IT_ , observed, \"Even more important than daily work is the improvement of daily work.\"\n",
      "\n",
      "We improve daily work by explicitly reserving time to pay down technical debt, fix defects, and refactor and improve problematic areas of our code and environments—we do this by reserving cycles in each development interval, or by scheduling _kaizen blitzes_ , which are periods when engineers self-organize into teams to work on fixing any problem they want.\n",
      "\n",
      "The result of these practices is that everyone finds and fixes problems in their area of control, all the time, as part of their daily work. When we finally fix the daily problems that we've worked around for months (or years), we can eradicate from our system the less obvious problems. By detecting and responding to these ever-weaker failure signals, we fix problems when it is not only easier and cheaper but also when the consequences are smaller.\n",
      "\n",
      "Consider the following example that improved workplace safety at Alcoa, an aluminum manufacturer with $7.8 billion in revenue in 1987. Aluminum manufacturing requires extremely high heat, high pressures, and corrosive chemicals. In 1987, Alcoa had a frightening safety record, with 2% of the ninety thousand employee workforce being injured each year—that's seven injuries per day. When Paul O'Neill started as CEO, his first goal was to have zero injuries to employees, contractors, and visitors.\n",
      "\n",
      "O'Neill wanted to be notified within twenty-four hours of anyone being injured on the job—not to punish, but to ensure and promote that learnings were being generated and incorporated to create a safer workplace. Over the course of ten years, Alcoa reduced their injury rate by 95%.\n",
      "\n",
      "The reduction in injury rates allowed Alcoa to focus on smaller problems and weaker failure signals—instead of notifying O'Neill only when injuries occurred, they started reporting any close calls as well.‡ By doing this, they improved workplace safety over the subsequent twenty years and have one of the most enviable safety records in the industry.\n",
      "\n",
      "As Dr. Spear writes, \"Alcoans gradually stopped working around the difficulties, inconveniences, and impediments they experienced. Coping, fire fighting, and making do were gradually replaced throughout the organization by a dynamic of identifying opportunities for process and product improvement. As those opportunities were identified and the problems were investigated, the pockets of ignorance that they reflected were converted into nuggets of knowledge.\" This helped give the company a greater competitive advantage in the market.\n",
      "\n",
      "Similarly, in the technology value stream, as we make our system of work safer, we find and fix problems from ever weaker failure signals. For example, we may initially perform blameless post-mortems only for customer-impacting incidents. Over time, we may perform them for lesser team-impacting incidents and near misses as well.\n",
      "\n",
      "## TRANSFORM LOCAL DISCOVERIES INTO GLOBAL IMPROVEMENTS\n",
      "\n",
      "When new learnings are discovered locally, there must also be some mechanism to enable the rest of the organization to use and benefit from that knowledge. In other words, when teams or individuals have experiences that create expertise, our goal is to convert that tacit knowledge (i.e., knowledge that is difficult to transfer to another person by means of writing it down or verbalizing) into explicit, codified knowledge, which becomes someone else's expertise through practice.\n",
      "\n",
      "This ensures that when anyone else does similar work, they do so with the cumulative and collective experience of everyone in the organization who has ever done the same work. A remarkable example of turning local knowledge into global knowledge is the US Navy's Nuclear Power Propulsion Program (also known as \"NR\" for \"Naval Reactors\"), which has over 5,700 reactor-years of operation without a single reactor-related casualty or escape of radiation.\n",
      "\n",
      "The NR is known for their intense commitment to scripted procedures and standardized work, and the need for incident reports for any departure from procedure or normal operations to accumulate learnings, no matter how minor the failure signal—they constantly update procedures and system designs based on these learnings.\n",
      "\n",
      "The result is that when a new crew sets out to sea on their first deployment, they and their officers benefit from the collective knowledge of 5,700 accident-free reactor-years. Equally impressive is that their own experiences at sea will be added to this collective knowledge, helping future crews safely achieve their own missions.\n",
      "\n",
      "In the technology value stream, we must create similar mechanisms to create global knowledge, such as making all our blameless post-mortem reports searchable by teams trying to solve similar problems, and by creating shared source code repositories that span the entire organization, where shared code, libraries, and configurations that embody the best collective knowledge of the entire organization can be easily utilized. All these mechanisms help convert individual expertise into artifacts that the rest of the organization can use.\n",
      "\n",
      "## INJECT RESILIENCE PATTERNS INTO OUR DAILY WORK\n",
      "\n",
      "Lower-performing manufacturing organizations buffer themselves from disruptions in many ways—in other words, they bulk up or add flab. For instance, to reduce the risk of a work center being idle (due to inventory arriving late, inventory that had to be scrapped, etc.), managers may choose to stockpile more inventory at each work center. However, that inventory buffer also increases WIP, which has all sorts of undesired outcomes, as previously discussed.\n",
      "\n",
      "Similarly, to reduce the risk of a work center going down due to machinery failure, managers may increase capacity by buying more capital equipment, hiring more people, or even increasing floor space. All these options increase costs.\n",
      "\n",
      "In contrast, high performers achieve the same results (or better) by improving daily operations, continually introducing tension to elevate performance, as well as engineering more resilience into their system.\n",
      "\n",
      "Consider a typical experiment at one of Aisin Seiki Global's mattress factories, one of Toyota's top suppliers. Suppose they had two production lines, each capable of producing one hundred units per day. On slow days, they would send all production onto one line, experimenting with ways to increase capacity and identify vulnerabilities in their process, knowing that if overloading the line caused it to fail, they could send all production to the second line.\n",
      "\n",
      "By relentless and constant experimentation in their daily work, they were able to continually increase capacity, often without adding any new equipment or hiring more people. The emergent pattern that results from these types of improvement rituals not only improves performance but also improves resilience, because the organization is always in a state of tension and change. This process of applying stress to increase resilience was named _antifragility_ by author and risk analyst Nassim Nicholas Taleb.\n",
      "\n",
      "In the technology value stream, we can introduce the same type of tension into our systems by seeking to always reduce deployment lead times, increase test coverage, decrease test execution times, and even by re-architecting if necessary to increase developer productivity or increase reliability.\n",
      "\n",
      "We may also perform _Game Day_ exercises, where we rehearse large scale failures, such as turning off entire data centers. Or we may inject ever-larger scale faults into the production environment (such as the famous Netflix \"Chaos Monkey,\" which randomly kills processes and compute servers in production) to ensure that we're as resilient as we want to be.\n",
      "\n",
      "## LEADERS REINFORCE A LEARNING CULTURE\n",
      "\n",
      "Traditionally, leaders were expected to be responsible for setting objectives, allocating resources for achieving those objectives, and establishing the right combination of incentives. Leaders also establish the emotional tone for the organizations they lead. In other words, leaders lead by \"making all the right decisions.\"\n",
      "\n",
      "However, there is significant evidence that shows greatness is not achieved by leaders making all the right decisions—instead, the leader's role is to create the conditions so their team can discover greatness in their daily work. In other words, creating greatness requires both leaders and workers, each of whom are mutually dependent upon each other.\n",
      "\n",
      "Jim Womack, author of _Gemba Walks,_ described the complementary working relationship and mutual respect that must occur between leaders and frontline workers. According to Womack, this relationship is necessary because neither can solve problems alone—leaders are not close enough to the work, which is required to solve any problem, and frontline workers do not have the broader organizational context or the authority to make changes outside of their area of work.§\n",
      "\n",
      "Leaders must elevate the value of learning and disciplined problem solving. Mike Rother formalized these methods in what he calls the _coaching kata._ The result is one that mirrors the scientific method, where we explicitly state our True North goals, such as \"sustain zero accidents\" in the case of Alcoa, or \"double throughput within a year\" in the case of Aisin.\n",
      "\n",
      "These strategic goals then inform the creation of iterative, shorter term goals, which are cascaded and then executed by establishing target conditions at the value stream or work center level (e.g., \"reduce lead time by 10% within the next two weeks\").\n",
      "\n",
      "These target conditions frame the scientific experiment: we explicitly state the problem we are seeking to solve, our hypothesis of how our proposed countermeasure will solve it, our methods for testing that hypothesis, our interpretation of the results, and our use of learnings to inform the next iteration.\n",
      "\n",
      "The leader helps coach the person conducting the experiment with questions that may include:\n",
      "\n",
      "  * What was your last step and what happened?\n",
      "  * What did you learn?\n",
      "  * What is your condition now?\n",
      "  * What is your next target condition?\n",
      "  * What obstacle are you working on now?\n",
      "  * What is your next step?\n",
      "  * What is your expected outcome?\n",
      "  * When can we check?\n",
      "\n",
      "This problem-solving approach in which leaders help workers see and solve problems in their daily work is at the core of the Toyota Production System, of learning organizations, the Improvement Kata, and high-reliability organizations. Mike Rother observes that he sees Toyota \"as an organization defined primarily by the unique behavior routines it continually teaches to all its members.\"\n",
      "\n",
      "In the technology value stream, this scientific approach and iterative method guides all of our internal improvement processes, but also how we perform experiments to ensure that the products we build actually help our internal and external customers achieve their goals.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "The principles of the Third Way address the need for valuing organizational learning, enabling high trust and boundary-spanning between functions, accepting that failures will always occur in complex systems, and making it acceptable to talk about problems so we can create a safe system of work. It also requires institutionalizing the improvement of daily work, converting local learnings into global learnings that can be used by the entire organization, as well as continually injecting tension into our daily work.\n",
      "\n",
      "Although fostering a culture of continual learning and experimentation is the principle of the Third Way, it is also interwoven into the First and Second Ways. In other words, improving flow and feedback requires an iterative and scientific approach that includes framing of a target condition, stating a hypothesis of what will help us get there, designing and conducting experiments, and evaluating the results.\n",
      "\n",
      "The results are not only better performance but also increased resilience, higher job satisfaction, and improved organization adaptability.\n",
      "\n",
      "## PART I CONCLUSION\n",
      "\n",
      "In Part I of _The DevOps Handbook_ we looked back at several movements in history that helped lead to the development of DevOps. We also looked at the three main principles that form the foundation for successful DevOps organizations: the principles of Flow, Feedback, and Continual Learning and Experimentation. In Part II, we will begin to look at how to start a DevOps movement in your organization.\n",
      "\n",
      "* * *\n",
      "\n",
      "† The \"name, blame, shame\" pattern is part of the Bad Apple Theory criticized by Dr. Sydney Dekker and extensively discussed in his book _The Field Guide to Understanding Human Error._\n",
      "\n",
      "‡ It is astonishing, instructional, and truly moving to see the level of conviction and passion that Paul O'Neill has about the moral responsibility leaders have to create workplace safety.\n",
      "\n",
      "§ Leaders are responsible for the design and operation of processes at a higher level of aggregation where others have less perspective and authority.\n",
      "\n",
      "# Part II\n",
      "\n",
      "## Introduction\n",
      "\n",
      "How do we decide where to start a DevOps transformation in our organization? Who needs to be involved? How should we organize our teams, protect their work capacity, and maximize their chances of succeess? These are the questions we aim to answer in Part II of _The DevOps Handbook_.\n",
      "\n",
      "In the following chapters we will walk through the process of initiating a DevOps transformation. We begin by evaluating the value streams in our organization, locating a good place to start, and forming a strategy to create a dedicated transformation team with specific improvement goals and eventual expansion. For each value stream being transformed, we identify the work being performed and then look at organizational design strategies and organizational archetypes that best support the transformation goals.\n",
      "\n",
      "Primary focuses in these chapters include:\n",
      "\n",
      "  * Selecting which value streams to start with\n",
      "  * Understanding the work being done in our candidate value streams\n",
      "  * Designing our organization and architecture with Conway's Law in mind\n",
      "  * Enabling market-oriented outcomes through more effective collaboration between functions throughout the value stream\n",
      "  * Protecting and enabling our teams\n",
      "\n",
      "Beginning any transformation is full of uncertainty—we are charting a journey to an ideal end state, but where virtually all the intermediate steps are unknown. These next chapters are intended to provide a thought process to guide our decisions, provide actionable steps we can take, and illustrate case studies as examples.\n",
      "'\n",
      "\n",
      "# 5Selecting Which Value Stream to Start With\n",
      "\n",
      "Choosing a value stream for DevOps transformation deserves careful consideration. Not only does the value stream we choose dictate the difficulty of our transformation, but it also dictates who will be involved in the transformation. It will affect how we need to organize into teams and how we can best enable the teams and individuals in them.\n",
      "\n",
      "Another challenge was noted by Michael Rembetsy, who helped lead the DevOps transformation as the Director of Operations at Etsy in 2009. He observed, \"We must pick our transformation projects carefully—when we're in trouble, we don't get very many shots. Therefore, we must carefully pick and then protect those improvement projects that will most improve the state of our organization.\"\n",
      "\n",
      "Let us examine how the Nordstrom team started their DevOps transformation initiative in 2013, which Courtney Kissler, their VP of E-Commerce and Store Technologies, described at the DevOps Enterprise Summit in 2014 and 2015.\n",
      "\n",
      "Founded in 1901, Nordstrom is a leading fashion retailer that is focused on delivering the best possible shopping experience to their customers. In 2015, Nordstrom had annual revenue of $13.5 billion.\n",
      "\n",
      "The stage for Nordstrom's DevOps journey was likely set in 2011 during one of their annual board of directors meetings. That year, one of the strategic topics discussed was the need for online revenue growth. They studied the plight of Blockbusters, Borders, and Barnes & Nobles, which demonstrated the dire consequences when traditional retailers were late creating competitive e-commerce capabilities—these organizations were clearly at risk of losing their position in the marketplace or even going out of business entirely.†\n",
      "\n",
      "At that time, Courtney Kissler was the senior director of Systems Delivery and Selling Technology, responsible for a significant portion of the technology organization, including their in-store systems and online e-commerce site. As Kissler described, \"In 2011, the Nordstrom technology organization was very much optimized for cost—we had outsourced many of our technology functions, we had an annual planning cycle with large batch, 'waterfall' software releases. Even though we had a 97% success rate of hitting our schedule, budget, and scope goals, we were ill-equipped to achieve what the five-year business strategy required from us, as Nordstrom started optimizing for speed instead of merely optimizing for cost.\"\n",
      "\n",
      "Kissler and the Nordstrom technology management team had to decide where to start their initial transformation efforts. They didn't want to cause upheaval in the whole system. Instead, they wanted to focus on very specific areas of the business so that they could experiment and learn. Their goal was to demonstrate early wins, which would give everyone confidence that these improvements could be replicated in other areas of the organization. How exactly that would be achieved was still unknown.\n",
      "\n",
      "They focused on three areas: the customer mobile application, their in-store restaurant systems, and their digital properties. Each of these areas had business goals that weren't being met; thus, they were more receptive to considering a different way of working. The stories of the first two are described below.\n",
      "\n",
      "The Nordstrom mobile application had experienced an inauspicious start. As Kissler said, \"Our customers were extremely frustrated with the product, and we had uniformly negative reviews when we launched it in the App Store. Worse, the existing structure and processes (aka \"the system\") had designed their processes so that they could only release updates twice per year.\" In other words, any fixes to the application would have to wait months to reach the customer.\n",
      "\n",
      "Their first goal was to enable faster or on-demand releases, providing faster iteration and the ability to respond to customer feedback. They created a dedicated product team that was solely dedicated to supporting the mobile application, with the goal of enabling that team to be able to independently implement, test, and deliver value to the customer. By doing this, they would no longer have to depend on and coordinate with scores of other teams inside Nordstrom. Furthermore, they moved from planning once per year to a continuous planning process. The result was a single prioritized backlog of work for the mobile app based on customer need—gone were all the conflicting priorities when the team had to support multiple products.\n",
      "\n",
      "Over the following year, they eliminated testing as a separate phase of work, instead integrating it into everyone's daily work.‡ They doubled the features being delivered per month and halved the number of defects—creating a successful outcome.\n",
      "\n",
      "Their second area of focus was the systems supporting their in-store _Café Bistro_ restaurants. Unlike the mobile app value stream where the business need was to reduce time to market and increase feature throughput, the business need here was to decrease cost and increase quality. In 2013, Nordstrom had completed eleven \"restaurant re-concepts\" which required changes to the in-store applications, causing a number of customer-impacting incidents. Disturbingly, they had planned forty-four more of these re-concepts for 2014—four times as many as in the previous year.\n",
      "\n",
      "As Kissler stated, \"One of our business leaders suggested that we triple our team size to handle these new demands, but I proposed that we had to stop throwing more bodies at the problem and instead improve the way we worked.\"\n",
      "\n",
      "They were able to identify problematic areas, such as in their work intake and deployment processes, which is where they focused their improvement efforts. They were able to reduce code deployment lead times by 60% and reduce the number of production incidents 60% to 90%.\n",
      "\n",
      "These successes gave the teams confidence that DevOps principles and practices were applicable to a wide variety of value streams. Kissler was promoted to VP of E-Commerce and Store Technologies in 2014.\n",
      "\n",
      "In 2015, Kissler said that in order for the selling or customer-facing technology organization to enable the business to meet their goals, \"...we needed to increase productivity in all our technology value streams, not just in a few. At the management level, we created an across-the-board mandate to reduce cycle times by 20% for all customer-facing services.\"\n",
      "\n",
      "She continued, \"This is an audacious challenge. We have many problems in our current state—process and cycle times are not consistently measured across teams, nor are they visible. Our first target condition requires us to help all our teams measure, make it visible, and perform experiments to start reducing their process times, iteration by iteration.\"\n",
      "\n",
      "Kissler concluded, \"From a high level perspective, we believe that techniques such as value stream mapping, reducing our batch sizes toward single-piece flow, as well as using continuous delivery and microservices will get us to our desired state. However, while we are still learning, we are confident that we are heading in the right direction, and everyone knows that this effort has support from the highest levels of management.\"\n",
      "\n",
      "In this chapter, various models are presented that will enable us to replicate the thought processes that the Nordstrom team used to decide which value streams to start with. We will evaluate our candidate value streams in many ways, including whether they are a _greenfield_ or _brownfield_ service, a _system of engagement_ or a _system of record_. We will also estimate the risk/reward balance of transforming and assess the likely level of resistance we may get from the teams we would work with.\n",
      "\n",
      "## GREENFIELD VS. BROWNFIELD SERVICES\n",
      "\n",
      "We often categorize our software services or products as either greenfield or brownfield. These terms were originally used for urban planning and building projects. Greenfield development is when we build on undeveloped land. Brownfield development is when we build on land that was previously used for industrial purposes, potentially contaminated with hazardous waste or pollution. In urban development, many factors can make greenfield projects simpler than brownfield projects—there are no existing structures that need to be demolished nor are there toxic materials that need to be removed.\n",
      "\n",
      "In technology, a greenfield project is a new software project or initiative, likely in the early stages of planning or implementation, where we build our applications and infrastructure anew, with few constraints. Starting with a greenfield software project can be easier, especially if the project is already funded and a team is either being created or is already in place. Furthermore, because we are starting from scratch, we can worry less about existing code bases, processes, and teams.\n",
      "\n",
      "Greenfield DevOps projects are often pilots to demonstrate feasibility of public or private clouds, piloting deployment automation, and similar tools. An example of a greenfield DevOps project is the Hosted LabVIEW product in 2009 at National Instruments, a thirty-year-old organization with five thousand employees and $1 billion in annual revenue. To bring this product to market quickly, a new team was created and allowed to operate outside of the existing IT processes and explore the use of public clouds. The initial team included an applications architect, a systems architect, two developers, a system automation developer, an operations lead, and two offshore operations staff. By using DevOps practices, they were able to deliver Hosted LabVIEW to market in half the time of their normal product introductions.\n",
      "\n",
      "On the other end of the spectrum are brownfield DevOps projects, these are existing products or services that are already serving customers and have potentially been in operation for years or even decades. Brownfield projects often come with significant amounts of technical debt, such as having no test automation or running on unsupported platforms. In the Nordstrom example presented earlier in this chapter, both the in-store restaurant systems and e-commerce systems were brownfield projects.\n",
      "\n",
      "Although many believe that DevOps is primarily for greenfield projects, DevOps has been used to successfully transform brownfield projects of all sorts. In fact, over 60% of the transformation stories shared at the DevOps Enterprise Summit in 2014 were for brownfield projects. In these cases, there was a large performance gap between what the customer needed and what the organization was currently delivering, and the DevOps transformations created tremendous business benefit.\n",
      "\n",
      "Indeed, one of the findings in the _2015 State of DevOps Report_ validated that the age of the application was not a significant predictor of performance; instead, what predicted performance was whether the application was architected (or could be re-architected) for testability and deployability.\n",
      "\n",
      "Teams supporting brownfield projects may be very receptive to experimenting with DevOps, particularly when there is a widespread belief that traditional methods are insufficient to achieve their goals—and especially if there is a strong sense of urgency around the need for improvement.§\n",
      "\n",
      "When transforming brownfield projects, we may face significant impediments and problems, especially when no automated testing exists or when there is a tightly-coupled architecture that prevents small teams from developing, testing, and deploying code independently. How we overcome these issues are discussed throughout this book.\n",
      "\n",
      "Examples of successful brownfield transformations include:\n",
      "\n",
      "  * **CSG (2013):** In 2013, CSG International had $747 million in revenue and over 3,500 employees, enabling over ninety thousand customer service agents to provide billing operations and customer care to over fifty million video, voice, and data customers, executing over six billion transactions, and printing and mailing over seventy million paper bill statements every month. Their initial scope of improvement was bill printing, one of their primary businesses, and involved a COBOL mainframe application and the twenty surrounding technology platforms. As part of their transformation, they started performing daily deployments into a production-like environment, and doubled the frequency of customer releases from twice annually to four times annually. As a result, they significantly increased the reliability of the application and reduced code deployment lead times from two weeks to less than one day.\n",
      "  * **Etsy (2009):** In 2009, Etsy had thirty-five employees and was generating $87 million in revenue, but after they \"barely survived the holiday retail season,\" they started transforming virtually every aspect of how the organization worked, eventually turning the company into one of the most admired DevOps organizations and set the stage for a successful 2015 IPO.\n",
      "\n",
      "## CONSIDER BOTH SYSTEMS OF RECORD AND SYSTEMS OF ENGAGEMENT\n",
      "\n",
      "The Gartner research firm has recently popularized the notion of _bimodal IT_ , referring to the wide spectrum of services that typical enterprises support. Within bimodal IT there are _systems of record_ , the ERP-like systems that run our business (e.g., MRP, HR, financial reporting systems), where the correctness of the transactions and data are paramount; and _systems of engagement_ , which are customer-facing or employee-facing systems, such as e-commerce systems and productivity applications.\n",
      "\n",
      "Systems of record typically have a slower pace of change and often have regulatory and compliance requirements (e.g., SOX). Gartner calls these types of systems \"Type 1,\" where the organization focuses on \"doing it right.\"\n",
      "\n",
      "Systems of engagement typically have a much higher pace of change to support rapid feedback loops that enable them to conduct experimentation to discover how to best meet customer needs. Gartner calls these types of systems \"Type 2,\" where the organization focuses on \"doing it fast.\"\n",
      "\n",
      "It may be convenient to divide up our systems into these categories; however, we know that the core, chronic conflict between \"doing it right\" and \"doing it fast\" can be broken with DevOps. The data from Puppet Labs' State of DevOps Reports—following the lessons of Lean manufacturing—shows that high-performing organizations are able to simultaneously deliver higher levels of throughput and reliability.\n",
      "\n",
      "Furthermore, because of how interdependent our systems are, our ability to make changes to any of these systems is limited by the system that is most difficult to safely change, which is almost always a system of record.\n",
      "\n",
      "Scott Prugh, VP of Product Development at CSG, observed, \"We've adopted a philosophy that rejects bi-modal IT, because every one of our customers deserve speed and quality. This means that we need technical excellence, whether the team is supporting a 30 year old mainframe application, a Java application, or a mobile application.\"\n",
      "\n",
      "Consequently, when we improve brownfield systems, we should not only strive to reduce their complexity and improve their reliability and stability, we should also make them faster, safer, and easier to change. Even when new functionality is added just to greenfield systems of engagement, they often cause reliability problems in the brownfield systems of record they rely on. By making these downstream systems safer to change, we help the entire organization more quickly and safely achieve its goals.\n",
      "\n",
      "## START WITH THE MOST SYMPATHETIC AND INNOVATIVE GROUPS\n",
      "\n",
      "Within every organization, there will be teams and individuals with a wide range of attitudes toward the adoption of new ideas. Geoffrey A. Moore first depicted this spectrum in the form of the technology adoption life cycle in _Crossing The Chasm_ , where the chasm represents the classic difficulty of reaching groups beyond the _innovators_ and _early adopters_ (see figure 9).\n",
      "\n",
      "In other words, new ideas are often quickly embraced by innovators and early adopters, while others with more conservative attitudes resist them (the _early majority_, _late majority_ , and _laggards_ ). Our goal is to find those teams that already believe in the need for DevOps principles and practices, and who possess a desire and demonstrated ability to innovate and improve their own processes. Ideally, these groups will be enthusiastic supporters of the DevOps journey.\n",
      "\n",
      "  **Figure 9:** The Technology Adoption Curve (Source: Moore and McKenna, _Crossing The Chasm_ , 15.)\n",
      "\n",
      "Especially in the early stages, we will not spend much time trying to convert the more conservative groups. Instead, we will focus our energy on creating successes with less risk-averse groups and build out our base from there (a process that is discussed further in the next section). Even if we have the highest levels of executive sponsorship, we will avoid the _big bang approach_ (i.e., starting everywhere all at once), choosing instead to focus our efforts in a few areas of the organization, ensuring that those initiatives are successful, and expanding from there.¶\n",
      "\n",
      "## EXPANDING DEVOPS ACROSS OUR ORGANIZATION\n",
      "\n",
      "Regardless of how we scope our initial effort, we must demonstrate early wins and broadcast our successes. We do this by breaking up our larger improvement goals into small, incremental steps. This not only creates our improvements faster, it also enables us to discover when we have made the wrong choice of value stream—by detecting our errors early, we can quickly back up and try again, making different decisions armed with our new learnings.\n",
      "\n",
      "As we generate successes, we earn the right to expand the scope of our DevOps initiative. We want to follow a safe sequence that methodically grows our levels of credibility, influence, and support. The following list, adapted from a course taught by Dr. Roberto Fernandez, a William F. Pounds Professor in Management at MIT, describes the ideal phases used by change agents to build and expand their coalition and base of support:\n",
      "\n",
      "  1. **Find Innovators and Early Adopters:** In the beginning, we focus our efforts on teams who actually want to help—these are our kindred spirits and fellow travelers who are the first to volunteer to start the DevOps journey. In the ideal, these are also people who are respected and have a high degree of influence over the rest of the organization, giving our initiative more credibility.\n",
      "  2. **Build Critical Mass and Silent Majority:** In the next phase, we seek to expand DevOps practices to more teams and value streams with the goal of creating a stable base of support. By working with teams who are receptive to our ideas, even if they are not the most visible or influential groups, we expand our coalition who are generating more successes, creating a \"bandwagon effect\" that further increases our influence. We specifically bypass dangerous political battles that could jeopardize our initiative. \n",
      "  3. **Identify the Holdouts:** The \"holdouts\" are the high profile, influential detractors who are most likely to resist (and maybe even sabotage) our efforts. In general, we tackle this group only after we have achieved a silent majority, when we have established enough successes to successfully protect our initiative.\n",
      "\n",
      "Expanding DevOps across an organization is no small task. It can create risk to individuals, departments, and the organization as a whole. But as Ron van Kemenade, CIO of ING, who helped transform the organization into one of the most admired technology organizations, said, \"Leading change requires courage, especially in corporate environments where people are scared and fight you. But if you start small, you really have nothing to fear. Any leader needs to be brave enough to allocate teams to do some calculated risk-taking.\"\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Peter Drucker, a leader in the development of management education, observed that \"little fish learn to be big fish in little ponds.\" By choosing carefully where and how to start, we are able to experiment and learn in areas of our organization that create value without jeopardizing the rest of the organization. By doing this, we build our base of support, earn the right to expand the use of DevOps in our organization, and gain the recognition and gratitude of an ever-larger constituency.\n",
      "\n",
      "* * *\n",
      "\n",
      "† These organizations were sometimes known as the \"Killer B's that are Dying.\"\n",
      "\n",
      "‡ The practice of relying on a stabilization phase or hardening phase at the end of a project often has very poor outcomes, because it means problems are not being found and fixed as part of daily work and are left unaddressed, potentially snowballing into larger issues.\n",
      "\n",
      "§ That the services that have the largest potential business benefit are brownfield systems shouldn't be surprising. After all, these are the systems that are most relied upon and have the largest number of existing customers or highest amount of revenue depending upon them.\n",
      "\n",
      "¶ Big bang, top-down transformations are possible, such as the Agile transformation at PayPal in 2012 that was led by their vice president of technology, Kirsten Wolberg. However, as with any sustainable and successful transformation, this required the highest level of management support and a relentless, sustained focus on driving the necessary outcomes.\n",
      "\n",
      "# 6Understanding the Work in Our Value Stream, Making it Visible, and Expanding it Across the Organization\n",
      "\n",
      "Once we have identified a value stream to which we want to apply DevOps principles and patterns, our next step is to gain a sufficient understanding of how value is delivered to the customer: what work is performed and by whom, and what steps can we take to improve flow.\n",
      "\n",
      "In the previous chapter, we learned about the DevOps transformation led by Courtney Kissler and the team at Nordstrom. Over the years, they have learned that one of the most efficient ways to start improving any value stream is to conduct a workshop with all the major stakeholders and perform a value stream mapping exercise—a process (described later in this chapter) designed to help capture all the steps required to create value.\n",
      "\n",
      "Kissler's favorite example of the valuable and unexpected insights that can come from value stream mapping is when they tried to improve the long lead times associated with requests going through the Cosmetics Business Office application, a COBOL mainframe application that supported all the floor and department managers of their in-store beauty and cosmetic departments.\n",
      "\n",
      "This application allowed department managers to register new salespeople for various product lines carried in their stores, so that they could track sales commissions, enable vendor rebates, and so forth.\n",
      "\n",
      "Kissler explained:\n",
      "\n",
      "I knew this particular mainframe application well—earlier in my career, I supported this technology team, so I know firsthand that for nearly a decade, during each annual planning cycle, we would debate about how we needed to get this application off the mainframe. Of course, like in most organizations, even when there was full management support, we never seemed to get around to migrating it.\n",
      "\n",
      "My team wanted to conduct a value stream mapping exercise to determine whether the COBOL application really was the problem, or maybe there was a larger problem that we needed to address. They conducted a workshop that assembled everyone with any accountability for delivering value to our internal customers, including our business partners, the mainframe team, the shared service teams, and so forth.\n",
      "\n",
      "What they discovered was that when department managers were submitting the 'product line assignment' request form, we were asking them for an employee number, which they didn't have—so they would either leave it blank or put in something like 'I don't know.' Worse, in order to fill out the form, department managers would have to inconveniently leave the store floor in order to use a PC in the back office. The end result was all this wasted time, with work bouncing back and forth in the process.\n",
      "\n",
      "During the workshop, the participants conducted several experiments, including deleting the employee number field in the form and letting another department get that information in a downstream step. These experiments, conducted with the help of department managers, showed a four-day reduction in processing time. The team later replaced the PC application with an iPad application, which allowed managers to submit the necessary information without leaving the store floor, and the processing time was further reduced to seconds.\n",
      "\n",
      "She said proudly, \"With those amazing improvements, all the demands to get this application off the mainframe disappeared. Furthermore, other business leaders took notice and started coming to us with a whole list of further experiments they wanted to conduct with us in their own organizations. Everyone in the business and technology teams were excited by the outcome because they solved a real business problem, and, most importantly, they learned something in the process.\"\n",
      "\n",
      "In the remainder of this chapter, we will go through the following steps: identifying all the teams required to create customer value, creating a value stream map to make visible all the required work, and using it to guide the teams in how to better and more quickly create value. By doing this, we can replicate the amazing outcomes described in this Nordstrom example.\n",
      "\n",
      "## IDENTIFYING THE TEAMS SUPPORTING OUR VALUE STREAM\n",
      "\n",
      "As this Nordstrom example demonstrates, in value streams of any complexity, no one person knows all the work that must be performed in order to create value for the customer—especially since the required work must be performed by many different teams, often far removed from each other on the organization charts, geographically, or by incentives.\n",
      "\n",
      "As a result, after we select a candidate application or service for our DevOps initiative, we must identify all the members of the value stream who are responsible for working together to create value for the customers being served. In general, this includes:\n",
      "\n",
      "  * **Product owner:** the internal voice of the business that defines the next set of functionality in the service\n",
      "  * **Development:** the team responsible for developing application functionality in the service\n",
      "  * **QA:** the team responsible for ensuring that feedback loops exist to ensure the service functions as desired\n",
      "  * **Operations:** the team often responsible for maintaining the production environment and helping ensure that required service levels are met\n",
      "  * **Infosec:** the team responsible for securing systems and data\n",
      "  * **Release managers:** the people responsible for managing and coordinating the production deployment and release processes\n",
      "  * **Technology executives or value stream manager:** in Lean literature, someone who is responsible for \"ensuring that the value stream meets or exceeds the customer [and organizational] requirements for the overall value stream, from start to finish\"\n",
      "\n",
      "## CREATE A VALUE STREAM MAP TO SEE THE WORK\n",
      "\n",
      "After we identify our value stream members, our next step is to gain a concrete understanding of how work is performed, documented in the form of a value stream map. In our value stream, work likely begins with the product owner, in the form of a customer request or the formulation of a business hypothesis. Some time later, this work is accepted by Development, where features are implemented in code and checked in to our version control repository. Builds are then integrated, tested in a production-like environment, and finally deployed into production, where they (ideally) create value for our customer.\n",
      "\n",
      "In many traditional organizations, this value stream will consist of hundreds, if not thousands, of steps, requiring work from hundreds of people. Because documenting any value stream map this complex likely requires multiple days, we may conduct a multi-day workshop, where we assemble all the key constituents and remove them from the distractions of their daily work.\n",
      "\n",
      "Our goal is not to document every step and associated minutiae, but to sufficiently understand the areas in our value stream that are jeopardizing our goals of fast flow, short lead times, and reliable customer outcomes. Ideally, we have assembled those people with the authority to change their portion of the value stream.†\n",
      "\n",
      "Damon Edwards, co-host of _DevOps Café_ podcast, observed, \"In my experience, these types of value stream mapping exercises are always an eye-opener. Often, it is the first time when people see how much work and heroics are required to deliver value to the customer. For Operations, it may be the first time that they see the consequences that result when developers don't have access to correctly configured environments, which contributes to even more crazy work during code deployments. For Development, it may be the first time they see all the heroics that are required by Test and Operations in order to deploy their code into production, long after they flag a feature as 'completed.'\"\n",
      "\n",
      "Using the full breadth of knowledge brought by the teams engaged in the value stream, we should focus our investigation and scrutiny on the following areas:\n",
      "\n",
      "  * Places where work must wait weeks or even months, such as getting production-like environments, change approval processes, or security review processes\n",
      "  * Places where significant rework is generated or received\n",
      "\n",
      "Our first pass of documenting our value stream should only consist of high-level process blocks. Typically, even for complex value streams, groups can create a diagram with five to fifteen process blocks within a few hours. Each process block should include the lead time and process time for a work item to be processed, as well as the %C/A as measured by the downstream consumers of the output.‡\n",
      "\n",
      "  **Figure 10:** An example of a value stream map   \n",
      "(Source: Humble, Molesky, and O'Reilly, _Lean Enterprise_ , 139.)\n",
      "\n",
      "We use the metrics from our value stream map to guide our improvement efforts. In the Nordstrom example, they focused on the low %C/A rates on the request form submitted by department managers due to the absence of employee numbers. In other cases, it may be long lead times or low %C/A rates when delivering correctly configured test environments to Development teams, or it might be the long lead times required to execute and pass regression testing before each software release.\n",
      "\n",
      "Once we identify the metric we want to improve, we should perform the next level of observations and measurements to better understand the problem and then construct an idealized, future value stream map, which serves as a target condition to achieve by some date (e.g., usually three to twelve months).\n",
      "\n",
      "Leadership helps define this future state and then guides and enables the team to brainstorm hypotheses and countermeasures to achieve the desired improvement to that state, perform experiments to test those hypotheses, and interpret the results to determine whether the hypotheses were correct. The teams keep repeating and iterating, using any new learnings to inform the next experiments.\n",
      "\n",
      "## CREATING A DEDICATED TRANSFORMATION TEAM\n",
      "\n",
      "One of the inherent challenges with initiatives such as DevOps transformations is that they are inevitably in conflict with ongoing business operations. Part of this is a natural outcome of how successful businesses evolve. An organization that has been successful for any extended period of time (years, decades, or even centuries) has created mechanisms to perpetuate the practices that made them successful, such as product development, order administration, and supply chain operations.\n",
      "\n",
      "Many techniques are used to perpetuate and protect how current processes operate, such as specialization, focus on efficiency and repeatability, bureaucracies that enforce approval processes, and controls to protect against variance. In particular, bureaucracies are incredibly resilient and are designed to survive adverse conditions—one can remove half the bureaucrats, and the process will still survive.\n",
      "\n",
      "While this is good for preserving status quo, we often need to change how we work to adapt to changing conditions in the marketplace. Doing this requires disruption and innovation, which puts us at odds with groups who are currently responsible for daily operations and the internal bureaucracies, and who will almost always win.\n",
      "\n",
      "In their book _The Other Side of Innovation: Solving the Execution Challenge,_ Dr. Vijay Govindarajan and Dr. Chris Trimble, both faculty members of Dartmouth College's Tuck School of Business, described their studies of how disruptive innovation is achieved despite these powerful forces of daily operations. They documented how customer-driven auto insurance products were successfully developed and marketed at Allstate, how the profitable digital publishing business was created at the _Wall Street Journal_ , the development of the breakthrough trail-running shoe at Timberland, and the development of the first electric car at BMW.\n",
      "\n",
      "Based on their research, Dr. Govindarajan and Dr. Trimble assert that organizations need to create a dedicated transformation team that is able to operate outside of the rest of the organization that is responsible for daily operations (which they call the \"dedicated team\" and \"performance engine\" respectively).\n",
      "\n",
      "First and foremost, we will hold this dedicated team accountable for achieving a clearly defined, measurable, system-level result (e.g., reduce the deployment lead time from \"code committed into version control to successfully running in production\" by 50%). In order to execute such an initiative, we do the following:\n",
      "\n",
      "  * Assign members of the dedicated team to be solely allocated to the DevOps transformation efforts (as opposed to \"maintain all your current responsibilities, but spend 20% of your time on this new DevOps thing.\").\n",
      "  * Select team members who are generalists, who have skills across a wide variety of domains.\n",
      "  * Select team members who have longstanding and mutually respectful relationships with the rest of the organization.\n",
      "  * Create a separate physical space for the dedicated team, if possible, to maximize communication flow within the team, and creating some isolation from the rest of the organization.\n",
      "\n",
      "If possible, we will free the transformation team from many of the rules and policies that restrict the rest of the organization, as National Instruments did, described in the previous chapter. After all, established processes are a form of institutional memory—we need the dedicated team to create the new processes and learnings required to generate our desired outcomes, creating new institutional memory.\n",
      "\n",
      "Creating a dedicated team is not only good for the team, but also good for the performance engine. By creating a separate team, we create the space for them to experiment with new practices, protecting the rest of the organization from the potential disruptions and distractions associated with it.\n",
      "\n",
      "### AGREE ON A SHARED GOAL\n",
      "\n",
      "One of the most important parts of any improvement initiative is to define a measurable goal with a clearly defined deadline, between six months and two years in the future. It should require considerable effort but still be achievable. And achievement of the goal should create obvious value for the organization as a whole and to our customers.\n",
      "\n",
      "These goals and the time frame should be agreed upon by the executives and known to everyone in the organization. We also want to limit the number of these types of initiatives going on simultaneously to prevent us from overly taxing the organizational change management capacity of leaders and the organization. Examples of improvement goals might include:\n",
      "\n",
      "  * Reduce the percentage of the budget spent on product support and unplanned work by 50%.\n",
      "  * Ensure lead time from code check-in to production release is one week or less for 95% of changes.\n",
      "  * Ensure releases can always be performed during normal business hours with zero downtime.\n",
      "  * Integrate all the required information security controls into the deployment pipeline to pass all required compliance requirements.\n",
      "\n",
      "Once the high-level goal is made clear, teams should decide on a regular cadence to drive the improvement work. Like product development work, we want transformation work to be done in an iterative, incremental manner. A typical iteration will be in the range of two to four weeks. For each iteration, the teams should agree on a small set of goals that generate value and makes some progress toward the long-term goal. At the end of each iteration, teams should review their progress and set new goals for the next iteration.\n",
      "\n",
      "### KEEP OUR IMPROVEMENT PLANNING HORIZONS SHORT\n",
      "\n",
      "In any DevOps transformation project, we need to keep our planning horizons short, just as if we were in a startup doing product or customer development. Our initiative should strive to generate measurable improvements or actionable data within weeks (or, in the worst case, months).\n",
      "\n",
      "By keeping our planning horizons and iteration intervals short, we achieve the following:\n",
      "\n",
      "  * Flexibility and the ability to reprioritize and replan quickly\n",
      "  * Decrease the delay between work expended and improvement realized, which strengthens our feedback loop, making it more likely to reinforce desired behaviors—when improvement initiatives are successful, it encourages more investment\n",
      "  * Faster learning generated from the first iteration, meaning faster integration of our learnings into the next iteration\n",
      "  * Reduction in activation energy to get improvements\n",
      "  * Quicker realization of improvements that make meaningful differences in our daily work\n",
      "  * Less risk that our project is killed before we can generate any demonstrable outcomes\n",
      "\n",
      "### RESERVE 20% OF CYCLES FOR NON-FUNCTIONAL REQUIREMENTS AND REDUCING TECHNICAL DEBT\n",
      "\n",
      "A problem common to any process improvement effort is how to properly prioritize it—after all, organizations that need it most are those that have the least amount of time to spend on improvement. This is especially true in technology organizations because of technical debt.\n",
      "\n",
      "Organizations that struggle with financial debt only make interest payments and never reduce the loan principal, and may eventually find themselves in situations where they can no longer service the interest payments. Similarly, organizations that don't pay down technical debt can find themselves so burdened with daily workarounds for problems left unfixed that they can no longer complete any new work. In other words, they are now only making the interest payment on their technical debt.\n",
      "\n",
      "We will actively manage this technical debt by ensuring that we invest at least 20% of all Development and Operations cycles on refactoring, investing in automation work and architecture and non-functional requirements (NFRs, sometimes referred to as the \"ilities\"), such as maintainability, manageability, scalability, reliability, testability, deployability, and security.\n",
      "\n",
      "  **Figure 11:** Invest 20% of cycles on those that create positive, user-invisible value   \n",
      "(Source: \"Machine Learning and Technical Debt with D. Sculley,\" _Software Engineering Daily_ podcast, November 17, 2015, <http://softwareengineeringdaily.com/2015/11/17/machine-learning-and-technical-debt-with-d-sculley/>.)\n",
      "\n",
      "After the near-death experience of eBay in the late 1990s, Marty Cagan, author of _Inspired: How To Create Products Customers Love,_ the seminal book on product design and management, codified the following lesson:\n",
      "\n",
      "The deal [between product owners and] engineering goes like this: Product management takes 20% of the team's capacity right off the top and gives this to engineering to spend as they see fit. They might use it to rewrite, re-architect, or re-factor problematic parts of the code base...whatever they believe is necessary to avoid ever having to come to the team and say, 'we need to stop and rewrite [all our code].' If you're in really bad shape today, you might need to make this 30% or even more of the resources. However, I get nervous when I find teams that think they can get away with much less than 20%.\n",
      "\n",
      "Cagan notes that when organizations do not pay their \"20% tax,\" technical debt will increase to the point where an organization inevitably spends all of its cycles paying down technical debt. At some point, the services become so fragile that feature delivery grinds to a halt because all the engineers are working on reliability issues or working around problems.\n",
      "\n",
      "By dedicating 20% of our cycles so that Dev and Ops can create lasting countermeasures to the problems we encounter in our daily work, we ensure that technical debt doesn't impede our ability to quickly and safely develop and operate our services in production. Elevating added pressure of technical debt from workers can also reduce levels of burnout.\n",
      "\n",
      "Case Study   \n",
      "Operation InVersion at LinkedIn (2011)\n",
      "\n",
      "LinkedIn's Operation InVersion presents an interesting case study that illustrates the need to pay down technical debt as a part of daily work. Six months after their successful IPO in 2011, LinkedIn continued to struggle with problematic deployments that became so painful that they launched Operation InVersion, where they stopped all feature development for two months in order to overhaul their computing environments, deployments, and architecture.\n",
      "\n",
      "LinkedIn was created in 2003 to help users \"connect to your network for better job opportunities.\" By the end of their first week of operation, they had 2,700 members. One year later, they had over one million members, and have grown exponentially since then. By November 2015, LinkedIn had over 350 million members, who generate tens of thousands of requests per second, resulting in millions of queries per second on the LinkedIn back-end systems.\n",
      "\n",
      "From the beginning, LinkedIn primarily ran on their homegrown Leo application, a monolithic Java application that served every page through servlets and managed JDBC connections to various back-end Oracle databases. However, to keep up with growing traffic in their early years, two critical services were decoupled from Leo: the first handled queries around the member connection graph entirely in-memory, and the second was member search, which layered over the first.\n",
      "\n",
      "By 2010, most new development was occurring in new services, with nearly one hundred services running outside of Leo. The problem was that Leo was only being deployed once every two weeks.\n",
      "\n",
      "Josh Clemm, a senior engineering manager at LinkedIn, explained that by 2010, the company was having significant problems with Leo. Despite vertically scaling Leo by adding memory and CPUs, \"Leo was often going down in production, it was difficult to troubleshoot and recover, and difficult to release new code....It was clear we needed to 'Kill Leo' and break it up into many small functional and stateless services.\"\n",
      "\n",
      "In 2013, journalist Ashlee Vance of Bloomberg described how \"when LinkedIn would try to add a bunch of new things at once, the site would crumble into a broken mess, requiring engineers to work long into the night and fix the problems.\" By Fall 2011, late nights were no longer a rite of passage or a bonding activity, because the problems had become intolerable. Some of LinkedIn's top engineers, including Kevin Scott, who had joined as the LinkedIn VP of Engineering three months before their initial public offering, decided to completely stop engineering work on new features and dedicate the whole department to fixing the site's core infrastructure. They called the effort Operation InVersion.\n",
      "\n",
      "Scott launched Operation InVersion as a way to \"inject the beginnings of a cultural manifesto into his team's engineering culture. There would be no new feature development until LinkedIn's computing architecture was revamped—it's what the business _and_ his team needed.\"\n",
      "\n",
      "Scott described one downside: \"You go public, have all the world looking at you, and then we tell management that we're not going to deliver anything new while all of engineering works on this [InVersion] project for the next two months. It was a scary thing.\"\n",
      "\n",
      "However, Vance described the massively positive results of Operation InVersion. \"LinkedIn created a whole suite of software and tools to help it develop code for the site. Instead of waiting weeks for their new features to make their way onto LinkedIn's main site, engineers could develop a new service, have a series of automated systems examine the code for any bugs and issues the service might have interacting with existing features, and launch it right to the live LinkedIn site...LinkedIn's engineering corps [now] performs major upgrades to the site three times a day.\" By creating a safer system of work, the value they created included fewer late night cram sessions, with more time to develop new, innovative features.\n",
      "\n",
      "As Josh Clemm described in his article on scaling at LinkedIn, \"Scaling can be measured across many dimensions, including organizational.... [Operation InVersion] allowed the entire engineering organization to focus on improving tooling and deployment, infrastructure, and developer productivity. It was successful in enabling the engineering agility we need to build the scalable new products we have today....[In] 2010, we already had over 150 separate services. Today, we have over 750 services.\"\n",
      "\n",
      "Kevin Scott stated, \"Your job as an engineer and your purpose as a technology team is to help your company win. If you lead a team of engineers, it's better to take a CEO's perspective. Your job is to figure out what it is that your company, your business, your marketplace, your competitive environment needs. Apply that to your engineering team in order for your company to win.\"\n",
      "\n",
      "By allowing LinkedIn to pay down nearly a decade of technical debt, Project InVersion enabled stability and safety, while setting the next stage of growth for the company. However, it required two months of total focus on non-functional requirements, at the expense of all the promised features made to the public markets during an IPO. By finding and fixing problems as part of our daily work, we manage our technical debt so that we avoid these \"near death\" experiences.\n",
      "\n",
      "### INCREASE THE VISIBILITY OF WORK\n",
      "\n",
      "In order to be able to know if we are making progress toward our goal, it's essential that everyone in the organization knows the current state of work. There are many ways to make the current state visible, but what's most important is that the information we display is up to date, and that we constantly revise what we measure to make sure it's helping us understand progress toward our current target conditions.\n",
      "\n",
      "The following section discusses patterns that can help create visibility and alignment across teams and functions.\n",
      "\n",
      "## USE TOOLS TO REINFORCE DESIRED BEHAVIOR\n",
      "\n",
      "As Christopher Little, a software executive and one of the earliest chroniclers of DevOps, observed, \"Anthropologists describe tools as a cultural artifact. Any discussion of culture after the invention of fire must also be about tools.\" Similarly, in the DevOps value stream, we use tools to reinforce our culture and accelerate desired behavior changes.\n",
      "\n",
      "One goal is that our tooling reinforces that Development and Operations not only have shared goals, but have a common backlog of work, ideally stored in a common work system and using a shared vocabulary, so that work can be prioritized globally.\n",
      "\n",
      "By doing this, Development and Operations may end up creating a shared work queue, instead of each silo using a different one (e.g., Development uses JIRA while Operations uses ServiceNow). A significant benefit of this is that when production incidents are shown in the same work systems as development work, it will be obvious when ongoing incidents should halt other work, especially when we have a kanban board.\n",
      "\n",
      "Another benefit of having Development and Operations using a shared tool is a unified backlog, where everyone prioritizes improvement projects from a global perspective, selecting work that has the highest value to the organization or most reduces technical debt. As we identify technical debt, we add it to our prioritized backlog if we can't address it immediately. For issues that remain unaddressed, we can use our \"20% time for non-functional requirements\" to fix the top items from our backlog.\n",
      "\n",
      "Other technologies that reinforce shared goals are chat rooms, such as IRC channels, HipChat, Campfire, Slack, Flowdock, and OpenFire. Chat rooms allow the fast sharing of information (as opposed to filling out forms that are processed through predefined workflows), the ability to invite other people as needed, and history logs that are automatically recorded for posterity and can be analyzed during post-mortem sessions.\n",
      "\n",
      "An amazing dynamic is created when we have a mechanism that allows any team member to quickly help other team members, or even people outside their team—the time required to get information or needed work can go from days to minutes. In addition, because everything is being recorded, we may not need to ask someone else for help in the future—we simply search for it.\n",
      "\n",
      "However, the rapid communication environment facilitated by chat rooms can also be a drawback. As Ryan Martens, the founder and CTO of Rally Software, observes, \"In a chat room, if someone doesn't get an answer in a couple of minutes, it's totally accepted and expected that you can bug them again until they get what they need.\"\n",
      "\n",
      "The expectations of immediate response can, of course, lead to undesired outcomes. A constant barrage of interruptions and questions can prevent people from getting necessary work done. As a result, teams may decide that certain types of requests should go through more structured and asynchronous tools.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we identified all the teams supporting our value stream and captured in a value stream map what work is required in order to deliver value to the customer. The value stream map provides the basis for understanding our current state, including our lead time and %C/A metrics for problematic areas, and informs how we set a future state.\n",
      "\n",
      "This enables dedicated transformation teams to rapidly iterate and experiment to improve performance. We also make sure that we allocate a sufficient amount of time for improvement, fixing known problems and architectural issues, including our non-functional requirements. The case studies from Nordstrom and LinkedIn demonstrate how dramatic improvements can be made in lead times and quality when we find problems in our value stream and pay down technical debt.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Which makes it all the more important that we limit the level of detail being collected—everyone's time is valuable and scarce.\n",
      "\n",
      "‡ Conversely, there are many examples of using tools in a way that guarantees no behavior changes occur. For instance, an organization commits to an agile planning tool but then configures it for a waterfall process, which merely maintains status quo.\n",
      "\n",
      "# 7How to Design Our Organization and Architecture with Conway's Law in Mind\n",
      "\n",
      "In the previous chapters, we identified a value stream to start our DevOps transformation and established shared goals and practices to enable a dedicated transformation team to improve how we deliver value to the customer.\n",
      "\n",
      "In this chapter, we will start thinking about how to organize ourselves to best achieve our value stream goals. After all, how we organize our teams affects how we perform our work. Dr. Melvin Conway performed a famous experiment in 1968 with a contract research organization that had eight people who were commissioned to produce a COBOL and an ALGOL compiler. He observed, \"After some initial estimates of difficulty and time, five people were assigned to the COBOL job and three to the ALGOL job. The resulting COBOL compiler ran in five phases, the ALGOL compiler ran in three.\"\n",
      "\n",
      "These observations led to what is now known as Conway's Law, which states that \"organizations which design systems...are constrained to produce designs which are copies of the communication structures of these organizations....The larger an organization is, the less flexibility it has and the more pronounced the phenomenon.\" Eric S. Raymond, author of the book _The Cathedral and the Bazaar: Musings on Linux and Open Source by an Accidental Revolutionary_ , crafted a simplified (and now, more famous) version of Conway's Law in his Jargon File: \"The organization of the software and the organization of the software team will be congruent; commonly stated as 'if you have four groups working on a compiler, you'll get a 4-pass compiler.'\"\n",
      "\n",
      "In other words, how we organize our teams has a powerful effect on the software we produce, as well as our resulting architectural and production outcomes. In order to get fast flow of work from Development into Operations, with high quality and great customer outcomes, we must organize our teams and our work so that Conway's Law works to our advantage. Done poorly, Conway's Law will prevent teams from working safely and independently; instead, they will be tightly-coupled together, all waiting on each other for work to be done, with even small changes creating potentially global, catastrophic consequences.\n",
      "\n",
      "An example of how Conway's Law can either impede or reinforce our goals can be seen in a technology that was developed at Etsy called Sprouter. Etsy's DevOps journey began in 2009, and is one of the most admired DevOps organizations, with 2014 revenue of nearly $200 million and a successful IPO in 2015.\n",
      "\n",
      "Originally developed in 2007, Sprouter connected people, processes, and technology in ways that created many undesired outcomes. Sprouter, shorthand for \"stored procedure router,\" was originally designed to help make life easier for the developers and database teams. As Ross Snyder, a senior engineer at Etsy, said during his presentation at Surge 2011, \"Sprouter was designed to allow the Dev teams to write PHP code in the application, the DBAs to write SQL inside Postgres, with Sprouter helping them meet in the middle.\"\n",
      "\n",
      "Sprouter resided between their front-end PHP application and the Postgres database, centralizing access to the database and hiding the database implementation from the application layer. The problem was that adding any changes to business logic resulted in significant friction between developers and the database teams. As Snyder observed, \"For nearly any new site functionality, Sprouter required that the DBAs write a new stored procedure. As a result, every time developers wanted to add new functionality, they would need something from the DBAs, which often required them to wade through a ton of bureaucracy.\" In other words, developers creating new functionality had a dependency on the DBA team, which needed to be prioritized, communicated, and coordinated, resulting in work sitting in queues, meetings, longer lead times, and so forth. This is because Sprouter created a tight coupling between the development and database teams, preventing developers from being able to independently develop, test, and deploy their code into production.\n",
      "\n",
      "Also, the database stored procedures were tightly-coupled to Sprouter—any time a stored procedure was changed, it required changes to Sprouter too. The result was that Sprouter became an ever-larger single point of failure. Snyder explained that everything was so tightly-coupled and required such a high level of synchronization as a result, that almost every deployment caused a mini-outage.\n",
      "\n",
      "Both the problems associated with Sprouter and their eventual solution can be explained by Conway's Law. Etsy initially had two teams, the developers and the DBAs, who were each responsible for two layers of the service, the application logic layer and stored procedure layer. Two teams working on two layers, as Conway's Law predicts. Sprouter was intended to make life easier for both teams, but it didn't work as expected—when business rules changed, instead of changing only two layers, they now needed to make changes to three layers (in the application, in the stored procedures, and now in Sprouter). The resulting challenges of coordinating and prioritizing work across three teams significantly increased lead times and caused reliability problems.\n",
      "\n",
      "In the spring of 2009, as part of what Snyder called \"the great Etsy cultural transformation,\" Chad Dickerson joined as their new CTO. Dickerson put into motion many things, including a massive investment into site stability, having developers perform their own deployments into production, as well as beginning a two-year journey to eliminate Sprouter.\n",
      "\n",
      "To do this, the team decided to move all the business logic from the database layer into the application layer, removing the need for Sprouter. They created a small team that wrote a PHP Object Relational Mapping (ORM) layer,† enabling the front-end developers to make calls directly to the database and reducing the number of teams required to change business logic from three teams down to one team.\n",
      "\n",
      "As Snyder described, \"We started using the ORM for any new areas of the site and migrated small parts of our site from Sprouter to the ORM over time. It took us two years to migrate the entire site off of Sprouter. And even though we all grumbled about Sprouter the entire time, it remained in production throughout.\"\n",
      "\n",
      "By eliminating Sprouter, they also eliminated the problems associated with multiple teams needing to coordinate for business logic changes, decreased the number of handoffs, and significantly increased the speed and success of production deployments, improving site stability. Furthermore, because small teams could independently develop and deploy their code without requiring another team to make changes in other areas of the system, developer productivity increased.\n",
      "\n",
      "Sprouter was finally removed from production and Etsy's version control repositories in early 2001. As Snyder said, \"Wow, it felt good.\"‡\n",
      "\n",
      "As Snyder and Etsy experienced, how we design our organization dictates how work is performed, and, therefore, the outcomes we achieve. Throughout the rest of this chapter we will explore how Conway's Law can negatively impact the performance of our value stream, and, more importantly, how we organize our teams to use Conway's Law to our advantage.\n",
      "\n",
      "## ORGANIZATIONAL ARCHETYPES\n",
      "\n",
      "In the field of decision sciences, there are three primary types of organizational structures that inform how we design our DevOps value streams with Conway's Law in mind: _functional_ , _matrix,_ and _market_. They are defined by Dr. Roberto Fernandez as follows:\n",
      "\n",
      "  * Functional-oriented organizations optimize for expertise, division of labor, or reducing cost. These organizations centralize expertise, which helps enable career growth and skill development, and often have tall hierarchical organizational structures. This has been the prevailing method of organization for Operations (i.e., server admins, network admins, database admins, and so forth are all organized into separate groups).\n",
      "  * Matrix-oriented organizations attempt to combine functional and market orientation. However, as many who work in or manage matrix organizations observe, matrix organizations often result in complicated organizational structures, such as individual contributors reporting to two managers or more, and sometimes achieving neither of the goals of functional or market orientation.\n",
      "  * Market-oriented organizations optimize for responding quickly to customer needs. These organizations tend to be flat, composed of multiple, cross-functional disciplines (e.g., marketing, engineering, etc.), which often lead to potential redundancies across the organization. This is how many prominent organizations adopting DevOps operate—in extreme examples, such as at Amazon or Netflix, each service team is simultaneously responsible for feature delivery and service support.§\n",
      "\n",
      "With these three categories of organizations in mind, let's explore further how an overly functional orientation, especially in Operations, can cause undesired outcomes in the technology value stream, as Conway's Law would predict.\n",
      "\n",
      "## PROBLEMS OFTEN CAUSED BY OVERLY FUNCTIONAL ORIENTATION (\"OPTIMIZING FOR COST\")\n",
      "\n",
      "In traditional IT Operations organizations, we often use functional orientation to organize our teams by their specialties. We put the database administrators in one group, the network administrators in another, the server administrators in a third, and so forth. One of the most visible consequences of this is long lead times, especially for complex activities like large deployments where we must open up tickets with multiple groups and coordinate work handoffs, resulting in our work waiting in long queues at every step.\n",
      "\n",
      "Compounding the issue, the person performing the work often has little visibility or understanding of how their work relates to any value stream goals (e.g., \"I'm just configuring servers because someone told me to.\"). This places workers in a creativity and motivation vacuum.\n",
      "\n",
      "The problem is exacerbated when each Operations functional area has to serve multiple value streams (i.e., multiple Development teams) who all compete for their scarce cycles. In order for Development teams to get their work done in a timely manner, we often have to escalate issues to a manager or director, and eventually to someone (usually an executive) who can finally prioritize the work against the global organizational goals instead of the functional silo goals. This decision must then get cascaded down into each of the functional areas to change the local priorities, and this, in turn, slows down other teams. When every team expedites their work, the net result is that every project ends up moving at the same slow crawl.\n",
      "\n",
      "In addition to long queues and long lead times, this situation results in poor handoffs, large amounts of re-work, quality issues, bottlenecks, and delays. This gridlock impedes the achievement of important organizational goals, which often far outweigh the desire to reduce costs.¶\n",
      "\n",
      "Similarly, functional orientation can also be found with centralized QA and Infosec functions, which may have worked fine (or at least, well enough) when performing less frequent software releases. However, as we increase the number of Development teams and their deployment and release frequencies, most functionally-oriented organizations will have difficulty keeping up and delivering satisfactory outcomes, especially when their work is being performed manually. Now we'll study how market oriented organizations work.\n",
      "\n",
      "## ENABLE MARKET-ORIENTED TEAMS (\"OPTIMIZING FOR SPEED\")\n",
      "\n",
      "Broadly speaking, to achieve DevOps outcomes, we need to reduce the effects of functional orientation (\"optimizing for cost\") and enable market orientation (\"optimizing for speed\") so we can have many small teams working safely and independently, quickly delivering value to the customer.\n",
      "\n",
      "Taken to the extreme, market-oriented teams are responsible not only for feature development, but also for testing, securing, deploying, and supporting their service in production, from idea conception to retirement. These teams are designed to be cross-functional and independent—able to design and run user experiments, build and deliver new features, deploy and run their service in production, and fix any defects without manual dependencies on other teams, thus enabling them to move faster. This model has been adopted by Amazon and Netflix and is touted by Amazon as one of the primary reasons behind their ability to move fast even as they grow.\n",
      "\n",
      "To achieve market orientation, we won't do a large, top-down reorganization, which often creates large amounts of disruption, fear, and paralysis. Instead, we will embed the functional engineers and skills (e.g., Ops, QA, Infosec) into each service team, or provide their capabilities to teams through automated self-service platforms that provide production-like environments, initiate automated tests, or perform deployments.\n",
      "\n",
      "This enables each service team to independently deliver value to the customer without having to open tickets with other groups, such as IT Operations, QA, or Infosec.**\n",
      "\n",
      "## MAKING FUNCTIONAL ORIENTATION WORK\n",
      "\n",
      "Having just recommended market-orientated teams, it is worth pointing out that it is possible to create effective, high-velocity organizations with functional orientation. Cross-functional and market-oriented teams are one way to achieve fast flow and reliability, but they are not the only path. We can also achieve our desired DevOps outcomes through functional orientation, as long as everyone in the value stream views customer and organizational outcomes as a shared goal, regardless of where they reside in the organization.\n",
      "\n",
      "  **Figure 12:** Functional vs. market orientation\n",
      "\n",
      "Left: Functional orientation: all work flows through centralized IT Operations; Right: Market orientation: all product teams can deploy their loosely-coupled components self-service into production. (Source: Humble, Molesky, and O'Reilly, _Lean Enterprise_ , Kindle edition, 4523 & 4592.)\n",
      "\n",
      "For example, high performance with a functional-oriented and centralized Operations group is possible, as long as service teams get what they need from Operations reliably and quickly (ideally on demand) and vice-versa. Many of the most admired DevOps organizations retain functional orientation of Operations, including Etsy, Google, and GitHub.\n",
      "\n",
      "What these organizations have in common is a high-trust culture that enables all departments to work together effectively, where all work is transparently prioritized and there is sufficient slack in the system to allow high-priority work to be completed quickly. This is, in part, enabled by automated self-service platforms that build quality into the products everyone is building.\n",
      "\n",
      "In the Lean manufacturing movement of the 1980s, many researchers were puzzled by Toyota's functional orientation, which was at odds with the best practice of having cross-functional, market-oriented teams. They were so puzzled it was called \"the second Toyota paradox.\"\n",
      "\n",
      "As Mike Rother wrote in _Toyota Kata_ , \"As tempting as it seems, one cannot reorganize your way to continuous improvement and adaptiveness. What is decisive is not the form of the organization, but how people act and react. The roots of Toyota's success lie not in its organizational structures, but in developing capability and habits in its people. It surprises many people, in fact, to find that Toyota is largely organized in a traditional, functional-department style.\" It is this development of habits and capabilities in people and the workforce that are the focus of our next sections.\n",
      "\n",
      "## TESTING, OPERATIONS, AND SECURITY AS EVERYONE'S JOB, EVERY DAY\n",
      "\n",
      "In high-performing organizations, everyone within the team shares a common goal—quality, availability, and security aren't the responsibility of individual departments, but are a part of everyone's job, every day.\n",
      "\n",
      "This means that the most urgent problem of the day may be working on or deploying a customer feature or fixing a Severity 1 production incident. Alternatively, the day may require reviewing a fellow engineer's change, applying emergency security patches to production servers, or making improvements so that fellow engineers are more productive.\n",
      "\n",
      "Reflecting on shared goals between Development and Operations, Jody Mulkey, CTO at Ticketmaster, said, \"For almost 25 years, I used an American football metaphor to describe Dev and Ops. You know, Ops is defense, who keeps the other team from scoring, and Dev is offense, trying to score goals. And one day, I realized how flawed this metaphor was, because they never all play on the field at the same time. They're not actually on the same team!\"\n",
      "\n",
      "He continued, \"The analogy I use now is that Ops are the offensive linemen, and Dev are the 'skill' positions (like the quarterback and wide receivers) whose job it is to move the ball down the field—the job of Ops is to help make sure Dev has enough time to properly execute the plays.\"\n",
      "\n",
      "A striking example of how shared pain can reinforce shared goals is when Facebook was undergoing enormous growth in 2009. They were experiencing significant problems related to code deployments—while not all issues caused customer-impacting issues, there was chronic firefighting and long hours. Pedro Canahuati, their director of production engineering, described a meeting full of Ops engineers where someone asked that all people not working on an incident close their laptops, and no one could.\n",
      "\n",
      "One of the most significant things they did to help change the outcomes of deployments was to have all Facebook engineers, engineering managers, and architects rotate through on-call duty for the services they built. By doing this, everyone who worked on the service experienced visceral feedback on the upstream architectural and coding decisions they made, which made an enormous positive impact on the downstream outcomes.\n",
      "\n",
      "## ENABLE EVERY TEAM MEMBER TO BE A GENERALIST\n",
      "\n",
      "In extreme cases of a functionally-oriented Operations organization, we have departments of specialists, such as network administrators, storage administrators, and so forth. When departments over-specialize, it causes _siloization_ , which Dr. Spear describes as when departments \"operate more like sovereign states.\" Any complex operational activity then requires multiple handoffs and queues between the different areas of the infrastructure, leading to longer lead times (e.g., because every network change must be made by someone in the networking department).\n",
      "\n",
      "Because we rely upon an ever increasing number of technologies, we must have engineers who have specialized and achieved mastery in the technology areas we need. However, we don't want to create specialists who are \"frozen in time,\" only understanding and able to contribute to that one area of the value stream.\n",
      "\n",
      "One countermeasure is to enable and encourage every team member to be a generalist. We do this by providing opportunities for engineers to learn all the skills necessary to build and run the systems they are responsible for, and regularly rotating people through different roles. The term _full stack engineer_ is now commonly used (sometimes as a rich source of parody) to describe generalists who are familiar—at least have a general level of understanding—with the entire application stack (e.g., application code, databases, operating systems, networking, cloud).\n",
      "\n",
      "**Table 2:** Specialists vs. Generalists vs. \"E-shaped\" Staff (experience, expertise, exploration, and execution)\n",
      "\n",
      "(Source: Scott Prugh, \"Continuous Delivery,\" ScaledAgileFramework.com, February 14, 2013, <http://scaledagileframework.com/continuous-delivery/>.)\n",
      "\n",
      "Scott Prugh writes that CSG International has undergone a transformation that brings most resources required to build and run the product onto one team, including analysis, architecture, development, test, and operations. \"By cross-training and growing engineering skills, generalists can do orders of magnitude more work than their specialist counterparts, and it also improves our overall flow of work by removing queues and wait time.\" This approach is at odds with traditional hiring practices, but, as Prugh explains, it is well worth it. \"Traditional managers will often object to hiring engineers with generalist skill sets, arguing that they are more expensive and that 'I can hire two server administrators for every multi-skilled operations engineer.'\" However, the business benefits of enabling faster flow are overwhelming. Furthermore, as Prugh notes, \"[I]nvesting in cross training is the right thing for [employees'] career growth, and makes everyone's work more fun.\"\n",
      "\n",
      "When we value people merely for their existing skills or performance in their current role rather than for their ability to acquire and deploy new skills, we (often inadvertently) reinforce what Dr. Carol Dweck describes as the _fixed mindset_ , where people view their intelligence and abilities as static \"givens\" that can't be changed in meaningful ways.\n",
      "\n",
      "Instead, we want to encourage learning, help people overcome learning anxiety, help ensure that people have relevant skills and a defined career road map, and so forth. By doing this, we help foster a _growth mindset_ in our engineers—after all, a learning organization requires people who are willing to learn. By encouraging everyone to learn, as well as providing training and support, we create the most sustainable and least expensive way to create greatness in our teams—by investing in the development of the people we already have.\n",
      "\n",
      "As Jason Cox, Director of Systems Engineering at Disney, described, \"Inside of Operations, we had to change our hiring practices. We looked for people who had 'curiosity, courage, and candor,' who were not only capable of being generalists but also renegades...We want to promote positive disruption so our business doesn't get stuck and can move into the future.\" As we'll see in the next section, how we fund our teams also affects our outcomes.\n",
      "\n",
      "## FUND NOT PROJECTS, BUT SERVICES AND PRODUCTS\n",
      "\n",
      "Another way to enable high-performing outcomes is to create stable service teams with ongoing funding to execute their own strategy and road map of initiatives. These teams have the dedicated engineers needed to deliver on concrete commitments made to internal and external customers, such as features, stories, and tasks.\n",
      "\n",
      "Contrast this to the more traditional model where Development and Test teams are assigned to a \"project\" and then reassigned to another project as soon as the project is completed and funding runs out. This leads to all sorts of undesired outcomes, including developers being unable to see the long-term consequences of decisions they make (a form of feedback) and a funding model that only values and pays for the earliest stages of the software life cycle—which, tragically, is also the least expensive part for successful products or services.††\n",
      "\n",
      "Our goal with a product-based funding model is to value the achievement of organizational and customer outcomes, such as revenue, customer lifetime value, or customer adoption rate, ideally with the minimum of output (e.g., amount of effort or time, lines of code). Contrast this to how projects are typically measured, such as whether it was completed within the promised budget, time, and scope.\n",
      "\n",
      "## DESIGN TEAM BOUNDARIES IN ACCORDANCE WITH CONWAY'S LAW\n",
      "\n",
      "As organizations grow, one of the largest challenges is maintaining effective communication and coordination between people and teams. All too often, when people and teams reside on a different floor, in a different building, or in a different time zone, creating and maintaining a shared understanding and mutual trust becomes more difficult, impeding effective collaboration. Collaboration is also impeded when the primary communication mechanisms are work tickets and change requests, or worse, when teams are separated by contractual boundaries, such as when work is performed by an outsourced team.\n",
      "\n",
      "As we saw in the Etsy Sprouter example at the beginning of this chapter, the way we organize teams can create poor outcomes, a side effect of Conway's Law. These include splitting teams by function (e.g., by putting developers and testers in different locations or by outsourcing testers entirely) or by architectural layer (e.g., application, database).\n",
      "\n",
      "These configurations require significant communication and coordination between teams, but still results in a high amount of rework, disagreements over specifications, poor handoffs, and people sitting idle waiting for somebody else.\n",
      "\n",
      "Ideally, our software architecture should enable small teams to be independently productive, sufficiently decoupled from each other so that work can be done without excessive or unnecessary communication and coordination.\n",
      "\n",
      "## CREATE LOOSELY-COUPLED ARCHITECTURES TO ENABLE DEVELOPER PRODUCTIVITY AND SAFETY\n",
      "\n",
      "When we have a tightly-coupled architecture, small changes can result in large scale failures. As a result, anyone working in one part of the system must constantly coordinate with anyone else working in another part of the system they may affect, including navigating complex and bureaucratic change management processes.\n",
      "\n",
      "Furthermore, to test that the entire system works together requires integrating changes with the changes from hundreds, or even thousands, of other developers, which may, in turn, have dependencies on tens, hundreds, or thousands of interconnected systems. Testing is done in scarce integration test environments, which often require weeks to obtain and configure. The result is not only long lead times for changes (typically measured in weeks or months) but also low developer productivity and poor deployment outcomes.\n",
      "\n",
      "In contrast, when we have an architecture that enables small teams of developers to independently implement, test, and deploy code into production safely and quickly, we can increase and maintain developer productivity and improve deployment outcomes. These characteristics can be found in _service-oriented architectures_ (SOAs) first described in the 1990s, in which services are independently testable and deployable. A key feature of SOAs is that they're composed of _loosely-coupled_ services with _bounded contexts_.‡‡\n",
      "\n",
      "Having architecture that is loosely-coupled means that services can update in production independently, without having to update other services. Services must be decoupled from other services and, just as important, from shared databases (although they can share a database _service_ , provided they don't have any common schemas).\n",
      "\n",
      "Bounded contexts __are described in the book _Domain Driven Design_ by Eric J. Evans. The idea is that developers should be able to understand and update the code of a service without knowing anything about the internals of its peer services. Services interact with their peers strictly through APIs and thus don't share data structures, database schemata, or other internal representations of objects. Bounded contexts ensure that services are compartmentalized and have well-defined interfaces, which also enables easier testing.\n",
      "\n",
      "Randy Shoup, former Engineering Director for Google App Engine, observed that \"organizations with these types of service-oriented architectures, such as Google and Amazon, have incredible flexibility and scalability. These organizations have tens of thousands of developers where small teams can still be incredibly productive.\"\n",
      "\n",
      "### KEEP TEAM SIZES SMALL (THE \"TWO-PIZZA TEAM\" RULE)\n",
      "\n",
      "Conway's Law helps us design our team boundaries in the context of desired communication patterns, but it also encourages us to keep our team sizes small, reducing the amount of inter-team communication and encouraging us to keep the scope of each team's domain small and bounded.\n",
      "\n",
      "As part of its transformation initiative away from a monolithic code base in 2002, Amazon used the _two-pizza_ rule to keep team sizes small—a team only as large as can be fed with two pizzas—usually about five to ten people.\n",
      "\n",
      "This limit on size has four important effects:\n",
      "\n",
      "  1. It ensures the team has a clear, shared understanding of the system they are working on. As teams get larger, the amount of communication required for everybody to know what's going on scales in a combinatorial fashion.\n",
      "  2. It limits the growth rate of the product or service being worked on. By limiting the size of the team, we limit the rate at which their system can evolve. This also helps to ensure the team maintains a shared understanding of the system.\n",
      "  3. It decentralizes power and enables autonomy. Each two-pizza team (2PT) is as autonomous as possible. The team's lead, working with the executive team, decides on the key business metric that the team is responsible for, known as the fitness function, which becomes the overall evaluation criteria for the team's experiments. The team is then able to act autonomously to maximize that metric.§§\n",
      "  4. Leading a 2PT is a way for employees to gain some leadership experience in an environment where failure does not have catastrophic consequences. An essential element of Amazon's strategy was the link between the organizational structure of a 2PT and the architectural approach of a service-oriented architecture.\n",
      "\n",
      "Amazon CTO Werner Vogels explained the advantages of this structure to Larry Dignan of _Baseline_ in 2005. Dignan writes:\n",
      "\n",
      "\"Small teams are fast...and don't get bogged down in so-called administrivia....Each group assigned to a particular business is completely responsible for it....The team scopes the fix, designs it, builds it, implements it and monitors its ongoing use. This way, technology programmers and architects get direct feedback from the business people who use their code or applications—in regular meetings and informal conversations.\"\n",
      "\n",
      "Another example of how architecture can profoundly improve productivity is the API Enablement program at Target, Inc.\n",
      "\n",
      "Case Study   \n",
      "API Enablement at Target (2015)\n",
      "\n",
      "Target is the sixth-largest retailer in the US and spends over $1 billion on technology annually. Heather Mickman, a director of development for Target, described the beginnings of their DevOps journey: \"In the bad old days, it used to take ten different teams to provision a server at Target, and when things broke, we tended to stop making changes to prevent further issues, which of course makes everything worse.\"\n",
      "\n",
      "The hardships associated with getting environments and performing deployments created significant difficulties for development teams, as did getting access to data they needed. As Mickman described:\n",
      "\n",
      "The problem was that much of our core data, such as information on inventory, pricing, and stores, was locked up in legacy systems and mainframes. We often had multiple sources of truths of data, especially between e-commerce and our physical stores, which were owned by different teams, with different data structures and different priorities....The result was that if a new development team wanted to build something for our guests, it would take three to six months to build the integrations to get the data they needed. Worse, it would take another three to six months to do the manual testing to make sure they didn't break anything critical, because of how many custom point-to-point integrations we had in a very tightly-coupled system. Having to manage the interactions with the twenty to thirty different teams, along with all their dependencies, required lots of project managers, because of all the coordination and handoffs. It meant that development was spending all their time waiting in queues, instead of delivering results and getting stuff done.\n",
      "\n",
      "This long lead time for retrieving and creating data in their systems of record was jeopardizing important business goals, such as integrating the supply chain operations of Target's physical stores and their e-commerce site, which now required getting inventory to stores and customer homes. This pushed the Target supply chain well beyond what it was designed for, which was merely to facilitate the movement of goods from vendors to distribution centers and stores.\n",
      "\n",
      "In an attempt to solve the data problem, in 2012 Mickman led the API Enablement team to enable development teams to \"deliver new capabilities in days instead of months.\" They wanted any engineering team inside of Target to be able to get and store the data they needed, such as information on their products or their stores, including operating hours, location, whether there was as Starbucks on-site, and so forth.\n",
      "\n",
      "Time constraints played a large role in team selection. Mickman explained that:\n",
      "\n",
      "Because our team also needed to deliver capabilities in days, not months, I needed a team who could do the work, not give it to contractors—we wanted people with kickass engineering skills, not people who knew how to manage contracts. And to make sure our work wasn't sitting in queue, we needed to own the entire stack, which meant that we took over the Ops requirements as well....We brought in many new tools to support continuous integration and continuous delivery. And because we knew that if we succeeded, we would have to scale with extremely high growth, we brought in new tools such as the Cassandra database and Kafka message broker. When we asked for permission, we were told no, but we did it anyway, because we knew we needed it.\n",
      "\n",
      "In the following two years, the API Enablement team enabled fifty-three new business capabilities, including Ship to Store and Gift Registry, as well as their integrations with Instacart and Pinterest. As Mickman described, \"Working with Pinterest suddenly became very easy, because we just provided them our APIs.\"\n",
      "\n",
      "In 2014, the API Enablement team served over 1.5 billion API calls per month. By 2015, this had grown to seventeen billion calls per month spanning ninety different APIs. To support this capability, they routinely performed eighty deployments per week.\n",
      "\n",
      "These changes have created major business benefits for Target—digital sales increased 42% during the 2014 holiday season and increased another 32% in Q2. During the Black Friday weekend of 2015, over 280k in-store pickup orders were created. By 2015, their goal is to enable 450 of their 1,800 stores to be able to fulfill e-commerce orders, up from one hundred.\n",
      "\n",
      "\"The API Enablement team shows what a team of passionate change agents can do,\" Mickman says. \"And it help set us up for the next stage, which is to expand DevOps across the entire technology organization.\"\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Through the Etsy and Target case studies, we can see how architecture and organizational design can dramatically improve our outcomes. Done incorrectly, Conway's Law will ensure that the organization creates poor outcomes, preventing safety and agility. Done well, the organization enables developers to safely and independently develop, test, and deploy value to the customer.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Among many things, an ORM abstracts a database, enabling developers to do queries and data manipulation as if they were merely another object in the programming language. Popular ORMs include Hibernate for Java, SQLAlchemy for Python, and ActiveRecord for Ruby on Rails.\n",
      "\n",
      "‡ Sprouter was one of many technologies used in development and production that Etsy eliminated as part of their transformation.\n",
      "\n",
      "§ However, as will be explained later, equally prominent organizations such as Etsy and GitHub have functional orientation.\n",
      "\n",
      "¶ Adrian Cockcroft remarked, \"For companies who are now coming off of five-year IT outsourcing contracts, it's like they've been frozen in time, during one of the most disruptive times in technology.\" In other words, IT outsourcing is a tactic used to control costs through contractually-enforced stasis, with firm fixed prices that schedule annual cost reductions. However, it often results in organizations being unable to respond to changing business and technology needs.\n",
      "\n",
      "** For the remainder of this books, we will use _service teams_ interchangeably with _feature teams_ , _product teams_ , _development teams,_ and _delivery teams_. The intent is to specify the team primarily developing, testing, and securing the code so that value is delivered to the customer.\n",
      "\n",
      "†† As John Lauderbach, currently VP of Information Technology at Roche Bros. Supermarkets, quipped, \"Every new application is like a free puppy. It's not the upfront capital cost that kills you...It's the ongoing maintenance and support.\"\n",
      "\n",
      "‡‡ These properties are also found in \"microservices,\" which build upon the principles of SOA. One popular set of patterns for modern web architecture based on these principles is the \"12-factor app.\"\n",
      "\n",
      "§§ In the Netflix culture, one of the seven key values is \"highly aligned, loosely-coupled.\"\n",
      "\n",
      "# 8How to Get Great Outcomes by Integrating Operations into the Daily Work of Development\n",
      "\n",
      "Our goal is to enable market-oriented outcomes where many small teams can quickly and independently deliver value to the customer. This can be a challenge to achieve when Operations is centralized and functionally-oriented, having to serve the needs of many different development teams with potentially wildly different needs. The result can often be long lead times for needed Ops work, constant reprioritization and escalation, and poor deployment outcomes.\n",
      "\n",
      "We can create more market-oriented outcomes by better integrating Ops capabilities into Dev teams, making both more efficient and productive. In this chapter, we'll explore many ways to achieve this, both at the organizational level and through daily rituals. By doing this, Ops can significantly improve the productivity of Dev teams throughout the entire organization, as well as enable better collaboration and organizational outcomes.\n",
      "\n",
      "At Big Fish Games, which develops and supports hundreds of mobile and thousands of PC games and had more than $266 million in revenue in 2013, VP of IT Operations Paul Farrall was in charge of the centralized Operations organization. He was responsible for supporting many different business units that had a great deal of autonomy.\n",
      "\n",
      "Each of these business units had dedicated development teams who often chose wildly different technologies. When these groups wanted to deploy new functionality, they would have to compete for a common pool of scarce Ops resources. Furthermore, everyone was struggling with unreliable Test and Integration environments, as well as extremely cumbersome release processes.\n",
      "\n",
      "Farrall thought the best way to solve this problem was by embedding Ops expertise into Development teams. He observed, \"When Dev teams had problems with testing or deployment, they needed more than just technology or environments. What they also needed was help and coaching. At first, we embedded Ops engineers and architects into each of the Dev teams, but there simply weren't enough Ops engineers to cover that many teams. We were able to help more teams with what we called an _Ops liaison_ model and with fewer people.\"\n",
      "\n",
      "Farrall defined two types of Ops liaisons: the business relationship manager and the dedicated release engineer. The business relationship managers worked with product management, line-of-business owners, project management, Dev management, and developers. They became intimately familiar with product group business drivers and product road maps, acted as advocates for product owners inside of Operations, and helped their product teams navigate the Operations landscape to prioritize and streamline work requests.\n",
      "\n",
      "Similarly, the dedicated release engineer became intimately familiar with the product's Development and QA issues, and helped them get what they needed from the Ops organization to achieve their goals. They were familiar with the typical Dev and QA requests for Ops, and would often execute the needed work themselves. As needed, they would also pull in dedicated technical Ops engineers (e.g., DBAs, Infosec, storage engineers, network engineers), and help determine which self-service tools the entire Operations group should prioritize building.\n",
      "\n",
      "By doing this, Farrall was able to help Dev teams across the organization become more productive and achieve their team goals. Furthermore, he helped the teams prioritize around his global Ops constraints, reducing the number of surprises discovered mid-project and ultimately increasing the overall project throughput.\n",
      "\n",
      "Farrall notes that both working relationships with Operations and code release velocity were noticeably improved as a result of the changes. He concludes, \"The Ops liaison model allowed us to embed IT Operations expertise into the Dev and Product teams without adding new headcount.\"\n",
      "\n",
      "The DevOps transformation at Big Fish Games shows how a centralized Operations team was able to achieve the outcomes typically associated with market-oriented teams. We can employ the three following broad strategies:\n",
      "\n",
      "  * Create self-service capabilities to enable developers in the service teams to be productive.\n",
      "  * Embed Ops engineers into the service teams.\n",
      "  * Assign Ops liaisons to the service teams when embedding Ops is not possible.\n",
      "\n",
      "Lastly, we describe how Ops engineers can integrate into the Dev team rituals used in their daily work, including daily standups, planning, and retrospectives.\n",
      "\n",
      "## CREATE SHARED SERVICES TO INCREASE DEVELOPER PRODUCTIVITY\n",
      "\n",
      "One way to enable market-oriented outcomes is for Operations to create a set of centralized platforms and tooling services that any Dev team can use to become more productive, such as getting production-like environments, deployment pipelines, automated testing tools, production telemetry dashboards, and so forth.† By doing this, we enable Dev teams to spend more time building functionality for their customer, as opposed to obtaining all the infrastructure required to deliver and support that feature in production.\n",
      "\n",
      "All the platforms and services we provide should (ideally) be automated and available on demand, without requiring a developer to open up a ticket and wait for someone to manually perform work. This ensures that Operations doesn't become a bottleneck for their customers (e.g., \"We received your work request, and it will take six weeks to manually configure those test environments.\").‡\n",
      "\n",
      "By doing this, we enable the product teams to get what they need, when they need it, as well as reduce the need for communications and coordination. As Damon Edwards observed, \"Without these self-service Operations platforms, the cloud is just Expensive Hosting 2.0.\"\n",
      "\n",
      "In almost all cases, we will not mandate that internal teams use these platforms and services—these platform teams will have to win over and satisfy their internal customers, sometimes even competing with external vendors. By creating this effective internal marketplace of capabilities, we help ensure that the platforms and services we create are the easiest and most appealing choice available (the path of least resistance).\n",
      "\n",
      "For instance, we may create a platform that provides a shared version control repository with pre-blessed security libraries, a deployment pipeline that automatically runs code quality and security scanning tools, which deploys our applications into _known, good environments_ that already have production monitoring tools installed on them. Ideally, we make life so much easier for Dev teams that they will overwhelmingly decide that using our platform is the easiest, safest, and most secure means to get their applications into production.\n",
      "\n",
      "We build into these platforms the cumulative and collective experience of everyone in the organization, including QA, Operations, and Infosec, which helps to create an ever safer system of work. This increases developer productivity and makes it easy for product teams to leverage common processes, such as performing automated testing and satisfying security and compliance requirements.\n",
      "\n",
      "Creating and maintaining these platforms and tools is real product development—the customers of our platform aren't our external customer but our internal Dev teams. Like creating any great product, creating great platforms that everyone loves doesn't happen by accident. An internal platform team with poor customer focus will likely create tools that everyone will hate and quickly abandon for other alternatives, whether for another internal platform team or an external vendor.\n",
      "\n",
      "Dianne Marsh, Director of Engineering Tools at Netflix, states that her team's charter is to \"support our engineering teams' innovation and velocity. We don't build, bake, or deploy anything for these teams, nor do we manage their configurations. Instead, we build tools to enable self-service. It's okay for people to be dependent on our tools, but it's important that they don't become dependent on us.\"\n",
      "\n",
      "Often, these platform teams provide other services to help their customers learn their technology, migrate off of other technologies, and even provide coaching and consulting to help elevate the state of the practice inside the organization. These shared services also facilitate standardization, which enable engineers to quickly become productive, even if they switch between teams. For instance, if every product team chooses a different toolchain, engineers may have to learn an entirely new set of technologies to do their work, putting the team goals ahead of the global goals.\n",
      "\n",
      "In organizations where teams can only use approved tools, we can start by removing this requirement for a few teams, such as the transformation team, so that we can experiment and discover what capabilities make those teams more productive.\n",
      "\n",
      "Internal shared services teams should continually look for internal toolchains that are widely being adopted in the organization, deciding which ones make sense to be supported centrally and made available to everyone. In general, taking something that's already working somewhere and expanding its usage is far more likely to succeed than building these capabilities from scratch.§\n",
      "\n",
      "## EMBED OPS ENGINEERS INTO OUR SERVICE TEAMS\n",
      "\n",
      "Another way we can enable more market-oriented outcomes is by enabling product teams to become more self-sufficient by embedding Operations engineers within them, thus reducing their reliance on centralized Operations. These product teams may also be completely responsible for service delivery and service support.\n",
      "\n",
      "By embedding Operations engineers into the Dev teams, their priorities are driven almost entirely by the goals of the product teams they are embedded in—as opposed to Ops focusing inwardly on solving their own problems. As a result, Ops engineers become more closely connected to their internal and external customers. Furthermore, the product teams often have the budget to fund the hiring of these Ops engineers, although interviewing and hiring decisions will likely still be done from the centralized Operations group, to ensure consistency and quality of staff.\n",
      "\n",
      "Jason Cox said, \"In many parts of Disney we have embedded Ops (system engineers) inside the product teams in our business units, along with inside Development, Test, and even Information Security. It has totally changed the dynamics of how we work. As Operations Engineers, we create the tools and capabilities that transform the way people work, and even the way they think. In traditional Ops, we merely drove the train that someone else built. But in modern Operations Engineering, we not only help build the train, but also the bridges that the trains roll on.\"\n",
      "\n",
      "For new large Development projects, we may initially embed Ops engineers into those teams. Their work may include helping decide what to build and how to build it, influencing the product architecture, helping influence internal and external technology choices, helping create new capabilities in our internal platforms, and maybe even generating new operational capabilities. After the product is released to production, embedded Ops engineers may help with the production responsibilities of the Dev team.\n",
      "\n",
      "They will take part in all of the Dev team rituals, such as planning meetings, daily standups, and demonstrations where the team shows off new features and decides which ones to ship. As the need for Ops knowledge and capabilities decreases, Ops engineers may transition to different projects or engagements, following the general pattern that the composition within product teams changes throughout its life cycle.\n",
      "\n",
      "This paradigm has another important advantage: pairing Dev and Ops engineers together is an extremely efficient way to cross-train operations knowledge and expertise into a service team. It can also have the powerful benefit of transforming operations knowledge into automated code that can be far more reliable and widely reused.\n",
      "\n",
      "## ASSIGN AN OPS LIAISON TO EACH SERVICE TEAM\n",
      "\n",
      "For a variety of reasons, such as cost and scarcity, we may be unable to embed Ops engineers into every product team. However, we can get many of the same benefits by assigning a designated liaison for each product team.\n",
      "\n",
      "At Etsy, this model is called \"designated Ops.\" Their centralized Operations group continues to manage all the environments—not just production environments but also pre-production environments—to help ensure they remain consistent. The designated Ops engineer is responsible for understanding:\n",
      "\n",
      "  * What the new product functionality is and why we're building it\n",
      "  * How it works as it pertains to operability, scalability, and observability (diagramming is strongly encouraged)\n",
      "  * How to monitor and collect metrics to ensure the progress, success, or failure of the functionality\n",
      "  * Any departures from previous architectures and patterns, and the justifications for them\n",
      "  * Any extra needs for infrastructure and how usage will affect infrastructure capacity\n",
      "  * Feature launch plans\n",
      "\n",
      "Furthermore, just like in the embedded Ops model, this liaison attends the team standups, integrating their needs into the Operations road map and performing any needed tasks. We rely on these liaisons to escalate any resource contention or prioritization issue. By doing this, we identify any resource or time conflicts that should be evaluated and prioritized in the context of wider organizational goals.\n",
      "\n",
      "Assigning Ops liaisons allows us to support more product teams than the embedded Ops model. Our goal is to ensure that Ops is not a constraint for the product teams. If we find that Ops liaisons are stretched too thin, preventing the product teams from achieving their goals, then we will likely need to either reduce the number of teams each liaison supports or temporarily embed an Ops engineer into specific teams.\n",
      "\n",
      "## INTEGRATE OPS INTO DEV RITUALS\n",
      "\n",
      "When Ops engineers are embedded or assigned as liaisons into our product teams, we can integrate them into our Dev team rituals. In this section, our goal is to help Ops engineers and other non-developers better understand the existing Development culture and proactively integrate them into all aspects of planning and daily work. As a result, Operations is better able to plan and radiate any needed knowledge into the product teams, influencing work long before it gets into production. The following sections describe some of the standard rituals used by Development teams using agile methods and how we would integrate Ops engineers into them. By no means are agile practices a prerequisite for this step—as Ops engineers, our goal is to discover what rituals the product teams follow, integrate into them, and add value to them.¶\n",
      "\n",
      "As Ernest Mueller observed, \"I believe DevOps works a lot better if Operations teams adopt the same agile rituals that Dev teams have used—we've had fantastic successes solving many problems associated with Ops pain points, as well as integrating better with Dev teams.\"\n",
      "\n",
      "### INVITE OPS TO OUR DEV STANDUPS\n",
      "\n",
      "One of the Dev rituals popularized by Scrum is the daily standup, a quick meeting where everyone on the team gets together and presents to each other three things: what was done yesterday, what is going to be done today, and what is preventing you from getting your work done.**\n",
      "\n",
      "The purpose of this ceremony is to radiate information throughout the team and to understand the work that is being done and is going to be done. By having team members present this information to each other, we learn about any tasks that are experiencing roadblocks and discover ways to help each other move our work toward completion. Furthermore, by having managers present, we can quickly resolve prioritization and resource conflicts.\n",
      "\n",
      "A common problem is that this information is compartmentalized within the Development team. By having Ops engineers attend, Operations can gain an awareness of the Development team's activities, enabling better planning and preparation—for instance, if we discover that the product team is planning a big feature rollout in two weeks, we can ensure that the right people and resources are available to support the rollout. Alternatively, we may highlight areas where closer interaction or more preparation is needed (e.g., creating more monitoring checks or automation scripts). By doing this, we create the conditions where Operations can help solve our current team problems (e.g., improving performance by tuning the database, instead of optimizing code) or future problems before they turn into a crisis (e.g., creating more integration test environments to enable performance testing).\n",
      "\n",
      "### INVITE OPS TO OUR DEV RETROSPECTIVES\n",
      "\n",
      "Another widespread agile ritual is the retrospective. At the end of each development interval, the team discusses what was successful, what could be improved, and how to incorporate the successes and improvements in future iterations or projects. The team comes up with ideas to make things better and reviews experiments from the previous iteration. This is one of the primary mechanisms where organizational learning and the development of countermeasures occurs, with resulting work implemented immediately or added to the team's backlog.\n",
      "\n",
      "Having Ops engineers attend our project team retrospectives means they can also benefit from any new learnings. Furthermore, when there is a deployment or release in that interval, Operations should present the outcomes and any resulting learnings, creating feedback into the product team. By doing this, we can improve how future work is planned and performed, improving our outcomes. Examples of feedback that Operations can bring to a retrospective include:\n",
      "\n",
      "  * \"Two weeks ago, we found a monitoring blind-spot and agreed on how to fix it. It worked. We had an incident last Tuesday, and we were able to quickly detect and correct it before any customers were impacted.\"\n",
      "  * \"Last week's deployment was one of the most difficult and lengthy we've had in over a year. Here are some ideas on how it can be improved.\"\n",
      "  * \"The promotion campaign we did last week was far more difficult than we thought it would be, and we should probably not make an offer like that again. Here are some ideas on other offers we can make to achieve our goals.\"\n",
      "  * \"During the last deployment, the biggest problem we had was our firewall rules are now thousands of lines long, making it extremely difficult and risky to change. We need to re-architect how we prevent unauthorized network traffic.\"\n",
      "\n",
      "Feedback from Operations helps our product teams better see and understand the downstream impact of decisions they make. When there are negative outcomes, we can make the changes necessary to prevent them in the future. Operations feedback will also likely identify more problems and defects that should be fixed—it may even uncover larger architectural issues that need to be addressed.\n",
      "\n",
      "The additional work identified during project team retrospectives falls into the broad category of improvement work, such as fixing defects, refactoring, and automating manual work. Product managers and project managers may want to defer or deprioritize improvement work in favor of customer features.\n",
      "\n",
      "However, we must remind everyone that improvement of daily work is more important than daily work itself, and that all teams must have dedicated capacity for this (e.g., reserving 20% of all cycles for improvement work, scheduling one day per week or one week per month, etc.). Without doing this, the productivity of the team will almost certainly grind to a halt under the weight of its own technical and process debt.\n",
      "\n",
      "### MAKE RELEVANT OPS WORK VISIBLE ON SHARED KANBAN BOARDS\n",
      "\n",
      "Often, Development teams will make their work visible on a project board or kanban board. It's far less common, however, for work boards to show the relevant Operations work that must be performed in order for the application to run successfully in production, where customer value is actually created. As a result, we are not aware of necessary Operations work until it becomes an urgent crisis, jeopardizing deadlines or creating a production outage.\n",
      "\n",
      "Because Operations is part of the product value stream, we should put the Operations work that is relevant to product delivery on the shared kanban board. This enables us to more clearly see all the work required to move our code into production, as well as keep track of all Operations work required to support the product. Furthermore, it enables us to see where Ops work is blocked and where work needs escalation, highlighting areas where we may need improvement.\n",
      "\n",
      "Kanban boards are an ideal tool to create visibility, and visibility is a key component in properly recognizing and integrating Ops work into all the relevant value streams. When we do this well, we achieve market-oriented outcomes, regardless of how we've drawn our organization charts.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Throughout this chapter, we explored ways to integrate Operations into the daily work of Development, and looked at how to make our work more visible to Operations. To accomplish this, we explored three broad strategies, including creating self-service capabilities to enable developers in service teams to be productive, embedding Ops engineers into the service teams, and assigning Ops liaisons to the service teams when embedding Ops engineers was not possible. Lastly, we described how Ops engineers can integrate with the Dev team through inclusion in their daily work, including daily standups, planning, and retrospectives.\n",
      "\n",
      "## PART II CONCLUSION\n",
      "\n",
      "In Part II: _Where to Start_ , we explored a variety of ways to think about DevOps transformations, including how to choose where to start, relevant aspects of architecture and organizational design, and how to organize our teams. We also explored how to integrate Ops into all aspects of Dev planning and daily work.\n",
      "\n",
      "In Part III: The First Way, _The Technical Practices of Flow_ , we will now start to explore how to implement the specific technical practices to realize the principles of flow, which enable the fast flow of work from Development to Operations without causing chaos and disruption downstream.\n",
      "\n",
      "* * *\n",
      "\n",
      "† The terms _platform_ , _shared service_ , and _toolchain_ will be used interchangeably in this book.\n",
      "\n",
      "‡ Ernest Mueller observed, \"At Bazaarvoice, the agreement was that these platform teams that make tools accept requirements, but not work from other teams.\"\n",
      "\n",
      "§ After all, designing a system upfront for re-use is a common and expensive failure mode of many enterprise architectures.\n",
      "\n",
      "¶ However, if we discover that the entire Development organization merely sits at their desks all day without ever talking to each other, we may have to find a different way to engage them, such as buying them lunch, starting a book club, taking turns doing \"lunch and learn\" presentations, or having conversations to discover what everyone's biggest problems are, so that we can figure out how we can make their lives better.\n",
      "\n",
      "** Scrum is an agile development methodology, described as \"a flexible, holistic product development strategy where a development team works as a unit to reach a common goal.\" It was first fully described by Ken Schwaber and Mike Beedle in the book _Agile Software Development with Scrum_. In this book, we use the term \"agile development\" or \"iterative development\" to encompass the various techniques used by special methodologies such as Agile and Scrum.\n",
      "\n",
      "# Part III\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In Part III, our goal is to create the technical practices and architecture required to enable and sustain the fast flow of work from Development into Operations without causing chaos and disruption to the production environment or our customers. This means we need to reduce the risk associated with deploying and releasing changes into production. We will do this by implementing a set of technical practices known as _continuous delivery_.\n",
      "\n",
      "Continuous delivery includes creating the foundations of our automated deployment pipeline, ensuring that we have automated tests that constantly validate that we are in a deployable state, having developers integrate their code into trunk daily, and architecting our environments and code to enable low-risk releases. Primary focuses within these chapters include:\n",
      "\n",
      "  * Creating the foundation of our deployment pipeline\n",
      "  * Enabling fast and reliable automated testing\n",
      "  * Enabling and practicing continuous integration and testing\n",
      "  * Automating, enabling, and architecting for low-risk releases\n",
      "\n",
      "Implementing these practices reduces the lead time to get production-like environments, enables continuous testing that gives everyone fast feedback on their work, enables small teams to safely and independently develop, test, and deploy their code into production, and makes production deployments and releases a routine part of daily work.\n",
      "\n",
      "Furthermore, integrating the objectives of QA and Operations into everyone's daily work reduces firefighting, hardship, and toil, while making people more productive and increasing joy in the work we do. We not only improve outcomes, but our organization is better able to win in the marketplace.\n",
      "\n",
      "# 9Create the Foundations of Our Deployment Pipeline\n",
      "\n",
      "In order to create fast and reliable flow from Dev to Ops, we must ensure that we always use production-like environments at every stage of the value stream. Furthermore, these environments must be created in an automated manner, ideally on demand from scripts and configuration information stored in version control, and entirely self-serviced, without any manual work required from Operations. Our goal is to ensure that we can re-create the entire production environment based on what's in version control.\n",
      "\n",
      "All too often, the only time we discover how our applications perform in anything resembling a production-like environment is during production deployment—far too late to correct problems without the customer being adversely impacted. An illustrative example of the spectrum of problems that can be caused by inconsistently built applications and environments is the Enterprise Data Warehouse program led by Em Campbell-Pretty at a large Australian telecommunications company in 2009. Campbell-Pretty became the general manager and business sponsor for this $200 million program, inheriting responsibility for all the strategic objectives that relied upon this platform.\n",
      "\n",
      "In her presentation at the 2014 DevOps Enterprise Summit, Campbell-Pretty explained, \"At the time, there were ten streams of work in progress, all using waterfall processes, and all ten streams were significantly behind schedule. Only one of the ten streams had successfully reached User Acceptance Testing [UAT] on schedule, and it took another six months for that stream to complete UAT, with the resulting capability falling well short of business expectations. This under-performance was the main catalyst for the department's Agile transformation.\"\n",
      "\n",
      "However, after using Agile for nearly a year, they experienced only small improvements, still falling short of their needed business outcomes. Campbell-Pretty held a program-wide retrospective and asked, \"After reflecting on all our experiences over the last release, what are things we could do that would double our productivity?\"\n",
      "\n",
      "Throughout the project, there was grumbling about the \"lack of business engagement.\" However, during the retrospective, \"improve availability of environments\" was at the top of the list. In hindsight, it was obvious—Development teams needed provisioned environments in order to begin work, and were often waiting up to eight weeks.\n",
      "\n",
      "They created a new integration and build team that was responsible for \"building quality into our processes, instead of trying to inspect quality after the fact.\" It was initially comprised of database administrators (DBAs) and automation specialists tasked with automating their environment creation process. The team quickly made a surprising discovery: only 50% of the source code in their development and test environments matched what was running in production.\n",
      "\n",
      "Campbell-Pretty observed, \"Suddenly, we understood why we encountered so many defects each time we deployed our code into new environments. In each environment, we kept fixing forward, but the changes we made were not being put back into version control.\"\n",
      "\n",
      "The team carefully reverse-engineered all the changes that had been made to the different environments and put them all into version control. They also automated their environment creation process so they could repeatedly and correctly spin up environments.\n",
      "\n",
      "Campbell-Pretty described the results, noting that \"the time it took to get a correct environment went from eight weeks to one day. This was one of the key adjustments that allowed us to hit our objectives concerning our lead time, the cost to deliver, and the number of escaped defects that made it into production.\"\n",
      "\n",
      "Campbell-Pretty's story shows the variety of problems that can be traced back to inconsistently constructed environments and changes not being systematically put back into version control.\n",
      "\n",
      "Throughout the remainder of this chapter, we will discuss how to build the mechanisms that will enable us to create environments on demand, expand the use of version control to everyone in the value stream, make infrastructure easier to rebuild than to repair, and ensure that developers run their code in production-like environments along every stage of the software development life cycle.\n",
      "\n",
      "## ENABLE ON DEMAND CREATION OF DEV, TEST, AND PRODUCTION ENVIRONMENTS\n",
      "\n",
      "As seen in the enterprise data warehouse example above, one of the major contributing causes of chaotic, disruptive, and sometimes even catastrophic software releases, is the first time we ever get to see how our application behaves in a production-like environment with realistic load and production data sets is during the release.† In many cases, development teams may have requested test environments in the early stages of the project.\n",
      "\n",
      "However, when there are long lead times required for Operations to deliver test environments, teams may not receive them soon enough to perform adequate testing. Worse, test environments are often mis-configured or are so different from our production environments that we still end up with large production problems despite having performed pre-deployment testing.\n",
      "\n",
      "In this step, we want developers to run production-like environments on their own workstations, created on demand and self-serviced. By doing this, developers can run and test their code in production-like environments as part of their daily work, providing early and constant feedback on the quality their work.\n",
      "\n",
      "Instead of merely documenting the specifications of the production environment in a document or on a wiki page, we create a common build mechanism that creates all of our environments, such as for development, test, and production. By doing this, anyone can get production-like environments in minutes, without opening up a ticket, let alone having to wait weeks.‡\n",
      "\n",
      "To do this requires defining and automating the creation of our known, good environments, __which are stable, secure, and in a risk-reduced state, embodying the collective knowledge of the organization. All our requirements are embedded, not in documents or as knowledge in someone's head, but codified in our automated environment build process.\n",
      "\n",
      "Instead of Operations manually building and configuring the environment, we can use automation for any or all of the following:\n",
      "\n",
      "  * Copying a virtualized environment (e.g., a VMware image, running a Vagrant script, booting an Amazon Machine Image file in EC2)\n",
      "  * Building an automated environment creation process that starts from \"bare metal\" (e.g., PXE install from a baseline image)\n",
      "  * Using \"infrastructure as code\" configuration management tools (e.g., Puppet, Chef, Ansible, Salt, CFEngine, etc.)\n",
      "  * Using automated operating system configuration tools (e.g., Solaris Jumpstart, Red Hat Kickstart, Debian preseed)\n",
      "  * Assembling an environment from a set of virtual images or containers (e.g., Vagrant, Docker)\n",
      "  * Spinning up a new environment in a public cloud (e.g., Amazon Web Services, Google App Engine, Microsoft Azure), private cloud, or other PaaS (platform as a service, such as OpenStack or Cloud Foundry, etc.).\n",
      "\n",
      "Because we've carefully defined all aspects of the environment ahead of time, we are not only able to create new environments quickly, but also ensure that these environments will be stable, reliable, consistent, and secure. This benefits everyone.\n",
      "\n",
      "Operations benefits from this capability, to create new environments quickly, because automation of the environment creation process enforces consistency and reduces tedious, error-prone manual work. Furthermore, Development benefits by being able to reproduce all the necessary parts of the production environment to build, run, and test their code on their workstations. By doing this, we enable developers to find and fix many problems, even at the earliest stages of the project, as opposed to during integration testing or worse, in production.\n",
      "\n",
      "By providing developers an environment they fully control, we enable them to quickly reproduce, diagnose, and fix defects, safely isolated from production services and other shared resources. They can also experiment with changes to the environments, as well as to the infrastructure code that creates it (e.g., configuration management scripts), further creating shared knowledge between Development and Operations.§\n",
      "\n",
      "## CREATE OUR SINGLE REPOSITORY OF TRUTH FOR THE ENTIRE SYSTEM\n",
      "\n",
      "In the previous step, we enabled the on demand creation of the development, test, and production environments. Now we must ensure that all parts of our software system.\n",
      "\n",
      "For decades, comprehensive use of version control has increasingly become a mandatory practice of individual developers and development teams.¶ A version control system records changes to files or sets of files stored within the system. This can be source code, assets, or other documents that may be part of a software development project. We make changes in groups called commits or revisions. Each revision, along with metadata such as who made the change and when, is stored within the system in one way or another, allowing us to commit, compare, merge, and restore past revisions to objects to the repository. It also minimizes risks by establishing a way to revert objects in production to previous versions. (In this book, the following terms will be used interchangeably: checked in to version control, committed into version control, code commit, change commit, commit.)\n",
      "\n",
      "When developers put all their application source files and configurations in version control, it becomes the single repository of truth that contains the precise intended state of the system. However, because delivering value to the customer requires both our code and the environments they run in, we need our environments in version control as well. In other words, version control is for everyone in our value stream, including QA, Operations, Infosec, as well as developers. By putting all production artifacts into version control, our version control repository enables us to repeatedly and reliably reproduce all components of our working software system—this includes our applications and production environment, as well as all of our pre-production environments.\n",
      "\n",
      "To ensure that we can restore production service repeatedly and predictably (and, ideally, quickly) even when catastrophic events occur, we must check in the following assets to our shared version control repository:\n",
      "\n",
      "  * All application code and dependencies (e.g., libraries, static content, etc.)\n",
      "  * Any script used to create database schemas, application reference data, etc.\n",
      "  * All the environment creation tools and artifacts described in the previous step (e.g., VMware or AMI images, Puppet or Chef recipes, etc.)\n",
      "  * Any file used to create containers (e.g., Docker or Rocket definition or composition files)\n",
      "  * All supporting automated tests and any manual test scripts\n",
      "  * Any script that supports code packaging, deployment, database migration, and environment provisioning\n",
      "  * All project artifacts (e.g., requirements documentation, deployment procedures, release notes, etc.)\n",
      "  * All cloud configuration files (e.g., AWS Cloudformation templates, Microsoft Azure Stack DSC files, OpenStack HEAT)\n",
      "  * Any other script or configuration information required to create infrastructure that supports multiple services (e.g., enterprise service buses, database management systems, DNS zone files, configuration rules for firewalls, and other networking devices).**\n",
      "\n",
      "We may have multiple repositories for different types of objects and services, where they are labelled and tagged alongside our source code. For instance, we may store large virtual machine images, ISO files, compiled binaries, and so forth in artifact repositories (e.g., Nexus, Artifactory). Alternatively, we may put them in blob stores (e.g., Amazon S3 buckets) or put Docker images into Docker registries, and so forth.\n",
      "\n",
      "It is not sufficient to merely be able to re-create any previous state of the production environment; we must also be able to re-create the entire pre-production and build processes as well. Consequently, we need to put into version control everything relied upon by our build processes, including our tools (e.g., compilers, testing tools) and the environments they depend upon.††\n",
      "\n",
      "In Puppet Labs' _2014 State of DevOps Report_ , the use of version control by Ops was the highest predictor of both IT performance and organizational performance. In fact, whether Ops used version control was a higher predictor for both IT performance and organizational performance than whether Dev used version control.\n",
      "\n",
      "The findings from Puppet Labs' _2014 State of DevOps Report_ underscores the critical role version control plays in the software development process. We now know when all application and environment changes are recorded in version control, it enables us to not only quickly see all changes that might have contributed to a problem, but also provides the means to roll back to a previous known, running state, allowing us to more quickly recover from failures.\n",
      "\n",
      "But why does using version control for our environments predict IT and organizational performance better than using version control for our code?\n",
      "\n",
      "Because in almost all cases, there are orders of magnitude more configurable settings in our environment than in our code. Consequently, it is the environment that needs to be in version control the most.‡‡\n",
      "\n",
      "Version control also provides a means of communication for everyone working in the value stream—having Development, QA, Infosec, and Operations able to see each other's changes helps reduce surprises, creates visibility into each other's work, and helps build and reinforce trust. See Appendix 7.\n",
      "\n",
      "## MAKE INFRASTRUCTURE EASIER TO REBUILD THAN TO REPAIR\n",
      "\n",
      "When we can quickly rebuild and re-create our applications and environments on demand, we can also quickly rebuild them instead of repairing them when things go wrong. Although this is something that almost all large-scale web operations do (i.e., more than one thousand servers), we should also adopt this practice even if we have only one server in production.\n",
      "\n",
      "Bill Baker, a distinguished engineer at Microsoft, quipped that we used to treat servers like pets: \"You name them and when they get sick, you nurse them back to health. [Now] servers are [treated] like cattle. You number them and when they get sick, you shoot them.\"\n",
      "\n",
      "By having repeatable environment creation systems, we are able to easily increase capacity by adding more servers into rotation (i.e., horizontal scaling). We also avoid the disaster that inevitably results when we must restore service after a catastrophic failure of irreproducible infrastructure, created through years of undocumented and manual production changes.\n",
      "\n",
      "To ensure consistency of our environments, whenever we make production changes (configuration changes, patching, upgrading, etc.), those changes need to be replicated everywhere in our production and pre-production environments, as well as in any newly created environments.\n",
      "\n",
      "Instead of manually logging into servers and making changes, we must make changes in a way that ensures all changes are replicated everywhere automatically and that all our changes are put into version control.\n",
      "\n",
      "We can rely on our automated configuration systems to ensure consistency (e.g., Puppet, Chef, Ansible, Salt, Bosh, etc.), or we can create new virtual machines or containers from our automated build mechanism and deploy them into production, destroying the old ones or taking them out of rotation.§§\n",
      "\n",
      "The latter pattern is what has become known as _immutable infrastructure_ , where manual changes to the production environment are no longer allowed—the only way production changes can be made is to put the changes into version control and re-create the code and environments from scratch. By doing this, no variance is able to creep into production.\n",
      "\n",
      "To prevent uncontrolled configuration variances, we may disable remote logins to production servers¶¶ or routinely kill and replace production instances, ensuring that manually-applied production changes are removed. This action motivates everyone to put their changes in the correct way through version control. By applying such measures, we are systematically reducing the ways our infrastructure can drift from our known, good states (e.g., configuration drift, fragile artifacts, works of art, snowflakes, and so forth).\n",
      "\n",
      "Also, we must keep our pre-production environments up to date—specifically, we need developers to stay running on our most current environment. Developers will often want to keep running on older environments because they fear environment updates may break existing functionality. However, we want to update them frequently so we can find problems at the earliest part of the life cycle.***\n",
      "\n",
      "## MODIFY OUR DEFINITION OF DEVELOPMENT \"DONE\" TO INCLUDE RUNNING IN PRODUCTION-LIKE ENVIRONMENTS\n",
      "\n",
      "Now that our environments can be created on demand and everything is checked in to version control, our goal is to ensure that these environments are being used in the daily work of Development. We need to verify that our application runs as expected in a production-like environment long before the end of the project or before our first production deployment.\n",
      "\n",
      "Most modern software development methodologies prescribe short and iterative development intervals, as opposed to the big bang approach (e.g., the waterfall `model). In general, the longer the interval between deployment, the worse the outcomes. For example, in the Scrum methodology a _sprint_ is a time-boxed development interval (typically one month or less) within which we are required to be done, widely defined as when we have \"working and potentially shippable code.\"\n",
      "\n",
      "Our goal is to ensure that Development and QA are routinely integrating the code with production-like environments at increasingly frequent intervals throughout the project.††† We do this by expanding the definition of \"done\" beyond just correct code functionality (addition in bold text): at the end of each development interval, we have integrated, tested, working and potentially shippable code, **demonstrated in a production-like environment**.\n",
      "\n",
      "In other words, we will only accept development work as done when it can be successfully built, deployed, and confirmed that it runs as expected in a production-like environment, instead of merely when a developer believes it to be done—ideally, it runs under a production-like load with a production-like dataset, long before the end of a sprint. This prevents situations where a feature is called done merely because a developer can run it successfully on their laptop but nowhere else.\n",
      "\n",
      "By having developers write, test, and run their own code in a production-like environment, the majority of the work to successfully integrate our code and environments happens during our daily work, instead of at the end of the release. By the end of our first interval, our application can be demonstrated to run correctly in a production-like environment, with the code and environment having been integrated together many times over, ideally with all the steps automated (no manual tinkering required).\n",
      "\n",
      "Better yet, by the end of the project, we will have successfully deployed and run our code in production-like environments hundreds or even thousands of times, giving us confidence that most of our production deployment problems have been found and fixed.\n",
      "\n",
      "Ideally, we use the same tools, such as monitoring, logging, and deployment, in our pre-production environments as we do in production. By doing this, we have familiarity and experience that will help us smoothly deploy and run, as well as diagnose and fix, our service when it is in production.\n",
      "\n",
      "By enabling Development and Operations to gain a shared mastery of how the code and environment interact, and practicing deployments early and often, we significantly reduce the deployment risks that are associated with production code releases. This also allows us to eliminate an entire class of operational and security defects and architectural problems that are usually caught too late in the project to fix.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "The fast flow of work from Development to Operations requires that anyone can get production-like environments on demand. By allowing developers to use production-like environments even at the earliest stages of a software project, we significantly reduce the risk of production problems later. This is one of many practices that demonstrate how Operations can make developers far more productive. We enforce the practice of developers running their code in production-like environments by incorporating it into the definition of \"done.\"\n",
      "\n",
      "Furthermore, by putting all production artifacts into version control, we have a \"single source of truth\" that allows us to re-create the entire production environment in a quick, repeatable, and documented way, using the same development practices for Operations work as we do for Development work. And by making production infrastructure easier to rebuild than to repair, we make resolving problems easier and faster, as well as making it easier to expand capacity.\n",
      "\n",
      "Having these practices in place sets the stage for enabling comprehensive test automation, which is explored in the next chapter.\n",
      "\n",
      "* * *\n",
      "\n",
      "† In this context, environment is defined as everything in the application stack except for the application, including the databases, operating systems, networking, virtualization, and all associated configurations.\n",
      "\n",
      "‡ Most developers want to test their code, and they have often gone to extreme lengths to obtain test environments to do so. Developers have been known to reuse old test environments from previous projects (often years old) or ask someone who has a reputation of being able to find one—they just won't ask where it came from, because, invariably, someone somewhere is now missing a server.\n",
      "\n",
      "§ Ideally, we should be finding errors before integration testing when is too late in the testing cycle to create fast feedback for developers. If we are unable to do so, we likely have an architectural issue that needs to be addressed. Designing our systems for testability, to include the ability to discover most defects using a non-integrated virtual environment on a development workstation, is a key part of creating an architecture that supports fast flow and feedback.\n",
      "\n",
      "¶ The first version control system was likely UPDATE on the CDC6600 (1969). Later came SCCS (1972), CMS on VMS (1978), RCS (1982), and so forth.\n",
      "\n",
      "** One may observe that version control fulfills some of the ITIL constructs of the Definitive Media Library (DML) and Configuration Management Database (CMDB), inventorying everything required to re-create the production environment.\n",
      "\n",
      "†† In future steps, we will also check in to version control all the supporting infrastructure we build, such as the automated test suites and our continuous integration and deployment pipeline infrastructure.\n",
      "\n",
      "‡‡ Anyone who has done a code migration for an ERP system (e.g., SAP, Oracle Financials, etc.) may recognize the following situation: When a code migration fails, it is rarely due to a coding error. Instead, it's far more likely that the migration failed due to some difference in the environments, such as between Development and QA or QA and Production.\n",
      "\n",
      "§§ At Netflix, the average age of Netflix AWS instance is twenty-four days, with 60% being less than one week old.\n",
      "\n",
      "¶¶ Or allow it only in emergencies, ensuring that a copy of the console log is automatically emailed to the operations team.\n",
      "\n",
      "*** The entire application stack and environment can be bundled into containers, which can enable unprecedented simplicity and speed across the entire deployment pipeline.\n",
      "\n",
      "††† The term _integration_ has many slightly different usages in Development and Operations. In Development, integration typically refers to code integration, which is the integration of multiple code branches into trunk in version control. In continuous delivery and DevOps, _integration testing_ refers to the testing of the application in a production-like environment or integrated test environment.\n",
      "\n",
      "# 10Enable Fast and Reliable Automated Testing\n",
      "\n",
      "At this point, Development and QA are using production-like environments in their daily work, and we are successfully integrating and running our code into a production-like environment for every feature that is accepted, with all changes checked in to version control. However, we are likely to get undesired outcomes if we find and fix errors in a separate test phase, executed by a separate QA department only after all development has been completed. And, if testing is only performed a few times a year, developers learn about their mistakes months after they introduced the change that caused the error. By then, the link between cause and effect has likely faded, solving the problem requires firefighting and archaeology, and, worst of all, our ability to learn from the mistake and integrate it into our future work is significantly diminished.\n",
      "\n",
      "Automated testing addresses another significant and unsettling problem. Gary Gruver observes that \"without automated testing, the more code we write, the more time and money is required to test our code—in most cases, this is a totally unscalable business model for any technology organization.\"\n",
      "\n",
      "Although Google now undoubtedly exemplifies a culture that values automated testing at scale, this wasn't always the case. In 2005, when Mike Bland joined the organization, deploying to Google.com was often extremely problematic, especially for the Google Web Server (GWS) team.\n",
      "\n",
      "As Bland explains, \"The GWS team had gotten into a position in the mid 2000s where it was extremely difficult to make changes to the web server, a C++ application that handled all requests to Google's home page and many other Google web pages. As important and prominent as Google.com was, being on the GWS team was not a glamorous assignment—it was often the dumping ground for all the different teams who were creating various search functionality, all of whom were developing code independently of each other. They had problems such as builds and tests taking too long, code being put into production without being tested, and teams checking in large, infrequent changes that conflicted with those from other teams.\"\n",
      "\n",
      "The consequences of this were large—search results could have errors or become unacceptably slow, affecting thousands of search queries on Google.com. The potential result was not only loss of revenue, but customer trust.\n",
      "\n",
      "Bland describes how it affected developers deploying changes, \"Fear became the mind-killer. Fear stopped new team members from changing things because they didn't understand the system. But fear also stopped experienced people from changing things because they understood it all too well.\"† Bland was part of the group that was determined to solve this problem.\n",
      "\n",
      "GWS team lead Bharat Mediratta believed automated testing would help. As Bland describes, \"They created a hard line: no changes would be accepted into GWS without accompanying automated tests. They set up a continuous build and religiously kept it passing. They set up test coverage monitoring and ensured that their level of test coverage went up over time. They wrote up policy and testing guides, and insisted that contributors both inside and outside the team follow them.\"\n",
      "\n",
      "The results were startling. As Bland notes, \"GWS quickly became one of the most productive teams in the company, integrating large numbers of changes from different teams every week while maintaining a rapid release schedule. New team members were able to make productive contributions to this complex system quickly, thanks to good test coverage and code health. Ultimately, their radical policy enabled the Google.com home page to quickly expand its capabilities and thrive in an amazingly fast-moving and competitive technology landscape.\"\n",
      "\n",
      "But GWS was still a relatively small team in a large and growing company. The team wanted to expand these practices across the entire organization. Thus, the Testing Grouplet was born, an informal group of engineers who wanted to elevate automated testing practices across the entire organization. Over the next five years, they helped replicate this culture of automated testing across all of Google.‡\n",
      "\n",
      "Now when any Google developer commits code, it is automatically run against a suite of hundreds of thousands of automated tests. If the code passes, it is automatically merged into trunk, ready to be deployed into production. Many Google properties build hourly or daily, then pick which builds to release; others adopt a continuous \"Push on Green\" delivery philosophy.\n",
      "\n",
      "The stakes are higher than ever—a single code deployment error at Google can take down every property, all at the same time (such as a global infrastructure change or when a defect is introduced into a core library that every property depends upon).\n",
      "\n",
      "Eran Messeri, an engineer in the Google Developer Infrastructure group, notes, \"Large failures happen occasionally. You'll get a ton of instant messages and engineers knocking on your door. [When the deployment pipeline is broken,] we need to fix it right away, because developers can no longer commit code. Consequently, we want to make it very easy to roll back.\"\n",
      "\n",
      "What enables this system to work at Google is engineering professionalism and a high-trust culture that assumes everyone wants to do a good job, as well as the ability to detect and correct issues quickly. Messeri explains, \"There are no hard policies at Google, such as, 'If you break production for more than ten projects, you have an SLA to fix the issue within ten minutes.' Instead, there is mutual respect between teams and an implicit agreement that everyone does whatever it takes to keep the deployment pipeline running. We all know that one day, I'll break your project by accident; the next day, you may break mine.\"\n",
      "\n",
      "What Mike Bland and the Testing Grouplet team achieved has made Google one of the most productive technology organizations in the world. By 2013, automated testing and continuous integration at Google enabled over four thousand small teams to work together and stay productive, all simultaneously developing, integrating, testing, and deploying their code into production. All their code is in a single, shared repository, made up of billions of files, all being continuously built and integrated, with 50% of their code being changed each month. Some other impressive statistics on their performance include:\n",
      "\n",
      "  * 40,000 code commits/day\n",
      "  * 50,000 builds/day (on weekdays, this may exceed 90,000)\n",
      "  * 120,000 automated test suites\n",
      "  * 75 million test cases run daily\n",
      "  * 100+ engineers working on the test engineering, continuous integration, and release engineering tooling to increase developer productivity (making up 0.5% of the R&D workforce)\n",
      "\n",
      "In the remainder of this chapter, we will go through the continuous integration practices required to replicate these outcomes.\n",
      "\n",
      "## CONTINUOUSLY BUILD, TEST, AND INTEGRATE OUR CODE AND ENVIRONMENTS\n",
      "\n",
      "Our goal is to build quality into our product, even at the earliest stages, by having developers build automated tests as part of their daily work. This creates a fast feedback loop that helps developers find problems early and fix them quickly, when there are the fewest constraints (e.g., time, resources).\n",
      "\n",
      "In this step, we create automated test suites that increase the frequency of integration and testing of our code and our environments from periodic to continuous. We do this by building our deployment pipeline, which will perform integration of our code and environments and trigger a series of tests every time a new change is put into version control.§ (See figure 13.)\n",
      "\n",
      "The deployment pipeline, first defined by Jez Humble and David Farley in their book _Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation_, ensures that all code checked in to version control is automatically built and tested in a production-like environment. By doing this, we find any build, test, or integration errors as soon as a change is introduced, enabling us to fix them immediately. Done correctly, this allows us to always be assured that we are in a deployable and shippable state.\n",
      "\n",
      "To achieve this, we must create automated build and test processes that run in dedicated environments. This is critical for the following reasons:\n",
      "\n",
      "  * Our build and test process can run all the time, independent of the work habits of individual engineers.\n",
      "  * A segregated build and test process ensures that we understand all the dependencies required to build, package, run, and test our code (i.e., removing the \"it worked on the developer's laptop, but it broke in production\" problem). \n",
      "  * We can package our application to enable the repeatable installation of code and configurations into an environment (e.g., on Linux RPM, yum, npm; on Windows, OneGet; alternatively framework-specific packaging systems can be used, such as EAR and WAR files for Java, gems for Ruby, etc.). \n",
      "  * Instead of putting our code in packages, we may choose to package our applications into deployable containers (e.g., Docker, Rkt, LXD, AMIs).\n",
      "  * Environments can be made more production-like in a way that is consistent and repeatable (e.g., compilers are removed from the environment, debugging flags are turned off, etc.).\n",
      "\n",
      "Our deployment pipeline validates after every change that our code successfully integrates into a production-like environment. It becomes the platform through which testers request and certify builds during acceptance testing and usability testing, and it will run automated performance and security validations.\n",
      "\n",
      ">  **Figure 13:** The deployment pipeline (Source: Humble and Farley, _Continuous Delivery_ , 3.)\n",
      "\n",
      "Furthermore, it will be used to self-service builds to UAT (user acceptance testing), integration testing, and security testing environments. In future steps, as we evolve the deployment pipeline, it will also be used to manage all activities required to take our changes from version control to deployment.\n",
      "\n",
      "A variety of tools have been designed to provide deployment pipeline functionality, many of them open source (e.g., Jenkins, ThoughtWorks Go, Concourse, Bamboo, Microsoft Team Foundation Server, TeamCity, Gitlab CI, as well as cloud-based solutions such as Travis CI and Snap).¶\n",
      "\n",
      "We begin the deployment pipeline by running the commit stage, which builds and packages the software, runs automated unity tests, and performs additional validation such as static code analysis, duplication and test coverage analysis, and checking style.** If successful, this triggers the acceptance stage, which automatically deploys the packages created in the commit stage into a production-like environment and runs the automated acceptance tests.\n",
      "\n",
      "Once changes are accepted into version control, we want to package our code only once, so that the same packages are used to deploy code throughout our entire deployment pipeline. By doing this, code will be deployed into our integrated test and staging environments in the same way that it is deployed into production. This reduces variances that can avoid downstream errors that are difficult to diagnose (e.g., using different compilers, compiler flags, library versions, or configurations).††\n",
      "\n",
      "The goal of the deployment pipeline is to provide everyone in the value stream, especially developers, the fastest possible feedback that a change has taken us out of a deployable state. This could be a change to our code, to any of our environments, to our automated tests, or even to the deployment pipeline infrastructure (e.g., a Jenkins configuration setting).\n",
      "\n",
      "As a result, our deployment pipeline infrastructure becomes as foundational for our development processes as our version control infrastructure. Our deployment pipeline also stores the history of each code build, including information about which tests were performed on which build, which builds have been deployed to which environment, and what the test results were. In combination with the information in our version control history, we can quickly determine what caused our deployment pipeline to break and, likely, how to fix the error.\n",
      "\n",
      "This information also helps us fulfill evidence requirements for audit and compliance purposes, with evidence being automatically generated as part of daily work.\n",
      "\n",
      "Now that we have a working deployment pipeline infrastructure, we must create our _continuous integration_ practices, which require three capabilities:\n",
      "\n",
      "  * A comprehensive and reliable set of automated tests that validate we are in a deployable state.\n",
      "  * A culture that \"stops the entire production line\" when our validation tests fail.\n",
      "  * Developers working in small batches on trunk rather than long-lived feature branches.\n",
      "\n",
      "In the next section, we describe why fast and reliable automated testing is needed and how to build it.\n",
      "\n",
      "## BUILD A FAST AND RELIABLE AUTOMATED VALIDATION TEST SUITE\n",
      "\n",
      "In the previous step, we started to create the automated testing infrastructure that validates that we have a _green build_ (i.e., whatever is in version control is in a buildable and deployable state). To underscore why we need to perform this integration and testing step continuously, consider what happens when we only perform this operation periodically, such as during a nightly build process.\n",
      "\n",
      "Suppose we have a team of ten developers, with everyone checking their code into version control daily, and a developer introduces a change that breaks our nightly build and test job. In this scenario, when we discover the next day that we no longer have a green build, it will take minutes, or more likely hours, for our development team to figure out which change caused the problem, who introduced it, and how to fix it.\n",
      "\n",
      "Worse, suppose the problem wasn't caused by a code change, but was due to a test environment issue (e.g., an incorrect configuration setting somewhere). The development team may believe that they fixed the problem because all the unit tests pass, only to discover that the tests will still fail later that night.\n",
      "\n",
      "Further complicating the issue, ten more changes will have been checked in to version control by the team that day. Each of these changes has the potential to introduce more errors that could break our automated tests, further increasing the difficulty of successfully diagnosing and fixing the problem.\n",
      "\n",
      "In short, slow and periodic feedback kills. Especially for larger development teams. The problem becomes even more daunting when we have tens, hundreds, or even thousands of other developers checking their changes into version control each day. The result is that our builds and automated tests are frequently broken, and developers even stop checking their changes into version control (\"Why bother, since the builds and tests are always broken?\"). Instead they wait to integrate their code at the end of the project, resulting in all the undesired outcomes of large batch size, big bang integrations, and production deployments.‡‡\n",
      "\n",
      "To prevent this scenario, we need fast automated tests that run within our build and test environments whenever a new change is introduced into version control. In this way we can find and fix any problems immediately, as the Google Web Server example demonstrated. By doing this, we ensure our batches remains small, and, at any given point in time, we remain in a deployable state.\n",
      "\n",
      "In general, automated tests fall into one of the following categories, from fastest to slowest:\n",
      "\n",
      "  * **Unit tests** : These typically test a single method, class, or function in isolation, providing assurance to the developer that their code operates as designed. For many reasons, including the need to keep our tests fast and stateless, unit tests often \"stub out\" databases and other external dependencies (e.g., functions are modified to return static, predefined values, instead of calling the real database).§§\n",
      "  * **Acceptance tests** : These typically test the application as a whole to provide assurance that a higher level of functionality operates as designed (e.g., the business acceptance criteria for a user story, the correctness of an API), and that regression errors have not been introduced (i.e., we broke functionality that was previously operating correctly). Humble and Farley define the difference between unit and acceptance testing as, \"The aim of a unit test is to show that a single part of the application does what the programmer intends it to....The objective of acceptance tests is to prove that our application does what the customer meant it to, not that it works the way its programmers think it should.\" After a build passes our unit tests, our deployment pipeline runs it against our acceptance tests. Any build that passes our acceptance tests is then typically made available for manual testing (e.g., exploratory testing, UI testing, etc.), as well as for integration testing.\n",
      "  * **Integration tests** : Integration tests are where we ensure that our application correctly interacts with other production applications and services, as opposed to calling stubbed out interfaces. As Humble and Farley observe, \"much of the work in the SIT environment involves deploying new versions of each of the applications until they all cooperate. In this situation the smoke test is usually a fully fledged set of acceptance tests that run against the whole application.\" Integration tests are performed on builds that have passed our unit and acceptance tests. Because integration tests are often brittle, we want to minimize the number of integration tests and find as many of our defects as possible during unit and acceptance testing. The ability to use virtual or simulated versions of remote services when running acceptance tests becomes an essential architectural requirement.\n",
      "\n",
      "When facing deadline pressures, developers may stop creating unit tests as part of their daily work, regardless of how we've defined 'done.' To detect this, we may choose to measure and make visible our test coverage (as a function of number of classes, lines of code, permutations, etc.), maybe even failing our validation test suite when it drops below a certain level (e.g., when less than 80% of our classes have unit tests).¶¶\n",
      "\n",
      "Martin Fowler observes that, in general, \"a ten-minute build [and test process] is perfectly within reason...[We first] do the compilation and run tests that are more localized unit tests with the database completely stubbed out. Such tests can run very fast, keeping within the ten minute guideline. However any bugs that involve larger scale interactions, particularly those involving the real database, won't be found. The second stage build runs a different suite of tests [acceptance tests] that do hit the real database and involve more end-to-end behavior. This suite may take a couple of hours to run.\"\n",
      "\n",
      "### CATCH ERRORS AS EARLY IN OUR AUTOMATED TESTING AS POSSIBLE\n",
      "\n",
      "A specific design goal of our automated test suite is to find errors as early in the testing as possible. This is why we run faster-running automated tests (e.g., unit tests) before slower-running automated tests (e.g., acceptance and integration tests), which are both run before any manual testing.\n",
      "\n",
      "Another corollary of this principle is that any errors should be found with the fastest category of testing possible. If most of our errors are found in our acceptance and integration tests, the feedback we provide to developers is orders of magnitude slower than with unit tests—and integration testing requires using scarce and complex integration test environments, which can only be used by one team at a time, further delaying feedback.\n",
      "\n",
      "Furthermore, not only are errors detected during integration testing difficult and time-consuming for developers to reproduce, even validating that it has been fixed is difficult (i.e., a developer creates a fix but then needs to wait four hours to learn whether the integration tests now pass).\n",
      "\n",
      "Therefore, whenever we find an error with an acceptance or integration test, we should create a unit test that could find the error faster, earlier, and cheaper. Martin Fowler described the notion of the \"ideal testing pyramid,\" where we are able to catch most of our errors using our unit tests. (See figure 14.) In contrast, in many testing programs the inverse is true, where most of the investment is in manual and integration testing.\n",
      "\n",
      "  **Figure 14:** The ideal and non-ideal automated testing pyramids (Source: Martin Fowler, \"TestPyramid.\")\n",
      "\n",
      "If we find that unit or acceptance tests are too difficult and expensive to write and maintain, it's likely that we have an architecture that is too tightly-coupled, where strong separation between our module boundaries no longer exist (or maybe never existed). In this case, we will need to create a more loosely-coupled system so modules can be independently tested without integration environments. Acceptance test suites for even the most complex applications that run in minutes are possible.\n",
      "\n",
      "### ENSURE TESTS RUN QUICKLY (IN PARALLEL, IF NECESSARY)\n",
      "\n",
      "Because we want our tests to run quickly, we need to design our tests to run in parallel, potentially across many different servers. We may also want to run different categories of tests in parallel. For example, when a build passes our acceptance tests, we may run our performance testing in parallel with our security testing, as shown in figure 15. We may or may not allow manual exploratory testing until the build has passed all our automated tests—which enables faster feedback, but may also allow manual testing on builds that will eventually fail.\n",
      "\n",
      "We make any build that passes all our automated tests available to use for exploratory testing, as well as for other forms of manual or resource-intensive testing (such as performance testing). We want to do all such testing as frequently as possible and practical, either continually or on a schedule.\n",
      "\n",
      "  **Figure 15:** Running automated and manual tests in parallel   \n",
      "(Source: Humble and Farley, _Continuous Delivery_ , Kindle edition, location 3868.)\n",
      "\n",
      "Any tester (which includes all our developers) should use the latest build that has passed all the automated tests, as opposed to waiting for developers to flag a specific build as ready to test. By doing this, we ensure that testing happens as early in the process as possible.\n",
      "\n",
      "### WRITE OUR AUTOMATED TESTS BEFORE WE WRITE THE CODE (\"TEST-DRIVEN DEVELOPMENT\")\n",
      "\n",
      "One of the most effective ways to ensure we have reliable automated testing, is to write those tests as part of our daily work, using techniques such as _test-driven development_ (TDD) and _acceptance test-driven development_ (ATDD). This is when we begin every change to the system by first writing an automated test that validates the expected behavior _fails_ , and then we write the code to make the tests pass.\n",
      "\n",
      "This technique was developed by Kent Beck in the late 1990s as part of Extreme Programming, and has the following three steps:\n",
      "\n",
      "  1. Ensure the tests fail. \"Write a test for the next bit of functionality you want to add.\" Check in.\n",
      "  2. Ensure the tests pass. \"Write the functional code until the test passes.\"Check in.\n",
      "  3. \"Refactor both new and old code to make it well structured.\"Ensure the tests pass. Check in again.\n",
      "\n",
      "These automated test suites are checked in to version control alongside our code, which provides a living, up-to-date specification of the system. Developers wishing to understand how to use the system can look at this test suite to find working examples of how to use the system's API.***\n",
      "\n",
      "### AUTOMATE AS MANY OF OUR MANUAL TESTS AS POSSIBLE\n",
      "\n",
      "Our goal is to find as many code errors through our automated test suites, reducing our reliance on manual testing. In her 2013 presentation at Flowcon titled \"On the Care and Feeding of Feedback Cycles,\" Elisabeth Hendrickson observed, \"Although testing can be automated, creating quality cannot. To have humans executing tests that should be automated is a waste of human potential.\"\n",
      "\n",
      "By doing this, we enable all our testers (which, of course, includes developers) work on high-value activities that cannot be automated, such as exploratory testing or improving the test process itself.\n",
      "\n",
      "However, merely automating all our manual tests may create undesired outcomes—we do not want automated tests that are unreliable or generate false positives (i.e., tests that should have passed because the code is functionally correct but failed due to problems such as slow performance, causing timeouts, uncontrolled starting state, or unintended state due to using database stubs or shared test environments).\n",
      "\n",
      "Unreliable tests that generate false positives create significant problems—they waste valuable time (e.g., forcing developers to re-run the test to determine whether there is actually a problem), increase the overall effort of running and interpreting our test results, and often result in stressed developers ignoring test results entirely or turning off the automated tests in favor of focusing on creating code.\n",
      "\n",
      "The result is always the same: we detect the problems later, the problems are more difficult to fix, and our customers have worse outcomes, which in turn creates stress across the value stream.\n",
      "\n",
      "To mitigate of this, a small number of reliable, automated tests are almost always preferable over a large number of manual or unreliable automated tests. Therefore, we focus on automating only the tests that genuinely validate the business goals we are trying to achieve. If abandoning a test results in production defects, we should add it back to our manual test suite, with the ideal of eventually automating it.\n",
      "\n",
      "As Gary Gruver, formerly VP of Quality Engineering, Release Engineering, and Operations for Macys.com, described observes, \"For a large retailer e-commerce site, we went from running 1,300 manual tests that we ran every ten days to running only ten automated tests upon every code commit—it's far better to run a few tests that we trust than to run tests that aren't reliable. Over time, we grew this test suite to having hundreds of thousands of automated tests.\"\n",
      "\n",
      "In other words, we start with a small number of reliable automated tests and add to them over time, creating an ever-increasing level of assurance that we will quickly detect any changes to the system that take us out of a deployable state.\n",
      "\n",
      "### INTEGRATE PERFORMANCE TESTING INTO OUR TEST SUITE\n",
      "\n",
      "All too often, we discover that our application performs poorly during integration testing or after it has been deployed to production. Performance problems are often difficult to detect, such as when things slow down over time, going unnoticed until it is too late (e.g., database queries without an index). And many problems are difficult to solve, especially when they are caused by architectural decisions we made or unforeseen limitations of our networking, database, storage, or other systems.\n",
      "\n",
      "Our goal is to write and run automated performance tests that validate our performance across the entire application stack (code, database, storage, network, virtualization, etc.) as part of the deployment pipeline, so we detect problems early, when the fixes are cheapest and fastest.\n",
      "\n",
      "By understanding how our application and environments behave under a production-like load, we can do a far better job at capacity planning, as well as detecting conditions such as:\n",
      "\n",
      "  * When our database query times grow non-linearly (e.g., we forget to turn on database indexing, and page load goes from one hundred minutes to thirty seconds).\n",
      "  * When a code change causes the number of database calls, storage use, or network traffic to increase ten-fold.\n",
      "\n",
      "When we have acceptance tests that are able to be run in parallel, we can use them as the basis of our performance tests. For instance, suppose we run an e-commerce site and have identified \"search\" and \"checkout\" as two high-value operations that must perform well under load. To test this, we may run thousands of parallel search acceptance tests simultaneously with thousands of parallel checkout tests.\n",
      "\n",
      "Due to the large amount of compute and I/O that is required to run performance tests, creating a performance testing environment can easily be more complex than creating the production environment for the application itself. Because of this, we may build our performance testing environment at the start of any project and ensure that we dedicate whatever resources are required to build it early and correctly.\n",
      "\n",
      "To find performance problems early, we should log performance results and evaluate each performance run against previous results. For instance, we might fail the performance tests if performance deviates more than 2% from the previous run.\n",
      "\n",
      "### INTEGRATE NON-FUNCTIONAL REQUIREMENTS TESTING INTO OUR TEST SUITE\n",
      "\n",
      "In addition to testing that our code functions as designed and it performs under production-like loads, we also want to validate every other attribute of the system we care about. These are often called non-functional requirements, which include availability, scalability, capacity, security, and so forth.\n",
      "\n",
      "Many of these requirements are fulfilled by the correct configuration of our environments, so we must also build automated tests to validate that our environments have been built and configured properly. For example, we want to enforce the consistency and correctness of the following, which many non-functional requirements rely upon (e.g., security, performance, availability):\n",
      "\n",
      "  * Supporting applications, databases, libraries, etc.\n",
      "  * Language interpreters, compilers, etc.\n",
      "  * Operating systems (e.g., audit logging enabled, etc.)\n",
      "  * All dependencies\n",
      "\n",
      "When we use infrastructure as code configuration management tools (e.g., Puppet, Chef, Ansible, Salt, Bosh), we can use the same testing frameworks that we use to test our code to also test that our environments are configured and operating correctly (e.g., encoding environment tests into cucumber or gherkin tests).\n",
      "\n",
      "Furthermore, similar to how we run analysis tools on our application in our deployment pipeline (e.g., static code analysis, test coverage analysis), we should run tools that analyze the code that constructs our environments (e.g., Foodcritic for Chef, puppet-lint for Puppet). We should also run any security hardening checks as part of our automated tests to ensure that everything is configured securely and correctly (e.g., server-spec).\n",
      "\n",
      "At any point in time, our automated tests can validate that we have a green build and that we are in a deployable state. Now, we must create an Andon cord so that when someone breaks the deployment pipeline, we take all necessary steps to get back into a green build state.\n",
      "\n",
      "## PULL OUR ANDON CORD WHEN THE DEPLOYMENT PIPELINE BREAKS\n",
      "\n",
      "When we have a green build in our deployment pipeline, we have a high degree of confidence that our code and environment will operate as designed when we deploy our changes into production.\n",
      "\n",
      "In order to keep our deployment pipeline in a green state, we will create a virtual Andon Cord, similar to the physical one in the Toyota Production System. Whenever someone introduces a change that causes our build or automated tests to fail, no new work is allowed to enter the system until the problem is fixed. And if someone needs help to resolve the problem, they can bring in whatever help they need, as in the Google example at the beginning of this chapter.\n",
      "\n",
      "When our deployment pipeline is broken, at a minimum, we notify the entire team of the failure, so anyone can either fix the problem or roll-back the commit. We may even configure the version control system to prevent further code commits until the first stage (i.e., builds and unit tests) of the deployment pipeline is back in a green state. If the problem was due to an automated test generating a false positive error, the offending test should either be rewritten or removed.††† Every member of the team should be empowered to roll back the commit to get back into a green state.\n",
      "\n",
      "Randy Shoup, former engineering director for Google App Engine, wrote about the importance of bringing the deployment back into a green state. \"We prioritize the team goals over individual goals—whenever we help someone move their work forward, we help the entire team. This applies whether we're helping someone fix the build or an automated test, or even performing a code review for them. And of course, we know that they'll do the same for us, when we need help. This system worked without a lot of formality or policy—everyone knew that our job was not just 'write code,' but it was to 'run a service.' This is why we prioritized all quality issues, especially those related to reliability and scaling, at the highest level, treating them as a Priority 0 'show-stopper' problems. From a systems perspective, these practices keep us from slipping backwards.\"\n",
      "\n",
      "When later stages of the deployment pipeline fail, such as acceptance tests or performance tests, instead of stopping all new work, we will have developers and testers on-call who are responsible for fixing these problems immediately. They should also create new tests that run at an earlier stage in the deployment pipeline to catch any future regressions. For example, if we discover a defect in our acceptance tests, we should write a unit test to catch the problem. Similarly, if we discover a defect in exploratory testing, we should write a unit or acceptance test.\n",
      "\n",
      "To increase the visibility of automated test failures, we should create highly visible indicators so that the entire team can see when our build or automated tests are failing. Many teams have created highly visible build lights that get mounted on a wall, indicating the current build status, or other fun ways of telling the team the build is broken, including lava lamps, playing a voice sample or song, klaxons, traffic lights, and so forth.\n",
      "\n",
      "In many ways, this step is more challenging than creating our builds and test servers—those were purely technical activities, whereas this step requires changing human behavior and incentives. However, continuous integration and continuous delivery require these changes, as we explore in the next section.\n",
      "\n",
      "### WHY WE NEED TO PULL THE ANDON CORD\n",
      "\n",
      "The consequence of not pulling the Andon cord and immediately fixing any deployment pipeline issues results in the all too familiar problem where it becomes ever more difficult to bring our applications and environment back into a deployable state. Consider the following situation:\n",
      "\n",
      "  * Someone checks in code that breaks the build or our automated tests, but no one fixes it.\n",
      "  * Someone else checks in another change onto the broken build, which also doesn't pass our automated tests—but no one sees the failing test results which would have enabled us to see the new defect, let alone fix it.\n",
      "  * Our existing tests don't run reliably, so we are very unlikely to build new tests. (Why bother? We can't even get the current tests to run.)\n",
      "\n",
      "When this happens, our deployments to any environment become as unreliable as when we had no automated tests or were using a waterfall method, where the majority of our problems are being discovered in production. The inevitable outcome of this vicious cycle is that we end up where we started, with an unpredictable \"stabilization phase\" that takes weeks or months where our whole team is plunged into crisis, trying to get all our tests to pass, taking shortcuts because of deadline pressures, and adding to our technical debt.‡‡‡\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we have created a comprehensive set of automated tests to confirm that we have a green build that is still in a passing and deployable state. We have organized our test suites and testing activities into a deployment pipeline. We have also created the cultural norm of doing whatever it takes to get back into a green build state if someone introduces a change that breaks any of our automated tests.\n",
      "\n",
      "By doing this, we set the stage for implementing continuous integration, which allows many small teams to independently and safely develop, test, and deploy code into production, delivering value to customers.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Bland described that at Google, one of the consequences of having so many talented developers was that it created \"imposter syndrome,\" a term coined by psychologists to informally describe people who are unable to internalize their accomplishments. Wikipedia states that \"despite external evidence of their competence, those exhibiting the syndrome remain convinced that they are frauds and do not deserve the success they have achieved. Proof of success is dismissed as luck, timing, or as a result of deceiving others into thinking they are more intelligent and competent than they believe themselves to be.\"\n",
      "\n",
      "‡ They created training programs, pushed the famous _Testing on the Toilet_ newsletter (which they posted in the bathrooms), the Test Certified roadmap and certification program, and led multiple \"fix-it\" days (i.e., improvement blitzes), which helped teams improve their automated testing processes so they could replicate the amazing outcomes that the GWS team was able to achieve.\n",
      "\n",
      "§ In Development, _continuous integration_ often refers to the continuous integration of multiple code branches into trunk and ensuring that it passes unit tests. However, in the context of continuous delivery and DevOps, continuous integration also mandates running on production-like environments and passing acceptance and integration tests. Jez Humble and David Farley disambiguate these by calling the latter CI+. In this book, _continuous integration_ will always refer to CI+ practices.\n",
      "\n",
      "¶ If we create containers in our deployment pipeline and have an architecture such as microservices, we can enable each developer to build immutable artifacts where developers assemble and run all the service components in an environment identical to production on their workstation. This enables developers to build and run more tests on their workstation instead of on testing servers, giving us even faster feedback on their work.\n",
      "\n",
      "** We may even require that these tools are run before changes are accepted into version control (e.g., get pre-commit hooks). We may also run these tools within the developer _integrated development environment_ (IDE; where the developer edits, compiles, and runs code), which creates an even faster feedback loop.\n",
      "\n",
      "†† We can also use containers, such as Docker, as the packaging mechanism. Containers enable the capability to write once, run anywhere. These containers are created as part of our build process and can be quickly deployed and run in any environment. Because the same container is run in every environment, we help enforce the consistency of all our build artifacts.\n",
      "\n",
      "‡‡ It is exactly this problem that led to the development of continuous integration practices.\n",
      "\n",
      "§§ There is a broad category of architectural and testing techniques used to handle the problems of tests requiring input from external integration points, including \"stubs,\" \"mocks,\" \"service virtualization,\" and so forth. This becomes even more important for acceptance and integration testing, which place far more reliance on external states.\n",
      "\n",
      "¶¶ We should do this only when our teams already value automated testing—this type of metric is easily gamed by developers and managers.\n",
      "\n",
      "*** Nachi Nagappan, E. Michael Maximilien, and Laurie Williams (from Microsoft Research, IBM Almaden Labs, and North Carolina State University, respectively) conducted a study that showed teams using TDD produced code 60%–90% better in terms of defect density than non-TDD teams, while taking only 15%–35% longer.\n",
      "\n",
      "††† If the process for rolling back the code is not well-known, a potential countermeasure is to schedule a _pair programmed rollback_ , so that it can be better documented.\n",
      "\n",
      "‡‡‡ This is sometimes called the _water-Scrum-fall anti-pattern_ , which refers to when an organization claims to be using Agile-like practices, but, in reality, all testing and defect fixing are performed at the end of the project.\n",
      "\n",
      "# 11Enable and Practice Continuous Integration\n",
      "\n",
      "In the previous chapter, we created the automated testing practices to ensure that developers get fast feedback on the quality of their work. This becomes even more important as we increase the number of developers and the number of branches they work on in version control.\n",
      "\n",
      "The ability to \"branch\" in version control systems was created primarily to enable developers to work on different parts of the software system in parallel, without the risk of individual developers checking in changes that could destabilize or introduce errors into trunk (sometimes also called master or mainline).†\n",
      "\n",
      "However, the longer developers are allowed to work in their branches in isolation, the more difficult it becomes to integrate and merge everyone's changes back into trunk. In fact, integrating those changes becomes exponentially more difficult as we increase the number of branches and the number of changes in each code branch.\n",
      "\n",
      "Integration problems result in a significant amount of rework to get back into a deployable state, including conflicting changes that must be manually merged or merges that break our automated or manual tests, usually requiring multiple developers to successfully resolve. And because integration has traditionally been done at the end of the project, when it takes far longer then planned, we are often forced to cut corners to make the release date.\n",
      "\n",
      "This causes another downward spiral: when merging code is painful, we tend to do it less often, making future merges even worse. Continuous integration was designed to solve this problem by making merging into trunk a part of everyone's daily work.\n",
      "\n",
      "The surprising breadth of problems that continuous integration solves, as well as the solutions themselves, are exemplified in Gary Gruver's experience as the director of engineering for HP's LaserJet Firmware division, which builds the firmware that runs all their scanners, printers, and multifunction devices.\n",
      "\n",
      "The team consisted of four hundred developers distributed across the US, Brazil, and India. Despite the size of their team, they were moving far too slowly. For years, they were unable to deliver new features as quickly as the business needed.\n",
      "\n",
      "Gruver described the problem thusly: \"Marketing would come to us with a million ideas to dazzle our customer, and we'd just tell them, 'Out of your list, pick the two things you'd like to get in the next six to twelve months.'\"\n",
      "\n",
      "They were only completing two firmware releases per year, with the majority of their time spent porting code to support new products. Gruver estimated that only 5% of their time was spent creating new features—the rest of the time was spent on non-productive work associated with their technical debt, such as managing multiple code branches and manual testing, as shown below:\n",
      "\n",
      "  * 20% on detailed planning (Their poor throughput and high lead times were misattributed to faulty estimation, and so, hoping to get a better answer, they were asked to estimate the work in greater detail.)\n",
      "  * 25% spent porting code, all maintained on separate code branches\n",
      "  * 10% spent integrating their code between developer branches\n",
      "  * 15% spent completing manual testing\n",
      "\n",
      "Gruver and his team created a goal of increasing the time spent on innovation and new functionality by a factor of ten. The team hoped this goal could be achieved through:\n",
      "\n",
      "  * Continuous integration and trunk-based development\n",
      "  * Significant investment in test automation \n",
      "  * Creation of a hardware simulator so tests could be run on a virtual platform\n",
      "  * The reproduction of test failures on developer workstations\n",
      "  * A new architecture to support running all printers off a common build and release\n",
      "\n",
      "Before this, each product line would require a new code branch, with each model having a unique firmware build with capabilities defined at compile time.‡ The new architecture would have all developers working in a common code base, with a single firmware release supporting all LaserJet models built off of trunk, with printer capabilities being established at runtime in an XML configuration file.\n",
      "\n",
      "Four years later, they had one codebase supporting all twenty-four HP LaserJet product lines being developed on trunk. Gruver admits trunk-based development requires a big mindset shift. Engineers thought trunk-based development would never work, but once they started, they couldn't imagine ever going back. Over the years we've had several engineers leave HP, and they would call me to tell me about how backward development was in their new companies, pointing out how difficult it is to be effective and release good code when there is no feedback that continuous integration gives them.\n",
      "\n",
      "However, trunk-based development required them to build more effective automated testing. Gruver observed, \"Without automated testing, continuous integration is the fastest way to get a big pile of junk that never compiles or runs correctly.\" In the beginning, a full manual testing cycle required six weeks.\n",
      "\n",
      "In order to have all firmware builds automatically tested, they invested heavily in their printer simulators and created a testing farm in six weeks—within a few years two thousand printer simulators ran on six racks of servers that would load the firmware builds from their deployment pipeline. Their continuous integration (CI) system ran their entire set of automated unit, acceptance, and integration tests on builds from trunk, just as described in the previous chapter. Furthermore, they created a culture that halted all work anytime a developer broke the deployment pipeline, ensuring that developers quickly brought the system back into a green state.\n",
      "\n",
      "Automated testing created fast feedback that enabled developers to quickly confirm that their committed code actually worked. Unit tests would run on their workstations in minutes, three levels of automated testing would run on every commit as well as every two and four hours. The final full regression testing would run every twenty-four hours. During this process, they:\n",
      "\n",
      "  * Reduced the build to one build per day, eventually doing ten to fifteen builds per day\n",
      "  * Went from around twenty commits per day performed by a \"build boss\" to over one hundred commits per day performed by individual developers\n",
      "  * Enabled developers to change or add 75k–100k lines of code each day\n",
      "  * Reduced regression test times from six weeks to one day\n",
      "\n",
      "This level of productivity could never have been supported prior to adopting continuous integration, when merely creating a green build required days of heroics. The resulting business benefits were astonishing:\n",
      "\n",
      "  * Time spent on driving innovation and writing new features increased from 5% of developer time to 40%.\n",
      "  * Overall development costs were reduced by approximately 40%. \n",
      "  * Programs under development were increased by about 140%. \n",
      "  * Development costs per program were decreased by 78%.\n",
      "\n",
      "What Gruver's experience shows is that, after comprehensive use of version control, continuous integration is one of the most critical practices that enable the fast flow of work in our value stream, enabling many development teams to independently develop, test, and deliver value. Nevertheless, continuous integration remains a controversial practice. The remainder of this chapter describes the practices required to implement continuous integration, as well as how to overcome common objections.\n",
      "\n",
      "## SMALL BATCH DEVELOPMENT AND WHAT HAPPENS WHEN WE COMMIT CODE TO TRUNK INFREQUENTLY\n",
      "\n",
      "As described in the previous chapters, whenever changes are introduced into version control that cause our deployment pipeline to fail, we quickly swarm the problem to fix it, bringing our deployment pipeline back into a green state. However, significant problems result when developers work in long-lived private branches (also known as \"feature branches\"), only merging back into trunk sporadically, resulting in a large batch size of changes. As described in the HP LaserJet example, what results is significant chaos and rework in order to get their code into a releasable state.\n",
      "\n",
      "Jeff Atwood, founder of the Stack Overflow site and author of the _Coding Horror_ blog, observes that while there are many branching strategies, they can all be put on the following spectrum:\n",
      "\n",
      "  * **Optimize for individual productivity:** Every single person on the project works in their own private branch. Everyone works independently, and nobody can disrupt anyone else's work; however, merging becomes a nightmare. Collaboration becomes almost comically difficult—every person's work has to be painstakingly merged with everyone else's work to see even the smallest part of the complete system.\n",
      "  * **Optimize for team productivity:** Everyone works in the same common area. There are no branches, just a long, unbroken straight line of development. There's nothing to understand, so commits are simple, but each commit can break the entire project and bring all progress to a screeching halt.\n",
      "\n",
      "Atwood's observation is absolutely correct—stated more precisely, the required effort to successfully merge branches back together increases exponentially as the number of branches increase. The problem lies not only in the rework this \"merge hell\" creates, but also in the delayed feedback we receive from our deployment pipeline. For instance, instead of performance testing against a fully integrated system happening continuously, it will likely happen only at the end of our process.\n",
      "\n",
      "Furthermore, as we increase the rate of code production as we add more developers, we increase the probability that any given change will impact someone else and increase the number of developers who will be impacted when someone breaks the deployment pipeline.\n",
      "\n",
      "Here is one last troubling side effect of large batch size merges: when merging is difficult, we become less able and motivated to improve and refactor our code, because refactorings are more likely to cause rework for everyone else. When this happens, we are more reluctant to modify code that has dependencies throughout the codebase, which is (tragically) where we may have the highest payoffs.\n",
      "\n",
      "This is how Ward Cunningham, developer of the first wiki, first described technical debt: when we do not aggressively refactor our codebase, it becomes more difficult to make changes and to maintain over time, slowing down the rate at which we can add new features. Solving this problem was one of the primary reasons behind the creation of continuous integration and trunk-based development practices, to optimize for team productivity over individual productivity.\n",
      "\n",
      "## ADOPT TRUNK-BASED DEVELOPMENT PRACTICES\n",
      "\n",
      "Our countermeasure to large batch size merges is to institute continuous integration and trunk-based development practices, where all developers check in their code to trunk at least once per day. Checking code in this frequently reduces our batch size to the work performed by our entire developer team in a single day. The more frequently developers check in their code to trunk, the smaller the batch size and the closer we are to the theoretical ideal of single-piece flow.\n",
      "\n",
      "Frequent code commits to trunk means we can run all automated tests on our software system as a whole and receive alerts when a change breaks some other part of the application or interferes with the work of another developer. And because we can detect merge problems when they are small, we can correct them faster.\n",
      "\n",
      "We may even configure our deployment pipeline to reject any commits (e.g., code or environment changes) that take us out of a deployable state. This method is called _gated commits_ , where the deployment pipeline first confirms that the submitted change will successfully merge, build as expected, and pass all the automated tests before actually being merged into trunk. If not, the developer will be notified, allowing corrections to be made without impacting anyone else in the value stream.\n",
      "\n",
      "The discipline of daily code commits also forces us to break our work down into smaller chunks while still keeping trunk in a working, releasable state. And version control becomes an integral mechanism of how the team communicates with each other—everyone has a better shared understanding of the system, is aware of the state of the deployment pipeline, and can help each other when it breaks. As a result, we achieve higher quality and faster deployment lead times.\n",
      "\n",
      "Having these practices in place, we can now again modify our definition of \"done\" (addition in bold text): \"At the end of each development interval, we must have integrated, tested, working, and potentially shippable code, demonstrated in a production-like environment, **created from trunk using a one-click process, and validated with automated tests.** \"\n",
      "\n",
      "Adhering to this revised definition of done helps us further ensure the ongoing testability and deployability of the code we're producing. By keeping our code in a deployable state, we are able to eliminate the common practice of having a separate test and stabilization phase at the end of the project.\n",
      "\n",
      "Case Study   \n",
      "Continuous Integration at Bazaarvoice (2012)\n",
      "\n",
      "Ernest Mueller, who helped engineer the DevOps transformation at National Instruments, later helped transform the development and release processes at Bazaarvoice in 2012. Bazaarvoice supplies customer generated content (e.g., reviews, ratings) for thousands of retailers, such as Best Buy, Nike, and Walmart.\n",
      "\n",
      "At that time, Bazaarvoice had $120 million in revenue and was preparing for an IPO.§ The business was primarily driven by the Bazaarvoice Conversations application, a monolithic Java application comprised of nearly five million lines of code dating back to 2006, spanning fifteen thousand files. The service ran on 1,200 servers across four data centers and multiple cloud service providers.\n",
      "\n",
      "Partially as a result of switching to an Agile development process and to two-week development intervals, there was a tremendous desire to increase release frequency from their current ten-week production release schedule. They had also started to decouple parts of their monolithic application, breaking it down into microservices.\n",
      "\n",
      "Their first attempt at a two-week release schedule was in January of 2012. Mueller observed, \"It didn't go well. It caused massive chaos, with forty-four production incidents filed by our customers. The major reaction from management was basically 'Let's not ever do that again.'\"\n",
      "\n",
      "Mueller took over the release processes shortly afterward, with the goal of doing bi-weekly releases without causing customer downtime. The business objectives for releasing more frequently included enabling faster A/B testing (described in upcoming chapters) and increasing the flow of features into production. Mueller identified three core problems:\n",
      "\n",
      "  * Lack of test automation made any level of testing during the two-week intervals inadequate to prevent large-scale failures.\n",
      "  * The version control branching strategy allowed developers to check in new code right up to the production release.\n",
      "  * The teams running microservices were also performing independent releases, which were often causing issues during the monolith release or vice versa.\n",
      "\n",
      "Mueller concluded that the monolithic Conversations application deployment process needed to be stabilized, which required continuous integration. In the six weeks that followed, developers stopped doing feature work to focus instead on writing automated testing suites, including unit tests in JUnit, regression tests in Selenium, and getting a deployment pipeline running in TeamCity. \"By running these tests all the time, we felt like we could make changes with some level of safety. And most importantly, we could immediately find when someone broke something, as opposed to discovering it only after it's in production.\"\n",
      "\n",
      "They also changed to a trunk/branch release model, where every two weeks they created a new dedicated release branch, with no new commits allowed to that branch unless there was an emergency—all changes would be worked through a sign-off process, either per-ticket or per-team through their internal wiki. That branch would go through a QA process, which would then be promoted into production.\n",
      "\n",
      "The improvements to predictability and quality of the releases were startling:\n",
      "\n",
      "  * January 2012 release: forty-four customer incidents (continuous integration effort begins)\n",
      "  * March 6, 2012 release: five days late, five customer incidents\n",
      "  * March 22, 2012 release: on time, one customer incident\n",
      "  * April 5, 2012 release: on time, zero customer incidents\n",
      "\n",
      "Mueller further described how successful this effort was:\n",
      "\n",
      "We had such success with releases every two weeks, we went to weekly releases, which required almost no changes from the engineering teams. Because releases became so routine, it was as simple as doubling the number of releases on the calendar and releasing when the calendar told us to. Seriously, it was almost a non-event. The majority of changes required were in our customer service and marketing teams, who had to change their processes, such as changing the schedule of their weekly customer emails to make sure customers knew that feature changes were coming. After that, we started working toward our next goals, which eventually led to speeding up our testing times from three plus hours to less than an hour, reducing the number of environments from four to three (Dev, Test, Production, eliminating Staging), and moving to a full continuous delivery model where we enable fast, one-click deployments.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Trunk-based development is likely the most controversial practice discussed in this book. Many engineers will not believe that it's possible, even those that prefer working uninterrupted on a private branch without having to deal with other developers. However, the data from Puppet Labs' _2015 State of DevOps Report_ is clear: trunk-based development predicts higher throughput and better stability, and even higher job satisfaction and lower rates of burnout.\n",
      "\n",
      "While convincing developers may be difficult at first, once they see the extraordinary benefits, they will likely become lifetime converts, as the HP LaserJet and Bazaarvoice examples illustrate. Continuous integration practices set the stage for the next step, which is automating the deployment process and enabling low-risk releases.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Branching in version control has been used in many ways, but is typically used to divide work between team members by release, promotion, task, component, technology platforms, and so forth.\n",
      "\n",
      "‡ Compile flags (#define and #ifdef) were used to enable/disable code execution for presence of copiers, paper size supported, and so on.\n",
      "\n",
      "§ The production release was delayed due to their (successful) IPO.\n",
      "\n",
      "# 12Automate and Enable Low-Risk Releases\n",
      "\n",
      "Chuck Rossi is the director of release engineering at Facebook. One of his responsibilities is overseeing the daily code push. In 2012, Rossi described their process as follows: \"Starting around 1 p.m., I switch over to 'operations mode' and work with my team to get ready to launch the changes that are going out to Facebook.com that day. This is the more stressful part of the job and really relies heavily on my team's judgment and past experience. We work to make sure that everyone who has changes going out is accounted for and is actively testing and supporting their changes.\"\n",
      "\n",
      "Just prior to the production push, all developers with changes going out must be present and check in on their IRC chat channel—any developers not present have their changes automatically removed from the deployment package. Rossi continued, \"If everything looks good and our test dashboards and canary tests† are green, we push the big red button and the entire Facebook.com server fleet gets the new code delivered. Within twenty minutes, thousands and thousands of machines are up on new code with no visible impact to the people using the site.\"‡\n",
      "\n",
      "Later that year, Rossi doubled their software release frequency to twice daily. He explained that the second code push gave engineers not on the US West Coast the ability to \"move and ship as quickly as any other engineer in the company,\" and also gave everyone a second opportunity each day to ship code and launch features.\n",
      "\n",
      "  **Figure 16:** Number of developers deploying per week at Facebook   \n",
      "(Source: Chuck Rossi, \"Ship early and ship twice as often.\")\n",
      "\n",
      "Kent Beck, the creator of the Extreme Programming methodology, one of the leading proponents of Test Driven Development, and technical coach at Facebook, further comments on the their code release strategy in an article posted on his Facebook page: \"Chuck Rossi made the observation that there seem to be a fixed number of changes Facebook can handle in one deployment. If we want more changes, we need more deployments. This has led to a steady increase in deployment pace over the past five years, from weekly to daily to thrice daily deployments of our PHP code and from six to four to two week cycles for deploying our mobile apps. This improvement has been driven primarily by the release engineering team.\"\n",
      "\n",
      "By using continuous integration and making code deployment a low-risk process, Facebook has enabled code deployment to be a part of everyone's daily work and sustain developer productivity. This requires that code deployment be automated, repeatable, and predictable. In the practices described in the book so far, even though our code and environments have been tested together, most likely we are not deploying to production very often because deployments are manual, time-consuming, painful, tedious, and error-prone, and they often involve an inconvenient and unreliable handoff between Development and Operations.\n",
      "\n",
      "And because it is painful, we tend to do it less and less frequently, resulting in another self-reinforcing downward spiral. By deferring production deployments, we accumulate ever-larger differences between the code to be deployed and what's running in production, increasing the deployment batch size. As deployment batch size grows, so does the risk of unexpected outcomes associated with the change, as well as the difficulty fixing them.\n",
      "\n",
      "In this chapter, we reduce the friction associated with production deployments, ensuring that they can be performed frequently and easily, either by Operations or Development. We do this by extending our deployment pipeline.\n",
      "\n",
      "Instead, of merely continually integrating our code in a production-like environment, we will enable the promotion into production of any build that passes our automated test and validation process, either on demand (i.e., at the push of a button) or automatically (i.e., any build that passes all the tests is automatically deployed).\n",
      "\n",
      "Because of the number of practices presented, extensive footnotes are provided with numerous examples and additional information, without interrupting the presentation of concepts in the chapter.\n",
      "\n",
      "## AUTOMATE OUR DEPLOYMENT PROCESS\n",
      "\n",
      "Achieving outcomes like those at Facebook requires that we have an automated mechanism that deploys our code into production. Especially if we have a deployment process that has existed for years, we need to fully document the steps in the deployment process, such as in a value stream mapping exercise, which we can assemble in a workshop or document incrementally (e.g., in a wiki).\n",
      "\n",
      "Once we have the process documented, our goal is to simplify and automate as many of the manual steps as possible, such as:\n",
      "\n",
      "  * Packaging code in ways suitable for deployment\n",
      "  * Creating pre-configured virtual machine images or containers\n",
      "  * Automating the deployment and configuration of middleware\n",
      "  * Copying packages or files onto production servers\n",
      "  * Restarting servers, applications, or services\n",
      "  * Generating configuration files from templates\n",
      "  * Running automated smoke tests to make sure the system is working and correctly configured\n",
      "  * Running testing procedures\n",
      "  * Scripting and automating database migrations\n",
      "\n",
      "Where possible, we will re-architect to remove steps, particularly those that take a long time to complete. We also want to not only reduce our lead times but also the number of handoffs as much as possible in order to reduce errors and loss of knowledge.\n",
      "\n",
      "Having developers focus on automating and optimizing the deployment process can lead to significant improvements in deployment flow, such as ensuring that small application configuration changes no longer need new deployments or new environments.\n",
      "\n",
      "However, this requires that Development works closely with Operations to ensure that all the tools and processes we co-create can be used downstream, as opposed to alienating Operations or reinventing the wheel.\n",
      "\n",
      "Many tools that provide continuous integration and testing also support the ability to extend the deployment pipeline so that validated builds can be promoted into production, typically after the production acceptance tests are performed (e.g., the Jenkins Build Pipeline plugin, ThoughtWorks Go.cd and Snap CI, Microsoft Visual Studio Team Services, and Pivotal Concourse).\n",
      "\n",
      "The requirements for our deployment pipeline include:\n",
      "\n",
      "  * **Deploying the same way to every environment:** By using the same deployment mechanism for every environment (e.g., development, test, and production), our production deployments are likely to be far more successful, since we know that it has been successfully performed many times already earlier in the pipeline.\n",
      "  * **Smoke testing our deployments:** During the deployment process, we should test that we can connect to any supporting systems (e.g., databases, message buses, external services) and run a single test transaction through the system to ensure that our system is performing as designed. If any of these tests fail, we should fail the deployment.\n",
      "  * **Ensure we maintain consistent environments:** In previous steps, we created a single-step environment build process so that the development, test, and production environments had a common build mechanism. We must continually ensure that these environments remain synchronized.\n",
      "\n",
      "Of course, when any problems occur during deployment, we pull the Andon cord and swarm the problem until the problem is resolved, just as we do when our deployment pipeline fails in any of the earlier steps.\n",
      "\n",
      "Case Study   \n",
      "Daily Deployments at CSG International (2013)\n",
      "\n",
      "CSG International runs one of the largest bill printing operations in the US. Scott Prugh, their chief architect and VP of Development, in an effort to improve the predictability and reliability of their software releases, doubled their release frequency from two per year to four per year (halving their deployment interval from twenty-eight weeks to fourteen weeks).\n",
      "\n",
      "Although the Development teams were using continuous integration to deploy their code into test environments daily, the production releases were being performed by the Operations team. Prugh observed, \"It was as if we had a 'practice team' that practiced daily (or even more frequently) in low-risk test environments, perfecting their processes and tools. But our production 'game team' got very few attempts to practice, only twice per year. Worse, they were practicing in the high-risk production environments, which were often very different than the pre-production environments with different constraints—the development environments were missing many production assets such as security, firewalls, load balancers, and a SAN.\"\n",
      "\n",
      "To solve this problem, they created a Shared Operations Team (SOT) that was responsible for managing all the environments (development, test, production) performing daily deployments into those development and test environments, as well as doing production deployments and releases every fourteen weeks. Because the SOT was doing deployments every day, any problems they encountered that were left unfixed would simply occur again the next day. This created tremendous motivation to automate tedious or error-prone manual steps and to fix any issues that could potentially happen again. Because the deployments were performed nearly one hundred times before the production release, most problems were found and fixed long before then.\n",
      "\n",
      "Doing this revealed problems that were previously only experienced by the Ops team, which were then problems for the entire value stream to solve. The daily deployments enabled daily feedback on which practices worked and which didn't.\n",
      "\n",
      "They also focused on making all their environments look as similar as possible, including the restricted security access rights and load balancers. Prugh writes, \"We made non-production environments as similar to production as possible, and we sought to emulate production constraints in as many ways as possible. Early exposure to production-class environments altered the designs of the architecture to make them friendlier in these constrained or different environments. Everyone gets smarter from this approach.\"\n",
      "\n",
      "Prugh also observes:\n",
      "\n",
      "\"We have experienced many cases where changes to database schemas are either 1) handed off to a DBA team for them to 'go and figure it out' or 2) automated tests that run on unrealistically small data sets (i.e., \"100's of MB vs. 100's of GBs\"), which led to production failures. In our old way of working, this would become a late-night blame game between teams trying to unwind the mess. We created a development and deployment process that removed the need for handoffs to DBAs by cross-training developers, automating schema changes, and executing them daily. We created realistic load testing against sanitized customer data, ideally running migrations every day. By doing this, we run our service hundreds of times with realistic scenarios before seeing actual production traffic.\"§\n",
      "\n",
      "Their results were astonishing. By doing daily deployments and doubling the frequency of production releases, the number of production incidents went down by 91%, MTTR went down by 80%, and the deployment lead time required for the service to run in production in a \"fully hands-off state\" went from fourteen days to one day.\n",
      "\n",
      "Prugh reported that deployments became so routine that the Ops team was playing video games by the end of the first day. In addition to deployments going more smoothly for Dev and Ops, in 50% of the cases the customer received the value in half the time, underscoring how more frequent deployments can be good for Development, QA, Operations, and the customer.\n",
      "\n",
      "  **Figure 17:** Daily deployments and increasing release frequency resulted in decrease in # of production incidents and MTTR (Source: \"DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),\" YouTube video, 29:39, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=tKdIHCL0DUg>.)\n",
      "\n",
      "### ENABLE AUTOMATED SELF-SERVICE DEPLOYMENTS\n",
      "\n",
      "Consider the following quote from Tim Tischler, Director of Operations Automation at Nike, Inc., that describes the common experience of a generation of developers: \"As a developer, there has never been a more satisfying point in my career than when I wrote the code, when I pushed the button to deploy it, when I could see the production metrics confirm that it actually worked in production, and when I could fix it myself if it didn't.\"\n",
      "\n",
      "Developers' ability to self-deploy code into production, to quickly see happy customers when their feature works, and to quickly fix any issues without having to open up a ticket with Operations has diminished over the last decade—in part as a result of a need for control and oversight, perhaps driven by security and compliance requirements.\n",
      "\n",
      "The resulting common practice is for Operations to perform code deployments, because separation of duties is a widely accepted practice to reduce the risk of production outages and fraud. However, to achieve DevOps outcomes, our goal is to shift our reliance to other control mechanisms that can mitigate these risks equally or even more effectively, such as through automated testing, automated deployment, and peer review of changes.\n",
      "\n",
      "The Puppet Labs' _2013 State of DevOps Report_ , which surveyed over four thousand technology professionals, found that there was no statistically significant difference in the change success rates between organizations where Development deployed code and those where Operations deployed code.\n",
      "\n",
      "In other words, when there are shared goals that span Development and Operations, and there is transparency, responsibility, and accountability for deployment outcomes, it doesn't matter who performs the deployment. In fact, we may even have other roles, such as testers or project managers, able to deploy to certain environments so they can get their own work done quickly, such as setting up demonstrations of specific features in test or UAT environments.\n",
      "\n",
      "To better enable fast flow, we want a code promotion process that can be performed by either Development or Operations, ideally without any manual steps or handoffs. This affects the following steps:\n",
      "\n",
      "  * **Build:** Our deployment pipeline must create packages from version control that can be deployed to any environment, including production.\n",
      "  * **Test:** Anyone should be able to run any or all of our automated test suite on their workstation or on our test systems.\n",
      "  * **Deploy:** Anybody should be able to deploy these packages to any environment where they have access, executed by running scripts that are also checked in to version control.\n",
      "\n",
      "These are the practices that enable deployments to be performed successfully, regardless of who is performing the deployment.\n",
      "\n",
      "### INTEGRATE CODE DEPLOYMENT INTO THE DEPLOYMENT PIPELINE\n",
      "\n",
      "Once the code deployment process is automated, we can make it part of the deployment pipeline. Consequently, our deployment automation must provide the following capabilities:\n",
      "\n",
      "  * Ensure that packages created during the continuous integration process are suitable for deployment into production\n",
      "  * Show the readiness of production environments at a glance\n",
      "  * Provide a push-button, self-service method for any suitable version of the packaged code to be deployed into production\n",
      "  * Record automatically, for auditing and compliance purposes, which commands were run on which machines when, who authorized it, and what the output was\n",
      "  * Run a smoke test to ensure the system is operating correctly and the configuration settings, including items such as database connection strings, are correct\n",
      "  * Provide fast feedback for the deployer so they can quickly determine whether their deployment was successful (e.g., did the deployment succeed, is the application performing as expected in production, etc.)\n",
      "\n",
      "Our goal is ensure that deployments are fast—we don't want to have to wait hours to determine whether our code deployment succeeded or failed and then need hours to deploy any needed code fixes. Now that we have technologies such as containers, it is possible to complete even the most complex deployments in seconds or minutes. In Puppet Labs' _2014 State of DevOps Report_ , the data showed that high performers had deployment lead times measured in minutes or hours, while the lowest performers had deployment lead times measured in months.\n",
      "\n",
      "  **Figure 18:** High performers had much faster deployment lead times and much faster time to restore production service after incidents (Source: Puppet Labs, _2014 State of DevOps Report_.)\n",
      "\n",
      "By building this capability, we now have a \"deploy code\" button that allows us to safely and quickly promote changes to our code and our environments into production through our deployment pipeline.\n",
      "\n",
      "Case Study   \n",
      "Etsy—Self-Service Developer Deployment, an Example of Continuous Deployment (2014)\n",
      "\n",
      "Unlike at Facebook where deployments are managed by release engineers, at Etsy deployments are performed by anyone who wants to perform a deployment, such as Development, Operations, or Infosec. The deployment process at Etsy has become so safe and routine that new engineers will perform a production deployment on their first day at work—as have Etsy board members and even dogs!\n",
      "\n",
      "As Noah Sussman, a test architect at Etsy, wrote, \"By the time 8am rolls around on a normal business day, 15 or so people and dogs are starting to queue up, all of them expecting to collectively deploy up to 25 changesets before the day is done.\"\n",
      "\n",
      "Engineers who want to deploy their code first go to a chat room, where engineers add themselves to the deploy queue, see the deployment activity in progress, see who else is in the queue, broadcast their activities, and get help from other engineers when they need it. When it's an engineer's turn to deploy, they are notified in the chat room.\n",
      "\n",
      "The goal at Etsy has been to make it easy and safe to deploy into production with the fewest number of steps and the least amount of ceremony. Likely before the developer even checks in code, they will run on their workstation all 4,500 unit tests, which takes less than one minute. All calls to external systems, such as databases, have been stubbed out.\n",
      "\n",
      "After they check their changes in to trunk in version control, over seven thousand automated trunk tests are instantly run on their continuous integration (CI) servers. Sussman writes, \"Through trial-and-error, we've settled on about 11 minutes as the longest that the automated tests can run during a push. That leaves time to re-run the tests once during a deployment [if someone breaks something and needs to fix it], without going too far past the 20 minute time limit.\"\n",
      "\n",
      "If all the tests were run sequentially, Sussman states that \"the 7,000 trunk tests would take about half an hour to execute. So we split these tests up into subsets, and distribute those onto the 10 machines in our Jenkins [CI] cluster....Splitting up our test suite and running many tests in parallel, gives us the desired 11 minute runtime.\"\n",
      "\n",
      "  **Figure 19:** The Deployinator console at Etsy (Source: Erik Kastner, \"Quantum of Deployment,\" CodeasCraft.com, May 20, 2010, https://codeascraft.com/2010/05/20/quantum-of-deployment/.)\n",
      "\n",
      "The next tests to run are the _smoke tests_ , which are system level tests that run cURL to execute PHPUnit test cases. Following these tests, the functional tests are run, which execute end-to-end GUI-driven tests on a live server—this server is either their QA environment or staging environment (nicknamed \"Princess\"), which is actually a production server that has been taken out of rotation, ensuring that it exactly matches the production environment.\n",
      "\n",
      "Once it is an engineer's turn to deploy, Erik Kastner writes, \"you go to Deployinator [an internally developed tool, see figure 19] and push the button to get it on QA. From there it visits Princess....Then, when it's ready to go live, you hit the \"Prod\" button and soon your code is live, and everyone in IRC [chat channel] knows who pushed what code, complete with a link to the diff. For anyone not on IRC, there's the email that everyone gets with the same information.\"\n",
      "\n",
      "In 2009, the deployment process at Etsy was a cause of stress and fear. By 2011, it had become a routine operation, happening twenty-five to fifty times per day, helping engineers get their code quickly into production, delivering value to their customers.\n",
      "\n",
      "## DECOUPLE DEPLOYMENTS FROM RELEASES\n",
      "\n",
      "In the traditional launch of a software project, releases are driven by our marketing launch date. On the prior evening, we deploy our completed software (or as close to complete as we could get) into production. The next morning, we announce our new capabilities to the world, start taking orders, deliver the new functionality to customer, etc.\n",
      "\n",
      "However, all too often things don't go according to plan. We may experience production loads that we never tested or designed for, causing our service to fail spectacularly, both for our customers and our organization. Worse, restoring service may require a painful rollback process or an equally risky _fix forward_ operation, where we make changes directly in production, this can all be a truly miserable experience for workers. When everything is finally working, everyone breathes a sigh of relief, grateful that production deployments and releases don't happen more often.\n",
      "\n",
      "Of course, we know that we need to be deploying more frequently to achieve our desired outcome of smooth and fast flow, not less frequently. To enable this, we need to decouple our production deployments from our feature releases. In practice, the terms _deployment_ and _release_ are often used interchangeably. However, they are two distinct actions that serve two very different purposes:\n",
      "\n",
      "  * Deployment is the installation of a specified version of software to a given environment (e.g., deploying code into an integration test environment or deploying code into production). Specifically, a deployment may or may not be associated with a release of a feature to customers.\n",
      "  * Release is when we make a feature (or set of features) available to all our customers or a segment of customers (e.g., we enable the feature to be used by 5% of our customer base). Our code and environments should be architected in such a way that the release of functionality does not require changing our application code.¶\n",
      "\n",
      "In other words, when we conflate deployment and release, it makes it difficult to create accountability for successful outcomes—decoupling these two activities allows us to empower Development and Operations to be responsible for the success of fast and frequent deployments, while enabling product owners to be responsible for the successful business outcomes of the release (i.e., was building and launching the feature worth our time).\n",
      "\n",
      "The practices described so far in this book ensure that we are doing fast and frequent production deployments throughout feature development, with the goal of reducing the risk and impact of deployment errors. The remaining risk is release risk, which is whether the features we put into production achieve the desired customer and business outcomes.\n",
      "\n",
      "If we have extremely long deployment lead times, this dictates how frequently we can release new features to the marketplace. However, as we become able to deploy on demand, how quickly we expose new functionality to customers becomes a business and marketing decision, not a technical decision. There are two broad categories of release patterns we can use:\n",
      "\n",
      "  * **Environment-based release patterns:** This is where we have two or more environments that we deploy into, but only one environment is receiving live customer traffic (e.g., by configuring our load balancers). New code is deployed into a non-live environment, and the release is performed moving traffic to this environment. These are extremely powerful patterns, because they typically require little or no change to our applications. These patterns include _blue-green deployments_ , _canary releases_ , and _cluster immune systems_ , all of which will be discussed shortly. \n",
      "  * **Application-based release patterns:** This is where we modify our application so that we can selectively release and expose specific application functionality by small configuration changes. For instance, we can implement feature flags that progressively expose new functionality in production to the development team, all internal employees, 1% of our customers, or, when we are confident that the release will operate as designed, our entire customer base. As discussed earlier, this enables a technique called dark launching, where we stage all the functionality to be launched in production and test it with production traffic before our release. For instance, we may invisibly test our new functionality with production traffic for weeks before our launch in order to expose problems so that they can be fixed before our actual launch.\n",
      "\n",
      "### ENVIRONMENT-BASED RELEASE PATTERNS\n",
      "\n",
      "Decoupling deployments from our releases dramatically changes how we work. We no longer have to perform deployments in the middle of the night or on weekends to lower the risk of negatively impacting customers. Instead, we can do deployments during typical business hours, enabling Ops to finally have normal working hours, just like everyone else.\n",
      "\n",
      "This section focuses on environment-based release patterns, which require no changes to application code. We do this by having multiple environments to deploy into, but only one of them receives live customer traffic. By doing this, we can significantly decrease the risk associated with production releases and reduce the deployment lead time.\n",
      "\n",
      "#### The Blue-Green Deployment Pattern\n",
      "\n",
      "The simplest of the three patterns is called blue-green deployment. In this pattern, we have two production environments: blue and green. At any time, only one of these is serving customer traffic—in figure 20, the green environment is live.\n",
      "\n",
      "  **Figure 20:** Blue-green deployment patterns (Source: Humble and North, _Continuous Delivery_ , 261.)\n",
      "\n",
      "To release a new version of our service, we deploy to the inactive environment where we can perform our testing without interrupting the user experience. When we are confident that everything is functioning as designed, we execute our release by directing traffic to the blue environment. Thus, blue becomes live and green becomes staging. Roll back is performed by sending customer traffic back to the green environment.**\n",
      "\n",
      "The blue-green deployment pattern is simple, and it is extremely easy to retrofit onto existing systems. It also has incredible benefits, such as enabling the team to perform deployments during normal business hours and conduct simple changeovers (e.g., changing a router setting, changing a symlink) during off-peak times. This alone can dramatically improve the work conditions for the team performing the deployment.\n",
      "\n",
      "#### Dealing with Database Changes\n",
      "\n",
      "Having two versions of our application in production creates problems when they depend upon a common database—when the deployment requires database schema changes or adding, modifying, or deleting tables or columns, the database cannot support both versions of our application. There are two general approaches to solving this problem:\n",
      "\n",
      "  * **Create two databases (i.e., a blue and green database):** __Each version—blue (old) and green (new)—of the application has its own database. During the release, we put the blue database into read-only mode, perform a backup of it, restore onto the green database, and finally switch traffic to the green environment. The problem with this pattern is that if we need to roll back to the blue version, we can potentially lose transactions if we don't manually migrate them from the green version first.\n",
      "  * **Decouple database changes from application changes:** __Instead of supporting two databases, we decouple the release of database changes from the release of application changes by doing two things: First, we make only additive changes to our database, we never mutate existing database objects, and second, we make no assumptions in our application about which database version will be in production. This is very different than how we've been traditionally trained to think about databases, where we avoid duplicating data. The process of Decoupling database changes from application changes was used by IMVU (among others) around 2009, enabling them to do fifty deployments per day, some of which required database changes.††\n",
      "\n",
      "Case Study   \n",
      "Dixons Retail—Blue-Green Deployment for Point-Of-Sale System (2008)\n",
      "\n",
      "Dan North and Dave Farley, co-authors of _Continuous Delivery_ , were working on a project for Dixons Retail, a large British retailer involving thousands of point-of-sale (POS) systems that resided in hundreds of retail stores and operating under a number of different customer brands.\n",
      "\n",
      "Although blue-green deployments are mostly associated with online web services, North and Farley used this pattern to significantly reduce the risk and changeover times for POS upgrades.\n",
      "\n",
      "Traditionally, upgrading POS systems are a big bang, waterfall project: the POS clients and the centralized server are upgraded at the same time, which requires extensive downtime (often an entire weekend), as well as significant network bandwidth to push out the new client software to all the retail stores. When things don't go entirely according to plan, it can be incredibly disruptive to store operations.\n",
      "\n",
      "For this upgrade, there was not enough network bandwidth to upgrade all the POS systems simultaneously, which made the traditional strategy impossible. To solve this problem, they used the blue-green strategy and created two production versions of the centralized server software, enabling them to simultaneously support the old and new versions of the POS clients.\n",
      "\n",
      "After they did this, weeks before the planned POS upgrade, they started sending out new versions of client POS software installers to the retail stores over the slow network links, deploying the new software onto the POS systems in an inactive state. Meanwhile, the old version kept running as normal.\n",
      "\n",
      "When all the POS clients had everything staged for the upgrade (the upgraded client and server had tested together successfully, and new client software had been deployed to all clients), the store managers were empowered to decide when to release the new version.\n",
      "\n",
      "Depending on their business needs, some managers wanted to use the new features immediately and released right away, while others wanted to wait. In either case, whether releasing features immediately or waiting, itwas significantly better for the managers than having the centralized IT department choose for them when the release would occur.\n",
      "\n",
      "The result was a significantly smoother and faster release, higher satisfaction from the store managers, and far less disruption to store operations. Furthermore, this application of blue-green deployments to thick-client PC applications demonstrates how DevOps patterns can be universally applied to different technologies, often in very surprising ways but with the same fantastic outcomes.\n",
      "\n",
      "#### The Canary and Cluster Immune System Release Patterns\n",
      "\n",
      "The blue-green release pattern is easy to implement and can dramatically increase the safety of software releases. There are variants of this pattern that can further improve safety and deployment lead times using automation, but with the potential trade-off of additional complexity.\n",
      "\n",
      "_The canary release pattern_ automates the release process of promoting to successively larger and more critical environments as we confirm that the code is operating as designed.\n",
      "\n",
      "The term _canary release_ comes from the tradition of coal miners bringing caged canaries into mines to provide early detection of toxic levels of carbon monoxide. If there wastoo much gas in the cave, it would kill the canaries before it killed the miners, alerting them to evacuate.\n",
      "\n",
      "In this pattern, when we perform a release, we monitor how the software in each environment is performing. When something appears to be going wrong, we roll back; otherwise, we deploy to the next environment.‡‡\n",
      "\n",
      "Figure 21 shows the groups of environments Facebook created to support this release pattern:\n",
      "\n",
      "  * **A1 group:** Production servers that only serve internal employees\n",
      "  * **A2 group:** Production servers that only serve a small percentage of customers and are deployed when certain acceptance criteria have been met (either automated or manual)\n",
      "  * **A3 group:** The rest of the production servers, which are deployed after the software running in the A2 cluster meets certain acceptance criteria\n",
      "\n",
      "  **Figure 21:** The canary release pattern (Source: Humble and Farley, _Continuous Delivery_ , 263.)\n",
      "\n",
      "The cluster immune system expands upon the canary release pattern by linking our production monitoring system with our release process and by automating the roll back of code when the user-facing performance of the production system deviates outside of a predefined expected range, such as when the conversion rates for new users drops below our historical norms of 15%–20%.\n",
      "\n",
      "There are two significant benefits to this type of safeguard. First, we protect against defects that are hard to find through automated tests, such as a web page change that renders some critical page element invisible (e.g., CSS change). Second, we reduce the time required to detect and respond to the degraded performance created by our change.§§\n",
      "\n",
      "### APPLICATION-BASED PATTERNS TO ENABLE SAFER RELEASES\n",
      "\n",
      "In the previous section, we created environment-based patterns that allowed us to decouple our deployments from our releases by using multiple environments and by switching between which environment was live, which can be entirely implemented at the infrastructure level.\n",
      "\n",
      "In this section, we describe application-based release patterns that we can implement in our code, allowing even greater flexibility in how we safely release new features to our customer, often on a per-feature basis. Because application-based release patterns are implemented in the application, these require involvement from Development.\n",
      "\n",
      "#### Implement Feature Toggles\n",
      "\n",
      "The primary way we enable application-based release patterns is by implementing feature toggles, which provide us with the mechanism to selectively enable and disable features without requiring a production code deployment. Feature toggles can also control which features are visible and available to specific user segments (e.g., internal employees, segments of customers).\n",
      "\n",
      "Feature toggles are usually implemented by wrapping application logic or UI elements with a conditional statement, where the feature is enabled or disabled based on a configuration setting stored somewhere. This can be as simple as an application configuration file (e.g., configuration files in JSON, XML), or it might be through a directory service or even a web service specifically designed to manage feature toggling.¶¶\n",
      "\n",
      "Feature toggles also enable us to do the following:\n",
      "\n",
      "  * **Roll back easily:** Features that create problems or interruptions in production can be quickly and safely disabled by merely changing the feature toggle setting. This is especially valuable when deployments are infrequent—switching off one particular stakeholder's features is usually much easier than rolling back an entire release.\n",
      "  * **Gracefully degrade performance:** When our service experiences extremely high loads that would normally require us to increase capacity or, worse, risk having our service fail in production, we can use feature toggles to reduce the quality of service. In other words, we can increase the number of users we serve by reducing the level of functionality delivered (e.g., reduce the number of customers who can access a certain feature, disable CPU-intensive features such as recommendations, etc.).\n",
      "  * **Increase our resilience through a service-oriented architecture:** If we have a feature that relies on another service that isn't complete yet, we can still deploy our feature into production but hide it behind a feature toggle. When that service finally becomes available, we can toggle the feature on. Similarly, when a service we rely upon fails, we can turn off the feature to prevent calls to the downstream service while keeping the rest of the application running.\n",
      "\n",
      "To ensure that we find errors in features wrapped in feature toggles, our automated acceptance tests should run with all feature toggles on. (We should also test that our feature toggling functionality works correctly too!)\n",
      "\n",
      "Feature toggles enable the decoupling of code deployments and feature releases, later in the book we use feature toggles to enable hypothesis-driven development and A/B testing, furthering our ability to achieve our desired business outcomes.\n",
      "\n",
      "#### Perform Dark Launches\n",
      "\n",
      "Feature toggles allow us to deploy features into production without making them accessible to users, enabling a technique known as _dark launching_. This is where we deploy all the functionality into production and then perform testing of that functionality while it is still invisible to customers. For large or risky changes, we often do this for weeks before the production launch, enabling us to safely test with the anticipated production-like loads.\n",
      "\n",
      "For instance, suppose we dark launch a new feature that poses significant release risk, such as new search features, account creation processes, or new database queries. After all the code is in production, keeping the new feature disabled, we may modify user session code to make calls to new functions—instead of displaying the results to the user, we simply log or discard the results.\n",
      "\n",
      "For example, we may have 1% of our online users make invisible calls to a new feature scheduled to be launched to see how our new feature behaves under load. After we find and fix any problems, we progressively increase the simulated load by increasing the frequency and number of users exercising the new functionality. By doing this, we are able to safely simulate production-like loads, giving us confidence that our service will perform as it needs to.\n",
      "\n",
      "Furthermore, when we launch a feature, we can progressively roll out the feature to small segments of customers, halting the release if any problems are found. That way, we minimize the number of customers who are given a feature only to have it taken away because we find a defect or are unable to maintain the required performance.\n",
      "\n",
      "In 2009, when John Allspaw was VP of Operations at Flickr, he wrote to the Yahoo! executive management team about their dark launch process, saying that it \"increases _everyone's_ confidence almost to the point of apathy, as far as fear of load-related issues are concerned. I have no idea how many code deploys there were made to production on any given day in the past 5 years...because for the most part I don't care, because those changes made in production have such a low chance of causing issues. When they have caused issues, everyone on the Flickr staff can find on a webpage _when_ the change was made, _who_ made the change, and exactly (line-by-line) _what_ the change was.\"***\n",
      "\n",
      "Later, when we have built adequate production telemetry in our application and environments, we can also enable faster feedback cycles to validate our business assumptions and outcomes immediately after we deploy the feature into production.\n",
      "\n",
      "By doing this, we no longer wait until a big bang release to test whether customers want to use the functionality we build. Instead, by the time we announce and release our big feature, we have already tested our business hypotheses and run countless experiments to continually refine our product with real customers, which helps us validate that the features will achieve the desired customer outcomes.\n",
      "\n",
      "Case Study   \n",
      "Dark Launch of Facebook Chat (2008)\n",
      "\n",
      "For nearly a decade, Facebook has been one of the most widely visited Internet sites, as measured by pages viewed and unique site users. In 2008, it had over seventy million daily active users, which created a challenge for the team that was developing the new Facebook Chat functionality.†††\n",
      "\n",
      "Eugene Letuchy, an engineer on the Chat team, wrote about how the number of concurrent users presented a huge software engineering challenge: \"The most resource-intensive operation performed in a chat system is not sending messages. It is rather keeping each online user aware of the online-idle-offline states of their friends, so that conversations can begin.\"\n",
      "\n",
      "Implementing this computationally-intensive feature was one of the largest technical undertakings ever at Facebook and took almost a year to complete.‡‡‡ Part of the complexity of the project was due to the wide variety of technologies needed to achieve the desired performance, including C++, JavaScript, and PHP, as well as their first use of Erlang in their back-end infrastructure.\n",
      "\n",
      "Throughout the course of the year-long endeavor, the Chat team checked their code in to version control, where it would be deployed into production at least once per day. At first, the Chat functionality was visible only to the Chat team. Later, it was made visible to all internal employees, but it was completely hidden from external Facebook users through Gatekeeper, the Facebook feature toggling service.\n",
      "\n",
      "As part of their dark launch process, every Facebook user session, which runs JavaScript in the user browser, had a test harness loaded into it—the chat UI elements were hidden, but the browser client would send invisible test chat messages to the back-end chat service that was already in production, enabling them to simulate production-like loads throughout the entire project, allowing them to find and fix performance problems long before the customer release.\n",
      "\n",
      "By doing this, every Facebook user was part of a massive load testing program, which enabled the team to gain confidence that their systems could handle realistic production-like loads. The Chat release and launch required only two steps: modifying the Gatekeeper configuration setting to make the Chat feature visible to some portion of external users, and having Facebook users load new JavaScript code that rendered the Chat UI and disabled the invisible test harness. If something went wrong, the two steps would be reversed. When the launch day of Facebook Chat arrived, it was surprisingly successful and uneventful, seeming to scale effortlessly from zero to seventy million users overnight. During the release, they incrementally enabled the chat functionality to ever-larger segments of the customer population—first to all internal Facebook employees, then to 1% of the customer population, then to 5%, and so forth. As Letuchy wrote, \"The secret for going from zero to seventy million users overnight is to avoid doing it all in one fell swoop.\"\n",
      "\n",
      "## SURVEY OF CONTINUOUS DELIVERY AND CONTINUOUS DEPLOYMENT IN PRACTICE\n",
      "\n",
      "In _Continuous Delivery,_ Jez Humble and David Farley define the term _continuous delivery._ The term _continuous deployment_ was first mentioned by Tim Fitz in his blog post \"Continuous Deployment at IMVU: Doing the impossible fifty times a day.\" However, in 2015, during the construction of _The DevOps Handbook_ , Jez Humble commented, \"In the last five years, there has been confusion around the terms continuous delivery versus continuous deployment—and, indeed, my own thinking and definitions have changed since we wrote the book. Every organization should create their variations, based on what they need. The key thing we should care about is not the form, but the outcomes: deployments should be low-risk, push-button events we can perform on demand.\"\n",
      "\n",
      "His updated definitions of continuous delivery and continuous deployment are as follows:\n",
      "\n",
      "When all developers are working in small batches on trunk, or everyone is working off trunk in short-lived feature branches that get merged to trunk regularly, and when trunk is always kept in a releasable state, and when we can release on demand at the push of a button during normal business hours, we are doing continuous delivery. Developers get fast feedback when they introduce any regression errors, which include defects, performance issues, security issues, usability issues, etc. When these issues are found, they are fixed immediately so that trunk is always deployable.\n",
      "\n",
      "In addition to the above, when we are deploying good builds into production on a regular basis through self-service (being deployed by Dev or by Ops)—which typically means that we are deploying to production at least once per day per developer, or perhaps even automatically deploying every change a developer commits—this is when we are engaging in continuous deployment.\n",
      "\n",
      "Defined this way, continuous delivery is the prerequisite for continuous deployment—just as continuous integration is a prerequisite for continuous delivery. Continuous deployment is likely applicable in the context of web services that are delivered online. However, continuous delivery is applicable in almost every context where we desire deployments and releases that have high quality, fast lead times and have highly predictable, low-risk outcomes, including for embedded systems, COTS products, and mobile apps.\n",
      "\n",
      "At Amazon and Google, most teams practice continuous delivery, although some perform continuous deployment— thus, there is considerable variation between teams in how frequently they deploy code and how deployments are performed. Teams are empowered to choose how to deploy based on the risks they are managing. For example, the Google App Engine team often deploys once per day, while the Google Search property deploys several times per week.\n",
      "\n",
      "Similarly, most of the cases studies presented in this book are also continuous delivery, such as the embedded software running on HP LaserJet printers, the CSG bill printing operations running on twenty technology platforms including a COBOL mainframe application, Facebook, and Etsy. These same patterns can be used for software that runs on mobile phones, ground control stations that control satellites, and so forth.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "As the Facebook, Etsy, and CSG examples have shown, releases and deployments do not have to be high-risk, high-drama affairs that require tens or hundreds of engineers to work around the clock to complete. Instead, they can be made entirely routine and a part of everyone's daily work.\n",
      "\n",
      "By doing this, we can reduce our deployment lead times from months to minutes, allowing our organizations to quickly deliver value to our customer without causing chaos and disruption. Furthermore, by having Dev and Ops work together, we can finally make Operations work humane.\n",
      "\n",
      "* * *\n",
      "\n",
      "† A canary release test is when software is deployed to a small group of production servers to make sure nothing terrible happens to them with live customer traffic.\n",
      "\n",
      "‡ The Facebook front-end codebase is primarily written in PHP. In 2010, to increase site performance, the PHP code was converted into C++ by their internally developed HipHop compiler, which was then compiled into a 1.5 GB executable. This file was then copied onto production servers using BitTorrent, enabling the copy operation to be completed in fifteen minutes.\n",
      "\n",
      "§ In their experiments, they found that SOT teams were successful, regardless of whether they were managed by Development or Operations, as long as the teams were staffed with the right people and were dedicated to SOT success.\n",
      "\n",
      "¶ Operation Desert Shield may serve as an effective metaphor. Starting on August 7, 1990, thousands of men and materials were safely deployed over four months into the production theater, culminating in a single, multi-disciplinary, highly coordinated release.\n",
      "\n",
      "** Other ways that we can implement the blue-green pattern include setting up multiple Apache/NGINX web servers to listen on different physical or virtual interfaces; employing multiple virtual roots on Windows IIS servers bound to different ports; using different directories for every version of the system, with a symbolic link determining which one is live (e.g., as Capistrano does for Ruby on Rails); running multiple versions of services or middleware concurrently, with each listening on different ports; using two different data centers and switching traffic between the data centers, instead of using them merely as hot- or warm-spares for disaster recovery purposes (incidentally, by routinely using both environments, we are continually ensuring that our disaster recovery process works as designed); or using different availability zones in the cloud.\n",
      "\n",
      "†† This pattern is also commonly referred to as the expand/contract pattern, which Timothy Fitz described when he said, \"We do not change (mutate) database objects, such as columns or tables. Instead, we first expand, by adding new objects, then, later, contract by removing the old ones.\" Furthermore, increasingly, there are technologies that enable virtualization, versioning, labeling, and roll back of databases, such as Redgate, Delphix, DBMaestro, and Datical, as well as open source tools, such as DBDeploy, that make database changes dramatically safer and faster.\n",
      "\n",
      "‡‡ Note that canary releases require having multiple versions of our software running in production simultaneously. However, because each additional version we have in production creates additional complexity to manage, we should keep the number of versions to a minimum. This may require the use of the expand/contract database pattern described earlier.\n",
      "\n",
      "§§ The cluster immune system was first documented by Eric Ries while working at IMVU. This functionality is also supported by Etsy in their Feature API library, as well as by Netflix.\n",
      "\n",
      "¶¶ One sophisticated example of such a service is Facebook's Gatekeeper, an internally developed service that dynamically selects which features are visible to specific users based on demographic information such as location, browser type, and user profile data (age, gender, etc.). For instance, a particular feature could be configured so that it is only accessible by internal employees, 10% of their user base, or only users between the ages of twenty-five and thirty-five. Other examples include the Etsy Feature API and the Netflix Archaius library.\n",
      "\n",
      "*** Similarly, as Chuck Rossi, Director of Release Engineering at Facebook, described, \"All the code supporting every feature we're planning to launch over the next six months has already been deployed onto our production servers. All we need to do is turn it on.\"\n",
      "\n",
      "††† By 2015, Facebook had over one billion active users, growing 17% over the previous year.\n",
      "\n",
      "‡‡‡ This problem has a worst-case computational characteristic of O(n3). In other words, the compute time increases exponentially as the function of the number of online users, the size of their friend lists, and the frequency of online/offline state change.\n",
      "\n",
      "# 13Architect for Low-Risk Releases\n",
      "\n",
      "Almost every well-known DevOps exemplar has had near-death experiences due to architectural problems, such as in the stories presented about LinkedIn, Google, eBay, Amazon, and Etsy. In each case, they were able to successfully migrate to a more suitable architecture that addressed their current problems and organizational needs.\n",
      "\n",
      "This is the principle of evolutionary architecture—Jez Humble observes that architecture of \"any successful product or organization will necessarily evolve over its life cycle.\" Before his tenure at Google, Randy Shoup served as chief engineer and distinguished architect at eBay from 2004 to 2011. He observes that \"both eBay and Google are each on their fifth entire rewrite of their architecture from top to bottom.\"\n",
      "\n",
      "He reflects, \"Looking back with 20/20 hindsight, some technology and architectural choices] look prescient and others look shortsighted. Each decision most likely best served the organizational goals at the time. If we had tried to implement the 1995 equivalent of micro-services out of the gate, we would have likely failed, collapsing under our own weight and probably taking the entire company with us.\"[†\n",
      "\n",
      "The challenge is how to keep migrating from the architecture we have to the architecture we need. In the case of eBay, when they needed to re-architect, they would first do a small pilot project to prove to themselves that they understood the problem well enough to even undertake the effort. For instance, when Shoup's team was planning on moving certain portions of the site to full-stack Java in 2006, they looked for the area that would get them the biggest bang for their buck by sorting the site pages by revenue produced. They chose the highest revenue areas, stopping when there was not enough of a business return to justify the effort.\n",
      "\n",
      "What Shoup's team did at eBay is a textbook example of evolutionary design, using a technique called the _strangler application_ pattern—instead of \"ripping out and replacing\" old services with architectures that no longer support our organizational goals, we put the existing functionality behind an API and avoid making further changes to it. All new functionality is then implemented in the new services that use the new desired architecture, making calls to the old system when necessary.\n",
      "\n",
      "The strangler application pattern is especially useful for helping migrate portions of a monolithic application or tightly-coupled services to one that is more loosely-coupled. All too often, we find ourselves working within an architecture that has become too tightly-coupled and too interconnected, often having been created years (or decades) ago.\n",
      "\n",
      "The consequences of overly tight architectures are easy to spot: every time we attempt to commit code in to trunk or release code in to production, we risk creating global failures (e.g., we break everyone else's tests and functionality, or the entire site goes down). To avoid this, every small change requires enormous amounts of communication and coordination over days or weeks, as well as approvals from any group that could potentially be affected. Deployments become problematic as well—the number of changes that are batched together for each deployment grows, further complicating the integration and test effort, and increasing the already high likelihood of something going wrong.\n",
      "\n",
      "Even deploying small changes may require coordinating with hundreds (or even thousands) of other developers, with any one of them able to create a catastrophic failure, potentially requiring weeks to find and fix the problem. (This results in another symptom: \"My developers spend only 15% of their time coding—the rest of their time is spent in meetings.\")\n",
      "\n",
      "These all contribute to an extremely unsafe system of work, where small changes have seemingly unknowable and catastrophic consequences. It also often contributes to a fear of integrating and deploying our code, and the self-reinforcing downward spiral of deploying less frequently.\n",
      "\n",
      "From an enterprise architecture perspective, this downward spiral is the consequence of the Second Law of Architectural Thermodynamics, especially in large, complex organizations. Charles Betz, author of _Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children_, observes, \"[IT project owners] are not held accountable for their contributions to overall system entropy.\" In other words, reducing our overall complexity and increasing the productivity of all our development teams is rarely the goal of an individual project.\n",
      "\n",
      "In this chapter, we will describe steps we can take to reverse the downward spiral, review the major architectural archetypes, examine the attributes of architectures that enable developer productivity, testability, deployability, and safety, as well as evaluate strategies that allow us to safely migrate from whatever current architecture we have to one that better enables the achievement of our organizational goals.\n",
      "\n",
      "## AN ARCHITECTURE THAT ENABLES PRODUCTIVITY, TESTABILITY, AND SAFETY\n",
      "\n",
      "In contrast to a tightly-coupled architecture that can impede everyone's productivity and ability to safely make changes, a loosely-coupled architecture with well-defined interfaces that enforce how modules connect with each other promotes productivity and safety. It enables small, productive, two-pizza teams that are able to make small changes that can be safely and independently deployed. And because each service also has a well-defined API, it enables easier testing of services and the creation of contracts and SLAs between teams.\n",
      "\n",
      "  **Figure 22:** Google cloud datastore (Source: Shoup, \"From the Monolith to Micro-services.\")\n",
      "\n",
      "As Randy Shoup describes, \"This type of architecture has served Google extremely well—for a service like Gmail, there's five or six other layers of services underneath it, each very focused on a very specific function. Each service is supported by a small team, who builds it and runs their functionality, with each group potentially making different technology choices. Another example is the Google Cloud Datastore service, which is one of the largest NoSQL services in the world—and yet it is supported by a team of only about eight people, largely because it is based on layers upon layers of dependable services built upon each other.\"\n",
      "\n",
      "This kind of service-oriented architecture allows small teams to work on smaller and simpler units of development that each team can deploy independently, quickly, and safely. Shoup notes, \"Organizations with these types of architectures, such as Google and Amazon, show how it can impact organizational structures, [creating] flexibility and scalability. These are both organizations with tens of thousands of developers, where small teams can still be incredibly productive.\"\n",
      "\n",
      "## ARCHITECTURAL ARCHETYPES: MONOLITHS VS. MICROSERVICES\n",
      "\n",
      "At some point in their history, most DevOps organizations were hobbled by tightly-coupled, monolithic architectures that—while extremely successful at helping them achieve product/market fit—put them at risk of organizational failure once they had to operate at scale (e.g., eBay's monolithic C++ application in 2001, Amazon's monolithic OBIDOS application in 2001, Twitter's monolithic Rails front-end in 2009, and LinkedIn's monolithic Leo application in 2011). In each of these cases, they were able to re-architect their systems and set the stage not only to survive, but also to thrive and win in the marketplace.\n",
      "\n",
      "Monolithic architectures are not inherently bad—in fact, they are often the best choice for an organization early in a product life cycle. As Randy Shoup observes, \"There is no one perfect architecture for all products and all scales. Any architecture meets a particular set of goals or range of requirements and constraints, such as time to market, ease of developing functionality, scaling, etc. The functionality of any product or service will almost certainly evolve over time—it should not be surprising that our architectural needs will change as well. What works at scale 1x rarely works at scale 10x or 100x.\"\n",
      "\n",
      "**Table 3:** Architectural archetypes\n",
      "\n",
      "(Source: Shoup, \"From the Monolith to Micro-services.\")\n",
      "\n",
      "The major architectural archetypes are shown in table 3, each row indicates a different evolutionary need for an organization, with each column giving the pros and cons of each of the different archetypes. As the table shows, a monolithic architecture that supports a startup (e.g., rapid prototyping of new features, and potential pivots or large changes in strategies) is very different from an architecture that needs hundreds of teams of developers, each of whom must be able to independently deliver value to the customer. By supporting evolutionary architectures, we can ensure that our architecture always serves the current needs of the organization.\n",
      "\n",
      "Case Study   \n",
      "Evolutionary Architecture at Amazon (2002)\n",
      "\n",
      "One of the most studied architecture transformations occurred at Amazon. In an interview with ACM Turing Award-winner and Microsoft Technical Fellow Jim Gray, Amazon CTO Werner Vogels explains that Amazon.com started in 1996 as a \"monolithic application, running on a web server, talking to a database on the back end. This application, dubbed Obidos, evolved to hold all the business logic, all the display logic, and all the functionality that Amazon eventually became famous for: similarities, recommendations, Listmania, reviews, etc.\"\n",
      "\n",
      "As time went by, Obidos grew too tangled, with complex sharing relationships meaning individual pieces could not be scaled as needed. Vogels tells Gray that this meant \"many things that you would like to see happening in a good software environment couldn't be done anymore; there were many complex pieces of software combined into a single system. It couldn't evolve anymore.\"\n",
      "\n",
      "Describing the thought process behind the new desired architecture, he tells Gray, \"We went through a period of serious introspection and concluded that a service-oriented architecture would give us the level of isolation that would allow us to build many software components rapidly and independently.\"\n",
      "\n",
      "Vogels notes, \"The big architectural change that Amazon went through in the past five years [from 2001–2005] was to move from a two-tier monolith to a fully-distributed, decentralized, services platform serving many different applications. A lot of innovation was necessary to make this happen, as we were one of the first to take this approach.\" The lessons from Vogel's experience at Amazon that are important to our understanding of architecture shifts include the following:\n",
      "\n",
      "  * **Lesson 1:** When applied rigorously, strict service orientation is an excellent technique to achieve isolation; you achieve a level of ownership and control that was not seen before.\n",
      "  * **Lesson 2:** Prohibiting direct database access by clients makes performing scaling and reliability improvements to your service state possible without involving your clients.\n",
      "  * **Lesson 3:** Development and operational process greatly benefits from switching to service-orientation. The services model has been a key enabler in creating teams that can innovate quickly with a strong customer focus. Each service has a team associated with it, and that team is completely responsible for the service—from scoping out the functionality to architecting, building, and operating it.\n",
      "\n",
      "The extent to which applying these lessons enhances developer productivity and reliability is breathtaking. In 2011, Amazon was performing approximately fifteen thousands deployments per day. By 2015, they were performing nearly 136,000 deployments per day.\n",
      "\n",
      "## USE THE STRANGLER APPLICATION PATTERN TO SAFELY EVOLVE OUR ENTERPRISE ARCHITECTURE\n",
      "\n",
      "The term _strangler application_ was coined by Martin Fowler in 2004 after he was inspired by seeing massive strangler vines during a trip to Australia, writing, \"They seed in the upper branches of a fig tree and gradually work their way down the tree until they root in the soil. Over many years they grow into fantastic and beautiful shapes, meanwhile strangling and killing the tree that was their host.\"\n",
      "\n",
      "If we have determined that our current architecture is too tightly-coupled, we can start safely decoupling parts of the functionality from our existing architecture. By doing this, we enable teams supporting the decoupled functionality to independently develop, test, and deploy their code into production with autonomy and safety, and reduce architectural entropy.\n",
      "\n",
      "As described earlier, the strangler application pattern involves placing existing functionality behind an API, where it remains unchanged, and implementing new functionality using our desired architecture, making calls to the old system when necessary. When we implement strangler applications, we seek to access all services through versioned APIs, also called _versioned services_ or _immutable services_.\n",
      "\n",
      "Versioned APIs enable us to modify the service without impacting the callers, which allows the system to be more loosely-coupled—if we need to modify the arguments, we create a new API version and migrate teams who depend on our service to the new version. After all, we are not achieving our re-architecting goals if we allow our new strangler application to get tightly-coupled into other services (e.g., connecting directly to another service's database).\n",
      "\n",
      "If the services we call do not have cleanly-defined APIs, we should build them or at least hide the complexity of communicating with such systems within a client library that has a cleanly defined API.\n",
      "\n",
      "By repeatedly decoupling functionality from our existing tightly-coupled system, we move our work into a safe and vibrant ecosystem where developers can be far more productive resulting in the legacy application shrinking in functionality. It might even disappear entirely as all the needed functionality migrates to our new architecture.\n",
      "\n",
      "By creating strangler applications, we avoid merely reproducing existing functionality in some new architecture or technology—often, our business processes are far more complex than necessary due to the idiosyncrasies of the existing systems, which we will end up replicating. (By researching the user, we can often re-engineer the process so that we can design a far simpler and more streamlined means to achieving the business goal.)‡\n",
      "\n",
      "An observation from Martin Fowler underscores this risk: \"Much of my career has involved rewrites of critical systems. You would think such a thing is easy—just make the new one do what the old one did. Yet they are always much more complex than they seem, and overflowing with risk. The big cut-over date looms, and the pressure is on. While new features (there are always new features) are liked, old stuff has to remain. Even old bugs often need to be added to the rewritten system.\"\n",
      "\n",
      "As with any transformation, we seek to create quick wins and deliver early incremental value before continuing to iterate. Up-front analysis helps us identify the smallest possible piece of work that will usefully achieve a business outcome using the new architecture.\n",
      "\n",
      "Case Study   \n",
      "Strangler Pattern at Blackboard Learn (2011)\n",
      "\n",
      "Blackboard Inc. is one of the pioneers of providing technology for educational institutions, with annual revenue of approximately $650 million in 2011. At that time, the development team for their flagship Learn product, packaged software that was installed and run on-premise at their customer sites, was living with the daily consequences of a legacy J2EE codebase that went back to 1997. As David Ashman, their chief architect, observes, \"we still have fragments of Perl code still embedded throughout our codebase.\"\n",
      "\n",
      "In 2010, Ashman was focused on the complexity and growing lead times associated with the old system, observing that \"our build, integration, and testing processes kept getting more and more complex and error prone. And the larger the product got, the longer our lead times and the worse the outcomes for our customers. To even get feedback from our integration process would require twenty-four to thirty-six hours.\"\n",
      "\n",
      "  **Figure 23:** Blackboard Learn code repository: before Building Blocks (Source: \"DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,\" YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, <https://www.youtube.com/watch?v=SSmixnMpsI4>.)\n",
      "\n",
      "How this started to impact developer productivity was made visible to Ashman in graphs generated from their source code repository going all the way back to 2005.\n",
      "\n",
      "In figure 24, the top graph represents the number of lines of code in the monolithic Blackboard Learn code repository; the bottom graph represents the number of code commits. The problem that became evident to Ashman was that the number of code commits started to decrease, objectively showing the increasing difficulty of introducing code changes, while the number of lines of code continued to increase. Ashman noted, \"To me, it said we needed to do something, otherwise the problems would keep getting worse, with no end in sight.\"\n",
      "\n",
      "As a result, in 2012 Ashman focused on implementing a code re-architecturing project that used the strangler pattern. The team accomplished this by creating what they internally called _Building Blocks_ , which allowed developers to work in separate modules that were decoupled from the monolithic codebase and accessed through fixed APIs. This enabled them to work with far more autonomy, without having to constantly communicate and coordinate with other development teams.\n",
      "\n",
      "  **Figure 24:** Blackboard Learn code repository: after Building Blocks (Source: \"DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds.\" YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, <https://www.youtube.com/watch?v=SSmixnMpsI4>.)\n",
      "\n",
      "When Building Blocks were made available to developers, the size of the monolith source code repository began to decrease (as measured by number of lines of code). Ashman explained that this was because developers were moving their code into the Building Block modules source code repository. \"In fact,\" Ashman reported, \"every developer given a choice would work in the Building Block codebase, where they could work with more autonomy and freedom and safety.\"\n",
      "\n",
      "The graph above shows the connection between the exponential growth in the number of lines of code and the exponential growth of the number of code commits for the Building Blocks code repositories. The new Building Blocks codebase allowed developers to be more productive, and they made the work safer because mistakes resulted in small, local failures instead of major catastrophes that impacted the global system.\n",
      "\n",
      "Ashman concluded, \"Having developers work in the Building Blocks architecture made for impressive improvements in code modularity, allowing them to work with more independence and freedom. In combination with the updates to our build process, they also got faster, better feedback on their work, which meant better quality. \"\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "To a large extent, the architecture that our services operate within dictates how we test and deploy our code. This was validated in Puppet Labs' _2015 State of DevOps Report_ , showing that architecture is one of the top predictors of the productivity of the engineers that work within it and of how changes can be quickly and safely made.\n",
      "\n",
      "Because we are often stuck with architectures that were optimized for a different set of organizational goals, or for an era long-passed, we must be able to safely migrate from one architecture to another. The case studies presented in this chapter, as well as the Amazon case study previously presented, describe techniques like the strangler pattern that can help us migrate between architectures incrementally, enabling us to adapt to the needs of the organization.\n",
      "\n",
      "## PART III CONCLUSION\n",
      "\n",
      "Within the previous chapters of Part III, we have implemented the architecture and technical practices that enable the fast flow of work from Dev to Ops, so that value can be quickly and safely delivered to customers.\n",
      "\n",
      "In Part IV: The Second Way, _The Technical Practices of Feedback_ , we will create the architecture and mechanisms to enable the reciprocal fast flow of feedback from right to left, to find and fix problems faster, radiate feedback, and ensure better outcomes from our work. This enables our organization to further increase the rate at which it can adapt.\n",
      "\n",
      "* * *\n",
      "\n",
      "† eBay's architecture went through the following phases: Perl and files (v1, 1995), C++ and Oracle (v2, 1997), XSL and Java (v3, 2002), full-stack Java (v4, 2007), Polyglot microservices (2013+).\n",
      "\n",
      "‡ The strangler application pattern involves incrementally replacing a whole system, usually a legacy system, with a completely new one. Conversely, _branching by abstraction_ , a term coined by Paul Hammant, is a technique where we create an abstraction layer between the areas that we are changing. This enables evolutionary design of the application architecture while allowing everybody to work off trunk/master and practice continuous integration.\n",
      "\n",
      "# Part IV\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In Part III, we described the architecture and technical practices required to create fast flow from Development into Operations. Now in Part IV, we describe how to implement the technical practices of the Second Way, which are required to create fast and continuous feedback from Operations to Development.\n",
      "\n",
      "By doing this, we shorten and amplify feedback loops so that we can see problems as they occur and radiate this information to everyone in the value stream. This allows us to quickly find and fix problems earlier in the software development life cycle, ideally long before they cause a catastrophic failure.\n",
      "\n",
      "Furthermore, we will create a system of work where knowledge acquired downstream in Operations is integrated into the upstream work of Development and Product Management. This allows us to quickly create improvements and learnings, whether it's from a production issue, a deployment issue, early indicators of problems, or our customer usage patterns.\n",
      "\n",
      "Additionally, we will create a process that allows everyone to get feedback on their work, makes information visible to enable learning, and enables us to rapidly test product hypotheses, helping us determine if the features we are building are helping us achieve our organizational goals.\n",
      "\n",
      "We will also demonstrate how to create telemetry from our build, test, and deploy processes, as well as from user behavior, production issues and outages, audit issues, and security breaches. By amplifying signals as part of our daily work, we make it possible to see and solve problems as they occur, and we grow safe systems of work that allow us to confidently make changes and run product experiments, knowing we can quickly detect and remediate failures. We will do all of this by exploring the following:\n",
      "\n",
      "  * Creating telemetry to enable seeing and solving problems\n",
      "  * Using our telemetry to better anticipate problems and achieve goals\n",
      "  * Integrating user research and feedback into the work of product teams\n",
      "  * Enabling feedback so Dev and Ops can safely perform deployments\n",
      "  * Enabling feedback to increase the quality of our work through peer reviews and pair programming\n",
      "\n",
      "The patterns in this chapter help reinforce the common goals of Product Management, Development, QA, Operations, and Infosec, and encourage them to share in the responsibility of ensuring that services run smoothly in production and collaborate on the improvement of the system as a whole. Where possible, we want to link cause to effect. The more assumptions we can invalidate, the faster we can discover and fix problems, but also the greater our ability to learn and innovate.\n",
      "\n",
      "Throughout the following chapters, we will implement feedback loops, enabling everyone to work together toward shared goals, to see problems as they occur, enable quick detection and recovery, and ensure that features not only operate as designed in production, but also achieve organizational goals and support organizational learning.\n",
      "\n",
      "# 14Create Telemetry to Enable Seeing and Solving Problems\n",
      "\n",
      "A fact of life in Operations is that things go wrong—small changes may result in many unexpected outcomes, including outages and global failures that impact all our customers. This is the reality of operating complex systems; no single person can see the whole system and understand how all the pieces fit together.\n",
      "\n",
      "When production outages and other problems occur in our daily work, we don't often have the information we need to solve the problem. For example, during an outage we may not be able to determine whether the issue is due to a failure in our application (e.g., defect in the code), in our environment (e.g., a networking problem, server configuration problem), or something entirely external to us (e.g., a massive denial of service attack).\n",
      "\n",
      "In Operations, we may deal with this problem with the following rule of thumb: When something goes wrong in production, we just reboot the server. If that doesn't work, reboot the server next to it. If that doesn't work, reboot all the servers. If that doesn't work, blame the developers, they're always causing outages.\n",
      "\n",
      "In contrast, the Microsoft Operations Framework (MOF) study in 2001 found that organizations with the highest service levels rebooted their servers twenty times less frequently than average and had five times fewer \"blue screens of death.\" In other words, they found that the best-performing organizations were much better at diagnosing and fixing service incidents, in what Kevin Behr, Gene Kim, and George Spafford called a \"culture of causality\" in _The Visible Ops Handbook_. High performers used a disciplined approach to solving problems, using production telemetry to understand possible contributing factors to focus their problem solving, as opposed to lower performers who would blindly reboot servers.\n",
      "\n",
      "To enable this disciplined problem-solving behavior, we need to design our systems so that they are continually creating _telemetry_ , widely defined as \"an automated communications process by which measurements and other data are collected at remote points and are subsequently transmitted to receiving equipment for monitoring.\" Our goal is to create telemetry within our applications and environments, both in our production and pre-production environments as well as in our deployment pipeline.\n",
      "\n",
      "Michael Rembetsy and Patrick McDonnell described how production monitoring was a critical part of Etsy's DevOps transformation that started in 2009. This was because they were standardizing and transitioning their entire technology stack to the LAMP stack (Linux, Apache, MySQL, and PHP), abandoning a myriad of different technologies being used in production that were increasingly difficult to support.\n",
      "\n",
      "At the 2012 Velocity Conference, McDonnell described how much risk this created, \"We were changing some of our most critical infrastructure, which, ideally, customers would never notice. However, they'd definitely notice if we screwed something up. We needed more metrics to give us confidence that we weren't actually breaking things while we were doing these big changes, both for our engineering teams and for team members in the non-technical areas, such as marketing.\"\n",
      "\n",
      "McDonnell explained further, \"We started collecting all our server information in a tool called Ganglia, displaying all the information into Graphite, an open source tool we invested heavily into. We started aggregating metrics together, everything from business metrics to deployments. This is when we modified Graphite with what we called 'our unparalleled and unmatched vertical line technology' that overlaid onto every metric graph when deployments happened. By doing this, we could more quickly see any unintended deployment side effects. We even started putting TV screens all around the office so that everyone could see how our services were performing.\"\n",
      "\n",
      "By enabling developers to add telemetry to their features as part of their daily work, they created enough telemetry to help make deployments safe. By 2011, Etsy was tracking over two hundred thousand production metrics at every layer of the application stack (e.g., application features, application health, database, operating system, storage, networking, security, etc.) with the top thirty most important business metrics prominently displayed on their \"deploy dashboard.\" By 2014, they were tracking over eight hundred thousand metrics, showing their relentless goal of instrumenting everything and making it easy for engineers to do so.\n",
      "\n",
      "As Ian Malpass, an engineer at Etsy, quipped, \"If Engineering at Etsy has a religion, it's the Church of Graphs. If it moves, we track it. Sometimes we'll draw a graph of something that isn't moving yet, just in case it decides to make a run for it....Tracking everything is key to moving fast, but the only way to do it is to make tracking anything easy....We enable engineers to track what they need to track, at the drop of a hat, without requiring time-sucking configuration changes or complicated processes.\"\n",
      "\n",
      "One of the findings of the _2015 State of DevOps Report_ was that high performers could resolve production incidents 168 times faster than their peers, with the median high performer having a MTTR measured in minutes, while the median low performer had an MTTR measured in days. The top two technical practices that enabled fast MTTR were the use of version control by Operations and having telemetry and proactive monitoring in the production environment.\n",
      "\n",
      "**Figure 25:** Incident resolution time for high, medium, and low performers   \n",
      "(Source: Puppet Labs, _2014 State of DevOps Report_.)\n",
      "\n",
      "As was created at Etsy, our goal in this chapter is to ensure that we always have enough telemetry so that we can confirm that our services are correctly operating in production. And when problems do occur, make it possible to quickly determine what is going wrong and make informed decisions on how best to fix it, ideally long before customers are impacted. Furthermore, telemetry is what enables us to assemble our best understanding of reality and detect when our understanding of reality is incorrect.\n",
      "\n",
      "## CREATE OUR CENTRALIZED TELEMETRY INFRASTRUCTURE\n",
      "\n",
      "Operational monitoring and logging is by no means new—multiple generations of Operations engineers have used and customized monitoring frameworks (e.g., HP OpenView, IBM Tivoli, and BMC Patrol/BladeLogic) to ensure the health of production systems. Data was typically collected through agents that ran on servers or through agent-less monitoring (e.g., SNMP traps or polling based monitors). There was often a graphical user interface (GUI) front end, and back-end reporting was often augmented through tools such as Crystal Reports.\n",
      "\n",
      "Similarly, the practices of developing applications with effective logging and managing the resulting telemetry are not new—a variety of mature logging libraries exist for almost all programming languages.\n",
      "\n",
      "However, for decades we have ended up with silos of information, where Development only creates logging events that are interesting to developers, and Operations only monitors whether the environments are up or down. As a result, when inopportune events occur, no one can determine why the entire system is not operating as designed or which specific component is failing, impeding our ability to bring our system back to a working state.\n",
      "\n",
      "In order for us to see all problems as they occur, we must design and develop our applications and environments so that they generate sufficient telemetry, allowing us to understand how our system is behaving as a whole. When all levels of our application stack have monitoring and logging, we enable other important capabilities, such as graphing and visualizing our metrics, anomaly detection, proactive alerting and escalation, etc.\n",
      "\n",
      "In _The Art of Monitoring_ , James Turnbull describes a modern monitoring architecture, which has been developed and used by Operations engineers at web-scale companies (e.g., Google, Amazon, Facebook). The architecture often consisted of open source tools, such as Nagios and Zenoss, that were customized and deployed at a scale that was difficult to accomplish with licensed commercial software at the time. This architecture has the following components:\n",
      "\n",
      "  * **Data collection at the business logic, application, and environments layer:** In each of these layers, we are creating telemetry in the form of events, logs, and metrics. Logs may be stored in application-specific files on each server (e.g., /var/log/httpd-error.log), but preferably we want all our logs sent to a common service that enables easy centralization, rotation, and deletion. This is provided by most operating systems, such as syslog for Linux, the Event Log for Windows, etc. Furthermore, we gather metrics at all layers of the application stack to better understand how our system is behaving. At the operating system level, we can collect metrics such as CPU, memory, disk, or network usage over time using tools like collectd, Ganglia, etc. Other tools that collect performance information include AppDynamics, New Relic, and Pingdom.\n",
      "  * **An event router responsible for storing our events and metrics:** This capability potentially enables visualization, trending, alerting, anomaly detection, and so forth. By collecting, storing, and aggregating all our telemetry, we better enable further analysis and health checks. This is also where we store configurations related to our services (and their supporting applications and environments) and is likely where we do threshold-based alerting and health checks.†\n",
      "\n",
      "Once we have centralized our logs, we can transform them into metrics by counting them in the event router—for example, a log event such as \"child pid 14024 exit signal Segmentation fault\" can be counted and summarized as a single segfault metric across our entire production infrastructure.\n",
      "\n",
      "By transforming logs into metrics, we can now perform statistical operations on them, such as using anomaly detection to find outliers and variances even earlier in the problem cycle. For instance, we might configure our alerting to notify us if we went from \"ten segfaults last week\" to \"thousands of segfaults in the last hour,\" prompting us to investigate further.\n",
      "\n",
      "In addition to collecting telemetry from our production services and environments, we must also collect telemetry from our deployment pipeline when important events occur, such as when our automated tests pass or fail and when we perform deployments to any environment. We should also collect telemetry on how long it takes us to execute our builds and tests. By doing this, we can detect conditions that could indicate problems, such as if the performance test or our build takes twice as long as normal, allowing us to find and fix errors before they go into production.\n",
      "\n",
      "**Figure 26:** Monitoring framework (Source: Turnbull, _The Art of Monitoring_ , Kindle edition, chap. 2.)\n",
      "\n",
      "Furthermore, we should ensure that it is easy to enter and retrieve information from our telemetry infrastructure. Preferably, everything should be done through self-service APIs, as opposed to requiring people to open up tickets and wait to get reports.\n",
      "\n",
      "Ideally, we will create telemetry that tells us exactly when anything of interest happens, as well as where and how. Our telemetry should also be suitable for manual and automated analysis and should be able to be analyzed without having the application that produced the logs on hand. As Adrian Cockcroft pointed out, \"Monitoring is so important that our monitoring systems need to be more available and scalable than the systems being monitored.\"\n",
      "\n",
      "From here on, the term _telemetry_ will be used interchangeably with _metrics_ , which includes all event logging and metrics created by our services at all levels of our application stack and generated from all our production and pre-production environments, as well as from our deployment pipeline.\n",
      "\n",
      "## CREATE APPLICATION LOGGING TELEMETRY THAT HELPS PRODUCTION\n",
      "\n",
      "Now that we have a centralized telemetry infrastructure, we must ensure that the applications we build and operate are creating sufficient telemetry. We do this by having Dev and Ops engineers create production telemetry as part of their daily work, both for new and existing services.\n",
      "\n",
      "Scott Prugh, Chief Architect and Vice President of Development at CSG, said, \"Every time NASA launches a rocket, it has millions of automated sensors reporting the status of every component of this valuable asset. And yet, we often don't take the same care with software—we found that creating application and infrastructure telemetry to be one of the highest return investments we've made. In 2014, we created over one billion telemetry events per day, with over one hundred thousand code locations instrumented.\"\n",
      "\n",
      "In the applications we create and operate, every feature should be instrumented—if it was important enough for an engineer to implement, it is certainly important enough to generate enough production telemetry so that we can confirm that it is operating as designed and that the desired outcomes are being achieved.‡\n",
      "\n",
      "Every member of our value stream will use telemetry in a variety of ways. For example, developers may temporarily create more telemetry in their application to better diagnose problems on their workstation, while Ops engineers may use telemetry to diagnose a production problem. In addition, Infosec and auditors may review the telemetry to confirm the effectiveness of a required control, and a product manager may use them to track business outcomes, feature usage, or conversion rates.\n",
      "\n",
      "To support these various usage models, we have different logging levels, some of which may also trigger alerts, such as the following:\n",
      "\n",
      "  * **DEBUG level:** Information at this level is about anything that happens in the program, most often used during debugging. Often, debug logs are disabled in production but temporarily enabled during troubleshooting.\n",
      "  * **INFO level:** Information at this level consists of actions that are user-driven or system specific (e.g., \"beginning credit card transaction\").\n",
      "  * **WARN level:** Information at this level tells us of conditions that could potentially become an error (e.g., a database call taking longer than some predefined time). These will likely initiate an alert and troubleshooting, while other logging messages may help us better understand what led to this condition.\n",
      "  * **ERROR level:** Information at this level focuses on error conditions (e.g., API call failures, internal error conditions).\n",
      "  * **FATAL level:** Information at this level tells us when we must terminate (e.g., a network daemon can't bind a network socket).\n",
      "\n",
      "Choosing the right logging level is important. Dan North, a former ThoughtWorks consultant who was involved in several projects in which the core continuous delivery concepts took shape, observes, \"When deciding whether a message should be ERROR or WARN, imagine being woken up at 4 a.m. Low printer toner is not an ERROR.\"\n",
      "\n",
      "To help ensure that we have information relevant to the reliable and secure operations of our service, we should ensure that all potentially significant application events generate logging entries, including those provided on this list assembled by Anton A. Chuvakin, a research VP at Gartner's GTP Security and Risk Management group:\n",
      "\n",
      "  * Authentication/authorization decisions (including logoff)\n",
      "  * System and data access\n",
      "  * System and application changes (especially privileged changes)\n",
      "  * Data changes, such as adding, editing, or deleting data\n",
      "  * Invalid input (possible malicious injection, threats, etc.)\n",
      "  * Resources (RAM, disk, CPU, bandwidth, or any other resource that has hard or soft limits)\n",
      "  * Health and availability\n",
      "  * Startups and shutdowns\n",
      "  * Faults and errors\n",
      "  * Circuit breaker trips\n",
      "  * Delays\n",
      "  * Backup success/failure\n",
      "\n",
      "To make it easier to interpret and give meaning to all these log entries, we should (ideally) create logging hierarchical categories, such as for non-functional attributes (e.g., performance, security) and for attributes related to features (e.g., search, ranking).\n",
      "\n",
      "## USE TELEMETRY TO GUIDE PROBLEM SOLVING\n",
      "\n",
      "As described in the beginning of this chapter, high performers use a disciplined approach to solving problems. This is in contrast to the more common practice of using rumor and hearsay, which can lead to the unfortunate metric of _mean time until declared innocent_ —how quickly can we convince everyone else that we didn't cause the outage.\n",
      "\n",
      "When there is a culture of blame around outages and problems, groups may avoid documenting changes and displaying telemetry where everyone can see them to avoid being blamed for outages.\n",
      "\n",
      "Other negative outcomes due to lack of public telemetry include a highly charged political atmosphere, the need to deflect accusations, and, worse, the inability to create institutional knowledge around how the incidents occurred and the learnings needed to prevent these errors from happening again in the future.§\n",
      "\n",
      "In contrast, telemetry enables us to use the scientific method to formulate hypotheses about what is causing a particular problem and what is required to solve it. Examples of questions we can answer during problem resolution include:\n",
      "\n",
      "  * What evidence do we have from our monitoring that a problem is actually occurring?\n",
      "  * What are the relevant events and changes in our applications and environments that could have contributed to the problem?\n",
      "  * What hypotheses can we formulate to confirm the link between the proposed causes and effects?\n",
      "  * How can we prove which of these hypotheses are correct and successfully effect a fix?\n",
      "\n",
      "The value of fact-based problem solving lies not only in significantly faster MTTR (and better customer outcomes), but also in its reinforcement of the perception of a win/win relationship between Development and Operations.\n",
      "\n",
      "## ENABLE CREATION OF PRODUCTION METRICS AS PART OF DAILY WORK\n",
      "\n",
      "To enable everyone to be able to find and fix problems in their daily work, we need to enable everyone to create metrics in their daily work that can be easily created, displayed, and analyzed. To do this, we must create the infrastructure and libraries necessary to make it as easy as possible for anyone in Development or Operations to create telemetry for any functionality they build. In the ideal, it should be as easy as writing one line of code to create a new metric that shows up in a common dashboard where everyone in the value stream can see it.\n",
      "\n",
      "This was the philosophy that guided the development of one of the most widely used metrics libraries, called StatsD, which was created and open-sourced at Etsy. As John Allspaw described, \"We designed StatsD to prevent any developer from saying, 'It's too much of a hassle to instrument my code.' Now they can do it with one line of code. It was important to us that for a developer, adding production telemetry didn't feel as difficult as doing a database schema change.\"\n",
      "\n",
      "StatsD can generate timers and counters with one line of code (in Ruby, Perl, Python, Java, and other languages) and is often used in conjunction with Graphite or Grafana, which renders metric events into graphs and dashboards.\n",
      "\n",
      "  **Figure 27:** One line of code to generate telemetry using StatsD and Graphite at Etsy (Source: Ian Malpass, \"Measure Anything, Measure Everything.\")\n",
      "\n",
      "Figure 27 above shows an example of how a single line of code creates a user login event (in this case, one line of PHP code: \"StatsD::increment(\"login.successes\")). The resulting graph shows the number of successful and failed logins per minute, and overlaid on the graph are vertical lines that represent a production deployment.\n",
      "\n",
      "When we generate graphs of our telemetry, we will also overlay onto them when production changes occur, because we know that the significant majority of production issues are caused by production changes, which include code deployments. This is part of what allows us to have a high rate of change, while still preserving a safe system of work.\n",
      "\n",
      "Alternative libraries to StatsD that allow developers to generate production telemetry can be easily aggregated and analyzed include JMX and codahale metrics. Other tools that create metrics invaluable for problem solving include New Relic, AppDynamics, and Dynatrace. Tools such as munin and collectd can be used to create similar functionality.¶\n",
      "\n",
      "By generating production telemetry as part of our daily work, we create an ever-improving capability to not only see problems as they occur, but also to design our work so that problems in design and operations can be revealed, allowing an increasing number of metrics to be tracked, as we saw in the Etsy case study.\n",
      "\n",
      "## CREATE SELF-SERVICE ACCESS TO TELEMETRY AND INFORMATION RADIATORS\n",
      "\n",
      "In the previous steps, we enabled Development and Operations to create and improve production telemetry as part of their daily work. In this step, our goal is to radiate this information to the rest of the organization, ensuring that anyone who wants information about any of the services we are running can get it without needing production system access or privileged accounts, or having to open up a ticket and wait for days for someone to configure the graph for them.\n",
      "\n",
      "By making telemetry fast, easy to get, and sufficiently centralized, everyone in the value stream can share a common view of reality. Typically, this means that production metrics will be radiated on web pages generated by a centralized server, such as Graphite or any of the other technologies described in the previous section.\n",
      "\n",
      "We want our production telemetry to be highly visible, which means putting it in central areas where Development and Operations work, thus allowing everyone who is interested to see how our services are performing. At a minimum, this includes everyone in our value stream, such as Development, Operations, Product Management, and Infosec.\n",
      "\n",
      "This is often referred to as an _information radiator_ , defined by the Agile Alliance as \"the generic term for any of a number of handwritten, drawn, printed, or electronic displays which a team places in a highly visible location, so that all team members as well as passers-by can see the latest information at a glance: count of automated tests, velocity, incident reports, continuous integration status, and so on. This idea originated as part of the Toyota Production System.\"\n",
      "\n",
      "By putting information radiators in highly visible places, we promote responsibility among team members, actively demonstrating the following values:\n",
      "\n",
      "  * The team has nothing to hide from its visitors (customers, stakeholders, etc.)\n",
      "  * The team has nothing to hide from itself: it acknowledges and confronts problems\n",
      "\n",
      "Now that we possess the infrastructure to create and radiate production telemetry to the entire organization, we may also choose to broadcast this information to our internal customers and even to our external customers. For example, we might do this by creating publicly-viewable service status pages so that customers can learn how the services they depend upon are performing.\n",
      "\n",
      "Although there may be some resistance to providing this amount of transparency, Ernest Mueller describes the value of doing so:\n",
      "\n",
      "One of the first actions I take when starting in an organization is to use information radiators to communicate issues and detail the changes we are making—this is usually extremely well-received by our business units, who were often left in the dark before. And for Development and Operations groups who must work together to deliver a service to others, we need that constant communication, information, and feedback.\n",
      "\n",
      "We may even extend this transparency further—instead of trying to keep customer-impacting problems a secret, we can broadcast this information to our external customers. This demonstrates that we value transparency, thereby helping to build and earn customers' trust.** See Appendix 10.\n",
      "\n",
      "Case Study   \n",
      "Creating Self-Service Metrics at LinkedIn (2011)\n",
      "\n",
      "As described in Part III, LinkedIn was created in 2003 to help users connect \"to your network for better job opportunities.\" By November 2015, LinkedIn had over 350 million members generating tens of thousands of requests per second, resulting in millions of queries per second on the LinkedIn back-end systems.\n",
      "\n",
      "Prachi Gupta, Director of Engineering at LinkedIn, wrote in 2011 about the importance of production telemetry: \"At LinkedIn, we emphasize making sure the site is up and our members have access to complete site functionality at all times. Fulfilling this commitment requires that we detect and respond to failures and bottlenecks as they start happening. That's why we use these time-series graphs for site monitoring to detect and react to incidents within minutes...This monitoring technique has proven to be a great tool for engineers. It lets us move fast and buys us time to detect, triage, and fix problems.\"\n",
      "\n",
      "However, in 2010, even though there was an incredibly large volume of telemetry being generated, it was extremely difficult for engineers to get access to the data, let alone analyze it. Thus began Eric Wong's summer intern project at LinkedIn, which turned into the production telemetry initiative that created InGraphs.\n",
      "\n",
      "Wong wrote, \"To get something as simple as CPU usage of all the hosts running a particular service, you would need to file a ticket and someone would spend 30 minutes putting it [a report] together.\"\n",
      "\n",
      "At the time, LinkedIn was using Zenoss to collect metrics, but as Wong explains, \"Getting data from Zenoss required digging through a slow web interface, so I wrote some python scripts to help streamline the process. While there was still manual intervention in setting up metric collection, I was able to cut down the time spent navigating Zenoss' interface.\"\n",
      "\n",
      "Over the course of the summer, he continued to add functionality to InGraphs so that engineers could see exactly what they wanted to see, adding the ability to make calculations across multiple datasets, view week-over-week trending to compare historical performance, and even define custom dashboards to pick exactly which metrics would be displayed on a single page.\n",
      "\n",
      "In writing about the outcomes of adding functionality to InGraphs and the value of this capability, Gupta notes, \"The effectiveness of our monitoring system was highlighted in an instant where our InGraphs monitoring functionality tied to a major web-mail provider started trending downwards and the provider realized they had a problem in their system only after we reached out to them!\"\n",
      "\n",
      "What started off as a summer internship project is now one of the most visible parts of LinkedIn operations. InGraphs has been so successful that the real-time graphs are featured prominently in the company's engineering offices where visitors can't fail to see them.\n",
      "\n",
      "## FIND AND FILL ANY TELEMETRY GAPS\n",
      "\n",
      "We have now created the infrastructure necessary to quickly create production telemetry throughout our entire application stack and radiate it throughout our organization.\n",
      "\n",
      "In this step, we will identify any gaps in our telemetry that impede our ability to quickly detect and resolve incidents—this is especially relevant if Dev and Ops currently have little (or no) telemetry. We will use this data later to better anticipate problems, as well as to enable everyone to gather the information they need to make better decisions to achieve organizational goals.\n",
      "\n",
      "Achieving this requires that we create enough telemetry at all levels of the application stack for all our environments, as well as for the deployment pipelines that support them. We need metrics from the following levels:\n",
      "\n",
      "  * **Business level:** Examples include the number of sales transactions, revenue of sales transactions, user signups, churn rate, A/B testing results, etc.\n",
      "  * **Application level:** Examples include transaction times, user response times, application faults, etc.\n",
      "  * **Infrastructure level (e.g., database, operating system, networking, storage):** Examples include web server traffic, CPU load, disk usage, etc.\n",
      "  * **Client software level (e.g., JavaScript on the client browser, mobile application):** Examples include application errors and crashes, user measured transaction times, etc.\n",
      "  * **Deployment pipeline level:** Examples include build pipeline status (e.g., red or green for our various automated test suites), change deployment lead times, deployment frequencies, test environment promotions, and environment status.\n",
      "\n",
      "By having telemetry coverage in all of these areas, we will be able to see the health of everything that our service relies upon, using data and facts instead of rumors, finger-pointing, blame, and so forth.\n",
      "\n",
      "Further, we better enable detection of security-relevant events by monitoring any application and infrastructure faults (e.g., abnormal program terminations, application errors and exceptions, and server and storage errors). Not only does this telemetry better inform Development and Operations when our services are crashing, but these errors are often indicators that a security vulnerability is being actively exploited.\n",
      "\n",
      "By detecting and correcting problems earlier, we can fix them while they are small and easy to fix, with fewer customers impacted. Furthermore, after every production incident, we should identify any missing telemetry that could have enabled faster detection and recovery; or, better yet, we can identify these gaps during feature development in our peer review process.\n",
      "\n",
      "### APPLICATION AND BUSINESS METRICS\n",
      "\n",
      "At the application level, our goal is to ensure that we are generating telemetry not only around application health (e.g., memory usage, transaction counts, etc.), but also to measure to what extent we are achieving our organizational goals (e.g., number of new users, user login events, user session lengths, percent of users active, how often certain features are being used, and so forth).\n",
      "\n",
      "For example, if we have a service that is supporting e-commerce, we want to ensure that we have telemetry around all of the user events that lead up to a successful transaction that generates revenue. We can then instrument all the user actions that are required for our desired customer outcomes.\n",
      "\n",
      "These metrics will vary according to different domains and organizational goals. For instance, for e-commerce sites, we may want to maximize the time spent on the site; however, for search engines, we may want to reduce the time spent on the site, since long sessions may indicate that users are having difficulty finding what they're looking for.\n",
      "\n",
      "In general, business metrics will be part of a _customer acquisition funnel_ , which is the theoretical steps a potential customer will take to make a purchase. For instance, in an e-commerce site, the measurable journey events include total time on site, product link clicks, shopping cart adds, and completed orders.\n",
      "\n",
      "Ed Blankenship, Senior Product Manager for Microsoft Visual Studio Team Services, describes, \"Often, feature teams will define their goals in an acquisition funnel, with the goal of their feature being used in every customer's daily work. Sometimes they're informally described as 'tire kickers,' 'active users,' 'engaged users,' and 'deeply engaged users,' with telemetry supporting each stage.\"\n",
      "\n",
      "Our goal is to have every business metric be _actionable_ —these top metrics should help inform how to change our product and be amenable to experimentation and A/B testing. When metrics aren't actionable, they are likely vanity metrics that provide little useful information—these we want to store, but likely not display, let alone alert on.\n",
      "\n",
      "Ideally, anyone viewing our information radiators will be able to make sense of the information we are showing in the context of desired organizational outcomes, such as goals around revenue, user attainment, conversion rates, etc. We should define and link each metric to a business outcome metric at the earliest stages of feature definition and development, and measure the outcomes after we deploy them in production. Furthermore, doing this helps product owners describe the business context of each feature for everyone in the value stream.\n",
      "\n",
      "  **Figure 28:** Amount of user excitement of new features in user forum posts after deployments (Source: Mike Brittain, \"Tracking Every Release,\" CodeasCraft.com, December 8, 2010, <https://codeascraft.com/2010/12/08/track-every-release/>.)\n",
      "\n",
      "Further business context can be created by being aware of and visually displaying time periods relevant to high-level business planning and operations, such as high transaction periods associated with peak holiday selling seasons, end-of-quarter financial close periods, or scheduled compliance audits. This information may be used as a reminder to avoid scheduling risky changes when availability is critical or avoid certain activities when audits are in progress.\n",
      "\n",
      "By radiating how customers interact with what we build in the context of our goals, we enable fast feedback to feature teams so they can see whether the capabilities we are building are actually being used and to what extent they are achieving business goals. As a result, we reinforce the cultural expectations that instrumenting and analyzing customer usage is also a part of our daily work, so we better understand how our work contributes to our organizational goals.\n",
      "\n",
      "### INFRASTRUCTURE METRICS\n",
      "\n",
      "Just as we did for application metrics, our goal for production and non-production infrastructure is to ensure that we are generating enough telemetry so that if a problem occurs in any environment, we can quickly determine whether infrastructure is a contributing cause of the problem. Furthermore, we must be able to pinpoint exactly what in the infrastructure is contributing to the problem (e.g., database, operating system, storage, networking, etc.).\n",
      "\n",
      "We want to make as much infrastructure telemetry visible as possible, across all the technology stakeholders, ideally organized by service or application. In other words, when something goes wrong with something in our environment, we need to know exactly what applications and services could be or are being affected.††\n",
      "\n",
      "In decades past, creating links between a service and the production infrastructure it depended on was often a manual effort (such as ITIL CMDBs or creating configuration definitions inside alerting tools in tools such as Nagios). However, increasingly these links are now registered automatically within our services, which are then dynamically discovered and used in production through tools such as Zookeeper, Etcd, Consul, etc.\n",
      "\n",
      "These tools enable services to register themselves, storing information that other services need to interact with it (e.g., IP address, port numbers, URIs). This solves the manual nature of the ITIL CMDB and is absolutely necessary when services are made up of hundreds (or thousands or even millions) of nodes, each with dynamically assigned IP addresses.‡‡\n",
      "\n",
      "Regardless of how simple or complex our services are, graphing our business metrics alongside our application and infrastructure metrics allow us to detect when things go wrong. For instance, we may see that new customer signups drop to 20% of daily norms, and then immediately also see that all our database queries are taking five times longer than normal, enabling us to focus our problem solving.\n",
      "\n",
      "Furthermore, business metrics create context for our infrastructure metrics, enabling Development and Operations to better work together toward common goals. As Jody Mulkey, CTO of Ticketmaster/LiveNation, observes, \"Instead of measuring Operations against the amount of downtime, I find it's much better to measure both Dev and Ops against the real business consequences of downtime: how much revenue should we have attained, but didn't.\"§§\n",
      "\n",
      "Note that in addition to monitoring our production services, we also need telemetry for those services in our pre-production environments (e.g., development, test, staging, etc.). Doing this enables us to find and fix issues before they go into production, such as detecting when we have ever-increasing database insert times due to a missing table index.\n",
      "\n",
      "### OVERLAYING OTHER RELEVANT INFORMATION ONTO OUR METRICS\n",
      "\n",
      "Even after we have created our deployment pipeline that allows us to make small and frequent production changes, changes still inherently create risk. Operational side effects are not just outages, but also significant disruptions and deviations from standard operations.\n",
      "\n",
      "To make changes visible, we make work visible by overlaying all production deployment activities on our graphs. For instance, for a service that handles a large number of inbound transactions, production changes can result in a significant _settling period_ , where performance degrades substantially as all cache lookups miss.\n",
      "\n",
      "To better understand and preserve quality of service, we want to understand how quickly performance returns to normal, and if necessary, take steps to improve performance.\n",
      "\n",
      "Similarly, we want to overlay other useful operational activities, such as when the service is under maintenance or being backed up, in places where we may want to display or suppress alerts.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "The improvements enabled by production telemetry from Etsy and LinkedIn show us how critical it is to see problems as they occur, so we can search out the cause and quickly remedy the situation. By having all elements of our service emitting telemetry that can be analyzed, whether it is in our application, database, or in our environment, and making that telemetry widely available, we can find and fix problems long before they cause something catastrophic, ideally long before a customer even notices that something is wrong. The result is not only happier customers, but, by reducing the amount of firefighting and crises when things go wrong, we have a happier and more productive workplace with less stress and lower levels of burnouts.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Example tools include Sensu, Nagios, Zappix, LogsStash, Splunk, Sumo Logic, Datadog, and Riemann.\n",
      "\n",
      "‡ A variety of application logging libraries exist that make it easy for developers to create useful telemetry, and we should choose one that allows us to send all our application logs to the centralized logging infrastructure that we created in the previous section. Popular examples include rrd4j and log4j for Java, and log4r and ruby-cabin for Ruby.\n",
      "\n",
      "§ In 2004, Gene Kim, Kevin Behr and George Spafford described this as a symptom of lacking a \"culture of causality,\" noting that high-performing organizations recognize that 80% of all outages are caused by change and 80% of MTTR is spent trying to determine what changed.\n",
      "\n",
      "¶ A whole other set of tools to aid in monitoring, aggregation, and collection include Splunk, Zabbix, Sumo Logic, DataDog, as well as Nagios, Cacti, Sensu, RRDTool, Netflix Atlas, Riemann, and others. Analysts often call this broad category of tools \"application performance monitors.\"\n",
      "\n",
      "** Creating a simple dashboard should be part of creating any new product or service—automated tests should confirm that both the service and dashboard are working correctly, helping both our customers and our ability to safely deploy code.\n",
      "\n",
      "†† Exactly as an ITIL Configuration Management Database (CMDB) would prescribe.\n",
      "\n",
      "‡‡ Consul may be of specific interest, as it creates an abstraction layer that easily enables service mapping, monitoring, locks, and key-value configuration stores, as well as host clustering and failure detection.\n",
      "\n",
      "§§ This could be the cost of production downtime or the costs associated with a late feature. In product development terms, the second metric is known as cost of delay, and is key to making effective prioritization decisions.\n",
      "\n",
      "# 15Analyze Telemetry to Better Anticipate Problems and Achieve Goals\n",
      "\n",
      "As we saw in the previous chapter, we need sufficient production telemetry in our applications and infrastructure to see and solve problems as they occur. In this chapter, we will create tools that allow us to discover variances and ever-weaker failure signals hidden in our production telemetry so we can avert catastrophic failures. Numerous statistical techniques will be presented, along with case studies demonstrating their use.\n",
      "\n",
      "A great example of analyzing telemetry to proactively find and fix problems before customers are impacted can be seen at Netflix, a global provider of streaming films and television series. Netflix had revenue of $6.2 billion from seventy-five million subscribers in 2015. One of their goals is to provide the best experience to those watching videos online around the world, which requires a robust, scalable, and resilient delivery infrastructure. Roy Rapoport describes one of the challenges of managing the Netflix cloud-based video delivery service: \"Given a herd of cattle that should all look and act the same, which cattle look different from the rest? Or more concretely, if we have a thousand-node stateless compute cluster, all running the same software and subject to the same approximate traffic load, our challenge is to find any nodes that don't look like the rest of the nodes.\"\n",
      "\n",
      "One of the statistical techniques that the team used at Netflix in 2012 was _outlier detection,_ defined by Victoria J. Hodge and Jim Austin of the University of York as detecting \"abnormal running conditions from which significant performance degradation may well result, such as an aircraft engine rotation defect or a flow problem in a pipeline.\"\n",
      "\n",
      "Rapoport explains that Netflix \"used outlier detection in a very simple way, which was to first compute what was the 'current normal' right now, given population of nodes in a compute cluster. And then we identified which nodes didn't fit that pattern, and removed those nodes from production.\"\n",
      "\n",
      "Rapoport continues, \"We can automatically flag misbehaving nodes without having to actually define what the 'proper' behavior is in any way. And since we're engineered to run resiliently in the cloud, we don't tell anyone in Operations to do something—instead, we just kill the sick or misbehaving compute node, and then log it or notify the engineers in whatever form they want.\"\n",
      "\n",
      "By implementing the Server Outlier Detection process, Rapoport states, Netflix has \"massively reduced the effort of finding sick servers, and, more importantly, massively reduced the time require Rapoport states d to fix them, resulting in improved service quality. The benefit of using these techniques to preserve employee sanity, work/life balance, and service quality cannot be overstated.\" The work done at Netflix highlights one very specific way we can use telemetry to mitigate problems before they impact our customer.\n",
      "\n",
      "Throughout this chapter we will explore many statistical and visualization techniques (including outlier detection) that we can use to analyze our telemetry to better anticipate problems. This enables us to solve problems faster, cheaper, and earlier than ever, before our customer or anyone in our organization is impacted; furthermore, we will also create more context for our data to help us make better decisions and achieve our organizational goals.\n",
      "\n",
      "## USE MEANS AND STANDARD DEVIATIONS TO DETECT POTENTIAL PROBLEMS\n",
      "\n",
      "One of the simplest statistical techniques that we can use to analyze a production metric is computing its _mean_ (or average) and _standard deviations_. By doing this, we can create a filter that detects when this metric is significantly different from its norm, and even configure our alerting so that we can take corrective action (e.g., notify on-call production staff at 2 a.m. to investigate when database queries are significantly slower than average).\n",
      "\n",
      "When critical production services have problems, waking people at 2 a.m. may be the right thing to do. However, when we create alerts that are not actionable or are false-positives, we've unnecessarily woken up people in the middle of the night. As John Vincent, an early leader in the DevOps movement, observed, \"Alert fatigue is the single biggest problem we have right now...We need to be more intelligent about our alerts or we'll all go insane.\"\n",
      "\n",
      "We create better alerts by increasing the signal-to-noise ratio, focusing on the variances or outliers that matter. Suppose we are analyzing the number of unauthorized login attempts per day. Our collected data has a Gaussian distribution (i.e., normal or bell curve distribution) that matches the graph in the figure 29. The vertical line in the middle of the bell curve is the mean, and the first, second, and third standard deviations indicated by the other vertical lines contain 68%, 95%, and 99.7% of the data, respectively.\n",
      "\n",
      "  **Figure 29:** Standard deviations (σ) & mean (µ) with Gaussian distribution (Source: Wikipedia's \"Normal Distribution\" entry, https://en.wikipedia.org/wiki/Normal_distribution.)\n",
      "\n",
      "A common use of standard deviations is to periodically inspect the data set for a metric and alert if it has significantly varied from the mean. For instance, we may set an alert for when the number of unauthorized login attempts per day is three standard deviations greater than the mean. Provided that this data set has Gaussian distribution, we would expect that only 0.3% of the data points would trigger the alert.\n",
      "\n",
      "Even this simple type of statistical analysis is valuable, because no one had to define a static threshold value, something which is infeasible if we are tracking thousands or hundreds of thousands of production metrics.\n",
      "\n",
      "For the remainder of this book, we will use the terms _telemetry, metric,_ and _data sets_ interchangeably—in other words, a _metric_ (e.g., \"page load times\") will map to a _data set_ (e.g., 2 ms, 8 ms, 11 ms, etc.), the term used by statisticians to describe a matrix of data points where each column represents a variable of which statistical operations are performed.\n",
      "\n",
      "## INSTRUMENT AND ALERT ON UNDESIRED OUTCOMES\n",
      "\n",
      "Tom Limoncelli, co-author of _The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems_ and a former Site Reliability Engineer at Google, relates the following story on monitoring: \"When people ask me for recommendations on what to monitor, I joke that in an ideal world, we would delete all the alerts we currently have in our monitoring system. Then, after each user-visible outage, we'd ask what indicators would have predicted that outage and then add those to our monitoring system, alerting as needed. Repeat. Now we only have alerts that prevent outages, as opposed to being bombarded by alerts after an outage already occurred.\"\n",
      "\n",
      "In this step, we will replicate the outcomes of such an exercise. One of the easiest ways to do this is to analyze our most severe incidents in the recent past (e.g., 30 days) and create a list of telemetry that could have enabled earlier and faster detection and diagnosis of the problem, as well as easier and faster confirmation that an effective fix had been implemented.\n",
      "\n",
      "For instance, if we had an issue where our NGINX web server stopped responding to requests, we would look at the leading indicators that could have warned us earlier that we were starting to deviate from standard operations, such as:\n",
      "\n",
      "  * **Application level:** increasing web page load times, etc.\n",
      "  * **OS level:** server free memory running low, disk space running low, etc.\n",
      "  * **Database level:** database transaction times taking longer than normal, etc.\n",
      "  * **Network level:** number of functioning servers behind the load balancer dropping, etc.\n",
      "\n",
      "Each of these metrics is a potential precursor to a production incident. For each, we would configure our alerting systems to notify them when they deviate sufficiently from the mean, so that we can take corrective action.\n",
      "\n",
      "By repeating this process on ever-weaker failure signals, we find problems ever earlier in the life cycle, resulting in fewer customer impacting incidents and near misses. In other words, we are preventing problems as well as enabling quicker detection and correction.\n",
      "\n",
      "## PROBLEMS THAT ARISE WHEN OUR TELEMETRY DATA HAS NON-GAUSSIAN DISTRIBUTION\n",
      "\n",
      "Using means and standard deviations to detect variance can be extremely useful. However, using these techniques on many of the telemetry data sets that we use in Operations will not generate the desired results. As Dr. Toufic Boubez observes, \"Not only will we get wakeup calls at 2 a.m., we'll get them at 2:37 a.m., 4:13 a.m., 5:17 a.m. This happens when the underlying data that we're monitoring doesn't have a Gaussian distribution.\"\n",
      "\n",
      "In other words, when the distribution of the data set does not have the Gaussian bell curve described earlier, the properties associated with standard deviations do not apply. For example, consider the scenario in which we are monitoring the number of file downloads per minute from our website. We want to detect periods when we have unusually high numbers of downloads, such as when our download rate is greater than three standard deviations from our average, so that we can proactively add more capacity.\n",
      "\n",
      "Figure 30 shows our number of simultaneous downloads per minute over time, with a bar overlaid on top. When the bar is black, the number of downloads within a given period (sometimes called a \"sliding window\") is at least three standard deviations from the average. Otherwise, it is gray.\n",
      "\n",
      "  **Figure 30:** Downloads per minute: over-alerting when using \"3 standard deviation\" rule   \n",
      "(Source: Dr. Toufic Boubez, \"Simple math for anomaly detection.\")\n",
      "\n",
      "The obvious problem that the graph shows is that we are alerting almost all of the time. This is because in almost any given period of time, we have instances when the download count exceeds our three standard deviation threshold.\n",
      "\n",
      "To confirm this, when we create a histogram (see figure 31) that shows the frequency of downloads per minute, we can see that it does not have the classic, symmetrical bell curve shape. Instead, it is obvious that the distribution is skewed toward the lower end, showing that the majority of the time we have very few downloads per minute but that download counts frequently spike three standard deviations higher.\n",
      "\n",
      "  **Figure 31:** Downloads per minute: histogram of data showing non-Gaussian distribution   \n",
      "(Source: Dr. Toufic Boubez, \"Simple math for anomaly detection.\")\n",
      "\n",
      "Many production data sets are non-Gaussian distribution. Dr. Nicole Forsgren explains, \"In Operations, many of our data sets have what we call 'chi squared' distribution. Using standard deviations for this data not only results in over- or under-alerting, but it also results in nonsensical results.\" She continues, \"When you compute the number of simultaneous downloads that are three standard deviations below the mean, you end up with a negative number, which obviously doesn't make sense.\"\n",
      "\n",
      "Over-alerting causes Operations engineers to be woken up in the middle of the night for protracted periods of time, even when there are few actions that they can appropriately take. The problem associated with under-alerting is just as significant. For instance, suppose we are monitoring the number of completed transactions, and the completed transaction count drops by 50% in the middle of the day due to a software component failure. If this is still within three standard deviations of the mean, no alert will be generated, meaning that our customers will discover the problem before we do, at which point the problem may be much more difficult to solve.\n",
      "\n",
      "Fortunately, there are techniques we can use to detect anomalies in even non-Gaussian data sets, which are described next.\n",
      "\n",
      "Case Study   \n",
      "Auto-Scaling Capacity at Netflix (2012)\n",
      "\n",
      "Another tool developed at Netflix to increase service quality, Scryer, addresses some of the shortcomings of Amazon Auto Scaling (AAS), which dynamically increases and decreases AWS compute server counts based on workload data. Scryer works by predicting what customer demands will be based on historical usage patterns and provisions the necessary capacity.\n",
      "\n",
      "Scryer addressed three problems with AAS. The first was dealing with rapid spikes in demand. Because AWS instance startup times can be ten to forty-five minutes, additional compute capacity was often delivered too late to deal with spikes in demand. The second problem was that after outages, the rapid decrease in customer demand led to AAS removing too much compute capacity to handle future incoming demand. The third problem was that AAS didn't factor in known usage traffic patterns when scheduling compute capacity.\n",
      "\n",
      "  **Figure 32:** Netflix customer viewing demand for five days (Source: Daniel Jacobson, Danny Yuan, and Neeraj Joshi, \"Scryer: Netflix's Predictive Auto Scaling Engine,\" _The Netflix Tech Blog_ , November 5, 2013, <http://techblog.netflix.com/2013/11/scryer-netflixs-predictive-auto-scaling.html>.)\n",
      "\n",
      "Netflix took advantage of the fact that their consumer viewing patterns were surprisingly consistent and predictable, despite not having Gaussian distributions. Below is a chart reflecting customer requests per second throughout the work week, showing regular and consistent customer viewing patterns Monday through Friday.\n",
      "\n",
      "Scryer uses a combination of outlier detections to throw out spurious data points and then uses techniques such as Fast Fourier Transform (FFT) and linear regression to smooth the data while preserving legitimate traffic spikes that recur in their data. The result is that Netflix can forecast traffic demand with surprising accuracy.\n",
      "\n",
      "  **Figure 33:** Netflix Scryer forecasting customer traffic and the resulting AWS schedule of compute resources (Source: Jacobson, Yuan, Joshi, \"Scryer: Netflix's Predictive Auto Scaling Engine.\")\n",
      "\n",
      "Only months after first using Scryer in production, Netflix significantly improved their customer viewing experience, improved service availability, and reduced Amazon EC2 costs.\n",
      "\n",
      "## USING ANOMALY DETECTION TECHNIQUES\n",
      "\n",
      "When our data does not have Gaussian distribution, we can still find noteworthy variances using a variety of methods. These techniques are broadly categorized as _anomaly detection,_ often defined as \"the search for items or events which do not conform to an expected pattern.\" Some of these capabilities can be found inside our monitoring tools, while others may require help from people with statistical skills.\n",
      "\n",
      "Tarun Reddy, VP of Development and Operations at Rally Software, actively advocates this active collaboration between Operations and statistics, observing, \"To better enable service quality, we put all our production metrics into Tableau, a statistical analysis software package. We even have an Ops engineer trained in statistics who writes R code (another statistical package)—this engineer has her own backlog, filled with requests from other teams inside the company who want to find variance ever earlier, before it causes an even larger variance that could affect our customers.\"\n",
      "\n",
      "One of the statistical techniques we can use is called _smoothing_ , which is especially suitable if our data is a time series, meaning each data point has a time stamp (e.g., download events, completed transaction events, etc.). Smoothing often involves using moving averages (or rolling averages), which transform our data by averaging each point with all the other data within our sliding window. This has the effect of smoothing out short-term fluctuations and highlighting longer-term trends or cycles.†\n",
      "\n",
      "An example of this smoothing effect is shown in the figure 34. The black line represents the raw data, while the blue line indicates the thirty day moving average (i.e., the average of the trailing thirty days).‡\n",
      "\n",
      "  **Figure 34:** Autodesk share price and thirty day moving average filter (Source: Jacobson, Yuan, Joshi, \"Scryer: Netflix's Predictive Auto Scaling Engine.\")\n",
      "\n",
      "More exotic filtering techniques exist, such as Fast Fourier Transforms, which has been widely used in image processing, and the Kolmogorov-Smirnov test (found in Graphite and Grafana), which is often used to find similarities or differences in periodic/seasonal metric data.\n",
      "\n",
      "We can expect that a large percentage of telemetry concerning user data will have periodic/seasonal similarities—web traffic, retail transactions, movie watching, and many other user behaviors have very regular and surprisingly predictable daily, weekly, and yearly patterns. This enables us to be able to detect situations that vary from historical norms, such as when our order transaction rate on a Tuesday afternoon drops to 50% of our weekly norms.\n",
      "\n",
      "Because of the usefulness of these techniques in forecasting, we may be able to find people in the Marketing or Business Intelligence departments with the knowledge and skills necessary to analyze this data. We may want to seek these people out and explore working together to identify shared problems and use improved anomaly detection and incident prediction to solve them.§\n",
      "\n",
      "Case Study   \n",
      "Advanced Anomaly Detection (2014)\n",
      "\n",
      "At Monitorama in 2014, Dr. Toufic Boubez described the power of using anomaly detection techniques, specifically highlighting the effectiveness of the Komogorov-Smirnov test, a technique that is often used in statistics to determine whether two data sets differ significantly and is found in the popular Graphite and Grafana tool. The purpose of presenting this case study here is not as a tutorial, but to demonstrate how a class of statistical techniques can be used in our work, as well as how it's likely being used in our organizations in completely different applications.\n",
      "\n",
      "Figure 35 shows the number of transactions per minute at an e-commerce site. Note the weekly periodicity of the graph, with transaction volume dropping on the weekends. By visual inspection, we can see that something peculiar seems to happen on the fourth week when normal transaction volume doesn't return to normal levels on Monday. This suggests an event we should investigate.\n",
      "\n",
      "  **Figure 35:** Transaction volume: under-alerting using \"3 standard deviation\" rule   \n",
      "(Source: Dr. Toufic Boubez, \"Simple math for anomaly detection.\")\n",
      "\n",
      "Using the three standard deviation rule would only alert us twice, missing the critical Monday dropoff in transaction volume. Ideally, we would also want to be alerted that the data has drifted from our expected Monday pattern.\n",
      "\n",
      "\"Even saying 'Kolmogorov-Smirnov' is a great way to impress everyone,\" Dr. Boubez jokes. \"But what Ops engineers should tell statisticians is that these types of _non-parametric_ techniques are great for Operations data, because it makes no assumptions about normality or any other probability distribution, which is crucial for us to understand what's going on in our very complex systems. These techniques compare two probability distributions, allowing us to compare periodic or seasonal data, which helps us find variances in data that varies from day to day or week to week.\"\n",
      "\n",
      "Figure 36, on the following page, shows is the same data set with the K-S filter applied, with the third area highlighting the anomalous Monday where transaction volume didn't return to normal levels. This would have alerted us of a problem in our system that would have been virtually impossible to detect using visual inspection or using standard deviations. In this scenario, this early detection could prevent a customer impacting event, as well as better enable us to achieve our organizational goals.\n",
      "\n",
      "  **Figure 36:** Transaction volume: using Kolmogorov-Smirnov test to alert on anomalies   \n",
      "(Source: Dr. Toufic Boubez, \"Simple math for anomaly detection.\")\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we explored several different statistical techniques that can be used to analyze our production telemetry so we can find and fix problems earlier than ever, often when they are still small and long before they cause catastrophic outcomes. This enables us to find ever-weaker failure signals that we can then act upon, creating an ever safer system of work, as well as increasing our ability to achieve our goals.\n",
      "\n",
      "Specific case studies were presented, including how Netflix used these techniques to proactively remove compute servers from production and auto-scale their compute infrastructure. We also discussed how to use a moving average and the Kolmogorov-Smirnov filter, both of which can be found in popular telemetry graphing tools.\n",
      "\n",
      "In the next chapter, we will describe how to integrate production telemetry into the daily work of Development in order to make deployments safer and improve the system as a whole.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Smoothing and other statistical techniques are also used to manipulate graphic and audio files. For instance, image smoothing (or blurring) as each pixel is replaced by the average of all its neighbors.\n",
      "\n",
      "‡ Other examples of smoothing filters include weighted moving averages or exponential smoothing (which linearly or exponentially weight more recent data points over older data points, respectively), and so forth.\n",
      "\n",
      "§ Tools we can using to solve these types of problems include Microsoft Excel (which remains one of the easiest and fastest ways to manipulate data for one-time purposes), as well as statistical packages such as SPSS, SAS, and the open source R project, now one of the most widely used statistical packages. Many other tools have been created, including several that Etsy has open-sourced, such as Oculus, which finds graphs with similar shapes that may indicate correlation; Opsweekly, which tracks alert volumes and frequencies; and Skyline, which attempts to identify anomalous behavior in system and application graphs.\n",
      "\n",
      "# 16Enable Feedback So Development and Operations Can Safely Deploy Code\n",
      "\n",
      "In 2006, Nick Galbreath was VP of Engineering at Right Media, responsible for both the Development and Operations departments for an online advertising platform that displayed and served over ten billion impressions daily.\n",
      "\n",
      "Galbreath described the competitive landscape they operated in:\n",
      "\n",
      "In our business, ad inventory levels were extremely dynamic, so we needed to respond to market conditions within minutes. This meant that Development had to be able to quickly make code changes and get them into production as soon as possible, otherwise we would lose to faster competitors. We found that having a separate group for testing, and even deployment, was simply too slow. We had to integrate all these functions into one group, with shared responsibilities and goals. Believe it or not, our biggest challenge was getting developers to overcome their fear of deploying their own code!\n",
      "\n",
      "There is an interesting irony here: Dev often complains about Ops being afraid to deploy code. But in this case, when given the power to deploy their own code, developers became just as afraid to perform code deployments.\n",
      "\n",
      "The fear of deploying code that was shared by both Dev and Ops at Right Media is not unusual. However, Galbreath observed that providing faster and more frequent feedback to engineers performing deployments (whether Dev or Ops), as well as reducing the batch size of their work, created safety and then confidence.\n",
      "\n",
      "After observing many teams go through this transformation, Galbreath describes their progression as follows:\n",
      "\n",
      "We start with no one in Dev or Ops being willing to push the \"deploy code\" button that we've built that automates the entire code deployment process, because of the paralyzing fear of being the first person to potentially bring all of the production systems down. Eventually, when someone is brave enough to volunteer to push their code into production, inevitably, due to incorrect assumptions or production subtleties that weren't fully appreciated, the first production deployment doesn't go smoothly—and because we don't have enough production telemetry, we only find out about the problems when customers tell us.\n",
      "\n",
      "To fix the problem, our team urgently fixes the code and pushes it into production, but this time with more production telemetry added to our applications and environment. This way, we can actually confirm that our fix restored service correctly, and we'll be able to detect this type of problem before a customer tells us next time.\n",
      "\n",
      "Later, more developers start to push their own code into production. And because we're working in a complex system, we'll still probably break something in production, but this time we'll be able to quickly see what functionality broke, and quickly decide whether to roll back or fix-forward, resolving the problem. This is a huge victory for the entire team and everyone celebrates—we're now on a roll.\n",
      "\n",
      "However, the team wants to improve the outcomes of their deployments, so developers proactively get more peer reviews of their code changes (described in chapter 18), and everyone helps each other write better automated tests so we can find errors before deployment. And because everyone now knows that the smaller our production changes, the fewer problems we will have, developers start checking ever-smaller increments of code more frequently into the deployment pipeline, ensuring that their change is working successfully in production before moving to their next change.\n",
      "\n",
      "We are now deploying code more frequently than ever, and service stability is better than ever too. We have re-discovered that the secret to smooth and continuous flow is making small, frequent changes that anyone can inspect and easily understand.\n",
      "\n",
      "Galbreath observes that the above progression benefits everyone, including Development, Operations, and Infosec. \"As the person who is also responsible for security, it's reassuring to know that we can deploy fixes into production quickly, because changes are going into production throughout the entire day. Furthermore, it always amazes me how interested every engineer becomes in security when you find problems in their code that they are responsible for and that they can quickly fix themselves.\"\n",
      "\n",
      "The Right Media story shows that it is not enough to merely automate the deployment process—we must also integrate the monitoring of production telemetry into our deployment work, as well as establish the cultural norms that everyone is equally responsible for the health of the entire value stream.\n",
      "\n",
      "In this chapter, we create the feedback mechanisms that enable us to improve the health of the value stream at every stage of the service life cycle, from product design through development and deployment and into operation and eventually retirement. By doing this, we ensure that our services are \"production ready,\" even at the earliest stages of the project, as well as integrating the learnings from each release and production problem into our future work, resulting in better safety and productivity for everyone.\n",
      "\n",
      "## USE TELEMETRY TO MAKE DEPLOYMENTS SAFER\n",
      "\n",
      "In this step, we ensure that we are actively monitoring our production telemetry when anyone performs a production deployment, as was illustrated in the Right Media story. This allows whoever is doing the deployment, be it Dev or Ops, to quickly determine whether features are operating as designed after the new release is running in production. After all, we should never consider our code deployment or production change to be done until it is operating as designed in the production environment.\n",
      "\n",
      "We do this by actively monitoring the metrics associated with our feature during our deployment to ensure we haven't inadvertently broken our service—or worse, that we broke another service. If our change breaks or impairs any functionality, we quickly work to restore service, bringing in whoever else is required to diagnose and fix the issue.†\n",
      "\n",
      "As described in Part III, our goal is to catch errors in our deployment pipeline before they get into production. However, there will still be errors that we don't detect, and we rely on production telemetry to quickly restore service. We may choose to turn off broken features with feature toggles (which is often the easiest and least risky option since it involves no deployments to production), or _fix forward_ (i.e., make code changes to fix the defect, which are then pushed into production through the deployment pipeline), or _roll back_ (e.g., switch back to the previous release by using feature toggles or by taking broken servers out of rotation using the blue-green or canary release patterns, etc.)\n",
      "\n",
      "Although fixing forward can often be dangerous, it can be extremely safe when we have automated testing and fast deployment processes, and sufficient telemetry that allows us to quickly confirm whether everything is functioning correctly in production.\n",
      "\n",
      "Figure 37 shows a deployment of PHP code change at Etsy that generated a spike in PHP runtime warnings—in this case, the developer quickly noticed the problem within minutes, and generated a fix and deployed it into production, resolving the issue in less than ten minutes.\n",
      "\n",
      "Because production deployments are one of the top causes of production issues, each deployment and change event is overlaid onto our metric graphs to ensure that everyone in the value stream is aware of relevant activity, enabling better communication and coordination, as well as faster detection and recovery.\n",
      "\n",
      "  **Figure 37:** Deployment to Etsy.com causes PHP run-time warnings and is quickly fixed   \n",
      "(Source: Mike Brittain, \"Tracking Every Release.\")\n",
      "\n",
      "## DEV SHARES PAGER ROTATION DUTIES WITH OPS\n",
      "\n",
      "Even when our production deployments and releases go flawlessly, in any complex service we will still have unexpected problems, such as incidents and outages that happen at inopportune times (every night at 2 a.m.). Left unfixed, these can cause recurring problems and suffering for Ops engineers downstream, especially when these problems are not made visible to the upstream engineers responsible for creating the problem.\n",
      "\n",
      "Even if the problem results in a defect being assigned to the feature team, it may be prioritized below the delivery of new features. The problem may keep recurring for weeks, months, or even years, causing continual chaos and disruption in Operations. This is an example of how upstream work centers can locally optimize for themselves but actually degrade performance for the entire value stream.\n",
      "\n",
      "To prevent this from happening, we will have everyone in the value stream share the downstream responsibilities of handling operational incidents. We can do this by putting developers, development managers, and architects on pager rotation, just as Pedro Canahuati, Facebook Director of Production Engineering, did in 2009. This ensures everyone in the value stream gets visceral feedback on any upstream architectural and coding decisions they make.\n",
      "\n",
      "By doing this, Operations doesn't struggle, isolated and alone with code-related production issues; instead, everyone is helping find the proper balance between fixing production defects and developing new functionality, regardless of where we reside in the value stream. As Patrick Lightbody, SVP of Product Management at New Relic, observed in 2011, \"We found that when we woke up developers at 2 a.m., defects were fixed faster than ever.\"\n",
      "\n",
      "One side effect of this practice is that it helps Development management see that business goals are not achieved simply because features have been marked as \"done.\" Instead, the feature is only done when it is performing as designed in production, without causing excessive escalations or unplanned work for either Development or Operations.‡\n",
      "\n",
      "This practice is equally applicable for market-oriented teams, responsible for both developing the feature and running it in production, and for functionally-oriented teams. As Arup Chakrabarti, Operations Engineering Manager at PagerDuty, observed during a 2014 presentation, \"It's becoming less and less common for companies to have dedicated on-call teams; instead, everyone who touches production code and environments is expected to be reachable in the event of downtime.\"\n",
      "\n",
      "Regardless of how we've organized our teams, the underlying principles remain the same: when developers get feedback on how their applications perform in production, which includes fixing it when it breaks, they become closer to the customer, this creates a buy-in that everyone in the value stream benefits from.\n",
      "\n",
      "## HAVE DEVELOPERS FOLLOW WORK DOWNSTREAM\n",
      "\n",
      "One of the most powerful techniques in interaction and user experience design (UX) is contextual inquiry. This is when the product team watches a customer use the application in their natural environment, often working at their desk. Doing so often uncovers startling ways that customers struggle with the application, such as requiring scores of clicks to perform simple tasks in their daily work, cutting and pasting text from multiple screens, or writing down notes on paper. All of these are examples of compensatory behaviors and workarounds for usability issues.\n",
      "\n",
      "The most common reaction for developers after participating in a customer observation is dismay, often stating \"how awful it was seeing the many ways we have been inflicting pain on our customers.\" These customer observations almost always result in significant learning and a fervent desire to improve the situation for the customer.\n",
      "\n",
      "Our goal is to use this same technique to observe how our work affects our internal customers. Developers should follow their work downstream, so they can see how downstream work centers must interact with their product to get it running into production.§\n",
      "\n",
      "Developers want to follow their work downstream—by seeing customer difficulties firsthand, they make better and more informed decisions in their daily work.\n",
      "\n",
      "By doing this, we create feedback on the non-functional aspects of our code—all the elements that are not related to the customer-facing feature—and identify ways that we can improve deployability, manageability, operability, and so on.\n",
      "\n",
      "UX observation often has a powerful impact on the observers. When describing his first customer observation, Gene Kim, the founder and CTO at Tripwire for thirteen years and co-author of this book, said:\n",
      "\n",
      "One of the worst moments of my professional career was in 2006 when I spent an entire morning watching one of our customers use our product. I was watching him perform an operation that we expected customers to do weekly, and, to our extreme horror, we discovered that it required sixty-three clicks. This person kept apologizing, saying things like, \"Sorry, there's probably a better way to do this.\"\n",
      "\n",
      "Unfortunately, there wasn't a better way to do that operation. Another customer described how initial product setup took 1,300 steps. Suddenly, I understood why the job of managing our product was always assigned to the newest engineer on the team—no one wanted the job of running our product. That was one of the reasons I helped create the UX practice at my company, to help atone for the pain we were inflicting on our customers.\n",
      "\n",
      "UX observation enables the creation of quality at the source and results in far greater empathy for fellow team members in the value stream. Ideally, UX observation helps us as we create codified non-functional requirements to add to our shared backlog of work, eventually allowing us to proactively integrate them into every service we build, which is an important part of creating a DevOps work culture.¶\n",
      "\n",
      "## HAVE DEVELOPERS INITIALLY SELF-MANAGE THEIR PRODUCTION SERVICE\n",
      "\n",
      "Even when Developers are writing and running their code in production-like environments in their daily work, Operations may still experience disastrous production releases because it is the first time we actually see how our code behaves during a release and under true production conditions. This result occurs because operational learnings often occur too late in the software life cycle.\n",
      "\n",
      "Left unaddressed, the result is often production software that is difficult to operate. As an anonymous Ops engineer once said, \"In our group, most system administrators lasted only six months. Things were always breaking in production, the hours were insane, and application deployments were painful beyond belief—the worst part was pairing the application server clusters, which would take us six hours. During each moment, we all felt like the developers personally hated us.\"\n",
      "\n",
      "This can be an outcome of not having enough Ops engineers to support all the product teams and the services we already have in production, which can happen in both functionally- and market-oriented teams.\n",
      "\n",
      "One potential countermeasure is to do what Google does, which is have Development groups self-manage their services in production before they become eligible for a centralized Ops group to manage. By having developers be responsible for deployment and production support, we are far more likely to have a smooth transition to Operations.**\n",
      "\n",
      "To prevent the possibility of problematic, self-managed services going into production and creating organizational risk, we may define launch requirements that must be met in order for services to interact with real customers and be exposed to real production traffic. Furthermore, to help the product teams, Ops engineers should act as consultants to help them make their services production-ready.\n",
      "\n",
      "By creating launch guidance, we help ensure that every product team benefits from the cumulative and collective experience of the entire organization, especially Operations. Launch guidance and requirements will likely include the following:\n",
      "\n",
      "  * **Defect counts and severity:** Does the application actually perform as designed?\n",
      "  * **Type/frequency of pager alerts:** Is the application generating an unsupportable number of alerts in production?\n",
      "  * **Monitoring coverage:** Is the coverage of monitoring sufficient to restore service when things go wrong?\n",
      "  * **System architecture:** Is the service loosely-coupled enough to support a high rate of changes and deployments in production?\n",
      "  * **Deployment process:** Is there a predictable, deterministic, and sufficiently automated process to deploy code into production?\n",
      "  * **Production hygiene:** Is there evidence of enough good production habits that would allow production support to be managed by anyone else?\n",
      "\n",
      "Superficially, these requirements may appear similar to traditional production checklists we have used in the past. However, the key differences are we require effective monitoring to be in place, deployments to be reliable and deterministic, and an architecture that supports fast and frequent deployments.\n",
      "\n",
      "If any deficiencies are found during the review, the assigned Ops engineer should help the feature team resolve the issues or even help re-engineer the service if necessary, so that it can be easily deployed and managed in production.\n",
      "\n",
      "At this time, we may also want to learn whether this service is subject to any regulatory compliance objectives or if it is likely to be in the future:\n",
      "\n",
      "  * Does the service generate a significant amount of revenue? (For example, if it is more than 5% of total revenue of a publicly-held US corporation, it is a \"significant account\" and in-scope for compliance with Section 404 of the Sarbanes-Oxley Act of 2002 [SOX].)\n",
      "  * Does the service have high user traffic or have high outage/impairment costs? (i.e., do operational issues risk creating availability or reputational risk?)\n",
      "  * Does the service store payment cardholder information, such as credit card numbers, or personally identifiable information, such as Social Security numbers or patient care records? Are there other security issues that could create regulatory, contractual obligation, privacy, or reputation risk?\n",
      "  * Does the service have any other regulatory or contractual compliance requirements associated with it, such as US export regulations, PCI-DSS, HIPAA, and so forth?\n",
      "\n",
      "This information helps ensure that we effectively manage not only the technical risks associated with this service, but also any potential security and compliance risks. It also provides essential input into the design of the production control environment.\n",
      "\n",
      "  **Figure 38:** The \"Service Handback\" at Google (Source: \"SRE@Google: Thousands of DevOps Since 2004,\" YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n",
      "\n",
      "By integrating operability requirements into the earliest stages of the development process and having Development initially self-manage their own applications and services, the process of transitioning new services into production becomes smoother, becoming far easier and more predictable to complete. However, for services already in production, we need a different mechanism to ensure that Operations is never stuck with an unsupportable service in production. This is especially relevant for functionally-oriented Operations organizations.\n",
      "\n",
      "In this step, we may create a _service handback mechanism_ —in other words, when a production service becomes sufficiently fragile, Operations has the ability to return production support responsibility back to Development.\n",
      "\n",
      "When a service goes back into a developer-managed state, the role of Operations shifts from production support to consultation, helping the team make the service production-ready.\n",
      "\n",
      "This mechanism serves as our pressure escape valve, ensuring that we never put Operations in a situation where they are trapped into managing a fragile service while an ever-increasing amount of technical debt buries them and amplifies a local problem into a global problem. This mechanism also helps ensure that Operations has enough capacity to work on improvement work and preventive projects.\n",
      "\n",
      "The hand-back remains a long-standing practice at Google and is perhaps one of the best demonstrations of the mutual respect between Dev and Ops engineers. By doing this, Development is able to quickly generate new services, with Ops engineers joining the team when the services become strategically important to the company and, in rare cases, handing them back when they become too troublesome to manage in production.†† The following case study of Site Reliability Engineering at Google describes how the Hand-off Readiness Review and Launch Readiness Review processes evolved, and the benefits that resulted.\n",
      "\n",
      "Case Study   \n",
      "The Launch and Hand-off Readiness Review at Google (2010)\n",
      "\n",
      "One of the many surprising facts about Google is that they have a functional orientation for their Ops engineers, who are referred to as \"Site Reliability Engineers\" (SRE), a term coined by Ben Treynor Sloss in 2004.‡‡ That year, Treynor Sloss started off with a staff of seven SREs that grew to over 1,200 SREs by 2014. As Treynor Sloss said, \"If Google ever goes down, it's my fault.\" Treynor Sloss has resisted creating a single sentence definition of what SREs are, but, he once described SREs as \"what happens when a software engineer is tasked with what used to be called operations.\"\n",
      "\n",
      "Every SRE reports to Treynor Sloss's organization to help ensure consistency of quality of staffing and hiring, and they are embedded into product teams across Google (which also provide their funding). However, SREs are still so scarce they are assigned only to the product teams that have the highest importance to the company or those that must comply with regulatory requirements. Furthermore, those services must have low operational burden. Products that don't meet the necessary criteria remain in a developer-managed state.\n",
      "\n",
      "Even when new products become important enough to the company to warrant being assigned an SRE, developers still must have self-managed their service in production for at least six months before it becomes eligible to have an SRE assigned to the team.\n",
      "\n",
      "To help ensure that these self-managed product teams can still benefit from the collective experience of the SRE organization, Google created two sets of safety checks for two critical stages of releasing new services called the _Launch Readiness Review_ and the _Hand-Off Readiness Review_ (LRR and HRR, respectively).\n",
      "\n",
      "The LRR must be performed and signed off on before any new Google service is made publicly available to customers and receives live production traffic, while the HRR is performed when the service is transitioned to an Ops-managed state, usually months after the LRR. The LRR and HRR checklists are similar, but the HRR is far more stringent and has higher acceptance standards, while the LRR is self-reported by the product teams.\n",
      "\n",
      "Any product team going through an LRR or HRR has an SRE assigned to them to help them understand the requirements and to help them achieve those requirements. The LRR and HRR launch checklists have evolved over time so every team can benefit from the collective experiences of all previous launches, whether successful or unsuccessful. Tom Limoncelli noted during his \"SRE@Google: Thousands of DevOps Since 2004\" presentation in 2012, \"Every time we do a launch, we learn something. There will always be some people who are less experienced than others doing releases and launches. The LRR and HRR checklists are a way to create that organizational memory.\"\n",
      "\n",
      "Requiring product teams to self-manage their own services in production forces Development to walk in the shoes of Ops, but guided by the LRR and HRR, which not only makes service transition easier and more predictable, but also helps create empathy between upstream and downstream work centers.\n",
      "\n",
      "  **Figure 39:** The \"Launch readiness review and hand-offs readiness review\" at Google (Source: \"SRE@Google: Thousands of DevOps Since 2004,\" YouTube video, 45:57, posted by USENIX, January 12, 2012, https://www.youtube.com/watch?v=iIuTnhdTzK0.)\n",
      "\n",
      "Limoncelli noted, \"In the best case, product teams have been using the LRR checklist as a guideline, working on fulfilling it in parallel with developing their service, and reaching out to SREs to get help when they need it.\"\n",
      "\n",
      "Furthermore, Limoncelli observed, \"The teams that have the fastest HRR production approval are the ones that worked with SREs earliest, from the early design stages up until launch. And the great thing is, it's always easy to get an SRE to volunteer to help with your project. Every SRE sees value in giving advice to project teams early, and will likely volunteer a few hours or days to do just that.\"\n",
      "\n",
      "The practice of SREs helping product teams early is an important cultural norm that is continually reinforced at Google. Limoncelli explained, \"Helping product teams is a long-term investment that will pay off many months later when it comes time to launch. It is a form of 'good citizenship' and 'community service' that is valued, it is routinely considered when evaluating engineers for SRE promotions.\"\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we discussed the feedback mechanisms that enable us to improve our service at every stage of our daily work, whether it is deploying changes into production, fixing code when things go wrong and engineers are paged, having developers follow their work downstream, creating non-functional requirements that help development teams write more production-ready code, or even handing problematic services back to be self-managed by Development.\n",
      "\n",
      "By creating these feedback loops, we make production deployments safer, increase the production readiness of code created by Development, and help create a better working relationship between Development and Operations by reinforcing shared goals, responsibilities, and empathy.\n",
      "\n",
      "In the next chapter, we explore how telemetry can enable hypothesis-driven development and A/B testing to perform experiments that help us achieve our organizational goals and win in the marketplace.\n",
      "\n",
      "* * *\n",
      "\n",
      "† By doing this, along with the required architecture, we \"optimize for MTTR, instead of MTBF,\" a popular DevOps maxim to describe our desire to optimize for recovering from failures quickly, as opposed to attempting to prevent failures.\n",
      "\n",
      "‡ ITIL defines warranty as when a service can run in production reliably without intervention for a predefined period of time (e.g., two weeks). This definition of warranty should ideally be integrated into our collective definition of \"done.\"\n",
      "\n",
      "§ By following work downstream, we may uncover ways to help improve flow, such as automating complex, manual steps (e.g., pairing application server clusters that require six hours to successfully complete); performing packaging of code once instead of creating it multiple times at different stages of QA and Production deployment; working with testers to automate manual test suites, thus removing a common bottleneck for more frequent deployment; and creating more useful documentation instead of having someone decipher developer application notes to build packaged installers.\n",
      "\n",
      "¶ More recently, Jeff Sussna attempted to further codify how to better achieve UX goals in what he calls \"digital conversations,\" which are intended to help organizations understand the customer journey as a complex system, broadening the context of quality. The key concepts include designing for service, not software; minimizing latency and maximizing strength of feedback; designing for failure and operating to learn; using Operations as an input to design; and seeking empathy.\n",
      "\n",
      "** We further increase the likelihood of production problems being fixed by ensuring that the Development teams remain intact, and not disbanded after the project is complete.\n",
      "\n",
      "†† In organizations with project-based funding, there may be no developers to hand the service back to, as the team has already been disbanded or may not have the budget or time to take on service responsibility. Potential countermeasures include holding an improvement blitz to improve the service, temporarily funding or staffing improvement efforts, or retiring the service.\n",
      "\n",
      "‡‡ In this book, we use the term \"Ops engineer,\" but the two terms, \"Ops Engineer\" and \"Site Reliability Engineer,\" are intended to be interchangeable.\n",
      "\n",
      "# 17Integrate Hypothesis-Driven Development and A/B Testing into Our Daily Work\n",
      "\n",
      "All too often in software projects, developers work on features for months or years, spanning multiple releases, without ever confirming whether the desired business outcomes are being met, such as whether a particular feature is achieving the desired results or even being used at all.\n",
      "\n",
      "Worse, even when we discover that a given feature isn't achieving the desired results, making corrections to the feature may be out-prioritized by other new features, ensuring that the under-performing feature will never achieve its intended business goal. In general, Jez Humble observes, \"the most inefficient way to test a business model or product idea is to build the complete product to see whether the predicted demand actually exists.\"\n",
      "\n",
      "Before we build a feature, we should rigorously ask ourselves, \"Should we build it, and why?\" We should then perform the cheapest and fastest experiments possible to validate through user research whether the intended feature will actually achieve the desired outcomes. We can use techniques such as hypothesis-driven development, customer acquisition funnels, and A/B testing, concepts we explore throughout this chapter. Intuit, Inc. provides a dramatic example of how organizations use these techniques to create products that customers love, to promote organizational learning, and to win in the marketplace.\n",
      "\n",
      "Intuit is focused on creating business and financial management solutions to simplify life for small businesses, consumers, and accounting professionals. In 2012, they had $4.5 billion in revenue and 8,500 employees, with flagship products that include QuickBooks, TurboTax, Mint, and, until recently, Quicken.†\n",
      "\n",
      "Scott Cook, the founder of Intuit, has long advocated building a culture of innovation, encouraging teams to take an experimental approach to product development and exhorting leadership to support them. As he said, \"Instead of focusing on the boss's vote...the emphasis is on getting real people to really behave in real experiments, and basing your decisions on that.\" This is the epitome of a scientific approach to product development.\n",
      "\n",
      "Cook explained that what is needed is \"a system where every employee can do rapid, high-velocity experiments....Dan Maurer runs our consumer division....[which] runs the TurboTax website. When he took over, we did about seven experiments a year.\"\n",
      "\n",
      "He continued, \"By installing a rampant innovation culture [in 2010], they now do 165 experiments in the three months of the [US] tax season. Business result? [The] conversion rate of the website is up 50 percent.... The folks [team members] just love it, because now their ideas can make it to market.\"\n",
      "\n",
      "Aside from the effect on the website conversion rate, one of the most surprising elements of this story is that TurboTax performed production experiments during their peak traffic seasons. For decades, especially in retailing, the risk of revenue-impacting outages during the holiday season were so high that we would often put into place a change freeze from mid-October to mid-January.\n",
      "\n",
      "However, by making software deployments and releases fast and safe, the TurboTax team made online user experimentation and any required production changes a low-risk activity that could be performed during the highest traffic and revenue generating periods.\n",
      "\n",
      "This highlights the notion that the period when experimentation has the highest value is during peak traffic seasons. Had the TurboTax team waited until April 16th, the day after the US tax filing deadline, to implement these changes, the company could have lost many of its prospective customers, and even some of its existing customers, to the competition.\n",
      "\n",
      "The faster we can experiment, iterate, and integrate feedback into our product or service, the faster we can learn and out-experiment the competition. And how quickly we can integrate our feedback depends on our ability to deploy and release software.\n",
      "\n",
      "The Intuit example shows that the Intuit TurboTax team was able to make this situation work for them and won in the marketplace as a result.\n",
      "\n",
      "## A BRIEF HISTORY OF A/B TESTING\n",
      "\n",
      "As the Intuit TurboTax story highlights, an extremely powerful user research technique is defining the customer acquisition funnel and performing A/B testing. A/B testing techniques were pioneered in _direct response marketing_ , which is one of the two major categories of marketing strategies. The other is called _mass marketing_ or _brand marketing_ and often relies on placing as many ad impressions in front of people as possible to influence buying decisions.\n",
      "\n",
      "In previous eras, before email and social media, direct response marketing meant sending thousands of postcards or flyers via postal mail, and asking prospects to accept an offer by calling a telephone number, returning a postcard, or placing an order.\n",
      "\n",
      "In these campaigns, experiments were performed to determine which offer had the highest conversion rates. They experimented with modifying and adapting the offer, re-wording the offer, varying the copywriting styles, design and typography, packaging, and so forth, to determine which was most effective in generating the desired action (e.g., calling a phone number, ordering a product).\n",
      "\n",
      "Each experiment often required doing another design and print run, mailing out thousands of offers, and waiting weeks for responses to come back. Each experiment typically cost tens of thousands of dollars per trial and required weeks or months to complete. However, despite the expense, iterative testing easily paid off if it significantly increased conversion rates (e.g., the percentage of respondents ordering a product going from 3%–12%).\n",
      "\n",
      "Well-documented cases of A/B testing include campaign fundraising, Internet marketing, and the Lean Startup methodology. Interestingly, it has also been used by the British government to determine which letters were most effective in collecting overdue tax revenue from delinquent citizens.‡\n",
      "\n",
      "## INTEGRATING A/B TESTING INTO OUR FEATURE TESTING\n",
      "\n",
      "The most commonly used A/B technique in modern UX practice involves a website where visitors are randomly selected to be shown one of two versions of a page, either a control (the \"A\") or a treatment (the \"B\"). Based on statistical analysis of the subsequent behavior of these two cohorts of users, we demonstrate whether there is a significant difference in the outcomes of the two, establishing a _causal_ link between the treatment (e.g., a change in a feature, design element, background color) and the outcome (e.g., conversion rate, average order size).\n",
      "\n",
      "For example, we may conduct an experiment to see whether modifying the text or color on a \"buy\" button increases revenue or whether slowing down the response time of a website (by introducing an artificial delay as the treatment) reduces revenue. This type of A/B testing allows us to establish a dollar value on performance improvements.\n",
      "\n",
      "Sometimes, A/B tests are also known as online controlled experiments and split tests. It's also possible to run experiments with more than one variable. This allows us to see how the variables interact, a technique known as multivariate testing.\n",
      "\n",
      "The outcomes of A/B tests are often startling. Ronny Kohavi, Distinguished Engineer and General Manager of the Analysis and Experimentation group at Microsoft, observed that after \"evaluating well-designed and executed experiments that were designed to improve a key metric, only about one-third were successful at improving the key metric!\" In other words, two-thirds of features either have a negligible impact or actually make things worse. Kohavi goes on to note that all these features were originally thought to be reasonable, good ideas, further elevating the need for user testing over intuition and expert opinions.\n",
      "\n",
      "The implications of the Kohavi data are staggering. If we are not performing user research, the odds are that two-thirds of the features we are building deliver zero or _negative_ value to our organization, even as they make our codebase ever more complex, thus increasing our maintenance costs over time and making our software more difficult to change. Furthermore, the effort to build these features is often made at the expense of delivering features that _would_ deliver value (i.e., opportunity cost). Jez Humble joked, \"Taken to an extreme, the organization and customers would have been better off giving the entire team a vacation, instead of building one of these non–value-adding features.\"\n",
      "\n",
      "Our countermeasure is to integrate A/B testing into the way we design, implement, test, and deploy our features. Performing meaningful user research and experiments ensures that our efforts help achieve our customer and organizational goals, and help us win in the marketplace.\n",
      "\n",
      "## INTEGRATE A/B TESTING INTO OUR RELEASE\n",
      "\n",
      "Fast and iterative A/B testing is made possible by being able to quickly and easily do production deployments on demand, using feature toggles and potentially delivering multiple versions of our code simultaneously to customer segments. Doing this requires useful production telemetry at all levels of the application stack.\n",
      "\n",
      "By hooking into our feature toggles, we can control which percentage of users see the treatment version of an experiment. For example, we may have one-half of our customers be our treatment group and one-half get shown the following: \"Similar items link on unavailable items in the cart.\" As part of our experiment, we compare the behavior of the control group (no offer made) against the treatment group (offer made), perhaps measuring number of purchases made in that session.\n",
      "\n",
      "Etsy open-sourced their experimentation framework Feature API (formerly known as the Etsy A/B API), which not only supports A/B testing but also online ramp-ups, enabling throttling exposure to experiments. Other A/B testing products include Optimizely, Google Analytics, etc.\n",
      "\n",
      "In a 2014 interview with Kendrick Wang of Apptimize, Lacy Rhoades at Etsy described their journey: \"Experimentation at Etsy comes from a desire to make informed decisions, and ensure that when we launch features for our millions of members, they work. Too often, we had features that took a lot of time and had to be maintained without any proof of their success or any popularity among users. A/B testing allows us to...say a feature is worth working on as soon as it's underway.\"\n",
      "\n",
      "## INTEGRATING A/B TESTING INTO OUR FEATURE PLANNING\n",
      "\n",
      "Once we have the infrastructure to support A/B feature release and testing, we must ensure that product owners think about each feature as a hypothesis and use our production releases as experiments with real users to prove or disprove that hypothesis. Constructing experiments should be designed in the context of the overall customer acquisition funnel. Barry O'Reilly, co-author of _Lean Enterprise: How High Performance Organizations Innovate at Scale_ , described how we can frame hypotheses in feature development in the following form:\n",
      "\n",
      "**We Believe** that increasing the size of hotel images on the booking page\n",
      "\n",
      "**Will Result** in improved customer engagement and conversion\n",
      "\n",
      "**We Will Have Confidence To Proceed When** we see a 5% increase in customers who review hotel images who then proceed to book in forty-eight hours.\n",
      "\n",
      "Adopting an experimental approach to product development requires us to not only break down work into small units (stories or requirements), but also validate whether each unit of work is delivering the expected outcomes. If it does not, we modify our road map of work with alternative paths that will actually achieve those outcomes.\n",
      "\n",
      "Case Study  \n",
      "Doubling Revenue Growth through Fast Release Cycle Experimentation at Yahoo! Answers (2010)\n",
      "\n",
      "The faster we can iterate and integrate feedback into the product or service we are delivering to customers, the faster we can learn and the bigger the impact we can create. How dramatically outcomes can be affected by faster cycle times was evident at Yahoo! Answers as they went from one release every six weeks to multiple releases every week.\n",
      "\n",
      "In 2009, Jim Stoneham was General Manager of the Yahoo! Communities group that included Flickr and Answers. Previously, he had been primarily responsible for Yahoo! Answers, competing against other Q&A companies such as Quora, Aardvark, and Stack Exchange.\n",
      "\n",
      "At that time, Answers had approximately 140 million monthly visitors, with over twenty million active users answering questions in over twenty different languages. However, user growth and revenue had flattened, and user engagement scores were declining.\n",
      "\n",
      "Stoneham observes that \"Yahoo Answers was and continues to be one of the biggest social games on the Internet; tens of millions of people are actively trying to 'level up' by providing quality answers to questions faster than the next member of the community. There were many opportunities to tweak the game mechanic, viral loops, and other community interactions. When you're dealing with these human behaviors, you've got to be able to do quick iterations and testing to see what clicks with people.\"\n",
      "\n",
      "He continues, \"These [experiments] are the things that Twitter, Facebook, and Zynga did so well. Those organizations were doing experiments at least twice per week—they were even reviewing the changes they made before their deployments, to make sure they were still on track. So here I am, running [the] largest Q&A site in the market, wanting to do rapid iterative feature testing, but we can't release any faster than once every 4 weeks. In contrast, the other people in the market had a feedback loop 10x faster than us.\"\n",
      "\n",
      "Stoneham observed that as much as product owners and developers talk about being metrics-driven, if experiments are not performed frequently (daily or weekly), the focus of daily work is merely on the feature they're working on, as opposed to customer outcomes.\n",
      "\n",
      "As the Yahoo! Answers team was able to move to weekly deployments, and later multiple deployments per week, their ability to experiment with new features increased dramatically. Their astounding achievements and learnings over the next twelve months of experimentation included increased monthly visits of 72%, increased user engagement of threefold, and the team doubled their revenue. To continue their success, the team focused on optimizing the following top metrics:\n",
      "\n",
      "  * Time to first answer: How quickly was an answer posted to a user question?\n",
      "  * Time to best answer: How quickly did the user community award a best answer?\n",
      "  * Upvotes per answer: How many times was an answer upvoted by the user community?\n",
      "  * Answers/week/person: How many answers were users creating?\n",
      "  * Second search rate: How often did visitors have to search again to get an answer? (Lower is better.)\n",
      "\n",
      "Stoneham concluded, \"This was exactly the learning that we needed to win in the marketplace—and it changed more than our feature velocity. We transformed from a team of employees to a team of owners. When you move at that speed, and are looking at the numbers and the results daily, your investment level radically changes.\"\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Success requires us to not only deploy and release software quickly, but also to out-experiment our competition. Techniques such as hypothesis-driven development, defining and measuring out customer acquisition funnel, and A/B testing allow us to perform user-experiments safely and easily, enabling us to unleash creativity and innovation, and create organizational learning. And, while succeeding is important, the organizational learning that comes from experimentation also gives employees ownership of business objectives and customer satisfaction. In the next chapter, we examine and create review and coordination processes as a way to increase the quality of our current work.\n",
      "\n",
      "* * *\n",
      "\n",
      "† In 2016, Intuit sold the Quicken business to the private equity firm H.I.G. Capital.\n",
      "\n",
      "‡ There are many other ways to conduct user research before embarking on development. Among the most inexpensive methods include performing surveys, creating prototypes (either mock-ups using tools such as Balsamiq or interactive versions written in code), and performing usability testing. Alberto Savoia, Director of Engineering at Google, coined the term pretotyping for the practice of using prototypes to validate whether we are building the right thing. User research is so inexpensive and easy relative to the effort and cost of building a useless feature in code that, in almost every case, we shouldn't prioritize a feature without some form of validation.\n",
      "\n",
      "# 18Create Review and Coordination Processes to Increase Quality of Our Current Work\n",
      "\n",
      "In the previous chapters, we created the telemetry necessary to see and solve problems in production and at all stages of our deployment pipeline, and created fast feedback loops from customers to help enhance organizational learning—learning that encourages ownership and responsibility for customer satisfaction and feature performance, which helps us succeed.\n",
      "\n",
      "Our goal in this chapter is to enable Development and Operations to reduce the risk of production changes before they are made. Traditionally, when we review changes for deployment, we tend to rely heavily on reviews, inspections, and approvals just prior to deployment. Frequently those approvals are given by external teams who are often too far removed from the work to make informed decisions on whether a change is risky or not, and the time required to get all the necessary approvals also lengthens our change lead times.\n",
      "\n",
      "The peer review process at GitHub is a striking example of how inspection can increase quality, make deployments safe, and be integrated into the flow of everyone's daily work. They pioneered the process called _pull request_ , one of the most popular forms of peer review that span Dev and Ops.\n",
      "\n",
      "Scott Chacon, CIO and co-founder of GitHub, wrote on his website that pull requests are the mechanism that lets engineers tell others about changes they have pushed to a repository on GitHub. Once a pull request is sent, interested parties can review the set of changes, discuss potential modifications, and even push follow-up commits if necessary. Engineers submitting a pull request will often request a \"+1,\" \"+2,\" or so forth, depending on how many reviews they need, or \"@mention\" engineers that they'd like to get reviews from.\n",
      "\n",
      "At GitHub, pull requests are also the mechanism used to deploy code into production through a collective set of practices they call \"GitHub Flow\"—it's how engineers request code reviews, gather and integrate feedback, and announce that code will be deployed to production (i.e., \"master\" branch).\n",
      "\n",
      "  **Figure 40:** Comments and suggestions on a GitHub pull request   \n",
      "(Source: Scott Chacon, \"GitHub Flow,\" ScottChacon.com, August 31, 2011, <http://scottchacon.com/2011/08/31/github-flow.html>.)\n",
      "\n",
      "GitHub Flow is composed of five steps:\n",
      "\n",
      "  1. To work on something new, the engineer creates a descriptively named branch off of master (e.g., \"new-oauth2-scopes\").\n",
      "  2. The engineer commits to that branch locally, regularly pushing their work to the same named branch on the server.\n",
      "  3. When they need feedback or help, or when they think the branch is ready for merging, they open a pull request.\n",
      "  4. When they get their desired reviews and get any necessary approvals of the feature, the engineer can then merge it into master.\n",
      "  5. Once the code changes are merged and pushed to master, the engineer deploys them into production.\n",
      "\n",
      "These practices, which integrate review and coordination into daliy work, have allowed GitHub to quickly and reliably deliver features to market with high quality and security. For example, in 2012 they performed an amazing 12,602 deployments. In particular, on August 23rd, after a company-wide summit where many exciting ideas were brainstormed and discussed, the company had their busiest deployment day of the year, with 563 builds and 175 successful deployments into production, all made possible through the pull request process.\n",
      "\n",
      "Throughout this chapter we will integrate practices, such as those used at GitHub, to shift our reliance away from periodic inspections and approvals, and moving to integrated peer review performed continually as a part of our daily work. Our goal is to ensure that Development, Operations, and Infosec are continuously collaborating so that changes we make to our systems will operate reliably, securely, safely, and as designed.\n",
      "\n",
      "## THE DANGERS OF CHANGE APPROVAL PROCESSES\n",
      "\n",
      "The Knight Capital failure is one of the most prominent software deployment errors in recent memory. A fifteen minute deployment error resulted in a $440 million trading loss, during which the engineering teams were unable to disable the production services. The financial losses jeopardized the firm's operations and forced the company to be sold over the weekend so they could continue operating without jeopardizing the entire financial system.\n",
      "\n",
      "John Allspaw observed that when high-profile incidents occur, such as the Knight Capital deployment accident, there are typically two _counterfactual_ narratives for why the accident occurred.†\n",
      "\n",
      "The first narrative is that the accident was due to a change control failure, which seems valid because we can imagine a situation where better change control practices could have detected the risk earlier and prevented the change from going into production. And if we couldn't prevent it, we might have taken steps to enable faster detection and recovery.\n",
      "\n",
      "The second narrative is that the accident was due to a testing failure. This also seems valid, with better testing practices we could have identified the risk earlier and canceled the risky deployment, or we could have at least taken steps to enable faster detection and recovery.\n",
      "\n",
      "The surprising reality is that in environments with low-trust, command-and-control cultures, the outcomes of these types of change control and testing countermeasures often result in an increased likelihood that problems will occur again, potentially with even worse outcomes.\n",
      "\n",
      "Gene Kim (co-author of this book) describes his realization that change and testing controls can potentially have the opposite effect than intended as \"one of the most important moments of my professional career. This 'aha' moment was the result of a conversation in 2013 with John Allspaw and Jez Humble about the Knight Capital accident, making me question some of my core beliefs that I've formed over the last ten years, especially having been trained as an auditor.\"\n",
      "\n",
      "He continues, \"However upsetting it was, it was also a very formative moment for me. Not only did they convince me that they were correct, we tested these beliefs in the _2014 State of DevOps Report_ , which led to some astonishing findings that reinforce that building high-trust cultures is likely the largest management challenge of this decade.\"\n",
      "\n",
      "## POTENTIAL DANGERS OF \"OVERLY CONTROLLING CHANGES\"\n",
      "\n",
      "Traditional change controls can lead to unintended outcomes, such as contributing to long lead times, and reducing the strength and immediacy of feedback from the deployment process. In order to understand how this happens, let us examine the controls we often put in place when change control failures occur:\n",
      "\n",
      "  * Adding more questions that need to be answered to the change request form\n",
      "  * Requiring more authorizations, such as one more level of management approval (e.g., instead of merely the VP of Operations approving, we now require that the CIO also approve) or more stakeholders (e.g., network engineering, architecture review boards, etc.)\n",
      "  * Requiring more lead time for change approvals so that change requests can be properly evaluated\n",
      "\n",
      "These controls often add more friction to the deployment process by multiplying the number of steps and approvals, and increasing batch sizes and deployment lead times, which we know reduces the likelihood of successful production outcomes for both Dev and Ops. These controls also reduce how quickly we get feedback from our work.\n",
      "\n",
      "One of the core beliefs in the Toyota Production System is that \"people closest to a problem typically know the most about it.\" This becomes more pronounced as the work being performed and the system the work occurs in become more complex and dynamic, as is typical in DevOps value streams. In these cases, creating approval steps from people who are located further and further away from the work may actually reduce the likelihood of success. As has been proven time and again, the further the distance between the person doing the work (i.e., the change implementer) and the person deciding to do the work (i.e., the change authorizer), the worse the outcome.\n",
      "\n",
      "In Puppet Labs' _2014 State of DevOps Report_ , one of the key findings was that high-performing organizations relied more on peer review and less on external approval of changes. Figure 41 shows that the more organizations rely on change approvals, the worse their IT performance in terms of both stability (mean time to restore service and change fail rate) and throughput (deployment lead times, deployment frequency).\n",
      "\n",
      "In many organizations, change advisory boards serve an important role in coordinating and governing the delivery process, but their job should not be to manually evaluate every change, nor does ITIL mandate such a practice.\n",
      "\n",
      "To understand why this is the case, consider the predicament of being on a change advisory board, reviewing a complex change composed of hundreds of thousands of lines of code changes, and created by hundreds of engineers.\n",
      "\n",
      "At one extreme, we cannot reliably predict whether a change will be successful either by reading a hundred-word description of the change or by merely validating that a checklist has been completed. At the other extreme, painfully scrutinizing thousands of lines of code changes is unlikely to reveal any new insights. Part of this is the nature of making changes inside of a complex system. Even the engineers who work inside the codebase as part of their daily work are often surprised by the side effects of what should be low-risk changes.\n",
      "\n",
      "  **Figure 41:** Organizations that rely on peer review outperform those with change approvals (Source: Puppet Labs, DevOps _Survey Of Practice 2014_ )\n",
      "\n",
      "For all these reasons, we need to create effective control practices that more closely resemble peer review, reducing our reliance on external bodies to authorize our changes. We also need to coordinate and schedule changes effectively. We explore both of these in the next two sections.\n",
      "\n",
      "## ENABLE COORDINATION AND SCHEDULING OF CHANGES\n",
      "\n",
      "Whenever we have multiple groups working on systems that share dependencies, our changes will likely need to be coordinated to ensure that they don't interfere with each other (e.g., marshaling, batching, and sequencing the changes). In general, the more loosely-coupled our architecture, the less we need to communicate and coordinate with other component teams—when the architecture is truly service-oriented, teams can make changes with a high degree of autonomy, where local changes are unlikely to create global disruptions.\n",
      "\n",
      "However, even in a loosely-coupled architecture, when many teams are doing hundreds of independent deployments per day, there may be a risk of changes interfering with each other (e.g., simultaneous A/B tests). To mitigate these risks, we may use chat rooms to announce changes and proactively find collisions that may exist.\n",
      "\n",
      "For more complex organizations and organizations with more tightly-coupled architectures, we may need to deliberately schedule our changes, where representatives from the teams get together, not to authorize changes, but to schedule and sequence their changes in order to minimize accidents.\n",
      "\n",
      "However, certain areas, such as global infrastructure changes (e.g., core network switch changes) will always have a higher risk associated with them. These changes will always require technical countermeasures, such as redundancy, failover, comprehensive testing, and (ideally) simulation.\n",
      "\n",
      "## ENABLE PEER REVIEW OF CHANGES\n",
      "\n",
      "Instead of requiring approval from an external body prior to deployment, we may require engineers to get peer reviews of their changes. In Development, this practice has been called _code review_ , but it is equally applicable to any change we make to our applications or environments, including servers, networking, and databases.‡ The goal is to find errors by having fellow engineers close to the work scrutinize our changes. This review improves the quality of our changes, which also creates the benefits of cross-training, peer learning, and skill improvement.\n",
      "\n",
      "A logical place to require reviews is prior to committing code to trunk in source control, where changes could potentially have a team-wide or global impact. At a minimum, fellow engineers should review our change, but for higher risk areas, such as database changes or business-critical components with poor automated test coverage, we may require further review from a subject matter expert (e.g., information security engineer, database engineer) or multiple reviews (e.g., \"+2\" instead of merely \"+1\").\n",
      "\n",
      "The principle of small batch sizes also applies to code reviews. The larger the size of the change that needs to be reviewed, the longer it takes to understand and the larger the burden on the reviewing engineer. As Randy Shoup observed, \"There is a non-linear relationship between the size of the change and the potential risk of integrating that change—when you go from a ten line code change to a one hundred line code, the risk of something going wrong is more than ten times higher, and so forth.\" This is why it's so essential for developers to work in small, incremental steps rather than on long-lived feature branches.\n",
      "\n",
      "Furthermore, our ability to meaningfully critique code changes goes down as the change size goes up. As Giray Özil tweeted, \"Ask a programmer to review ten lines of code, he'll find ten issues. Ask him to do five hundred lines, and he'll say it looks good.\"\n",
      "\n",
      "Guidelines for code reviews include:\n",
      "\n",
      "  * Everyone must have someone to review their changes (e.g., to the code, environment, etc.) before committing to trunk.\n",
      "  * Everyone should monitor the commit stream of their fellow team members so that potential conflicts can be identified and reviewed.\n",
      "  * Define which changes qualify as high risk and may require review from a designated subject matter expert (e.g., database changes, security-sensitive modules such as authentication, etc.).§\n",
      "  * If someone submits a change that is too large to reason about easily—in other words, you can't understand its impact after reading through it a couple of times, or you need to ask the submitter for clarification—it should be split up into multiple, smaller changes that can be understood at a glance.\n",
      "\n",
      "To ensure that we are not merely rubber stamping reviews, we may also want to inspect the code review statistics to determine the number of proposed changes approved versus not approved, and perhaps sample and inspect specific code reviews.\n",
      "\n",
      "Code reviews come in various forms:\n",
      "\n",
      "  * **Pair programming:** programmers work in pairs (see section below)\n",
      "  * **\"Over-the-shoulder\":** One developer looks over the author's shoulder as the latter walks through the code.\n",
      "  * **Email pass-around:** A source code management system emails code to reviewers automatically after the code is checked in.\n",
      "  * **Tool-assisted code review:** Authors and reviewers use specialized tools designed for peer code review (e.g., Gerrit, GitHub pull requests, etc.) or facilities provided by the source code repositories (e.g., GitHub, Mercurial, Subversion, as well as other platforms such as Gerrit, Atlassian Stash, and Atlassian Crucible).\n",
      "\n",
      "Close scrutiny of changes in many forms is effective in locating errors previously overlooked. Code reviews can facilitate increased code commits and production deployments, and support trunk-based deployment and continuous delivery at scale, as we will see in the following case study.\n",
      "\n",
      "Case Study  \n",
      "Code Reviews at Google (2010)\n",
      "\n",
      "Google is an excellent example of a company that employees trunk-based development and continuous delivery at scale. As noted earlier in this book, Eran Messeri described that in 2013 the processes at Google enabled over thirteen thousand developers to work off of trunk on a single source code tree, performing over 5,500 code commits per week, resulting in hundreds of production deployments per week. In 2010, there were 20+ changes being checked in to trunk every minute, resulting in 50% of the codebase being changed every month.\n",
      "\n",
      "This requires considerable discipline from Google team members and mandatory code reviews, which cover the following areas:\n",
      "\n",
      "  * Code readability for languages (enforces style guide)\n",
      "  * Ownership assignments for code sub-trees to maintain consistency and correctness\n",
      "  * Code transparency and code contributions across teams\n",
      "\n",
      "Figure 42 shows how code review lead times are affected by the change size. On the x-axis is the size of the change, and on the y-axis is the lead time required for code review process. In general, the larger the change submitted for code reviews, the longer the lead time required to get the necessary sign offs. And the data points in the upper-left corner represent the more complex and potentially risky changes that required more deliberation and discussion.\n",
      "\n",
      "  **Figure 42:** Size of change vs. lead time for reviews at Google (Source: Ashish Kumar, \"Development at the Speed and Scale of Google,\" presentation at QCon, San Francisco, CA, 2010, https://qconsf.com/sf2010/dl/qcon-sanfran-2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf.)\n",
      "\n",
      "While he was working as a Google engineering director, Randy Shoup started a personal project to solve a technical problem that the organization was facing. He said, \"I worked on that project for weeks and finally got around to asking a subject matter expert to review my code. It was nearly three thousand lines of code, which took the reviewer days of work to go through. He told me, 'Please don't do that to me again.' I was grateful that this engineer took the time to do that. That was also when I learned how to make code reviews a part of my daily work.\"\n",
      "\n",
      "## POTENTIAL DANGERS OF DOING MORE MANUAL TESTING AND CHANGE FREEZES\n",
      "\n",
      "Now that we have created peer reviews that reduce our risk, shorten lead times associated with change approval processes, and enable continuous delivery at scale, such as we saw in the Google case study, let us examine the effects of how testing countermeasure can sometimes backfire. When testing failures occur, our typical reaction is to do more testing. However, if we are merely performing more testing at the end of the project, we may worsen our outcomes.\n",
      "\n",
      "This is especially true if we are doing manual testing, because manual testing is naturally slower and more tedious than automated testing and performing \"additional testing\" often has the consequence of taking significantly longer to test, which means we are deploying less frequently, thus increasing our deployment batch size. And we know from both theory and practice that when we increase our deployment batch size, our change success rates go down and our incident counts and MTTR go up—the opposite of the outcome we want.\n",
      "\n",
      "Instead of performing testing on large batches of changes that are scheduled around change freeze periods, we want to fully integrate testing our daily work as part of the smooth and continual flow into production, and increase our deployment frequency. By doing this, we build in quality, which allows us to test, deploy, and release in ever smaller batch sizes.\n",
      "\n",
      "## ENABLE PAIR PROGRAMMING TO IMPROVE ALL OUR CHANGES\n",
      "\n",
      "_Pair programming_ is when two engineers work together at the same workstation, a method popularized by Extreme Programming and Agile in the early 2000s. As with code reviews, this practice started in Development but is equally applicable to the work that any engineer does in our value stream. In this book, we will use the term _pairing_ and _pair programming_ interchangeably, to indicate that the practice is not just for developers.\n",
      "\n",
      "In one common pattern of pairing, one engineer fills the role of the _driver_ , the person who actually writes the code, while the other engineer acts as the _navigator, observer,_ or _pointer_ , the person who reviews the work as it is being performed. While reviewing, the observer may also consider the strategic direction of the work, coming up with ideas for improvements and likely future problems to address. This frees the driver to focus all of his or her attention on the tactical aspects of completing the task, using the observer as a safety net and guide. When the two have differing specialties, skills are transferred as an automatic side effect, whether it's through ad-hoc training or by sharing techniques and workarounds.\n",
      "\n",
      "Another pair programming pattern reinforces test-driven development (TDD) by having one engineer write the automated test and the other engineer implement the code. Jeff Atwood, one of the founders of Stack Exchange, wrote, \"I can't help wondering if pair programming is nothing more than code review on steroids....The advantage of pair programming is its gripping immediacy: it is impossible to ignore the reviewer when he or she is sitting right next to you.\"\n",
      "\n",
      "He continued, \"Most people will passively opt out [of reviewing code] if given the choice. With pair programming, that's not possible. Each half of the pair _has_ to understand the code, right then and there, as it's being written. Pairing may be invasive, but it can also force a level of communication that you'd otherwise never achieve.\"\n",
      "\n",
      "Dr. Laurie Williams performed a study in 2001 that showed \"paired programmers are 15% slower than two independent individual programmers, while 'error-free' code increased from 70% to 85%. Since testing and debugging are often many times more costly than initial programming, this is an impressive result. Pairs typically consider more design alternatives than programmers working alone and arrive at simpler, more maintainable designs; they also catch design defects early.\" Dr. Williams also reported that 96% of her respondents stated that they enjoyed their work more when they programmed in pairs than when they programmed alone.¶\n",
      "\n",
      "Pair programming has the additional benefit of spreading knowledge throughout the organization and increasing information flow within the team. Having more experienced engineers review while the less experienced engineer codes is also an effective way to teach and be taught.\n",
      "\n",
      "Case Study   \n",
      "Pair Programming Replacing Broken Code Review Processes at Pivotal Labs (2011)\n",
      "\n",
      "Elisabeth Hendrickson, VP of Engineering at Pivotal Software, Inc. and author of _Explore It!: Reduce Risk and Increase Confidence with Exploratory Testing_ , has spoken extensively about making every team responsible for their own quality, as opposed to making separate departments responsible. She argues that doing so not only increase quality, but significantly increases the flow of work into production.\n",
      "\n",
      "In her 2015 DevOps Enterprise Summit presentation, she described how in 2011, there were two accepted methods of code review at Pivotal: pair programming (which ensured that every line of code was inspected by two people) or a code review process that was managed by Gerrit (which ensured that every code commit had two designated people \"+1\" the change before it was allowed into trunk).\n",
      "\n",
      "The problem Hendrickson observed with the Gerrit code review process was that it would often take an entire week for developers to receive their required reviews. Worse, skilled developers were experiencing the \"frustrating and soul crushing experience of not being able to get simple changes into the codebase, because we had inadvertently created intolerable bottlenecks.\"\n",
      "\n",
      "Hendrickson lamented that \"the only people who had the ability to '+1' the changes were senior engineers, who had many other responsibilities and often didn't care as much about the fixes the more junior developers were working on or their productivity. It created a terrible situation—while you were waiting for your changes to get reviewed, other developers were checking in their changes. So for a week, you would have to merge all their code changes onto your laptop, re-run all the tests to ensure that everything still worked, and (sometimes) you'd have to resubmit your changes for review again!\"\n",
      "\n",
      "To fix the problem and eliminate all of these delays, they ended up dismantling the entire Gerrit code review process, instead requiring pair programming to implement code changes into the system. By doing this, they reduced the amount of time required to get code reviewed from weeks to hours.\n",
      "\n",
      "Hendrickson is quick to note that code reviews work fine in many organizations, but it requires a culture that values reviewing code as highly as it values writing the code in the first place. When that culture is not yet in place, pair programming can serve as a valuable interim practice.\n",
      "\n",
      "### EVALUATING THE EFFECTIVENESS OF PULL REQUEST PROCESSES\n",
      "\n",
      "Because the peer review process is an important part of our control environment, we need to be able to determine whether it is working effectively or not. One method is to look at production outages and examine the peer review process for any relevant changes.\n",
      "\n",
      "Another method comes from Ryan Tomayko, CIO and co-founder of GitHub and one of the inventors of the pull request process. When asked to describe the difference between a bad pull request and a good pull request, he said it has little to do with the production outcome. Instead, a bad pull request is one that doesn't have enough context for the reader, having little or no documentation of what the change is intended to do. For example, a pull request that merely has the following text: \"Fixing issue #3616 and #3841.\"**\n",
      "\n",
      "That was an actual internal GitHub pull request, which Tomayko critiqued, \"This was probably written by a new engineer here. First off, no specific engineers were specifically @mentioned—at a minimum, the engineer should have mentioned their mentor or a subject matter expert in the area that they're modifying to ensure that someone appropriate reviews their change. Worse, there isn't any explanation of what the changes actually are, why it's important, or exposing any of the implementer's thinking.\"\n",
      "\n",
      "On the other hand, when asked to describe a great pull request that indicates an effective review process, Tomayko quickly listed off the essential elements: there must be sufficient detail on why the change is being made, how the change was made, as well as any identified risks and resulting countermeasures.\n",
      "\n",
      "Tomayko also looks for good discussion of the change, enabled by all the context that the pull request provided—often, there will be additional risks pointed out, ideas on better ways to implement the desired change, ideas on how to better mitigate the risk, and so forth. And if something bad or unexpected happens upon deployment, it is added to the pull request, with a link to the corresponding issue. All discussion happens without placing blame; instead, there is a candid conversation on how to prevent the problem from recurring in the future.\n",
      "\n",
      "As an example, Tomayko produced another internal GitHub pull request for a database migration. It was many pages long, with lengthy discussions about the potential risks, leading up to the following statement by the pull request author: \"I am pushing this now. Builds are now failing for the branch, because of a missing column in the CI servers. (Link to Post-Mortem: MySQL outage)\"\n",
      "\n",
      "The change submitter then apologized for the outage, describing what conditions and mistaken assumptions led to the accident, as well as a list of proposed countermeasures to prevent it from happening again. This was followed by pages and pages of discussion. Reading through the pull request, Tomayko smiled, \"Now _that_ is a great pull request.\"\n",
      "\n",
      "As described above, we can evaluate the effectiveness of our peer review process by sampling and examining pull requests, either from the entire population of pull requests or those that are relevant to production incidents.\n",
      "\n",
      "## FEARLESSLY CUT BUREAUCRATIC PROCESSES\n",
      "\n",
      "So far, we have discussed peer review and pair programming processes that enable us to increase the quality of our work without relying on external approvals for changes. However, many companies still have long-standing processes for approval that require months to navigate. These approval processes can significantly increase lead times, not only preventing us from delivering value quickly to customers, but potentially increasing the risk to our organizational objectives. When this happens, we must re-engineer our processes so that we can achieve our goals more quickly and safely.\n",
      "\n",
      "As Adrian Cockcroft observed, \"A great metric to publish widely is how many meetings and work tickets are mandatory to perform a release—the goal is to relentlessly reduce the effort required for engineers to perform work and deliver it to the customer.\"\n",
      "\n",
      "Similarly, Dr. Tapabrata Pal, technical fellow at Capital One, described a program at Capital One called Got Goo?, which involves a dedicated team removing obstacles—including tools, processes, and approvals—that impede work completion. Jason Cox, Senior Director of Systems Engineering at Disney, described in his presentation at the DevOps Enterprise Summit in 2015 a program called Join The Rebellion that aimed to remove toil and obstacles from daily work.\n",
      "\n",
      "At Target in 2012, a combination of the Technology Enterprise Adoption Process and Lead Architecture Review Board (TEAP-LARB process) resulted in complicated, long approval times for anyone attempting to bring in new technology. The TEAP form needed to be filled out by anyone wanting to propose new technologies to be adopted, such as a new database or monitoring technologies. These proposals were evaluated, and those deemed appropriate were put onto the monthly LARB meeting agenda.\n",
      "\n",
      "Heather Mickman and Ross Clanton, Director of Development and Director of Operations at Target, Inc., respectively, were helping to lead the DevOps movement at Target. During their DevOps initiative, Mickman had identified a technology needed to enable an initiative from the lines of business (in this case, Tomcat and Cassandra). The decision from the LARB was that Operations could not support it at the time. However, because Mickman was so convinced that this technology was essential, she proposed that her Development team be responsible for service support as well as integration, availability, and security, instead of relying on the Operations team.\n",
      "\n",
      "\"As we went through the process, I wanted to better understand why the TEAP-LARB process took so long to get through, and I used the technique of 'the five why's'....Which eventually led to the question of why TEAP-LARB existed in the first place. The surprising thing was that no one knew, outside of a vague notion that we needed some sort of governance process. Many knew that there had been some sort of disaster that could never happen again years ago, but no one could remember exactly what that disaster was, either,\" Mickman observed.\n",
      "\n",
      "Mickman concluded that this process was not necessary for her group if they were responsible for the operational responsibilities of the technology she was introducing. She added, \"I let everyone know that any future technologies that we would support wouldn't have to go through the TEAP-LARB process, either.\"\n",
      "\n",
      "The outcome was that Cassandra was successfully introduced inside Target and eventually widely adopted. Furthermore, the TEAP-LARB process was eventually dismantled. Out of appreciation, her team awarded Mickman the Lifetime Achievement Award for removing barriers to get technology work done within Target.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "In this chapter, we discussed how to integrate practices into our daily work that increase the quality of our changes and reduce the risk of poor deployment outcomes, reducing our reliance on approval processes. Case studies from GitHub and Target show that these practices not only improve our outcomes, but also significantly reduce lead times and increase developer productivity. To do this kind of work requires a high-trust culture.\n",
      "\n",
      "Consider a story that John Allspaw told about a newly hired junior engineer: The engineer asked if it was okay to deploy a small HTML change, and Allspaw responded, \"I don't know, is it?\" He then asked \"Did you have someone review your change? Do you know who the best person to ask is for changes of this type? Did you do everything you absolutely could to assure yourself that this change operates in production as designed? If you did, then don't ask me—just make the change!\"\n",
      "\n",
      "By responding this way, Allspaw reminded the engineer that she was solely responsibility for the quality of her change—if she did everything she felt she could to give herself confidence that the change would work, then she didn't need to ask anyone for approval, she should make the change.\n",
      "\n",
      "Creating the conditions that enable change implementers to fully own the quality of their changes is an essential part of the high-trust, generative culture we are striving to build. Furthermore, these conditions enable us to create an ever-safer system of work, where we are all helping each other achieve our goals, spanning whatever boundaries necessary to get there.\n",
      "\n",
      "## PART IV CONCLUSION\n",
      "\n",
      "Part IV has shown us that by implementing feedback loops we can enable everyone to work together toward shared goals, see problems as they occur, and, with quick detection and recovery, ensure that features not only operate as designed in production, but also achieve organizational goals and organizational learning. We have also examined how to enable shared goals spanning Dev and Ops so that they can improve the health of the entire value stream.\n",
      "\n",
      "We are now ready to enter Part V: The Third Way, _The Technical Practices of Learning_ , so we can create opportunities for learning that happen earlier and ever more quickly and cheaply, and so that we can unleash a culture of innovation and experimentation that enables everyone to do meaningful work that helps our organization succeed.\n",
      "\n",
      "* * *\n",
      "\n",
      "† _Counterfactual thinking_ is a term used in psychology that involves the human tendency to create possible alternatives to life events that have already occurred. In reliability engineering, it often involves narratives of the \"system as imagined\" as opposed to the \"system in reality.\"\n",
      "\n",
      "‡ In this book, the terms _code review_ and _change review_ will be used interchangeably.\n",
      "\n",
      "§ Incidentally, a list of high-risk areas of code and environments has likely already been created by the change advisory board.\n",
      "\n",
      "¶ Some organizations may require pair programming, while in others, engineers find someone to pair program with when working in areas where they want more scrutiny (such as before checking in) or for challenging tasks. Another common practice is to set _pairing hours_ for a subset of the working day, perhaps four hours from mid-morning to mid-afternoon.\n",
      "\n",
      "** Gene Kim is grateful to Shawn Davenport, James Fryman, Will Farr, and Ryan Tomayko at GitHub for discussing the differences between good and bad pull requests.\n",
      "\n",
      "# Part V\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In Part III, The First Way: _The Technical Practices of Flow_ , we discussed implementing the practices required to create fast flow in our value stream. In Part IV, The Second Way: _The Technical Practices of Feedback_ , our goal was to create as much feedback as possible, from as many areas in our system as possible—sooner, faster, and cheaper.\n",
      "\n",
      "In Part V, The Third Way: _The Technical Practices of Learning_ , we present the practices that create opportunities for learning, as quickly, frequently, cheaply, and as soon as possible. This includes creating learnings from accidents and failures, which are inevitable when we work within complex systems, as well as organizing and designing our systems of work so that we are constantly experimenting and learning, continually making our systems safer. The results include higher resilience and an ever-growing collective knowledge of how our system actually works, so that we are better able to achieve our goals.\n",
      "\n",
      "In the following chapters, we will institutionalize rituals that increase safety, continuous improvement, and learning by doing the following:\n",
      "\n",
      "  * Establish a just culture to make safety possible \n",
      "  * Inject production failures to create resilience\n",
      "  * Convert local discoveries into global improvements\n",
      "  * Reserve time to create organizational improvements and learning\n",
      "\n",
      "We will also create mechanisms so that any new learnings generated in one area of the organization can be rapidly used across the entire organization, turning local improvements into global advancements. In this way, we not only learn faster than our competition, helping us win in the marketplace, but also create a safer, more resilient work culture that people are excited to be a part of and that helps them achieve their highest potential.\n",
      "\n",
      "# 19Enable and Inject Learning into Daily Work\n",
      "\n",
      "When we work within a complex system, it is impossible for us to predict all the outcomes for the actions we take. This contributes to unexpected and sometimes catastrophic accidents, even when we use static precautionary tools, such as checklists and runbooks, which codify our current understanding of the system.\n",
      "\n",
      "To enable us to safely work within complex systems, our organizations must become ever better at self-diagnostics and self-improvement and must be skilled at detecting problems, solving them, and multiplying the effects by making the solutions available throughout the organization. This creates a dynamic system of learning that allows us to understand our mistakes and translate that understanding into actions that prevent those mistakes from recurring in the future.\n",
      "\n",
      "The result is what Dr. Steven Spear describes as resilient organizations, who are \"skilled at detecting problems, solving them, and multiplying the effect by making the solutions available throughout the organization.\" These organizations can heal themselves. \"For such an organization, responding to crises is not idiosyncratic work. It is something that is done all the time. It is this responsiveness that is their source of reliability.\"\n",
      "\n",
      "A striking example of the incredible resilience that can result from these principles and practices was seen on April 21, 2011, when the entire Amazon AWS US-EAST availability zone went down, taking down virtually all of their customers who depended on it, including Reddit and Quora.† However, Netflix was a surprising exception, seemingly unaffected by this massive AWS outage.\n",
      "\n",
      "Following the event, there was considerable speculation about how Netflix kept their services running. A popular theory was since Netflix was one of the largest customers of Amazon Web Services, it was given some special treatment that allowed them to keep running. However, a _Netflix Engineering_ blog post explained that it was their architectural design decisions in 2009 enabled their exceptional resilience.\n",
      "\n",
      "Back in 2008, Netflix's online video delivery service ran on a monolithic J2EE application hosted in one of their data centers. However, starting in 2009, they began re-architecting this system to be what they called _cloud native—_ it was designed to run entirely in the Amazon public cloud and to be resilient enough to survive significant failures.\n",
      "\n",
      "One of their specific design objectives was to ensure Netflix services kept running, even if an entire AWS availability zone went down, such as happened with US-EAST. To do this required that their system be loosely-coupled, with each component having aggressive timeouts to ensure that failing components didn't bring the entire system down.Instead, each feature and component was designed to gracefully degrade. For example, during traffic surges that created CPU-usage spikes, instead of showing a list of movies personalized to the user, they would show static content, such as cached or un-personalized results, which required less computation.\n",
      "\n",
      "Furthermore, the blog post explained that, in addition to implementing these architectural patterns, they also built and had been running a surprising and audacious service called _Chaos Monkey_ , which simulated AWS failures by constantly and randomly killing production servers. They did so because they wanted all \"engineering teams to be used to a constant level of failure in the cloud\" so that services could \"automatically recover without any manual intervention.\"\n",
      "\n",
      "In other words, the Netflix team ran Chaos Monkey to gain assurance that they had achieved their operational resilience objectives, constantly injecting failures into their pre-production and production environments.\n",
      "\n",
      "As one might expect, when they first ran Chaos Monkey in their production environments, services failed in ways they never could have predicted or imagined—by constantly finding and fixing these issues during normal working hours, Netflix engineers quickly and iteratively created a more resilient service, while simultaneously creating organizational learnings (during normal working hours!) that enabled them to evolve their systems far beyond their competition.\n",
      "\n",
      "Chaos Monkey is just one example of how learning can be integrated into daily work. The story also shows how learning organizations think about failures, accidents, and mistakes—as an opportunity for learning and not something to be punished. This chapter explores how to create a system of learning and how to establish a _just culture_ , as well as how to routinely rehearse and deliberately create failures to accelerate learning.\n",
      "\n",
      "## ESTABLISH A JUST, LEARNING CULTURE\n",
      "\n",
      "One of the prerequisites for a learning culture is that when accidents occur (which they undoubtedly will), the response to those accidents is seen as \"just.\" Dr. Sidney Dekker, who helped codify some of the key elements of safety culture and coined the term _just culture_ , writes, \"When responses to incidents and accidents are seen as unjust, it can impede safety investigations, promoting fear rather than mindfulness in people who do safety-critical work, making organizations more bureaucratic rather than more careful, and cultivating professional secrecy, evasion, and self-protection.\"\n",
      "\n",
      "This notion of punishment is present, either subtly or prominently, in the way many managers have operated during the last century. The thinking goes, in order to achieve the goals of the organization, leaders must command, control, establish procedures to eliminate errors, and enforce compliance of those procedures.\n",
      "\n",
      "Dr. Dekker calls this notion of eliminating error by eliminating the people who caused the errors the _Bad Apple Theory_. He asserts that this is invalid, because \"human error is not our cause of troubles; instead, human error is a consequence of the design of the tools that we gave them.\"\n",
      "\n",
      "If accidents are not caused by \"bad apples,\" but rather are due to inevitable design problems in the complex system that we created, then instead of \"naming, blaming, and shaming\" the person who caused the failure, our goal should always be to maximize opportunities for organizational learning, continually reinforcing that we value actions that expose and share more widely the problems in our daily work. This is what enables us to improve the quality and safety of the system we operate within and reinforce the relationships between everyone who operates within that system.\n",
      "\n",
      "By turning information into knowledge and building the results of the learning into our systems, we start to achieve the goals of a just culture, balancing the needs for safety and accountability. As John Allspaw, CTO of Etsy, states, \"Our goal at Etsy is to view mistakes, errors, slips, lapses, and so forth with a perspective of learning.\"\n",
      "\n",
      "When engineers make mistakes and feel safe when giving details about it, they are not only willing to be held accountable, but they are also enthusiastic in helping the rest of the company avoid the same error in the future. This is what creates organizational learning. On the other hand, if we punish that engineer, everyone is dis-incentivized to provide the necessary details to get an understanding of the mechanism, pathology, and operation of the failure, which guarantees that the failure will occur again.\n",
      "\n",
      "Two effective practices that help create a just, learning-based culture are blameless post-mortems and the controlled introduction of failures into production to create opportunities to practice for the inevitable problems that arise within complex systems. We will first look at blameless post-mortems and follow that with an exploration of why failure can be a good thing.\n",
      "\n",
      "## SCHEDULE BLAMELESS POST-MORTEM MEETINGS AFTER ACCIDENTS OCCUR\n",
      "\n",
      "To help enable a just culture, when accidents and significant incidents occur (e.g., failed deployment, production issue that affected customers), we should conduct a _blameless post-mortem_ after the incident has been resolved.‡ Blameless post-mortems, a term coined by John Allspaw, help us examine \"mistakes in a way that focuses on the situational aspects of a failure's mechanism and the decision-making process of individuals proximate to the failure.\"\n",
      "\n",
      "To do this, we schedule the post-mortem as soon as possible after the accident occurs and before memories and the links between cause and effect fade or circumstances change. (Of course, we wait until after the problem has been resolved so as not to distract the people who are still actively working on the issue.)\n",
      "\n",
      "In the blameless post-mortem meeting, we will do the following:\n",
      "\n",
      "  * Construct a timeline and gather details from multiple perspectives on failures, ensuring we don't punish people for making mistakes\n",
      "  * Empower all engineers to improve safety by allowing them to give detailed accounts of their contributions to failures\n",
      "  * Enable and encourage people who do make mistakes to be the experts who educate the rest of the organization on how not to make them in the future\n",
      "  * Accept that there is always a discretionary space where humans can decide to take action or not, and that the judgment of those decisions lies in hindsight\n",
      "  * Propose countermeasures to prevent a similar accident from happening in the future and ensure these countermeasures are recorded with a target date and an owner for follow-up\n",
      "\n",
      "To enable us to gain this understanding, the following stakeholders need to be present at the meeting:\n",
      "\n",
      "  * The people involved in decisions that may have contributed to the problem\n",
      "  * The people who identified the problem\n",
      "  * The people who responded to the problem\n",
      "  * The people who diagnosed the problem\n",
      "  * The people who were affected by the problem\n",
      "  * And anyone else who is interested in attending the meeting.\n",
      "\n",
      "Our first task in the blameless post-mortem meeting is to record our best understanding of the timeline of relevant events as they occurred. This includes all actions we took and what time (ideally supported by chat logs, such as IRC or Slack), what effects we observed (ideally in the form of the specific metrics from our production telemetry, as opposed to merely subjective narratives), all investigation paths we followed, and what resolutions were considered.\n",
      "\n",
      "To enable these outcomes, we must be rigorous about recording details and reinforcing a culture that information can be shared without fear of punishment or retribution. Because of this, especially for our first few post-mortems, it may be helpful to have the meeting led by a trained facilitator who wasn't involved in the accident.\n",
      "\n",
      "During the meeting and the subsequent resolution, we should explicitly disallow the phrases \"would have\" or \"could have,\" as they are _counterfactual_ statements that result from our human tendency to create possible alternatives to events that have already occurred.\n",
      "\n",
      "Counterfactual statements, such as \"I could have...\" or \"If I had known about that, I should have...,\" frame the problem in terms of the _system as imagined_ instead of in terms of the _system that actually exists_ , which is the context we need to restrict ourselves to. See Appendix 8.\n",
      "\n",
      "One of the potentially surprising outcomes of these meetings is that people will often blame themselves for things outside of their control or question their own abilities. Ian Malpass, an engineer at Etsy observes, \"In that moment when we do something that causes the entire site to go down, we get this 'ice water down the spine' feeling, and likely the first thought through our head is, 'I suck and I have no idea what I'm doing.' We need to stop ourselves from doing that, as it is route to madness, despair, and feelings of being an imposter, which is something that we can't let happen to good engineers. The better question to focus on is, 'Why did it make sense to me when I took that action?'\"\n",
      "\n",
      "In the meeting, we must reserve enough time for brainstorming and deciding which countermeasures to implement. Once the countermeasures have been identified, they must be prioritized and given an owner and timeline for implementation. Doing this further demonstrates that we value improvement __of our daily work more than daily work itself.\n",
      "\n",
      "Dan Milstein, one of the principal engineers at Hubspot, writes that he begins all blameless post-mortem meetings \"by saying, 'We're trying to prepare for a future where we're as stupid as we are today.'\" In other words, it is not acceptable to have a countermeasure to merely \"be more careful\" or \"be less stupid\"—instead, we must design real countermeasures to prevent these errors from happening again.\n",
      "\n",
      "Examples of such countermeasures include new automated tests to detect dangerous conditions in our deployment pipeline, adding further production telemetry, identifying categories of changes that require additional peer review, and conducting rehearsals of this category of failure as part of regularly scheduled Game Day exercises.\n",
      "\n",
      "## PUBLISH OUR POST-MORTEMS AS WIDELY AS POSSIBLE\n",
      "\n",
      "After we conduct a blameless post-mortem meeting, we should widely announce the availability of the meeting notes and any associated artifacts (e.g., timelines, IRC chat logs, external communications). This information should (ideally) be placed in a centralized location where our entire organization can access it and learn from the incident. Conducting post-mortems is so important that we may even prohibit production incidents from being closed until the post-mortem meeting has been completed.\n",
      "\n",
      "Doing this helps us translate local learnings and improvements into global learnings and improvements. Randy Shoup, former engineering director for Google App Engine, describes how documentation of post-mortem meetings can have tremendous value to others in the organization, \"As you can imagine at Google, everything is searchable. All the post-mortem documents are in places where other Googlers can see them. And trust me, when any group has an incident that sounds similar to something that happened before, these post-mortem documents are among the first documents being read and studied.\"§\n",
      "\n",
      "Widely publishing post-mortems and encouraging others in the organization to read them increases organizational learning, and it also becoming increasingly commonplace for online service companies to publish post-mortems for customer-impacting outages. This often significantly increases the transparency we have with our internal and external customers, which in turn increases their trust in us.\n",
      "\n",
      "This desire to conduct as many blameless post-mortem meetings as necessary at Etsy led to some problems—over the course of four years, Etsy accumulated a large number of post-mortem meeting notes in wiki pages, which became increasingly difficult to search, save, and collaborate from.\n",
      "\n",
      "To help with this issue, they developed a tool called Morgue to easily record aspects of each accident, such as the incident MTTR and severity, better address time zones (which became relevant as more Etsy employees were working remotely), and include other data, such as rich text in Markdown format, embedded images, tags, and history.\n",
      "\n",
      "Morgue was designed to make it easy for the team to record:\n",
      "\n",
      "  * Whether the problem was due to a scheduled or an unscheduled incident\n",
      "  * The post-mortem owner\n",
      "  * Relevant IRC chat logs (especially important for 3 a.m. issues when accurate note-taking may not happen)\n",
      "  * Relevant JIRA tickets for corrective actions and their due dates (information particularly important to management)\n",
      "  * Links to customer forum posts (where customers complain about issues)\n",
      "\n",
      "After developing and using Morgue, the number of recorded post-mortems at Etsy increased significantly compared to when they used wiki pages, especially for P2, P3, and P4 incidents (i.e., lower severity problems). This result reinforced the hypothesis that if they made it easier to document post-mortems through tools such as Morgue, more people would record and detail the outcomes of their post-mortem meetings, enabling more organizational learning.\n",
      "\n",
      "Dr. Amy C. Edmondson, Novartis Professor of Leadership and Management at Harvard Business School and co-author of _Building the Future: Big Teaming for Audacious Innovation,_ writes:\n",
      "\n",
      "Again, the remedy—which does not necessarily involve much time and expense—is to reduce the stigma of failure. Eli Lilly has done this since the early 1990s by holding 'failure parties' to honor intelligent, high-quality scientific experiments that fail to achieve the desired results. The parties don't cost much, and redeploying valuable resources—particularly scientists—to new projects earlier rather than later can save hundreds of thousands of dollars, not to mention kickstart potential new discoveries.\n",
      "\n",
      "## DECREASE INCIDENT TOLERANCES TO FIND EVER-WEAKER FAILURE SIGNALS\n",
      "\n",
      "Inevitably, as organizations learn how to see and solve problems efficiently, they need to decrease the threshold of what constitutes a problem in order to keep learning. To do this, we seek to amplify weak failure signals. For example, as described in chapter 4, when Alcoa was able to reduce the rate of workplace accidents so that they were no longer commonplace, Paul O'Neill, CEO of Alcoa, started to be notified of accident near-misses in addition to actual workplace accidents.\n",
      "\n",
      "Dr. Spear summarizes O'Neill's accomplishments at Alcoa when he writes, \"Though it started by focusing on problems related to workplace safety, it soon found that safety problems reflected process ignorance and that this ignorance would also manifest itself in other problems such as quality, timeliness, and yield versus scrap.\"\n",
      "\n",
      "When we work within complex systems, this need to amplify weak failure signals is critical to averting catastrophic failures. The way NASA handled failure signals during the space shuttle era serves as an illustrative example: In 2003, sixteen days into the _Columbia_ space shuttle mission, it exploded as it re-entered the earth's atmosphere. We now know that a piece of insulating foam had broken off the external fuel tank during takeoff.\n",
      "\n",
      "However, prior to _Columbia's_ re-entry, a handful of mid-level NASA engineers had reported this incident, but their voices had gone unheard. They observed the foam strike on video monitors during a post-launch review session and immediately notified NASA's managers, but they were told that the foam issue was nothing new. Foam dislodgement had damaged shuttles in previous launches, but had never resulted in an accident. It was considered a maintenance problem and not acted upon until it was too late.\n",
      "\n",
      "Michael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson wrote in a 2006 article for _Harvard Business Review_ how NASA culture contributed to this problem. They describe how organizations are typically structured in one of two models: a _standardized model_ , where routine and systems govern everything, including strict compliance with timelines and budgets, or an _experimental model_ , where every day every exercise and every piece of new information is evaluated and debated in a culture that resembles a research and design (R&D) laboratory.\n",
      "\n",
      "They observe, \"Firms get into trouble when they apply the wrong mind-set to an organization [which dictates how they respond to _ambiguous threats_ or, in the terminology of this book, _weak failure signals_ ]....By the 1970s, NASA had created a culture of rigid standardization, promoting to Congress the space shuttle as a cheap and reusable spacecraft.\" NASA favored strict process compliance instead of an experimental model where every piece of information needed to be evaluated as it occured without bias. The absence of continuous learning and experimentation had dire consequences. The authors conclude that it is culture and mind-set that matters, not just \"being careful\"—as they write, \"vigilance alone will not prevent ambiguous threats [weak failure signals] from turning into costly (and sometimes tragic) failures.\"\n",
      "\n",
      "Our work in the technology value stream, like space travel, should be approached as a fundamentally experimental endeavor and managed that way. All work we do is a potentially important hypothesis and a source of data, rather than a routine application and validation of past practice. Instead of treating technology work as entirely standardized, where we strive for process compliance, we must continually seek to find ever-weaker failure signals so that we can better understand and manage the system we operate in.\n",
      "\n",
      "## REDEFINE FAILURE AND ENCOURAGE CALCULATED RISK-TAKING\n",
      "\n",
      "Leaders of an organization, whether deliberately or inadvertently, reinforce the organizational culture and values through their actions. Audit, accounting, and ethics experts have long observed that the \"tone at the top\" predicts the likelihood of fraud and other unethical practices. To reinforce our culture of learning and calculated risk-taking, we need leaders to continually reinforce that everyone should feel both comfortable with and responsible for surfacing and learning from failures.\n",
      "\n",
      "On failures, Roy Rapoport from Netflix observes, \"What the _2014 State of DevOps Report_ proved to me is that high-performing DevOps organizations will fail and make mistakes more often. Not only is this okay, it's what organizations need! You can even see it in the data: if high performers are performing thirty times more frequently but with only half the change failure rate, they're obviously having more failures.\"\n",
      "\n",
      "He continues, \"I was talking with a co-worker about a massive outage we just had at Netflix—it was caused by, frankly, a dumb mistake. In fact, it was caused by an engineer who had taken down Netflix twice in the last eighteen months. But, of course, this is a person we'd never fire. In that same eighteen months, this engineer moved the state of our operations and automation forward not by miles but by light-years. That work has enabled us to do deployments safely on a daily basis, and has personally performed huge numbers of production deployments.\"\n",
      "\n",
      "He concludes, \"DevOps must allow this sort of innovation and the resulting risks of people making mistakes. Yes, you'll have more failures in production. But that's a good thing, and should not be punished.\"\n",
      "\n",
      "## INJECT PRODUCTION FAILURES TO ENABLE RESILIENCE AND LEARNING\n",
      "\n",
      "As we saw in the chapter introduction, injecting faults into the production environment (such as Chaos Monkey) is one way we can increase our resilience. In this section, we describe the processes involved in rehearsing and injecting failures into our system to confirm that we have designed and architected our systems properly, so that failures happen in specific and controlled ways. We do this by regularly (or even continuously) performing tests to make certain that our systems fail gracefully.\n",
      "\n",
      "As Michael Nygard, author of _Release It! Design and Deploy Production-Ready Software,_ comments, \"Like building crumple zones into cars to absorb impacts and keep passengers safe, you can decide what features of the system are indispensable and build in failure modes that keep cracks away from those features. If you do not design your failure modes, then you will get whatever unpredictable—and usually dangerous—ones happen to emerge.\"\n",
      "\n",
      "Resilience requires that we first define our failure modes and then perform testing to ensure that these failure modes operate as designed. One way we do this is by injecting faults into our production environment and rehearsing large-scale failures so we are confident we can recover from accidents when they occur, ideally without even impacting our customers.\n",
      "\n",
      "The 2012 story about Netflix and the Amazon AWS-EAST outage presented in the introduction is just one example. An even more interesting example of resilience at Netflix was during the \"Great Amazon Reboot of 2014,\" when nearly 10% of the entire Amazon EC2 server fleet had to be rebooted to apply an emergency Xen security patch. As Christos Kalantzis of Netflix Cloud Database Engineering recalled, \"When we got the news about the emergency EC2 reboots, our jaws dropped. When we got the list of how many Cassandra nodes would be affected, I felt ill.\"But, Kalantzis continues, \"Then I remembered all the Chaos Monkey exercises we've gone through. My reaction was, 'Bring it on!'\"\n",
      "\n",
      "Once again, the outcomes were astonishing. Of the 2,700+ Cassandra nodes used in production, 218 were rebooted, and twenty-two didn't reboot successfully. As Kalantzis and Bruce Wong from Netflix Chaos Engineering wrote, \"Netflix experienced 0 downtime that weekend. Repeatedly and regularly exercising failure, even in the persistence [database] layer, should be part of every company's resilience planning. If it wasn't for Cassandra's participation in Chaos Monkey, this story would have ended much differently.\"\n",
      "\n",
      "Even more surprising, not only was no one at Netflix working active incidents due to failed Cassandra nodes, no one was even in the office—they were in Hollywood at a party celebrating an acquisition milestone. This is another example demonstrating that proactively focusing on resilience often means that a firm can handle events that may cause crises for most organizations in a manner that is routine and mundane.¶ See Appendix 9.\n",
      "\n",
      "## INSTITUTE GAME DAYS TO REHEARSE FAILURES\n",
      "\n",
      "In this section, we describe specific disaster recovery rehearsals called Game Days, a term popularized by Jesse Robbins, one of the founders of the Velocity Conference community and co-founder of Chef, for the work he did at Amazon, where he was responsible for programs to ensure site availability and was widely known internally as the \"Master of Disaster.\"The concept of Game Days comes from the discipline of _resilience engineering_. Robbins defines resilience engineering as \"an exercise designed to increase resilience through large-scale fault injection across critical systems.\"\n",
      "\n",
      "Robbins observes that \"whenever you set out to engineer a system at scale, the best you can hope for is to build a reliable software platform on top of components that are completely unreliable. That puts you in an environment where complex failures are both inevitable and unpredictable.\"\n",
      "\n",
      "Consequently, we must ensure that services continue to operate when failures occur, potentially throughout our system, ideally without crisis or even manual intervention. As Robbins quips, \"a service is not really tested until we break it in production.\"\n",
      "\n",
      "Our goal for Game Day is to help teams simulate and rehearse accidents to give them the ability to practice. First, we schedule a catastrophic event, such as the simulated destruction of an entire data center, to happen at some point in the future. We then give teams time to prepare, to eliminate all the single points of failure, and to create the necessary monitoring procedures, failover procedures, etc.\n",
      "\n",
      "Our Game Day team defines and executes drills, such as conducting database failovers (i.e., simulating a database failure and ensuring that the secondary database works) or turning off an important network connection to expose problems in the defined processes. Any problems or difficulties that are encountered are identified, addressed, and tested again.\n",
      "\n",
      "At the scheduled time, we then execute the outage. As Robbins describes, at Amazon they \"would literally power off a facility—without notice—and then let the systems fail naturally and [allow] the people to follow their processes wherever they led.\"\n",
      "\n",
      "By doing this, we start to expose the _latent defects_ in our system, which are the problems that appear only because of having injected faults into the system. Robbins explains, \"You might discover that certain monitoring or management systems crucial to the recovery process end up getting turned off as part of the failure you've orchestrated. [Or] you would find some single points of failure you didn't know about that way.\" These exercises are then conducted in an increasingly intense and complex way with the goal of making them feel like just another part of an average day.\n",
      "\n",
      "By executing Game Days, we progressively create a more resilient service and a higher degree of assurance that we can resume operations when inopportune events occur, as well create more learnings and a more resilient organization.\n",
      "\n",
      "An excellent example of simulating disaster is Google's Disaster Recovery Program (DiRT). Kripa Krishnan is a technical program director at Google, and, at the time of this writing, has led the program for over seven years. During that time, they've simulated an earthquake in Silicon Valley, which resulted in the entire Mountain View campus being disconnected from Google; major data centers having complete loss of power; and even aliens attacking cities where engineers resided.\n",
      "\n",
      "As Krishnan wrote, \"An often-overlooked area of testing is business process and communications. Systems and processes are highly intertwined, and separating testing of systems from testing of business processes isn't realistic: a failure of a business system will affect the business process, and conversely a working system is not very useful without the right personnel.\"\n",
      "\n",
      "Some of the learnings gained during these disasters included:\n",
      "\n",
      "  * When connectivity was lost, the failover to the engineer workstations didn't work\n",
      "  * Engineers didn't know how to access a conference call bridge or the bridge only had capacity for fifty people or they needed a new conference call provider who would allow them to kick off engineers who had subjected the entire conference to hold music\n",
      "  * When the data centers ran out of diesel for the backup generators, no one knew the procedures for making emergency purchases through the supplier, resulting in someone using a personal credit card to purchase $50,000 worth of diesel.\n",
      "\n",
      "By creating failure in a controlled situation, we can practice and create the playbooks we need. One of the other outputs of Game Days is that people actually know who to call and know who to talk to—by doing this, they develop relationships with people in other departments so they can work together during an incident, turning conscious actions into unconscious actions that are able to become routine.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "To create a just culture that enables organizational learning, we have to re-contextualize so-called failures. When treated properly, errors that are inherent in complex systems can create a dynamic learning environment where all of the shareholders feel safe enough to come forward with ideas and observations, and where groups rebound more readily from projects that don't perform as expected.\n",
      "\n",
      "Both blameless post-mortems and injecting production failures reinforce a culture that everyone should feel both comfortable with and responsible for surfacing and learning from failures. In fact, when we sufficiently reduce the number of accidents, we decrease our tolerance so that we can keep learning. As Peter Senge is known to say, \"The only sustainable competitive advantage is an organization's ability to learn faster than the competition.\"\n",
      "\n",
      "* * *\n",
      "\n",
      "† In January 2013 at re:Invent, James Hamilton, VP and Distinguished Engineer for Amazon Web Services said that the US East region had more than ten data centers all by itself, and added that a typical data center has between fifty thousand and eighty thousand servers. By this math, the 2011 EC2 outage affected customers on more than half a million servers.\n",
      "\n",
      "‡ This practice has also been called _blameless post-incident reviews_ as well as _post-event retrospectives_. There is also a noteworthy similarity to the routine retrospectives that are a part of many iterative and agile development practices.\n",
      "\n",
      "§ We may also choose to extend the philosophies of Transparent Uptime to our post-mortem reports and, in addition to making a service dashboard available to the public, we may choose to publish (maybe sanitized) post-mortem meetings to the public. Some of the most widely admired public post-mortems include those posted by the Google App Engine team after a significant 2010 outage, as well as the post-mortem of the 2015 Amazon DynamoDB outage. Interestingly, Chef publishes their post-mortem meeting notes on their blog, as well as recorded videos of the actual post-mortem meetings.\n",
      "\n",
      "¶ Specific architectural patterns that they implemented included fail fasts (setting aggressive timeouts such that failing components don't make the entire system crawl to a halt), fallbacks (designing each feature to degrade or fall back to a lower quality representation), and feature removal (removing non-critical features when they run slowly from any given page to prevent them from impacting the member experience). Another astonishing example of the resilience that the Netflix team created beyond preserving business continuity during the AWS outage, was that Netflix went over six hours into the AWS outage before declaring a Sev 1 incident, assuming that AWS service would eventually be restored (i.e., \"AWS will come back... it usually does, right?\"). Only after six hours into the outage did they activate any business continuity procedures.\n",
      "\n",
      "# 20Convert Local Discoveries into Global Improvements\n",
      "\n",
      "In the previous chapter, we discussed developing a safe learning culture by encouraging everyone to talk about mistakes and accidents through blameless post-mortems. We also explored finding and fixing ever-weaker failure signals, as well as reinforcing and rewarding experimentation and risk-taking. Furthermore, we helped make our system of work more resilient by proactively scheduling and testing failure scenarios, making our systems safer by finding latent defects and fixing them.\n",
      "\n",
      "In this chapter, we will create mechanisms that make it possible for new learnings and improvements discovered locally to be captured and shared globally throughout the entire organization, multiplying the effect of global knowledge and improvement. By doing this, we elevate the state of the practice of the entire organization so that everyone doing work benefits from the cumulative experience of the organization.\n",
      "\n",
      "## USE CHAT ROOMS AND CHAT BOTS TO AUTOMATE AND CAPTURE ORGANIZATIONAL KNOWLEDGE\n",
      "\n",
      "Many organizations have created chat rooms to facilitate fast communication within teams. However, chat rooms are also used to trigger automation.\n",
      "\n",
      "This technique was pioneered in the ChatOps journey at GitHub. The goal was to put automation tools into the middle of the conversation in their chat rooms, helping create transparency and documentation of their work. As Jesse Newland, a systems engineer at GitHub, describes, \"Even when you're new to the team, you can look in the chat logs and see how everything is done. It's as if you were pair-programming with them all the time.\"\n",
      "\n",
      "They created _Hubot_ , a software application that interacted with the Ops team in their chat rooms, where it could be instructed to perform actions merely by sending it a command (e.g., \"@hubot deploy owl to production\"). The results would also be sent back into the chat room.\n",
      "\n",
      "Having this work performed by automation in the chat room (as opposed to running automated scripts via command line) had numerous benefits, including:\n",
      "\n",
      "  * Everyone saw everything that was happening.\n",
      "  * Engineers on their first day of work could see what daily work looked like and how it was performed.\n",
      "  * People were more apt to ask for help when they saw others helping each other.\n",
      "  * Rapid organizational learning was enabled and accumulated.\n",
      "\n",
      "Furthermore, beyond the above tested benefits, chat rooms inherently record and make all communications public; in contrast, emails are private by default, and the information in them cannot easily be discovered or propagated within an organization.\n",
      "\n",
      "Integrating our automation into chat rooms helps document and share our observations and problem solving as an inherent part of performing our work. This reinforces a culture of transparency and collaboration in everything we do.\n",
      "\n",
      "This is also an extremely effective way of converting local learning to global knowledge. At Github, all the Operations staff worked remotely—in fact, no two engineers worked in the same city. As Mark Imbriaco, former VP of Operations at GitHub, recalls, \"There was no physical water cooler at GitHub. The chat room was the water cooler.\"\n",
      "\n",
      "Github enabled Hubot to trigger their automation technologies, including Puppet, Capistrano, Jenkins, resque (a Redis-backed library for creating background jobs), and graphme (which generates graphs from Graphite).\n",
      "\n",
      "Actions performed through Hubot included checking the health of services, doing puppet pushes or code deployments into production, and muting alerts as services went into maintenance mode. Actions that were performed multiple times, such as pulling up the smoke test logs when a deployment failed, taking production servers out of rotation, reverting to master for production front-end services, or even apologizing to the engineers who were on call, also became Hubot actions.†\n",
      "\n",
      "Similarly, commits to the source code repository and the commands that trigger production deployments both emit messages to the chat room. Additionally, as changes move through the deployment pipeline, their status is posted in the chat room.\n",
      "\n",
      "A typical quick chat room exchange might look like:\n",
      "\n",
      "_\"@sr: @jnewland, how do you get that list of big repos? disk_hogs or something?\"_\n",
      "\n",
      "_\"@jnewland: /disk-hogs\"_\n",
      "\n",
      "Newland observes that certain questions that were previously asked during the course of a project are rarely asked now. For example, engineers may ask each other, \"How is that deploy going?\" or \"Are you deploying that, or should I?\" or \"How does the load look?\"\n",
      "\n",
      "Among all the benefits that Newland describes, which include faster onboarding of newer engineers and making all engineers more productive, the result that he felt was most important was that Ops work became more humane as Ops engineers were enabled to discover problems and help each other quickly and easily.\n",
      "\n",
      "GitHub created an environment for collaborative local learning that could be transformed into learnings across the organization. Throughout the rest of this chapter we will explore ways to create and accelerate the spread of new organizational learnings.\n",
      "\n",
      "## AUTOMATE STANDARDIZED PROCESSES IN SOFTWARE FOR RE-USE\n",
      "\n",
      "All too often, we codify our standards and processes for architecture, testing, deployment, and infrastructure management in prose, storing them in Word documents that are uploaded somewhere. The problem is that engineers who are building new applications or environments often don't know that these documents exist, or they don't have the time to implement the documented standards. The result is they create their own tools and processes, with all the disappointing outcomes we'd expect: fragile, insecure, and unmaintainable applications and environments that are expensive to run, maintain, and evolve.\n",
      "\n",
      "Instead of putting our expertise into Word documents, we need to transform these documented standards and processes, which encompass the sum of our organizational learnings and knowledge, into an executable form that makes them easier to reuse. One of the best ways we can make this knowledge re-usable is by putting it into a centralized source code repository, making the tool available for everyone to search and use.\n",
      "\n",
      "Justin Arbuckle was chief architect at GE Capital in 2013 when he said, \"We needed to create a mechanism that would allow teams to easily comply with policy—national, regional, and industry regulations across dozens of regulatory frameworks, spanning thousands of applications running on tens of thousands of servers in tens of data centers.\"\n",
      "\n",
      "The mechanism they created was called ArchOps, which \"enabled our engineers to be builders, not bricklayers. By putting our design standards into automated blueprints that were able to be used easily by anyone, we achieved consistency as a byproduct.\"\n",
      "\n",
      "By encoding our manual processes into code that is automated and executed, we enable the process to be widely adopted, providing value to anyone who uses them. Arbuckle concluded that \"the actual compliance of an organization is in direct proportion to the degree to which its policies are expressed as code.\"\n",
      "\n",
      "By making this automated process the easiest means to achieve the goal, we allow practices to be widely adopted—we may even consider turning them into shared services supported by the organization.\n",
      "\n",
      "## CREATE A SINGLE, SHARED SOURCE CODE REPOSITORY FOR OUR ENTIRE ORGANIZATION\n",
      "\n",
      "A firm-wide, shared source code repository is one of the most powerful mechanisms used to integrate local discoveries across the entire organization. When we update anything in the source code repository (e.g., a shared library), it rapidly and automatically propagates to every other service that uses that library, and it is integrated through each team's deployment pipeline.\n",
      "\n",
      "Google is one of the largest examples of using an organization-wide shared source code repository. By 2015, Google had a single shared source code repository with over one billion files and over two billion lines of code. This repository is used by every one of their twenty-five thousand engineers and spans every Google property, including Google Search, Google Maps, Google Docs, Google+, Google Calendar, Gmail, and YouTube.‡\n",
      "\n",
      "One of the valuable results of this is that engineers can leverage the diverse expertise of everyone in the organization. Rachel Potvin, a Google engineering manager overseeing the Developer Infrastructure group, told _Wired_ that every Google engineer can access \"a wealth of libraries\" because \"almost everything has already been done.\"\n",
      "\n",
      "Furthermore, as Eran Messeri, an engineer in the Google Developer Infrastructure group, explains, one of the advantages of using a single repository is that it allows users to easily access all of the code in its most up-to-date form, without the need for coordination.\n",
      "\n",
      "We put into our shared source code repository not only source code, but also other artifacts that encode knowledge and learning, including:\n",
      "\n",
      "  * Configuration standards for our libraries, infrastructure, and environments (Chef recipes, Puppet manifests, etc.)\n",
      "  * Deployment tools\n",
      "  * Testing standards and tools, including security \n",
      "  * Deployment pipeline tools\n",
      "  * Monitoring and analysis tools\n",
      "  * Tutorials and standards\n",
      "\n",
      "Encoding knowledge and sharing it through this repository is one of the most powerful mechanisms we have for propagating knowledge. As Randy Shoup describes, \"The most powerful mechanism for preventing failures at Google is the single code repository. Whenever someone checks in anything into the repo, it results in a new build, which always uses the latest version of everything. Everything is built from source rather than dynamically linked at runtime—there is always a single version of a library that is the current one in use, which is what gets statically linked during the build process.\"\n",
      "\n",
      "Tom Limoncelli is the co-author of _The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems_ and a former Site Reliability Engineer at Google. In his book, he states that the value of having a single repository for an entire organization is so powerful it is difficult to even explain.\n",
      "\n",
      "You can write a tool exactly once and have it be usable for all projects. You have 100% accurate knowledge of who depends on a library; therefore, you can refactor it and be 100% sure of who will be affected and who needs to test for breakage. I could probably list one hundred more examples. I can't express in words how much of a competitive advantage this is for Google.\n",
      "\n",
      "At Google, every library (e.g., libc, OpenSSL, as well internally developed libraries such as Java threading libraries) has an owner who is responsible for ensuring that the library not only compiles, but also successfully passes the tests for all projects that depend upon it, much like a real-world librarian. That owner is also responsible for migrating each project from one version to the next.\n",
      "\n",
      "Consider the real-life example of an organization that runs eighty-one different versions of the Java Struts framework library in production—all but one of those versions have critical security vulnerabilities, and maintaining all those versions, each with its own quirks and idiosyncrasies, creates significant operational burden and stress. Furthermore, all this variance makes upgrading versions risky and unsafe, which in turn discourages developers from upgrading. And the cycle continues.\n",
      "\n",
      "The single source repository solves much of this problem, as well as having automated tests that allow teams to migrate to new versions safely and confidently.\n",
      "\n",
      "If we are not able to build everything off a single source tree, we must find another means to maintain known good versions of the libraries and their dependencies. For instance, we may have an organization-wide repository such as Nexus, Artifactory, or a Debian or RPM repository, which we must then update where there are known vulnerabilities, both in these repositories and in production systems.\n",
      "\n",
      "## SPREAD KNOWLEDGE BY USING AUTOMATED TESTS AS DOCUMENTATION AND COMMUNITIES OF PRACTICE\n",
      "\n",
      "When we have shared libraries being used across the organization, we should enable rapid propagation of expertise and improvements. Ensuring that each of these libraries has significant amounts of automated testing included means these libraries become self-documenting and show other engineers how to use them.\n",
      "\n",
      "This benefit will be nearly automatic if we have test-driven development (TDD) practices in place, where automated tests are written before we write the code. This discipline turns our test suites into a living, up-to-date specification of the system. Any engineer wishing to understand how to use the system can look at the test suite to find working examples of how to use the system's API.\n",
      "\n",
      "Ideally, each library will have a single owner or a single team supporting it, representing where knowledge and expertise for the library resides. Furthermore, we should (ideally) only allow one version to be used in production, ensuring that whatever is in production leverages the best collective knowledge of the organization.\n",
      "\n",
      "In this model, the library owner is also responsible for safely migrating each group using the repository from one version to the next. This in turn requires quick detection of regression errors through comprehensive automated testing and continuous integration for all systems that use the library.\n",
      "\n",
      "In order to more rapidly propagate knowledge, we can also create discussion groups or chat rooms for each library or service, so anyone who has questions can get responses from other users, who are often faster to respond than the developers.\n",
      "\n",
      "By using this type of communication tool instead of having isolated pockets of expertise spread throughout the organization, we facilitate an exchange of knowledge and experience, ensuring that workers are able to help each other with problems and new patterns.\n",
      "\n",
      "## DESIGN FOR OPERATIONS THROUGH CODIFIED NON-FUNCTIONAL REQUIREMENTS\n",
      "\n",
      "When Development follows their work downstream and participates in production incident resolution activities, the application becomes increasingly better designed for Operations. Furthermore, as we start to deliberately design our code and application so that it can accommodate fast flow and deployability, we will likely identify a set of non-functional requirements that we will want to integrate into all of our production services.\n",
      "\n",
      "Implementing these non-functional requirements will enable our services to be easy to deploy and keep running in production, where we can quickly detect and correct problems, and ensure it degrades gracefully when components fail. Examples of non-functional requirements include ensuring that we have:\n",
      "\n",
      "  * Sufficient production telemetry in our applications and environments\n",
      "  * The ability to accurately track dependencies\n",
      "  * Services that are resilient and degrade gracefully\n",
      "  * Forward and backward compatibility between versions\n",
      "  * The ability to archive data to manage the size of the production data set\n",
      "  * The ability to easily search and understand log messages across services\n",
      "  * The ability to trace requests from users through multiple services\n",
      "  * Simple, centralized runtime configuration using feature flags and so forth\n",
      "\n",
      "By codifying these types of non-functional requirements, we make it easier for all our new and existing services to leverage the collective knowledge and experience of the organization. These are all responsibilities of the team building the service.\n",
      "\n",
      "## BUILD REUSABLE OPERATIONS USER STORIES INTO DEVELOPMENT\n",
      "\n",
      "When there is Operations work that cannot be fully automated or made self-service, our goal is to make this recurring work as repeatable and deterministic as possible. We do this by standardizing the needed work, automating as much as possible, and documenting our work so that we can best enable product teams to better plan and resource this activity.\n",
      "\n",
      "Instead of manually building servers and then putting them into production according to manual checklists, we should automate as much of this work as possible. Where certain steps cannot be automated (e.g., manually racking a server and having another team cable it), we should collectively define the handoffs as clearly as possible to reduce lead times and errors. This will also enable us to better plan and schedule these steps in the future. For instance, we can use tools such as Rundeck to automate and execute workflows, or work ticket systems such as JIRA or ServiceNow.\n",
      "\n",
      "Ideally, for all our recurring Ops work we will know the following: what work is required, who is needed to perform it, what the steps to complete it are, and so forth. For instance, \"We know a high-availability rollout takes fourteen steps, requiring work from four different teams, and the last five times we performed this, it took an average of three days.\"\n",
      "\n",
      "Just as we create user stories in Development that we put into the backlog and then pull into work, we can create well-defined \"Ops user stories\" that represent work activities that can be reused across all our projects (e.g., deployment, capacity, security, etc.). By creating these well defined Ops user stories, we expose repeatable IT Operations work in a manner where it shows up alongside Development work, enabling better planning and more repeatable outcomes.\n",
      "\n",
      "## ENSURE TECHNOLOGY CHOICES HELP ACHIEVE ORGANIZATIONAL GOALS\n",
      "\n",
      "When one of our goals is to maximize developer productivity and we have service-oriented architectures, small service teams can potentially build and run their service in whatever language or framework that best serves their specific needs. In some cases, this is what best enables us to achieve our organizational goals.\n",
      "\n",
      "However, there are scenarios when the opposite occurs, such as when expertise for a critical service resides only in one team, and only that team can make changes or fix problems, creating a bottleneck. In other words, we may have optimized for team productivity but inadvertently impeded the achievement of organizational goals.\n",
      "\n",
      "This often happens when we have a functionally-oriented Operations group that is responsible for any aspect of service support. In these scenarios, to ensure that we enable the deep skill sets in specific technologies, we want to make sure that Operations can influence which components are used in production, or give them the ability to not be responsible for unsupported platforms.\n",
      "\n",
      "If we do not have a list of technologies that Operations will support, collectively generated by Development and Operations, we should systematically go through the production infrastructure and services, as well as all their dependencies that are currently supported, to find which ones are creating a disproportionate amount of failure demand and unplanned work. Our goal is to identify the technologies that:\n",
      "\n",
      "  * Impede or slow down the flow of work\n",
      "  * Disproportionately create high levels of unplanned work\n",
      "  * Disproportionately create large numbers of support requests\n",
      "  * Are most inconsistent with our desired architectural outcomes (e.g. throughput, stability, security, reliability, business continuity)\n",
      "\n",
      "By removing these problematic infrastructures and platforms from the technologies supported by Ops, we enable them to focus on infrastructure that best helps achieve the global goals of the organization.\n",
      "\n",
      "As Tom Limoncelli describes, \"When I was at Google, we had one official compiled language, one official scripting language, and one official UI language. Yes, other languages were supported in some way or another, but sticking with 'the big three' meant support libraries, tools, and an easier way to find collaborators.\"§ These standards were also reinforced by the code review process, as well as what languages were supported by their internal platforms.\n",
      "\n",
      "In a presentation that he gave with Olivier Jacques and Rafael Garcia at the 2015 DevOps Enterprise Summit, Ralph Loura, CIO of HP, stated:\n",
      "\n",
      "Internally, we described our goal as creating \"buoys, not boundaries.\" Instead of drawing hard boundaries that everyone has to stay within, we put buoys that indicate deep areas of the channel where you're safe and supported. You can go past the buoys as long as you follow the organizational principles. After all, how are we ever going to see the next innovation that helps us win if we're not exploring and testing at the edges? As leaders, we need to navigate the channel, mark the channel, and allow people to explore past it.\n",
      "\n",
      "Case Study   \n",
      "Standardizing a New Technology Stack at Etsy (2010)\n",
      "\n",
      "In many organizations adopting DevOps, a common story developers tell is, \"Ops wouldn't provide us what we needed, so we just built and supported it ourselves.\" However, in the early stages of the Etsy transformation, technology leadership took the opposite approach, significantly reducing the number of supported technologies in production.\n",
      "\n",
      "In 2010, after a nearly disastrous peak holiday season, the Etsy team decided to massively reduce the number of technologies used in production, choosing a few that the entire organization could fully support and eradicating the rest.¶\n",
      "\n",
      "Their goal was to standardize and very deliberately reduce the supported infrastructure and configurations. One of the early decisions was to migrate Etsy's entire platform to PHP and MySQL. This was primarily a philosophical decision rather than a technological decision—they wanted both Dev and Ops to be able to understand the full technology stack so that everyone could contribute to a single platform, as well as enable everyone to be able to read, rewrite, and fix each other's code. Over the next several years, as Michael Rembetsy, who was Etsy's Director of Operations at the time, recalls, \"We retired some great technologies, taking them entirely out of production,\" including lighttpd, Postgres, MongoDB, Scala, CoffeeScript, Python, and many others.\n",
      "\n",
      "Similarly, Dan McKinley, a developer on the feature team that introduced MongoDB into Etsy in 2010, writes on his blog that all the benefits of having a schema-less database were negated by all the operational problems the team had to solve. These included problems concerning logging, graphing, monitoring, production telemetry, and backups and restoration, as well as numerous other issues that developers typically do not need to concern themselves with. The result was to abandon MongoDB, porting the new service to use the already supported MySQL database infrastructure.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "The techniques described in this chapter enable every new learning to be incorporated into the collective knowledge of the organization, multiplying its effect. We do this by actively and widely communicating new knowledge, such as through chat rooms and through technology such as architecture as code, shared source code repositories, technology standardization, and so forth. By doing this, we elevate the state of the practice of not just Dev and Ops, but also the entire organization, so everyone who performs work does so with the cumulative experience of the entire organization.\n",
      "\n",
      "* * *\n",
      "\n",
      "† Hubot often performed tasks by calling shell scripts, which could then be executed from the chat room anywhere, including from an engineer's phone.\n",
      "\n",
      "‡ The Chrome and Android projects reside in a separate source code repository, and certain algorithms that are kept secret, such as PageRank, are available only to certain teams.\n",
      "\n",
      "§ Google used C++ as their official compiled language, Python (and later Go) as their official scripting language, and Java and JavaScript via Google Web Toolkit as their official UI languages.\n",
      "\n",
      "¶ At that time, Etsy used PHP, lighttp, Postgres, MongoDB, Scala, CoffeeScript, Python, as well as many other platforms and languages.\n",
      "\n",
      "# 21Reserve Time to Create Organizational Learning and Improvement\n",
      "\n",
      "One of the practices that forms part of the Toyota Production System is called the _improvement blitz_ (or sometimes a _kaizen blitz_ ), defined as a dedicated and concentrated period of time to address a particular issue, often over the course of a several days. Dr. Spear explains, \"...blitzes often take this form: A group is gathered to focus intently on a process with problems...The blitz lasts a few days, the objective is process improvement, and the means are the concentrated use of people from outside the process to advise those normally inside the process.\"\n",
      "\n",
      "Spear observes that the output of the improvement blitz team will often be a new approach to solving a problem, such as new layouts of equipment, new means of conveying material and information, a more organized workspace, or standardized work. They may also leave behind a to-do list of changes to be made down the road.\n",
      "\n",
      "An example of a DevOps improvement blitz is the Monthly Challenge program at the Target DevOps Dojo. Ross Clanton, Director of Operations at Target, is responsible for accelerating the adoption of DevOps. One of his primary mechanisms for this is the Technology Innovation Center, more popularly known as the DevOps Dojo.\n",
      "\n",
      "The DevOps Dojo occupies about eighteen thousand square feet of open office space, where DevOps coaches help teams from across the Target technology organization elevate the state of their practice. The most intensive format is what they call \"30-Day Challenges,\" where internal development teams come in for a month and work together with dedicated Dojo coaches and engineers. The team brings their work with them, with the goal of solving an internal problem they have been struggling with and to create a breakthrough in thirty days.\n",
      "\n",
      "Throughout the thirty days, they work intensively with the Dojo coaches on the problem—planning, working, and doing demos in two-day sprints. When the 30-Day Challenge is complete, the internal teams return to their lines of business, not only having solved a significant problem, but bringing their new learnings back to their teams.\n",
      "\n",
      "Clanton describes, \"We currently have capacity to have eight teams doing 30-Day Challenges concurrently, so we are focused on the most strategic projects of the organization. So far, we've had some of our most critical capabilities come through the Dojo, including teams from Point Of Sale (POS), Inventory, Pricing, and Promotion.\"\n",
      "\n",
      "By having full-time assigned Dojo staff and being focused on only one objective, teams going through a 30-Day Challenge make incredible improvements.\n",
      "\n",
      "Ravi Pandey, a Target development manager who went through this program, explains, \"In the old days, we would have to wait six weeks to get a test environment. Now, we get it in minutes, and we're working side by side with Ops engineers who are helping us increase our productivity and building tooling for us to help us achieve our goals.\" Clanton expands on this idea, \"It is not uncommon for teams to achieve in days what would usually take them three to six months. So far, two hundred learners have come through the Dojo, having completed fourteen challenges.\"\n",
      "\n",
      "The Dojo also supports less intensive engagement models, including Flash Builds, where teams come together for one- to three-day events, with the goal of shipping a minimal viable product (MVP) or a capability by the end of the event. They also host Open Labs every two weeks, where anyone can visit the Dojo to talk to the Dojo coaches, attend demos, or receive training.\n",
      "\n",
      "In this chapter, we will describe this and other ways of reserving time for organizational learning and improvement, further institutionalizing the practice of dedicating time for improving daily work.\n",
      "\n",
      "## INSTITUTIONALIZE RITUALS TO PAY DOWN TECHNICAL DEBT\n",
      "\n",
      "In this section, we schedule rituals that help enforce the practice of reserving Dev and Ops time for improvement work, such as non-functional requirements, automation, etc. One of the easiest ways to do this is to schedule and conduct day- or week-long improvement blitzes, where everyone on a team (or in the entire organization) self-organizes to fix problems they care about—no feature work is allowed. It could be a problematic area of the code, environment, architecture, tooling, and so forth. These teams span the entire value stream, often combining Development, Operations, and Infosec engineers. Teams that typically don't work together combine their skills and effort to improve a chosen area and then demonstrate their improvement to the rest of the company.\n",
      "\n",
      "In addition to the Lean-oriented terms kaizen blitz and improvement blitz, the technique of dedicated rituals for improvement work has also been called _spring_ or _fall cleanings_ and _ticket queue inversion weeks_. Other terms have also been used, such as _hack days_ , _hackathons_ , and _20% innovation time_. Unfortunately, these specific rituals sometimes focus on product innovation and prototyping new market ideas, rather than on improvement work, and worse, they are often restricted to developers—which is considerably different than the goals of an improvement blitz.†\n",
      "\n",
      "Our goal during these blitzes is not to simply experiment and innovate for the sake of testing out new technologies, but to improve our daily work, such as solving our daily workarounds. While experiments can also lead to improvements, improvement blitzes are very focused on solving specific problems we encounter in our daily work.\n",
      "\n",
      "We may schedule week-long improvement blitzes that prioritize Dev and Ops working together toward improvement goals. These improvement blitzes are simple to administer: One week is selected where everyone in the technology organization works on an improvement activity at the same time. At the end of the period, each team makes a presentation to their peers that discusses the problem they were tackling and what they built. This practice reinforces a culture in which engineers work across the entire value stream to solve problems. Furthermore, it reinforces fixing problems as part of our daily work and demonstrates that we value paying down technical debt.\n",
      "\n",
      "What makes improvement blitzes so powerful is that we are empowering those closest to the work to continually identify and solve their own problems. Consider for a moment that our complex system is like a spider web, with intertwining strands that are constantly weakening and breaking. If the right combination of strands breaks, the entire web collapses. There is no amount of command-and-control management that can direct workers to fix each strand one by one. Instead, we must create the organizational culture and norms that lead to everyone continually finding and fixing broken strands as part of our daily work. As Dr. Spear observes, \"No wonder then that spiders repair rips and tears in the web as they occur, not waiting for the failures to accumulate.\"\n",
      "\n",
      "A great example of the success of the improvement blitz concept is described by Mark Zuckerberg, Facebook CEO. In an interview with Jessica Stillman of Inc.com, he says, \"Every few months we have a hackathon, where everyone builds prototypes for new ideas they have. At the end, the whole team gets together and looks at everything that has been built. Many of our most successful products came out of hackathons, including Timeline, chat, video, our mobile development framework and some of our most important infrastructure like the HipHop compiler.\"\n",
      "\n",
      "Of particular interest is the HipHop PHP compiler. In 2008, Facebook was facing significant capacity problems, with over one-hundred million active users and rapidly growing, creating tremendous problems for the entire engineering team. During a hack day, Haiping Zhao, Senior Server Engineer at Facebook, started experimenting with converting PHP code to compilable C++ code, with the hope of significantly increasing the capacity of their existing infrastructure. Over the next two years, a small team was assembled to build what became known as the HipHop compiler, converting all Facebook production services from interpreted PHP to compiled C++ binaries. HipHop enabled Facebook's platform to handle six times higher production loads than the native PHP.\n",
      "\n",
      "In an interview with Cade Metz of _Wired_ , Drew Paroski, one of the engineers who worked on the project, noted, \"There was a moment where, if HipHop hadn't been there, we would have been in hot water. We would probably have needed more machines to serve the site than we could have gotten in time. It was a Hail Mary pass that worked out.\"\n",
      "\n",
      "Later, Paroski and fellow engineers Keith Adams and Jason Evans decided that they could beat the performance of the HipHop compiler effort and reduce some of its limitations that reduced developer productivity. The resulting project was the HipHop virtual machine project (\"HHVM\"), taking a just-in-time compilation approach. By 2012, HHVM had completely replaced the HipHop compiler in production, with nearly twenty engineers contributing to the project.\n",
      "\n",
      "By performing regularly scheduled improvement blitzes and hack weeks, we enable everyone in the value stream to take pride and ownership in the innovations they create, and we continually integrate improvements into our system, further enabling safety, reliability, and learning.\n",
      "\n",
      "## ENABLE EVERYONE TO TEACH AND LEARN\n",
      "\n",
      "A dynamic culture of learning creates conditions so that everyone can not only learn, but also teach, whether through traditional didactic methods (e.g., people taking classes, attending training) or more experiential or open methods (e.g., conferences, workshops, mentoring). One way that we can foster this teaching and learning is to dedicate organizational time to it.\n",
      "\n",
      "Steve Farley, VP of Information Technology at Nationwide Insurance, said, \"We have five thousand technology professionals, who we call 'associates.' Since 2011, we have been committed to create a culture of learning—part of that is something we call Teaching Thursday, where each week we create time for our associates to learn. For two hours, each associate is expected to teach or learn. The topics are whatever our associates want to learn about—some of them are on technology, on new software development or process improvement techniques, or even on how to better manage their career. The most valuable thing any associate can do is mentor or learn from other associates.\"\n",
      "\n",
      "As has been made evident throughout this book, certain skills are becoming increasingly needed by all engineers, not just by developers. For instance, it is becoming more important for all Operations and Test engineers to be familiar with Development techniques, rituals, and skills, such as version control, automated testing, deployment pipelines, configuration management, and creating automation. Familiarity with Development techniques helps Operations engineers remain relevant as more technology value streams adopt DevOps principles and patterns.\n",
      "\n",
      "Although the prospect of learning something new may be intimidating or cause a sense of embarrassment or shame, it shouldn't. After all, we are all lifelong learners, and one of the best ways to learn is from our peers. Karthik Gaekwad, who was part of the National Instruments DevOps transformation, said, \"For Operations people who are trying to learn automation, it shouldn't be scary—just ask a friendly developer, because they would love to help.\"\n",
      "\n",
      "We can help further help teach skills through our daily work by jointly performing code reviews that include both parties so that we learn by doing, as well as by having Development and Operations work together to solve small problems. For instance, we might have Development show Operations how to authenticate an application, and login and run automated tests against various parts of the application to ensure that critical components are working correctly (e.g., key application functionality, database transactions, message queues). We would then integrate this new automated test into our deployment pipeline and run it periodically, sending the results to our monitoring and alerting systems so that we get earlier detection when critical components fail.\n",
      "\n",
      "As Glenn O'Donnell from Forrester Research quipped in his 2014 DevOps Enterprise Summit presentation, \"For all technology professionals who love innovating, love change, there is a wonderful and vibrant future ahead of us.\"\n",
      "\n",
      "## SHARE YOUR EXPERIENCES FROM DEVOPS CONFERENCES\n",
      "\n",
      "In many cost-focused organizations, engineers are often discouraged from attending conferences and learning from their peers. To help build a learning organization, we should encourage our engineers (both from Development and Operations) to attend conferences, give talks at them, and, when necessary, create and organize internal or external conferences themselves.\n",
      "\n",
      "DevOpsDays remains one of the most vibrant self-organized conference series today. Many DevOps practices have been shared and promulgated at these events. It has remained free or nearly free, supported by a vibrant community of practitioner communities and vendors.\n",
      "\n",
      "The DevOps Enterprise Summit was created in 2014 for technology leaders to share their experiences adopting DevOps principles and practices in large, complex organizations. The program is organized primarily around experience reports from technology leaders on the DevOps journey, as well as subject matter experts on topics selected by the community.\n",
      "\n",
      "Case Study   \n",
      "Internal Technology Conferences at Nationwide Insurance, Capital One, and Target (2014)\n",
      "\n",
      "Along with attending external conferences, many companies, including those described in this section, have internal conferences for their technology employees.\n",
      "\n",
      "Nationwide Insurance is a leading provider of insurance and financial services, and operates in heavily regulated industries. Their many offerings include auto and homeowners insurance, and they are the top provider of public-sector retirement plans and pet insurance. As of 2014, $195 billion in assets, with $24 billion in revenue. Since 2005, Nationwide has been adopting Agile and Lean principles to elevate the state of practice for their five thousand technology professionals, enabling grassroots innovation.\n",
      "\n",
      "Steve Farley, VP of Information Technology, remembers, \"Exciting technology conferences were starting to appear around that time, such as the Agile national conference. In 2011, the technology leadership at Nationwide agreed that we should create a technology conference, called TechCon. By holding this event, we wanted to create a better way to teach ourselves, as well as ensure that everything had a Nationwide context, as opposed to sending everyone to an external conference.\"\n",
      "\n",
      "Capital One, one of the largest banks in the US with over $298 billion in assets and $24 billion in revenue in 2015, held their first internal software engineering conference in 2015 as part of their goal to create a world-class technology organization. The mission was to promote a culture of sharing and collaboration, and to build relationships between the technology professionals and enable learning. The conference had thirteen learning tracks and fifty-two sessions, and over 1,200 internal employees attended.\n",
      "\n",
      "Dr. Tapabrata Pal, a technical fellow at Capital One and one of the organizers of the event, describes, \"We even had an expo hall, where we had twenty-eight booths, where internal Capital One teams were showing off all the amazing capabilities they were working on. We even decided very deliberately that there would be no vendors there, because we wanted to keep the focus on Capital One goals.\"\n",
      "\n",
      "Target is the sixth-largest retailer in the US, with $72 billion in revenue in 2014 and 1,799 retail stores and 347,000 employees worldwide. Heather Mickman, a director of Development, and Ross Clanton have held six internal DevOpsDays events since 2014 and have over 975 followers inside their internal technology community, modeled after the DevOpsDays held at ING in Amsterdam in 2013.‡\n",
      "\n",
      "After Mickman and Clanton attended the DevOps Enterprise Summit in 2014, they held their own internal conference, inviting many of the speakers from outside firms so that they could re-create their experience for their senior leadership. Clanton describes, \"2015 was the year when we got executive attention and when we built up momentum. After that event, tons of people came up to us, asking how they could get involved and how they could help.\"\n",
      "\n",
      "## CREATE INTERNAL CONSULTING AND COACHES TO SPREAD PRACTICES\n",
      "\n",
      "Creating an internal coaching and consulting organization is a method commonly used to spread expertise across an organization. This can come in many different forms. At Capital One, designated subject matter experts hold office hours where anyone can consult with them, ask questions, etc.\n",
      "\n",
      "Earlier in the book, we began the story of how the Testing Grouplet built a world-class automated testing culture at Google starting in 2005. Their story continues here, as they try to improve the state of automated testing across all of Google by using dedicated improvement blitzes, internal coaches, and even an internal certification program.\n",
      "\n",
      "Bland said, at that time, there was a _20%_ innovation time policy at Google, enabling developers to spend roughly one day per week on a Google-related project outside of their primary area of responsibility. Some engineers chose to form _grouplets_ , ad hoc teams of like-minded engineers who wanted to pool their 20% time, allowing them to do focused improvement blitzes.\n",
      "\n",
      "A testing grouplet was formed by Bharat Mediratta and Nick Lesiecki, with the mission of driving the adoption of automated testing across Google. Even though they had no budget or formal authority, as Mike Bland described, \"There were no explicit constraints put upon us, either. And we took advantage of that.\"\n",
      "\n",
      "They used several mechanisms to drive adoption, but one of the most famous was _Testing on the Toilet_ (or TotT), their weekly testing periodical. Each week, they published a newsletter in nearly every bathroom in nearly every Google office worldwide. Bland said, \"The goal was to raise the degree of testing knowledge and sophistication throughout the company. It's doubtful an online-only publication would've involved people to the same degree.\"\n",
      "\n",
      "Bland continues, \"One of the most significant TotT episodes was the one titled, 'Test Certified: Lousy Name, Great Results,' because it outlined two initiatives that had significant success in advancing the use of automated testing.\"\n",
      "\n",
      "Test Certified (TC) provided a road map to improve the state of automated testing. As Bland describes, \"It was intended to hack the measurement-focused priorities of Google culture...and to overcome the first, scary obstacle of not knowing where or how to start. Level 1 was to quickly establish a baseline metric, Level 2 was setting a policy and reaching an automated test coverage goal, and Level 3 was striving towards a long-term coverage goal.\"\n",
      "\n",
      "The second capability was providing Test Certified mentors to any team who wanted advice or help, and Test Mercenaries (i.e., a full-time team of internal coaches and consultants) to work hands-on with teams to improve their testing practices and code quality. The Mercenaries did so by applying the Testing Grouplet's knowledge, tools, and techniques to a team's own code, using TC as both a guide and a goal. Bland was eventually a leader of the Testing Grouplet from 2006 to 2007, and a member of the Test Mercenaries from 2007 to 2009.\n",
      "\n",
      "Bland continues, \"It was our goal to get every team to TC Level 3, whether they were enrolled in our program our not. We also collaborated closely with the internal testing tools teams, providing feedback as we tackled testing challenges with the product teams. We were boots on the ground, applying the tools we built, and eventually, we were able to remove 'I don't have time to test' as a legitimate excuse.\"\n",
      "\n",
      "He continues, \"The TC levels exploited the Google metrics-driven culture—the three levels of testing were something that people could discuss and brag about at performance review time. The Testing Grouplet eventually got funding for the Test Mercenaries, a staffed team of full-time internal consultants. This was an important step, because now management was fully onboard, not with edicts, but by actual funding.\"\n",
      "\n",
      "Another important construct was leveraging company-wide \"Fixit\" improvement blitzes. Bland describes Fixits as \"when ordinary engineers with an idea and a sense of mission recruit all of Google engineering for one-day, intensive sprints of code reform and tool adoption.\" He organized four company-wide Fixits, two pure Testing Fixits and two that were more tools-related, the last involving more than one hundred volunteers in over twenty offices in thirteen countries. He also led the Fixit Grouplet from 2007 to 2008.\n",
      "\n",
      "These Fixits, as Bland describes means that we should provide focused missions at critical points in time to generate excitement and energy, which helps advance the state-of-the-art. This will help the long-term culture change mission reach a new plateau with every big, visible effort.\n",
      "\n",
      "The results of the testing culture are self-evident in the amazing results Google has achieved, presented throughout the book.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "This chapter described how we can institute rituals that help reinforce the culture that we are all lifelong learners and that we value the improvement of daily work over daily work itself. We do this by reserving time to pay down technical debt, create forums that allow everyone to learn from and teach each other, both inside our organization and outside it. And we make experts available to help internal teams, either by coaching or consulting or even just holding office hours to answer questions.\n",
      "\n",
      "By having everyone help each other learn in our daily work, we out-learn the competition, helping us win in the marketplace. But also we help each other achieve our full potential as human beings.\n",
      "\n",
      "## CONCLUSION TO PART V\n",
      "\n",
      "Throughout Part V, we explored the practices that help create a culture of learning and experimentation in your organization. Learning from incidents, creating shared repositories, and sharing learnings is essential when we work in complex systems, helping to make our work culture more just and our systems safer and more resilient.\n",
      "\n",
      "In Part VI, we'll explore how to extend flow, feedback, and learning and experimentation by using them to simultaneously help us achieve our Information Security goals.\n",
      "\n",
      "* * *\n",
      "\n",
      "† From here on, the terms \"hack week\" and \"hackathon\" are used interchangeably with \"improvement blitz,\" and not in the context of \"you can work on whatever you want.\"\n",
      "\n",
      "‡ Incidentally, the first Target internal DevOpsDays event was modeled after the first ING DevOpsDays that was organized by Ingrid Algra, Jan-Joost Bouwman, Evelijn Van Leeuwen, and Kris Buytaert in 2013, after some of the ING team attended the 2013 Paris DevOpsDays.\n",
      "\n",
      "# Part VI\n",
      "\n",
      "## Introduction\n",
      "\n",
      "In the previous chapters, we discussed enabling the fast flow of work from check-in to release, as well as creating the reciprocal fast flow of feedback. We explored the cultural rituals that reinforce the acceleration of organizational learning and amplification of weak failure signals that help us create an ever safer system of work.\n",
      "\n",
      "In Part VI, we further extend these activities so that we not only achieve Development and Operations goals, but also simultaneously achieve Information Security goals, helping us create a high degree of assurance around the confidentiality, integrity, and availability of our services and data.\n",
      "\n",
      "Instead of inspecting security into our product at the end of the process, we will create and integrate security controls into the daily work of Development and Operations, so that security is part of everyone's job, every day. Ideally, this work will be automated and put into our deployment pipeline. Furthermore, we will augment our manual practices, acceptances, and approval processes with automated controls, relying less on controls such as separation of duties and change approval processes.\n",
      "\n",
      "By automating these activities, we can generate evidence on demand to demonstrate that our controls are operating effectively, whether to auditors, assessors, or anyone else working in our value stream.\n",
      "\n",
      "In the end, we will not only improve security, but also create processes that are easier to audit and that attest to the effectiveness of controls, in support of compliance with regulatory and contractual obligations. We do this by:\n",
      "\n",
      "  * Making security a part of everyone's job\n",
      "  * Integrating preventative controls into our shared source code repository\n",
      "  * Integrating security with our deployment pipeline\n",
      "  * Integrating security with our telemetry to better enable detection and recovery\n",
      "  * Protecting our deployment pipeline\n",
      "  * Integrating our deployment activities with our change approval processes\n",
      "  * Reducing reliance on separation of duty\n",
      "\n",
      "When we integrate security work into everyone's daily work, making it everyone's responsibility, we help the organization have better security. Better security means that we are defensible and sensible with our data. It means that we are reliable and have business continuity by being more available and more capable of easily recovering from issues. We are also able to overcome security problems before they cause catastrophic results, and we can increase the predictability of our systems. And, perhaps most importantly, we can secure our systems and data better than ever.\n",
      "\n",
      "# 22Information Security as Everyone's Job, Every Day\n",
      "\n",
      "One of the top objections to implementing DevOps principles and patterns has been, \"Information security and compliance won't let us.\" And yet, DevOps may be one of the best ways to better integrate information security into the daily work of everyone in the technology value stream.\n",
      "\n",
      "When Infosec is organized as a silo outside of Development and Operations, many problems arise. James Wickett, one of the creators of the Gauntlt security tool and organizer of DevOpsDays Austin and the Lonestar Application Security conference, observed:\n",
      "\n",
      "One interpretation of DevOps is that it came from the need to enable developers productivity, because as the number of developers grew, there weren't enough Ops people to handle all the resulting deployment work. This shortage is even worse in Infosec—the ratio of engineers in Development, Operations, and Infosec in a typical technology organization is 100:10:1. When Infosec is that outnumbered, without automation and integrating information security into the daily work of Dev and Ops, Infosec can only do compliance checking, which is the opposite of security engineering—and besides, it also makes everyone hate us.\n",
      "\n",
      "James Wickett and Josh Corman, former CTO of Sonatype and respected information security researcher, have written about incorporating information security objectives into DevOps, a set of practices and principles termed _Rugged DevOps_. Similar ideas were created by Dr. Tapabrata Pal, Director and Platform Engineering Technical Fellow at Capital One, and the Capital One team, who describe their processes as _DevOpsSec_ , where Infosec is integrated into all stages of the SDLC. Rugged DevOps traces some of its history to _Visible Ops Security_ , written by Gene Kim, Paul Love, and George Spafford.\n",
      "\n",
      "Throughout _The DevOps Handbook_ , we have explored how to fully integrate the QA and Operations objectives throughout our entire technology value stream. In this chapter, we describe how to similarly integrate Infosec objectives into our daily work, where we can increase developer and operational productivity, increase safety, and increase our security.\n",
      "\n",
      "## INTEGRATE SECURITY INTO DEVELOPMENT ITERATION DEMONSTRATIONS\n",
      "\n",
      "One of our goals is to have feature teams engaged with Infosec as early as possible, as opposed to primarily engaging at the end of the project. One way we can do this is by inviting Infosec to the product demonstrations at the end of each development interval so that they can better understand the team goals in the context of organizational goals, observe their implementations as they are being built, and provide guidance and feedback at the earliest stages of the project, when there is the most amount of time and freedom to make corrections.\n",
      "\n",
      "Justin Arbuckle, former chief architect at GE Capital, observes, \"When it came to information security and compliance, we found that blockages at the end of the project were much more expensive than at the beginning—and Infosec blockages were among the worst. 'Compliance by demonstration' became one of the rituals we used to shift all this complexity earlier in the process.\"\n",
      "\n",
      "He continues, \"By having Infosec involved throughout the creation of any new capability, we were able to reduce our use of static checklists dramatically and rely more on using their expertise throughout the entire software development process.\"\n",
      "\n",
      "This helped the organization achieve its goals. Snehal Antani, former CIO of Enterprise Architecture at GE Capital Americas, described their top three key business measurements were \"development velocity (i.e., speed of delivering features to market), failed customer interactions (i.e., outages, errors), and compliance response time (i.e., lead time from audit request to delivery of all quantitative and qualitative information required to fulfill the request).\"\n",
      "\n",
      "When Infosec is an assigned part of the team, even if they are only being kept informed and observing the process, they gain the business context they need to make better risk-based decisions. Furthermore, Infosec is able to help feature teams learn what is required to meet security and compliance objectives.\n",
      "\n",
      "## INTEGRATE SECURITY INTO DEFECT TRACKING AND POST-MORTEMS\n",
      "\n",
      "When possible, we want to track all open security issues in the same work tracking system that Development and Operations are using, ensuring the work is visible and can be prioritized against all other work. This is very different from how Infosec has traditionally worked, where all security vulnerabilities are stored in a GRC (governance, risk, and compliance) tool that only Infosec has access to. Instead, we will put any needed work in the systems that Dev and Ops use.\n",
      "\n",
      "In a presentation at the 2012 Austin DevOpsDays, Nick Galbreath, who headed up Information Security at Etsy for many years, describes how they treated security issues, \"We put all security issues into JIRA, which all engineers use in their daily work, and they were either 'P1' or 'P2,' meaning that they had to be fixed immediately or by the end of the week, even if the issue is only an internally-facing application.\"\n",
      "\n",
      "Furthermore, he states, \"Any time we had a security issue, we would conduct a post-mortem, because it would result in better educating our engineers on how to prevent it from happening again in the future, as well as a fantastic mechanism for transferring security knowledge to our engineering teams.\"\n",
      "\n",
      "## INTEGRATE PREVENTIVE SECURITY CONTROLS INTO SHARED SOURCE CODE REPOSITORIES AND SHARED SERVICES\n",
      "\n",
      "In chapter 20, we created a shared source code repository that makes it easy for anyone to discover and reuse the collective knowledge of our organization—not only for our code, but also for our toolchains, deployment pipeline, standards, etc. By doing this, anyone can benefit from the cumulative experience of everyone in the organization.\n",
      "\n",
      "Now we will add to our shared source code repository any mechanisms or tools that help enable us to ensure our applications and environments are secure. We will add libraries that are pre-blessed by security to fulfill specific Infosec objectives, such as authentication and encryption libraries and services. Because everyone in the DevOps value stream uses version control for anything they build or support, putting our information security artifacts there makes it much easier to influence the daily work of Dev and Ops, because anything we create is available, searchable, and reusable. Version control also serves as a omni-directional communication mechanism to keep all parties aware of changes being made.\n",
      "\n",
      "If we have a centralized shared services organization, we may also collaborate with them to create and operate shared security-relevant platforms, such as authentication, authorization, logging, and other security and auditing services that Dev and Ops require. When engineers use one of these predefined libraries or services, they won't need to schedule a separate security design review for that module; they'll be using the guidance we've created concerning configuration hardening, database security settings, key lengths, and so forth.\n",
      "\n",
      "To further increase the likelihood that the services and libraries we provide will be used correctly, we can provide security training to Dev and Ops, as well as review what they've created to help ensure that security objectives are being implemented correctly, especially for teams using these tools for the first time.\n",
      "\n",
      "Ultimately, our goal is to provide the security libraries or services that every modern application or environment requires, such as enabling user authentication, authorization, password management, data encryption, and so forth. Furthermore, we can provide Dev and Ops with effective security-specific configuration settings for the components they use in their application stacks, such as for logging, authentication, and encryption. We may include items such as:\n",
      "\n",
      "  * Code libraries and their recommended configurations (e.g., 2FA [two-factor authentication library], bcrypt password hashing, logging)\n",
      "  * Secret management (e.g., connection settings, encryption keys) using tools such as Vault, sneaker, Keywhiz, credstash, Trousseau, Red October, etc.\n",
      "  * OS packages and builds (e.g., NTP for time syncing, secure versions of OpenSSL with correct configurations, OSSEC or Tripwire for file integrity monitoring, syslog configuration to ensure logging of critical security into our centralized ELK stack)\n",
      "\n",
      "By putting all these into our shared source code repository, we make it easy for any engineer to correctly create and use logging and encryption standards in their applications and environments, with no further work from us.\n",
      "\n",
      "We should also collaborate with Ops teams to create a base cookbook or build image of our OS, databases, and other infrastructure (e.g., NGINX, Apache, Tomcat), showing they are in a known, secure, and risk-reduced state. Our shared repository not only becomes the place where we can get the latest versions, but also becomes a place where we can collaborate with other engineers and monitor and alert on changes made to security-sensitive modules.\n",
      "\n",
      "## INTEGRATE SECURITY INTO OUR DEPLOYMENT PIPELINE\n",
      "\n",
      "In previous eras, in order to harden and secure our application, we would start our security review after development was completed. Often, the output of this review would be hundreds of pages of vulnerabilities in a PDF, which we'd give to Development and Operations, which would be completely un-addressed due to project due date pressure or problems being found too late in the software life cycle to be easily corrected.\n",
      "\n",
      "In this step, we will automate as many of our information security tests as possible, so that they run alongside all our other automated tests in our deployment pipeline, being performed (ideally) upon every code commit by Dev or Ops, and even in the earliest stages of a software project.\n",
      "\n",
      "Our goal is to provide both Dev and Ops with fast feedback on their work so that they are notified whenever they commit changes that are potentially insecure. By doing this, we enable them to quickly detect and correct security problems as part of their daily work, which enables learning and prevents future errors.\n",
      "\n",
      "Ideally, these automated security tests will be run in our deployment pipeline alongside the other static code analysis tools.\n",
      "\n",
      "Tools such as Gauntlt have been designed to integrate into the deployment pipelines, which run automated security tests on our applications, our application dependencies, our environment, etc. Remarkably, Gauntlt even puts all its security tests in Gherkin syntax test scripts, which is widely used by developers for unit and functional testing. Doing this puts security testing in a framework they are likely already familiar with. This also allows security tests to easily run in a deployment pipeline on every committed change, such as static code analysis, checking for vulnerable dependencies, or dynamic testing.\n",
      "\n",
      "  **Figure 43:** Jenkins running automated security testing (Source: James Wicket and Gareth Rushgrove, \"Battle-tested code without the battle,\" Velocity 2014 conference presentation, posted to Speakerdeck.com, June 24, 2014, https://speakerdeck.com/garethr/battle-tested-code-without-the-battle.)\n",
      "\n",
      "By doing this, we provide everyone in the value stream with the fastest possible feedback about the security of what they are creating, enabling Dev and Ops engineers to find and fix issues quickly.\n",
      "\n",
      "## ENSURE SECURITY OF THE APPLICATION\n",
      "\n",
      "Often, Development testing focuses on the correctness of functionality, looking at positive logic flows. This type of testing is often referred to as the _happy path_ , which validates user journeys (and sometimes alternative paths) where everything goes as expected, with no exceptions or error conditions.\n",
      "\n",
      "On the other hand, effective QA, Infosec, and Fraud practitioners will often focus on the _sad paths_ , which happen when things go wrong, especially in relation to security-related error conditions. (These types of security-specific conditions are often jokingly referred to as the _bad paths_.)\n",
      "\n",
      "For instance, suppose we have an e-commerce site with a customer input form that accepts credit card numbers as part of generating a customer order. We want to define all the sad and bath paths required to ensure that invalid credit cards are properly rejected to prevent fraud and security exploits, such as SQL injections, buffer overruns, and other undesirable outcomes.\n",
      "\n",
      "Instead of performing these tests manually, we would ideally generate them as part of our automated unit or functional tests so that they can be run continuously in our deployment pipeline. As part of our testing, we will want to include the following:\n",
      "\n",
      "  * **Static analysis:** This is testing that we perform in a non-runtime environment, ideally in the deployment pipeline. Typically, a static analysis tool will inspect program code for all possible run-time behaviors and seek out coding flaws, back doors, and potentially malicious code (this is sometimes known as \"testing from the inside-out\"). Examples of tools include Brakeman, Code Climate, and searching for banned code functions (e.g., \"exec()\").\n",
      "  * **Dynamic analysis:** As opposed to static testing, dynamic analysis consists of tests executed while a program is in operation. Dynamic tests monitor items such as system memory, functional behavior, response time, and overall performance of the system. This method (sometimes known as \"testing from the outside-in\") is similar to the manner in which a malicious third party might interact with an application. Examples include Arachni and OWASP ZAP (Zed Attack Proxy).† Some types of penetration testing can also be performed in an automated fashion and should be included as part of dynamic analysis using tools such as Nmap and Metasploit. Ideally, we should perform automated dynamic testing during the automated functional testing phase of our deployment pipeline, or even against our services while they are in production. To ensure correct security handling, tools like OWASP ZAP can be configured to attack our services through a web browser proxy and inspect the network traffic within our test harness.\n",
      "  * **Dependency scanning:** Another type of static testing we would normally perform at build time inside of our deployment pipeline involves inventorying all our dependencies for binaries and executables, and ensuring that these dependencies, which we often don't have control over, are free of vulnerabilities or malicious binaries. Examples include Gemnasium and bundler audit for Ruby, Maven for Java, and the OWASP Dependency-Check.\n",
      "  * **Source code integrity and code signing:** All developers should have their own PGP key, perhaps created and managed in a system such as keybase.io. All commits to version control should be signed­—that is straightforward to configure using the open source tools gpg and git. Furthermore, all packages created by the CI process should be signed, and their hash recorded in the centralized logging service for audit purposes.\n",
      "\n",
      "Furthermore, we should define design patterns to help developers write code to prevent abuse, such as putting in rate limits for our services and graying out submit buttons after they have being pressed. OWASP publishes a great deal of useful guidance such as the Cheat Sheet series, which includes:\n",
      "\n",
      "  * How to store passwords\n",
      "  * How to handle forgotten passwords\n",
      "  * How to handle logging\n",
      "  * How to prevent cross-site scripting (XSS) vulnerabilities\n",
      "\n",
      "Case Study   \n",
      "Static Security Testing at Twitter (2009)\n",
      "\n",
      "The \"10 Deploys per Day: Dev and Ops Cooperation at Flickr\" presentation by John Allspaw and Paul Hammond is famous for catalyzing the Dev and Ops community in 2009. The equivalent for the information security community is likely the presentation that Justin Collins, Alex Smolen, and Neil Matatall gave on their information security transformation work at Twitter at the AppSecUSA conference in 2012.\n",
      "\n",
      "Twitter had many challenges due to hyper-growth. For years, the famous Fail Whale error page would be displayed when Twitter did not have sufficient capacity to keep up with user demand, showing a graphic of a whale being lifted by eight birds. The scale of user growth was breathtaking—between January and March 2009, the number of active Twitter users went from 2.5 million to 10 million.\n",
      "\n",
      "Twitter also had security problems during this period. In early 2009, two serious security breaches occurred. First, in January the @BarackObama Twitter account was hacked. Then in April, the Twitter administrative accounts were compromised through a brute-force dictionary attack. These events led the Federal Trade Commission to judge that Twitter was misleading its users into believing that their accounts were secure and issued an FTC consent order.\n",
      "\n",
      "The consent order required that Twitter comply within sixty days by instituting a set of processes that were to be enforced for the following twenty years and would do the following:\n",
      "\n",
      "  * Designate an employee or employees to be responsible for Twitter's information security plan\n",
      "  * Identify reasonably foreseeable risks, both internal and external, that could lead to an intrusion incident and create and implement a plan to address these risks‡\n",
      "  * Maintain the privacy of user information, not just from outside sources but also internally, with an outline of possible sources of verification and testing of the security and correctness of these implementations\n",
      "\n",
      "The group of engineers assigned to solve this problem had to integrate security into the daily work of Dev and Ops and close the security holes that allowed the breaches to happen in the first place.\n",
      "\n",
      "In their previously mentioned presentation, Collins, Smolen, and Matatall identified several problems they needed to address:\n",
      "\n",
      "  * **Prevent security mistakes from being repeated:** They found that they were fixing the same defects and vulnerabilities over and over again. They needed to modify the system of work and automation tools to prevent the issues from happening again.\n",
      "  * **Integrate security objectives into existing developer tools:** They identified early on that the major source of vulnerabilities were code issues. They couldn't run a tool that generated a huge PDF report and then email it to someone in Development or Operations. Instead, they needed to provide the developer who had created the vulnerability with the exact information needed to fix it.\n",
      "  * **Preserve trust of Development:** They needed to earn and maintain the trust of Development. That meant they needed to know when they sent Development false positives, so they could fix the error that prompted the false positive and avoid wasting Development's time.\n",
      "  * **Maintain fast flow through Infosec through automation:** Even when code vulnerability scanning was automated, Infosec still had to do lots of manual work and waiting. They had to wait for the scan to complete, get back the big stack of reports, interpret the reports, and then find the person responsible for fixing it. And when the code changed, it had to be done all over again. By automating the manual work, they did fewer dumb \"button-pushing\" tasks, enabling them to use more creativity and judgment.\n",
      "  * **Make everything security related self-service, if possible:** They trusted that most people wanted to do the right thing, so it was necessary to provide them with all the context and information they needed to fix any issues.\n",
      "  * **Take a holistic approach to achieving Infosec objectives:** Their goal was to do analysis from all the angles: source code, the production environment, and even what their customers were seeing.\n",
      "\n",
      "The first big breakthrough for the Infosec team occured during a company-wide hack week when they integrated static code analysis into the Twitter build process. The team used Brakeman, which scans Ruby on Rails applications for vulnerabilities. The goal was to integrate security scanning into the earliest stages of the Development process, not just when the code was committed into the source code repo.\n",
      "\n",
      "  **Figure 44:** Number of Brakeman security vulnerabilities detected\n",
      "\n",
      "The results of integrating security testing into the development process were breathtaking. Over the years, by creating fast feedback for developers when they write insecure code and showing them how to fix the vulnerabilities, Brakeman has reduced the rate of vulnerabilities found by 60%, as shown in figure 44. (The spikes are usually associated with new releases of Brakeman.)\n",
      "\n",
      "This cases study illustrates just how necessary it is to integrate security into the daily work and tools of DevOps and how effectively it can work. Doing so mitigates security risk, reduces the probability of vulnerabilities in the system, and helps teach developers to write more secure code.\n",
      "\n",
      "## ENSURE SECURITY OF OUR SOFTWARE SUPPLY CHAIN\n",
      "\n",
      "Josh Corman observed that as developers \"we are no longer writing customized software—instead, we assemble what we need from open source parts, which has become the software supply chain that we are very much reliant upon.\" In other words, when we use components or libraries­—either commercial or open source—in our software, we not only inherit their functionality, but also any security vulnerabilities they contain.\n",
      "\n",
      "When selecting software, we detect when our software projects are relying on components or libraries that have known vulnerabilities, and help developers choose the components they use deliberately and with due care, selecting those components (e.g., open source projects) that have a demonstrated history of quickly fixing software vulnerabilities. We also look for multiple versions of the same library being used across our production landscape, particularly the presence of older versions of libraries which contain known vulnerabilities.\n",
      "\n",
      "Examining cardholder data breaches shows how important the security of open source components we choose can be. Since 2008, the annual Verizon PCI Data Breach Investigation Report (DBIR) has been the most authoritative voice on data breaches where cardholder data was lost or stolen. In the 2014 report, they studied over eighty-five thousand breaches to better understand where attacks were coming from, how cardholder data was stolen, and factors leading to the breach.\n",
      "\n",
      "The DBIR found that ten vulnerabilities (i.e., CVEs) accounted for almost 97% of the exploits used in studied cardholder data breaches in 2014. Of these ten vulnerabilities, eight of them were over ten years old.\n",
      "\n",
      "The _2015 Sonatype State of the Software Supply Chain Report_ further analyzed the vulnerability data from the Nexus Central Repository. In 2015, this repository provided the build artifacts for over 605,000 open source projects, servicing over seventeen billion download requests of artifacts and dependencies primarily for the Java platform, originating from 106,000 organizations.\n",
      "\n",
      "The report included these startling findings:\n",
      "\n",
      "  * The typical organization relied upon 7,601 build artifacts (i.e., software suppliers or components) and used 18,614 different versions (i.e., software parts).\n",
      "  * Of those components being used, 7.5% had known vulnerabilities, with over 66% of those vulnerabilities being over two years old without having been resolved.\n",
      "\n",
      "The last statistic confirms another information security study by Dr. Dan Geer and Josh Corman, which showed that of the open source projects with known vulnerabilities registered in the National Vulnerability Database, only 41% were ever fixed and required, on average, 390 days to publish a fix. For those vulnerabilities that were labeled at the highest severity (i.e., those scored as CVSS level 10), fixes required 224 days.§\n",
      "\n",
      "## ENSURE SECURITY OF THE ENVIRONMENT\n",
      "\n",
      "In this step, we should do whatever is required to help ensure that the environments are in a hardened, risk-reduced state. Although we may have created known, good configurations already, we must put in monitoring controls to ensure that all production instances match these known good states.\n",
      "\n",
      "We do this by generating automated tests to ensure that all appropriate settings have been correctly applied for configuration hardening, database security settings, key lengths, and so forth. Furthermore, we will use tests to scan our environments for known vulnerabilities.¶\n",
      "\n",
      "Another category of security verification is understanding actual environments (i.e., \"as they actually are\"). Examples of tools for this include Nmap to ensure that only expected ports are open and Metasploit to ensure that we've adequately hardened our environments against known vulnerabilities, such as scanning with SQL injection attacks. The output of these tools should be put into our artifact repository and compared with the previous version as part of our functional testing process. Doing this will help us detect any undesirable changes as soon as they occur.\n",
      "\n",
      "Case Study   \n",
      "18F Automating Compliance for the Federal Government with Compliance Masonry\n",
      "\n",
      "US Federal Government agencies were projected to spend nearly $80 billion on IT in 2016, supporting the mission of all the executive branch agencies. Regardless of agency, to take any system from \"dev complete\" to \"live in production\" requires obtaining an Authority to Operate (ATO) from a Designated Approving Authority (DAA). The laws and policies that govern complience in government are comprised of tens of documents that together number over four thousand pages, littered with acronyms such as FISMA, FedRAMP, and FITARA. Even for systems that only require low levels of confidentiality, integrity, and availability, over one hundred controls must be implemented, documented, and tested. It typically takes between eight and fourteen months for an ATO to be granted following \"dev complete.\"\n",
      "\n",
      "The 18F team in the federal government's General Services Administration has taken a multi-pronged approach to solving this problem. Mike Bland explains, \"18F was created within the General Services Administration to capitalize on the momentum generated by the Healthcare.gov recovery to reform how the government builds and buys software.\"\n",
      "\n",
      "One 18F effort is a platform as a service called Cloud.gov, created from open source components. Cloud.gov runs on AWS GovCloud at present. Not only does the platform handle many of the operational concerns delivery teams might otherwise have to take care of, such as logging, monitoring, alerting, and service lifecycle management, it also handles the bulk of compliance concerns. By running on this platform, a large majority of the controls that government systems must implement can be taken care of at the infrastructure and platform level. Then, only the remaining controls that are in scope at the application layer have to be documented and tested, significantly reducing the compliance burden and the time it takes to receive an ATO.\n",
      "\n",
      "AWS GovCloud has already been approved for use for federal government systems of all types, including those which require high levels of confidentiality, integrity, and availability. By the time you read this book, it is expected that Cloud.gov will be approved for all systems that require moderate levels of confidentiality, integrity, and availability.**\n",
      "\n",
      "Furthermore, the Cloud.gov team is building a framework to automate the creation of system security plans (SSPs), which are \"comprehensive descriptions of the system's architecture, implemented controls, and general security posture...[which are] often incredibly complex, running several hundred pages in length.\" They developed a prototype tool called compliance masonry so that SSP data is stored in machine-readable YAML and then turned into GitBooks and PDFs automatically.\n",
      "\n",
      "18F is dedicated to working in the open and publishes its work open source in the public domain. You can find compliance masonry and the components that make up Cloud.gov in 18F's GitHub repositories—you can even stand up your own instance of Cloud.gov. The work on open documentation for SSPs is being done in close partnership with the OpenControl community.\n",
      "\n",
      "## INTEGRATE INFORMATION SECURITY INTO PRODUCTION TELEMETRY\n",
      "\n",
      "Marcus Sachs, one of the Verizon Data Breach researchers, observed in 2010, \"Year after year, in the vast majority of cardholder data breaches, the organization detected the security breach months or quarters after the breach occurred. Worse, the way the breach was detected was not an internal monitoring control, but was far more likely someone outside of the organization, usually a business partner or the customer who notices fraudulent transactions. One of the primary reasons for this is that no one in the organization was regularly reviewing the log files.\"\n",
      "\n",
      "In other words, internal security controls are often ineffective in successfully detecting breaches in a timely manner, either because of blind spots in our monitoring or because no one in our organization is examining the relevant telemetry in their daily work.\n",
      "\n",
      "In chapter 14, we discussed creating a culture in Dev and Ops where everyone in the value stream is creating production telemetry and metrics, making them visible in prominent public places so that everyone can see how our services are performing in production. Furthermore, we explored the necessity of relentlessly seeking ever-weaker failure signals so that we can find and fix problems before they result in a catastrophic failure.\n",
      "\n",
      "Here, we deploy the monitoring, logging, and alerting required to fulfill our information security objectives throughout our applications and environments, as well as ensure that it is adequately centralized to facilitate easy and meaningful analysis and response.\n",
      "\n",
      "We do this by integrating our security telemetry into the same tools that Development, QA, and Operations are using, giving everyone in the value stream visibility into how their application and environments are performing in a hostile threat environment where attackers are constantly attempting to exploit vulnerabilities, gain unauthorized access, plant backdoors, commit fraud, perform denials-of-service, and so forth.\n",
      "\n",
      "By radiating how our services are being attacked in the production environment, we reinforce that everyone needs to be thinking about security risks and designing countermeasures in their daily work.\n",
      "\n",
      "## CREATING SECURITY TELEMETRY IN OUR APPLICATIONS\n",
      "\n",
      "In order to detect problematic user behavior that could be an indicator or enabler of fraud and unauthorized access, we must create the relevant telemetry in our applications.\n",
      "\n",
      "Examples may include:\n",
      "\n",
      "  * Successful and unsuccessful user logins\n",
      "  * User password resets\n",
      "  * User email address resets\n",
      "  * User credit card changes\n",
      "\n",
      "For instance, as an early indicator of brute-force login attempts to gain unauthorized access, we might display the ratio of unsuccessful login attempts to successful logins. And, of course, we should create alerting around important events to ensure we can detect and correct issues quickly.\n",
      "\n",
      "## CREATING SECURITY TELEMETRY IN OUR ENVIRONMENT\n",
      "\n",
      "In addition to instrumenting our application, we also need to create sufficient telemetry in our environments so that we can detect early indicators of unauthorized access, especially in the components that are running on infrastructure that we do not control (e.g., hosting environments, in the cloud).\n",
      "\n",
      "We need to monitor and potentially alert on items, including the following:\n",
      "\n",
      "  * OS changes (e.g., in production, in our build infrastructure)\n",
      "  * Security group changes\n",
      "  * Changes to configurations (e.g., OSSEC, Puppet, Chef, Tripwire)\n",
      "  * Cloud infrastructure changes (e.g., VPC, security groups, users and privileges)\n",
      "  * XSS attempts (i.e., \"cross-site scripting attacks\")\n",
      "  * SQLi attempts (i.e., \"SQL injection attacks\")\n",
      "  * Web server errors (e.g., 4XX and 5XX errors)\n",
      "\n",
      "We also want to confirm that we've correctly configured our logging so that all telemetry is being sent to the right place. When we detect attacks, in addition to logging that it happened, we may also choose to block access and store information about the source to aid us in choosing the best mitigation actions.\n",
      "\n",
      "Case Study   \n",
      "Instrumenting the Environment at Etsy (2010)\n",
      "\n",
      "In 2010, Nick Galbreath was director of engineering at Etsy and responsible for information security, fraud control, and privacy. Galbreath defined _fraud_ as when \"the system works incorrectly, allowing invalid or un-inspected input into the system, causing financial loss, data loss/theft, system downtime, vandalism, or an attack on another system.\"\n",
      "\n",
      "To achieve these goals, Galbreath did not create a separate fraud control or information security department; instead, he embedded those responsibilities throughout the DevOps value stream.\n",
      "\n",
      "Galbreath created security-related telemetry that were displayed alongside all the other more Dev and Ops oriented metrics, which every Etsy engineer routinely saw:\n",
      "\n",
      "  * **Abnormal production program terminations (e.g., segmentation faults, core dumps, etc.):** \"Of particular concern was why certain processes kept dumping core across our entire production environment, triggered from traffic coming from the one IP address, over and over again. Of equal concern were those HTTP '500 Internal Server Errors.' These are indicators that a vulnerability was being exploited to gain unauthorized access to our systems, and that a patch needs to be urgently applied.\"\n",
      "  * **Database syntax error:** \"We were always looking for database syntax errors inside our code—these either enabled SQL Injection attacks or were actual attacks in progress. For this reason, we had zero-tolerance for database syntax errors in our code, because it remains one of the leading attack vectors used to compromise systems.\"\n",
      "  * **Indications of SQL injection attacks:** \"This was a ridiculously simple test—we'd merely alert whenever 'UNION ALL' showed up in user-input fields, since it almost always indicates a SQL injection attack. We also added unit tests to make sure that this type of uncontrolled user input could never be allowed into our database queries.\"\n",
      "\n",
      "  **Figure 45:** Developers would see SQL injection attempts in Graphite at Etsy (Source: \"DevOpsSec: Appling DevOps Priciples to Security, DevOpsDays Austin 2012,\" SlideShare.net, posted by Nick Galbreath, April 12, 2012, <http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security>.)\n",
      "\n",
      "Figure 45 is an example of a graph that every developer would see, which shows the number of potential SQL injection attacks that were attempted in the production environment. As Galbreath observed, \"Nothing helps developers understand how hostile the operating environment is than seeing their code being attacked in real-time.\"\n",
      "\n",
      "Galbreath observed, \"One of the results of showing this graph was that developers realized that they were being attacked all the time! And that was awesome, because it changed how developers thought about the security of their code as they were writing the code.\"\n",
      "\n",
      "## PROTECT OUR DEPLOYMENT PIPELINE\n",
      "\n",
      "The infrastructure that supports our continuous integration and continuous deployment processes also presents a new surface area vulnerable to attack. For instance, if someone compromises the servers running deployment pipeline that has the credentials for our version control system, it could enable someone to steal source code. Worse, if the deployment pipeline has write access, an attacker could also inject malicious changes into our version control repository, and, therefore, inject malicious changes into our application and services.\n",
      "\n",
      "As Jonathan Claudius, former Senior Security Tester at TrustWave SpiderLabs, observed, \"Continuous build and test servers are awesome, and I use them myself. But I started thinking about ways to use CI/CD as a way to inject malicious code. Which led to the question of where would be a good place to hide malicious code? The answer was obvious: in the unit tests. No one actually looks at the unit tests, and they're run every time someone commits code to the repo.\"\n",
      "\n",
      "This demonstrates that in order to adequately protect the integrity of our applications and environments, we must also mitigate the attack vectors on our deployment pipeline. Risks include developers introducing code that enables unauthorized access (which we've mitigated through controls such as code testing, code reviews, and penetration testing) and unauthorized users gaining access to our code or environment (which we've mitigated through controls such as ensuring configurations match known, good states, and effective patching).\n",
      "\n",
      "However, in order to protect our continuous build, integration, or deployment pipeline, our mitigation strategies may include:\n",
      "\n",
      "  * Hardening continuous build and integration servers and ensuring we can reproduce them in an automated manner, just as we would for infrastructure that supports customer-facing production services, to prevent our continuous build and integration servers from being compromised \n",
      "  * Reviewing all changes introduced into version control, either through pair programming at commit time or by a code review process between commit and merge into trunk, to prevent continuous integration servers from running uncontrolled code (e.g., unit tests may contain malicious code that allows or enables unauthorized access) \n",
      "  * Instrumenting our repository to detect when test code contains suspicious API calls (e.g., unit tests accessing the filesystem or network) is checked in to the repository, perhaps quarantining it and triggering an immediate code review\n",
      "  * Ensuring every CI process runs on its own isolated container or VM\n",
      "  * Ensuring the version control credentials used by the CI system are read-only\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Throughout this chapter we have described ways to integrate information security objectives into all stages of our daily work. We do this by integrating security controls into the mechanisms we've already created, ensuring that all on-demand environments are also in a hardened, risk-reduced state—by integrating security testing into the deployment pipeline and ensuring the creation of security telemetry in pre-production and production environments. By doing so, we enable developer and operational productivity to increase while simultaneously increasing our overall safety. Our next step is to protect the deployment pipeline.\n",
      "\n",
      "* * *\n",
      "\n",
      "† The Open Web Application Security Project (OWASP) is a non-profit organization focused on improving the security of software.\n",
      "\n",
      "‡ Strategies for managing these risks include providing employee training and management; rethinking the design of information systems, including network and software; and instituting processes designed to prevent, detect, and respond to attacks.\n",
      "\n",
      "§ Tools that can help ensure the integrity of our software dependencies include OWASP Dependency Check and Sonatype Nexus Lifecycle.\n",
      "\n",
      "¶ Examples of tools that can help with security correctness testing (i.e., \"as it should be\") include automated configuration management systems (e.g., Puppet, Chef, Ansible, Salt), as well as tools such as ServerSpec and the Netflix Simian Army (e.g., Conformity Monkey, Security Monkey, etc.).\n",
      "\n",
      "** These approvals are known as FedRAMP JAB P-ATOs.\n",
      "\n",
      "# 23Protecting the Deployment Pipeline\n",
      "\n",
      "Throughout this chapter we will look at how to protect our deployment pipeline, as well as how to acheive security and compliance objectives in our control environment, including change management and separation of duty.\n",
      "\n",
      "## INTEGRATE SECURITY AND COMPLIANCE INTO CHANGE APPROVAL PROCESSES\n",
      "\n",
      "Almost any IT organization of any significant size will have existing change management processes, which are the primary controls to reduce operations and security risks. Compliance manager and security managers place reliance on change management processes for compliance requirements, and they typically require evidence that all changes have been appropriately authorized.\n",
      "\n",
      "If we have constructed our deployment pipeline correctly so that deployments are low-risk, the majority of our changes won't need to go through a manual change approval process, because we will have placed our reliance on controls such as automated testing and proactive production monitoring.\n",
      "\n",
      "In this step, we will do what is required to ensure that we can successfully integrate security and compliance into any existing change management process. Effective change management policies will recognize that there are different risks associated with different types of changes and that those changes are all handled differently. These processes are defined in ITIL, which breaks changes down into three categories:\n",
      "\n",
      "  * **Standard changes:** These are lower-risk changes that follow an established and approved process, but can also be pre-approved. They include monthly updates of application tax tables or country codes, website content and styling changes, and certain types of application or operating system patches that have a well-understood impact. The change proposer does not require approval before deploying the change, and change deployments can be completely automated and should be logged so there is traceability.\n",
      "  * **Normal changes:** __These are higher-risk changes that require review or approval from the agreed upon change authority. In many organizations, this responsibility is inappropriately placed on the change advisory board (CAB) or emergency change advisory board (ECAB), which may lack the required expertise to understand the full impact of the change, often leading to unacceptably long lead times. This problem is especially relevant for large code deployments, which may contain hundreds of thousands (or even millions) of lines of new code, submitted by hundreds of developers over the course of several months. In order for normal changes to be authorized, the CAB will almost certainly have a well-defined request for change (RFC) form that defines what information is required for the go/no-go decision. The RFC form usually includes the desired business outcomes, planned utility and warranty,† a business case with risks and alternatives, and a proposed schedule.‡\n",
      "  * **Urgent changes:** These are emergency, and, consequently, potentially high risk, changes that must be put into production immediately (e.g., urgent security patch, restore service). These changes often require senior management approval, but allow documentation to be performed after the fact. A key goal of DevOps practices is to streamline our normal change process such that it is also suitable for emergency changes.\n",
      "\n",
      "## RE-CATEGORIZE THE MAJORITY OF OUR LOWER RISK CHANGES AS STANDARD CHANGES\n",
      "\n",
      "Ideally, by having a reliable deployment pipeline in place, we will have already earned a reputation for fast, reliable, and non-dramatic deployments. At this point, we should seek to gain agreement from Operations and the relevant change authorities that our changes have been demonstrated to be low risk enough to be defined as standard changes, pre-approved by the CAB. This enables us to deploy into production without need for further approval, although the changes should still be properly recorded.\n",
      "\n",
      "One way to support an assertion that our changes are low risk is to show a history of changes over a significant time period (e.g., months or quarters) and provide a complete list of production issues during that same period. If we can show high change success rates and low MTTR, we can assert that we have a control environment that is effectively preventing deployment errors, as well as prove that we can effectively and quickly detect and correct any resulting problems.\n",
      "\n",
      "Even when our changes are categorized as standard changes, they still need to be visual and recorded in our change management systems (e.g., Remedy or ServiceNow). Ideally, deployments will be performed automatically by our configuration management and deployment pipeline tools (e.g., Puppet, Chef, Jenkins) and the results will be automatically recorded. By doing this, everyone in our organization (DevOps or not) will have visibility into our changes in addition to all the other changes happening in the organization.\n",
      "\n",
      "We may automatically link these change request records to specific items in our work planning tools (e.g., JIRA, Rally, LeanKit, ThoughtWorks Mingle), allowing us to create more context for our changes, such as linking to feature defects, production incidents, or user stories. This can be accomplished in a lightweight way by including ticket numbers from planning tools in the comments associated with version control check ins.§ By doing this, we can trace a production deployment to the changes in version control and, from there, trace them further back to the planning tool tickets.\n",
      "\n",
      "Creating this traceability and context should be easy and should not create an overly onerous or time-consuming burden for engineers. Linking to user stories, requirements, or defects is almost certainly sufficient—any further detail, such as opening a ticket for each commit to version control, is likely not useful, and thus unnecessary and undesired, as it will impose a significant level of friction on their daily work.\n",
      "\n",
      "## WHAT TO DO WHEN CHANGES ARE CATEGORIZED AS NORMAL CHANGES\n",
      "\n",
      "For those changes that we cannot get classified as standard changes, they will be considered _normal changes_ and will require approval from at least a subset of the CAB before deployment. In this case, our goal is still to ensure that we can deploy quickly, even if it is not fully automated.\n",
      "\n",
      "In this case, we must ensure that any submitted change requests are as complete and accurate as possible, giving the CAB everything they need to properly evaluate our change—after all, if our change request is malformed or incomplete, it will be bounced back to us, increasing the time required for us to get into production and casting doubt on whether we actually understand the goals of the change management process.\n",
      "\n",
      "We can almost certainly automate the creation of complete and accurate RFCs, populating the ticket with details of exactly what is to be changed. For instance, we could automatically create a ServiceNow change ticket with a link to the JIRA user story, along with the build manifests and test output from our deployment pipeline tool and links to the Puppet/Chef scripts that will be run.\n",
      "\n",
      "Because our submitted changes will be manually evaluated by people, it is even more important that we describe the context of the change. This includes identifying why we are making the change (e.g., providing a link to the features, defects, or incidents), who is affected by the change, and what is going to be changed.\n",
      "\n",
      "Our goal is to share the evidence and artifacts that give us confidence that the change will operate in production as designed. Although RFCs typically have free-form text fields, we should provide links to machine-readable data to enable others to integrate and process our data (e.g., links to JSON files).\n",
      "\n",
      "In many toolchains, this can be done in a compliant and fully automated way. For example, ThoughtWorks' Mingle and Go can automatically link this information together, such as a list of defects fixed and new features completed that are associated with the change, and put it into an RFC.\n",
      "\n",
      "Upon submission of our RFC, the relevant members of the CAB will review, process, and approve these changes as they would any other submitted change request. If all goes well, the change authorities will appreciate the thoroughness and detail of our submitted changes, because we have allowed them to quickly validate the correctness of the information we've provided (e.g., viewing the links to artifacts from our deployment pipeline tools). However, our goal should be to continually show an exemplary track record of successful changes, so we can eventually gain their agreement that our automated changes can be safely classified as standard changes.\n",
      "\n",
      "Case Study   \n",
      "Automated Infrastructure Changes as Standard Changes at Salesforce.com (2012)\n",
      "\n",
      "Salesforce was founded in 2000 with the aim of making customer relationship management easily available and deliverable as a service. Salesforce's offerings were widely adopted by the marketplace, leading to a successful IPO in 2004. By 2007, the company had over fifty-nine thousand enterprise customers, processing hundreds of millions of transactions per day, with annual revenue of $497 million.\n",
      "\n",
      "However, around that same time, their ability to develop and release new functionality to their customers seemed to grind to a halt. In 2006, they had four major customer releases, but in 2007 they were only able to do one customer release despite having hired more engineers. The result was that the number of features delivered per team kept decreasing and the days between major releases kept increasing.\n",
      "\n",
      "And because the batch size of each release kept getting larger, the deployment outcomes also kept getting worse. Karthik Rajan, then VP of Infrastructure Engineering, reports in a 2013 presentation that 2007 marked \"the last year when software was created and shipped using a waterfall process and when we made our shift to a more incremental delivery process.\"\n",
      "\n",
      "At the 2014 DevOps Enterprise Summit, Dave Mangot and Reena Mathew described the resulting multi-year DevOps transformation that started in 2009. According to Mangot and Mathew, by implementing DevOps principles and practices, the company reduced their deployment lead times from six days to five minutes by 2013. As a result, they were able to scale capacity more easily, allowing them to process over one billion transactions per day.\n",
      "\n",
      "One of the main themes of the Salesforce transformation was to make quality engineering everyone's job, regardless of whether they were part of Development, Operations, or Infosec. To do this, they integrated automated testing into all stages of the application and environment creation, as well as into the continuous integration and deployment process, and created the open source tool Rouster to conduct functional testing of their Puppet modules.\n",
      "\n",
      "They also started to routinely perform _destructive testing_ , a term used in manufacturing to refer to performing prolonged endurance testing under the most severe operating conditions until the component being tested is destroyed. The Salesforce team started routinely testing their services under increasingly higher loads until the service broke, which helped them understand their failure modes and make appropriate corrections. Unsurprisingly, the result was significantly higher service quality with normal production loads.\n",
      "\n",
      "Information Security also worked with Quality Engineering at the earliest stages of their project, continually collaborating in critical phases such as architecture and test design, as well as properly integrating security tools into the automated testing process.\n",
      "\n",
      "For Mangot and Mathew, one of the key successes from all the repeatability and rigor they designed into the process was being told by their change management group that \"infrastructure changes made through Puppet would now be treated as 'standard changes,' requiring far less or even no further approvals from the CAB.\" Furthermore, they noted that \"manual changes to infrastructure would still require approvals.\"\n",
      "\n",
      "By doing this, they had not only integrated their DevOps processes with the change management process, but also created further motivation to automate the change process for more of their infrastructure.\n",
      "\n",
      "## REDUCE RELIANCE ON SEPARATION OF DUTY\n",
      "\n",
      "For decades, we have used separation of duty as one of our primary controls to reduce the risk of fraud or mistakes in the software development process. It has been the accepted practice in most SDLCs to require developer changes to be submitted to a code librarian, who would review and approve the change before IT Operations promoted the change into production.\n",
      "\n",
      "There are plenty of other less contentious examples of separation of duty in Ops work, such as server administrators ideally being able to view logs but not delete or modify them, in order to prevent someone with privileged access from deleting evidence of fraud or other issues.\n",
      "\n",
      "When we did production deployments less frequently (e.g., annually) and when our work was less complex, compartmentalizing our work and doing hand-offs were tenable ways of conducting business. However, as complexity and deployment frequency increase, performing production deployments successfully increasingly requires everyone in the value stream to quickly see the outcomes of their actions.\n",
      "\n",
      "Separation of duty often can impede this by slowing down and reducing the feedback engineers receive on their work. This prevents engineers from taking full responsibility for the quality of their work and reduces a firm's ability to create organizational learning.\n",
      "\n",
      "Consequently, wherever possible, we should avoid using separation of duties as a control. Instead, we should choose controls such as pair programming, continuous inspection of code check-ins, and code review. These controls can give us the necessary reassurance about the quality of our work. Furthermore, by putting these controls in place, if separation of duties is required, we can show that we achieve equivalent outcomes with the controls we have created.\n",
      "\n",
      "Case Study   \n",
      "PCI Compliance and a Cautionary Tale of Separating Duties at Etsy (2014)\n",
      "\n",
      "Bill Massie is a development manager at Etsy and is responsible for the payment application called ICHT (an abbreviation for \"I Can Haz Tokens\"). ICHT takes customer credit orders through a set of internally-developed payment processing applications that handle online order entry by taking customer-entered cardholder data, tokenizing it, communicating with the payment processor, and completing the order transaction.¶\n",
      "\n",
      "Because the scope of the Payment Card Industry Data Security Standards (PCI DSS) cardholder data environment (CDE) is \"the people, processes and technology that store, process or transmit cardholder data or sensitive authentication data,\" including any connected system components, the ICHT application has in scope for the PCI DSS.\n",
      "\n",
      "To contain the PCI DSS scope, the ICHT application is physically and logically separated from the rest of the Etsy organization and is managed by a completely separate application team of developers, database engineers, networking engineers, and ops engineers. Each team member is issued two laptops: one for ICHT (which are configured differently to meet the DSS requirements, as well as being locked in a safe when not in use) and one for the rest of Etsy.\n",
      "\n",
      "By doing this, they were able to decouple the CDE environment from the rest of the Etsy organization, limiting the scope of the PCI DSS regulations to one segregated area. The systems that form the CDE are separated (and managed differently) from the rest of Etsy's environments at the physical, network, source code, and logical infrastructure levels. Furthermore, the CDE is built and operated by a cross-functional team that is solely responsible for the CDE.\n",
      "\n",
      "The ICHT team had to modify their continuous delivery practices in order to accommodate the need for code approvals. According to Section 6.3.2 of the PCI DSS v3.1, teams should review:\n",
      "\n",
      "All custom code prior to release to production or customers in order to identify any potential coding vulnerability (using either manual or automated processes) as follows:\n",
      "\n",
      "  * Are code changes reviewed by individuals other than the originating code author, and by individuals knowledgeable about code-review techniques and secure coding practices?\n",
      "  * Do code reviews ensure code is developed according to secure coding guidelines?\n",
      "  * Are appropriate corrections implemented prior to release?\n",
      "  * Are code review results reviewed and approved by management prior to release?\n",
      "\n",
      "To fulfill this requirement, the team initially decided to designate Massie as the change approver responsible for deploying any changes into production. Desired deployments would be flagged in JIRA, and Massie would mark them as reviewed and approved, and manually deploy them into the ICHT production.\n",
      "\n",
      "This has enabled Etsy to meet their PCI DSS requirements and get their signed Report of Compliance from their assessors. However, with regard to the team, significant problems have resulted.\n",
      "\n",
      "Massie observes that one troubling side effect \"is a level of 'compartmentalization' that is happening in the ICHT team that no other group is having at Etsy. Ever since we implemented separation of duty and other controls required by the PCI DSS compliance, no one can be a full-stack engineer in this environment.\"\n",
      "\n",
      "As a result, while the rest of the Development and Operations teams at Etsy work together closely and deploy changes smoothly and with confidence, Massie notes that \"within our PCI environment, there is fear and reluctance around deployment and maintenance because no one has visibility outside their portion of the software stack. The seemingly minor changes we made to the way we work seem to have created an impenetrable wall between developers and ops, and creates an undeniable tension that no one at Etsy has had since 2008. Even if you have confidence in your portion, it's impossible to get confidence that someone else's change isn't going to break your part of the stack.\"\n",
      "\n",
      "This case study shows that compliance is possible in organizations using DevOps. However, the potentially cautionary tale here is that all the virtues that we associate with high-performing DevOps teams are fragile—even a team that has shared experiences with high trust and shared goals can begin to struggle when low trust control mechanisms are put into place.\n",
      "\n",
      "## ENSURE DOCUMENTATION AND PROOF FOR AUDITORS AND COMPLIANCE OFFICERS\n",
      "\n",
      "As technology organizations increasingly adopt DevOps patterns, there is more tension than ever between IT and audit. These new DevOps patterns challenge traditional thinking about auditing, controls, and risk mitigation.\n",
      "\n",
      "As Bill Shinn, a principal security solutions architect at Amazon Web Services, observes, \"DevOps is all about bridging the gap between Dev and Ops. In some ways, the challenge of bridging the gap between DevOps and auditors and compliance officers is even larger. For instance, how many auditors can read code and how many developers have read NIST 800-37 or the Gramm-Leach-Bliley Act? That creates a gap of knowledge, and the DevOps community needs to help bridge that gap.\"\n",
      "\n",
      "Case Study   \n",
      "Proving Compliance in Regulated Environments (2015)\n",
      "\n",
      "Helping large enterprise customers show that they can still comply with all relevant laws and regulations is among Bill Shinn's responsibilities as a principal security solutions architect at Amazon Web Services. Over the years, he has spent time with over one thousand enterprise customers, including Hearst Media, GE, Phillips, and Pacific Life, who have publicly referenced their use of public clouds in highly regulated environments.\n",
      "\n",
      "Shinn notes, \"One of the problems is that auditors have been trained in methods that aren't very suitable for DevOps work patterns. For example, if an auditor saw an environment with ten thousand productions servers, they have been traditionally trained to ask for a sample of one thousand servers, along with screenshot evidence of asset management, access control settings, agent installations, server logs, and so forth.\"\n",
      "\n",
      "\"That was fine with physical environments,\" Shinn continues. \"But when infrastructure is code, and when auto-scaling makes servers appear and disappear all the time, how do you sample that? You run into the same problems when you have a deployment pipeline, which is very different than the traditional software development process, where one group writes the code and another group deploys that code into production.\"\n",
      "\n",
      "He explains, \"In audit fieldwork, the most commonplace methods of gathering evidence are still screenshots and CSV files filled with configuration settings and logs. Our goal is to create alternative methods of presenting the data that clearly show auditors that our controls are operating and effective.\"\n",
      "\n",
      "To help bridge that gap, he has teams work with auditors in the control design process. They use an iterative approach, assigning a single control for each sprint to determine what is needed in terms of audit evidence. This has helped ensure that auditors get the information they need when the service is in production, entirely on demand.\n",
      "\n",
      "Shinn states that the best way to accomplish this is to \"send all data into our telemetry systems, such as Splunk or Kibana. This way auditors can get what they need, completely self-serviced. They don't need to request a data sample—instead, they log into Kibana, and then search for audit evidence they need for a given time range. Ideally, they'll see very quickly that there's evidence to support that our controls are working.\"\n",
      "\n",
      "Shinn continues, \"With modern audit logging, chat rooms, and deployment pipelines, there's unprecedented visibility and transparency into what's happening in production, especially compared to how Operations used to be done, with far lower probability of errors and security flaws being introduced. So, the challenge is to turn all that evidence into something an auditor recognizes.\"\n",
      "\n",
      "That requires deriving the engineering requirements from the actual regulations. Shinn explains, \"To discover what HIPAA requires from an information security perspective, you have to look into the forty-five CFR Part 160 legislation, go into Subparts A and C of Part 164. Even then, you need to keep reading until you get into 'technical safeguards and audit controls.' Only there will you see that what is required is that we need to determine activities that will be tracked and audited relevant to Patient Healthcare Information, document and implement those controls, select tools, and then finally review and capture the appropriate information.\"\n",
      "\n",
      "Shinn continues, \"How to fulfill that requirement is the discussion that needs to be happening between compliance and regulatory officers, and the security and DevOps teams, specifically around how to prevent, detect, and correct problems. Sometimes they can be fulfilled in a configuration setting in version control. Other times, it's a monitoring control.\"\n",
      "\n",
      "Shinn gives an example: \"We may choose to implement one of those controls using AWS CloudWatch, and we can test that the control is operating with one command line. Furthermore, we need to show where the logs are going—in the ideal, we push all this into our logging framework, where we can link the audit evidence with the actual control requirement.\"\n",
      "\n",
      "To help solve this problem, the _DevOps Audit Defense Toolkit_ describes the end-to-end narrative of the compliance and audit process for a fictitious organization (Parts Unlimited from _The Phoenix Project_ ). It starts by describing the entity's organizational goals, business processes, top risks, and resulting control environment, as well as how management could successfully prove that controls exist and are effective. A set of audit objections is also presented, as well as how to overcome them.\n",
      "\n",
      "The document describes how controls could be designed in a deployment pipeline to mitigate the stated risks, and provides examples of control attestations and control artifacts to demonstrate control effectiveness. It was intended to be general to all control objectives, including in support of accurate financial reporting, regulatory compliance (e.g., SEC SOX-404, HIPAA, FedRAMP, EU Model Contracts, and the proposed SEC Reg-SCI regulations), contractual obligations (e.g., PCI DSS, DOD DISA), and effective and efficient operations.\n",
      "\n",
      "Case Study   \n",
      "Relying on Production Telemetry for ATM Systems\n",
      "\n",
      "Mary Smith (a pseudonym) heads up the DevOps initiative for the consumer banking property of a large US financial services organization. She made the observation that information security, auditors, and regulators often put too much reliance on code reviews to detect fraud. Instead, they should be relying on production monitoring controls in addition to using automated testing, code reviews, and approvals, to effectively mitigate the risks associated with errors and fraud.\n",
      "\n",
      "She observed:\n",
      "\n",
      "Many years ago, we had a developer who planted a backdoor in the code that we deploy to our ATM cash machines. They were able to put the ATMs into maintenance mode at certain times, allowing them to take cash out of the machines. We were able to detect the fraud very quickly, and it wasn't through a code review. These types of backdoors are difficult, or even impossible, to detect when the perpetrators have sufficient means, motive, and opportunity.\n",
      "\n",
      "However, we quickly detected the fraud during our regularly operations review meeting when someone noticed that ATMs in a city were being put into maintenance mode at unscheduled times. We found the fraud even before the scheduled cash audit process, when they reconcile the amount of cash in the ATMs with authorized transactions.\n",
      "\n",
      "In this case study, the fraud occurred despite separation of duties between Development and Operations and a change approval process, but was quickly detected and corrected through effective production telemetry.\n",
      "\n",
      "## CONCLUSION\n",
      "\n",
      "Throughout this chapter, we have discussed practices that make information security everyone's job, where all of our information security objectives are integrated into the daily work of everyone in the value stream. By doing this, we significantly improve the effectiveness of our controls, so that we can better prevent security breaches, as well as detect and recover from them faster. And we significantly reduce the work associated with preparing and passing compliance audits.\n",
      "\n",
      "## PART VI CONCLUSION\n",
      "\n",
      "Throughout the previous chapters, we explored how to take DevOps principles and apply them to Information Security, helping us achieve our goals, and making sure security is a part of everyone's job, every day. Better security ensures that we are defensible and sensible with our data, that we can recover from security problems before they become catastrophic, and, most importantly, that we can make the security of our systems and data better than ever.\n",
      "\n",
      "* * *\n",
      "\n",
      "† ITIL defines utility as \"what the service does,\" while warranty is defined as \"how the service is delivered and can be used to determine whether a service is 'fit for use.'\"\n",
      "\n",
      "‡ To further manage risk changes, we may also have defined rules, such as certain changes can only be implemented by a certain group or individual (e.g., only DBAs can deploy database schema changes). Traditionally, the CAB meetings have been held weekly, where the change requests are approved and scheduled. From ITIL version 3 onward, it is acceptable for changes to be approved electronically in a just-in-time fashion through a change management tool. It also specifically recommends that \"standard changes should be identified early on when building the Change Management process to promote efficiency. Otherwise, a Change Management implementation can create unnecessarily high levels of administration and resistance to the Change Management process.\"\n",
      "\n",
      "§ The term _ticket_ is used generically to indicate any uniquely identifiable work item.\n",
      "\n",
      "¶ The authors thank Bill Massie and John Allspaw for spending an entire day with Gene Kim sharing their compliance experience.\n",
      "\n",
      "# A Call to Action\n",
      "\n",
      "## Conclusion to the DevOps Handbook\n",
      "\n",
      "We have come to the end of a detailed exploration of both the principles and technical practices of DevOps. At a time when every technology leader is challenged with enabling security, reliability, and agility, and at a time when security breaches, time to market, and massive technology transformation is taking place, DevOps offers a solution. Hopefully, this book has provided an in-depth understanding of the problem and a road map to creating relevant solutions.\n",
      "\n",
      "As we have explored throughout _The DevOps Handbook_ , we know that, left unmanaged, an inherent conflict can exist between Development and Operations that creates ever-worsening problems,which results in slower time to market for new products and features, poor quality, increased outages and technical debt, reduced engineering productivity, as well as increased employee dissatisfaction and burnout.\n",
      "\n",
      "DevOps principles and patterns enable us to break this core, chronic conflict. After reading this book, we hope you see how a DevOps transformation can enable the creation of dynamic learning organizations, achieving the amazing outcomes of fast flow and world-class reliability and security, as well as increased competitiveness and employee satisfaction.\n",
      "\n",
      "DevOps requires potentially new cultural and management norms, and changes in our technical practices and architecture. This requires a coalition that spans business leadership, Product Management, Development, QA, IT Operations, Information Security, and even Marketing, where many technology initiatives originate. When all these teams work together, we can create a safe system of work, enabling small teams to quickly and independently develop and validate code that can be safely deployed to customers. This results in maximizing developer productivity, organizational learning, high employee satisfaction, and the ability to win in the marketplace.\n",
      "\n",
      "Our goal in writing this book was to sufficiently codify DevOps principles and practices so that the amazing outcomes achieved within the DevOps community could be replicated by others. We hope to accelerate the adoption of DevOps initiatives and support their successful implementations while lowering the activation energy required for them to be completed.\n",
      "\n",
      "We know the dangers of postponing improvements and settling for daily work-arounds, as well as the difficulties of changing how we prioritize and perform our daily work. Furthermore, we understand the risks and effort required to get organizations to embrace a different way of working, as well as the perception that DevOps is another passing fad, soon to replaced by the next buzzword.\n",
      "\n",
      "We assert that DevOps is transformational to how we perform technology work, just as Lean forever transformed how manufacturing work was performed in the 1980s. Those that adopt DevOps will win in the marketplace, at the expense of those that do not. They will create energized and continually learning organizations that out-perform and out-innovate their competitors.\n",
      "\n",
      "Because of this, DevOps is not just a technology imperative, but also an organizational imperative. The bottom line is, DevOps is applicable and relevant to any and all organizations that must increase flow of planned work through the technology organization, while maintaining quality, reliability, and security for our customers.\n",
      "\n",
      "Our call to action is this: no matter what role you play in your organization, start finding people around you who want to change how work is performed. Show this book to others and create a coalition of like-minded thinkers to break out of the downward spiral. Ask organizational leaders to support these efforts, or, better yet, sponsor and lead these efforts yourself.\n",
      "\n",
      "Finally, since you've made it this far, we have a dirty secret to reveal. In many of our case studies, following the achievement of the breakthrough results presented, many of the change agents were promoted—but, in some cases, there was later a change of leadership which resulted in many of the people involved leaving, accompanied by a rolling back of the organizational changes they had created.\n",
      "\n",
      "We believe it's important not to be cynical about this possibility. The people involved in these transformations knew up front that what they were doing had a high chance of failure, and they did it anyway. In doing so, perhaps most importantly, they inspired the rest of us by showing us what can be done. Innovation is impossible without risk taking, and if you haven't managed to upset at least some people in management, you're probably not trying hard enough. Don't let your organization's immune system deter or distract you from your vision. As Jesse Robbins, previously \"master of disaster\" at Amazon, likes to say, \"Don't fight stupid, make more awesome.\"\n",
      "\n",
      "DevOps benefits all of us in the technology value stream, whether we are Dev, Ops, QA, Infosec, Product Owners, or customers. It brings joy back to developing great products, with fewer death marches. It enables humane work conditions with fewer weekends and missed holidays with our loved ones. It enables teams to work together to survive, learn, thrive, delight our customers, and help our organization succeed.\n",
      "\n",
      "We sincerely hope _The DevOps Handbook_ helps you achieve these goals.\n",
      "\n",
      "# Appendices\n",
      "\n",
      "## APPENDIX 1 THE CONVERGENCE OF DEVOPS\n",
      "\n",
      "We believe that DevOps is benefiting from an incredible convergence of management movements, which are all mutually reinforcing and can help create a powerful coalition to transform how organizations develop and deliver IT products and services.\n",
      "\n",
      "John Willis named this \"the Convergence of DevOps.\" The various elements of this convergence are described below in approximate chronological order. (Note that these descriptions are not intended to be an exhaustive description, but merely enough to show the progression of thinking and the rather improbable connections that led to DevOps.)\n",
      "\n",
      "### THE LEAN MOVEMENT\n",
      "\n",
      "The Lean Movement started in the 1980s as an attempt to codify the Toyota Production System with the popularization of techniques such as Value Stream Mapping, kanban boards, and Total Productive Maintenance.\n",
      "\n",
      "Two major tenets of Lean were the deeply held belief that lead time (i.e., the time required to convert raw materials into finished goods) was the best predictor of quality, customer satisfaction, and employee happiness; and that one of the best predictors of short lead times was small batch sizes, with the theoretical ideal being \"single piece flow\" (i.e., \"1x1\" flow: inventory of 1, batch size of 1).\n",
      "\n",
      "Lean principles focus on creating value for the customer—thinking systematically, creating constancy of purpose, embracing scientific thinking, creating flow and pull (versus push), assuring quality at the source, leading with humility, and respecting every individual.\n",
      "\n",
      "### THE AGILE MOVEMENT\n",
      "\n",
      "Started in 2001, the Agile Manifesto was created by seventeen of the leading thinkers in software development, with the goal of turning lightweight methods such as DP and DSDM into a wider movement that could take on heavyweight software development processes such as waterfall development and methodologies such as the Rational Unified Process.\n",
      "\n",
      "A key principle was to \"deliver working software frequently, from a couple of weeks to a couple of months, with a preference to the shorter timescale.\" Two other principles focus on the need for small, self-motivated teams, working in a high-trust management model and an emphasis on small batch sizes. Agile is also associated with a set of tools and practices such as Scrum, Standups, and so on.\n",
      "\n",
      "### THE VELOCITY CONFERENCE MOVEMENT\n",
      "\n",
      "Started in 2007, the Velocity Conference was created by Steve Souders, John Allspaw, and Jesse Robbins to provide a home for the IT Operations and Web Performance tribe. At the Velocity 2009 conference, John Allspaw and Paul Hammond gave the seminal \"10 Deploys per Day: Dev and Ops Cooperation at Flickr.\"\n",
      "\n",
      "### THE AGILE INFRASTRUCTURE MOVEMENT\n",
      "\n",
      "At the 2008 Agile Toronto conference, Patrick Dubois and Andrew Schafer held a \"birds of a feather\" session on applying Agile principles to infrastructure as opposed to application code. They rapidly gained a following of like-minded thinkers, including John Willis. Later, Dubois was so excited by Allspaw and Hammond's \"10 Deploys per Day: Dev and Ops Cooperation at Flickr\" presentation that he created the first DevOpsDays in Ghent, Belgium, in 2009, coining the word \"DevOps.\"\n",
      "\n",
      "### THE CONTINUOUS DELIVERY MOVEMENT\n",
      "\n",
      "Building upon the Development discipline of continuous build, test, and integration, Jez Humble and David Farley extended the concept of continuous delivery, which included a \"deployment pipeline\" to ensure that code and infrastructure are always in a deployable state and that all code checked in to truck is deployed into production.\n",
      "\n",
      "This idea was first presented at Agile 2006 and was also independently developed by Tim Fitz in a blog post titled \"Continuous Deployment.\"\n",
      "\n",
      "### THE TOYOTA KATA MOVEMENT\n",
      "\n",
      "In 2009, Mike Rother wrote _Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results_ , which described learnings over his twenty-year journey to understand and codify the causal mechanisms of the Toyota Production System. _ Toyota Kata_ describes the \"unseen managerial routines and thinking that lie behind Toyota's success with continuous improvement and adaptation... and how other companies develop similar routines and thinking in their organizations.\"\n",
      "\n",
      "His conclusion was that the Lean community missed the most important practice of all, which he described as the Improvement Kata. He explains that every organization has work routines, and the critical factor in Toyota was making improvement work habitual, and building it into the daily work of everyone in the organization. The Toyota Kata institutes an iterative, incremental, scientific approach to problem solving in the pursuit of a shared organizational true north.\n",
      "\n",
      "### THE LEAN STARTUP MOVEMENT\n",
      "\n",
      "In 2011, Eric Ries wrote _The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses_ , codifying his lessons learned at IMVU, a Silicon Valley startup, which built upon the work of Steve Blank in _The Four Steps to the Epiphany_ as well as continuous deployment techniques. Eric Ries also codified related practices and terms including Minimum Viable Product, the build-measure-learn cycle, and many continuous deployment technical patterns.\n",
      "\n",
      "### THE LEAN UX MOVEMENT\n",
      "\n",
      "In 2013, Jeff Gothelf wrote _Lean UX: Applying Lean Principles to Improve User Experience_ , which codified how to improve the \"fuzzy front end\" and explained how product owners can frame business hypotheses, experiment, and gain confidence in those business hypotheses before investing time and resources in the resulting features. By adding Lean UX, we now have the tools to fully optimize the flow between business hypotheses, feature development, testing, deployment, and service delivery to the customer.\n",
      "\n",
      "### THE RUGGED COMPUTING MOVEMENT\n",
      "\n",
      "In 2011, Joshua Corman, David Rice, and Jeff Williams examined the apparent futility of securing applications and environments late in the life cycle. In response, they created a philosophy called \"Rugged Computing,\" which attempts to frame the non-functional requirements of stability, scalability, availability, survivability, sustainability, security, supportability, manageability, and defensibility.\n",
      "\n",
      "Because of the potential for high release rates, DevOps can put incredible pressure on QA and Infosec, because when deploy rates go from monthly or quarterly to hundreds or thousands daily, no longer are two week turnaround times from Infosec or QA tenable. The Rugged Computing movement posited that the current approach to fighting the vulnerable industrial complex being employed by most information security programs is hopeless.\n",
      "\n",
      "## APPENDIX 2 THEORY OF CONSTRAINTS AND CORE, CHRONIC CONFLICTS\n",
      "\n",
      "The Theory of Constraints body of knowledge extensively discusses the use of creating core conflict clouds (often referred to as \"C3\"). Here is the conflict cloud for IT:\n",
      "\n",
      "  **Figure 46:** The core, chronic conflict facing every IT organization\n",
      "\n",
      "During the 1980s, there was a very well-known core, chronic conflict in manufacturing. Every plant manager had two valid business goals: protect sales and reduce costs. The problem was that in order to protect sales, sales management was incentivized to increase inventory to ensure that it was always possible to fulfill customer demand.\n",
      "\n",
      "On the other hand, in order to reduce cost, production management was incentivized to decrease inventory to ensure that money was not tied up in work in progress that wasn't immediately shippable to the customer in the form of fulfilled sales.\n",
      "\n",
      "They were able to break the conflict by adopting Lean principles, such as reducing batch sizes, reducing work in process, and shortening and amplifying feedback loops. This resulted in dramatic increases in plant productivity, product quality, and customer satisfaction.\n",
      "\n",
      "The principles behind DevOps work patterns are the same as those that transformed manufacturing, allowing us to optimize the IT value stream, converting business needs into capabilities and services that provide value for our customers.\n",
      "\n",
      "## APPENDIX 3 TABULAR FORM OF DOWNWARD SPIRAL\n",
      "\n",
      "The columnar form of the downward spiral depicted in _The Phoenix Project_ is shown below:\n",
      "\n",
      "**Table 4:** The Downward Spiral\n",
      "\n",
      "## APPENDIX 4 THE DANGERS OF HANDOFFS AND QUEUES\n",
      "\n",
      "The problem with high amounts of queue time is exacerbated when there are many handoffs, because that is where queues are created. Figure 47 shows wait time as a function of how busy a resource at a work center is. The asymptotic curve shows why a \"simple thirty-minute change\" often takes weeks to complete—specific engineers and work centers often become problematic bottlenecks when they operate at high utilization. As a work center approaches 100% utilization, any work required from it will languish in queues and won't be worked on without someone expediting/escalating.\n",
      "\n",
      "_ _**Figure 47:** Queue size and wait times as function of percent utilization (Source: Kim, Behr, and Spafford, _The Phoenix Project_ , ePub edition, 557.)\n",
      "\n",
      "In figure 47, the x-axis is the percent busy for a given resource at a work center, and the y-axis is the approximate wait time (or, more precisely stated, the queue length). What the shape of the line shows is that as resource utilization goes past 80%, wait time goes through the roof.\n",
      "\n",
      "In _The Phoenix Project_ , here's how Bill and his team realized the devastating consequences of this property on lead times for the commitments they were making to the project management office:\n",
      "\n",
      "I tell them about what Erik told me at MRP-8, about how wait times depend upon resource utilization. \"The wait time is the 'percentage of time busy' divided by the 'percentage of time idle.' In other words, if a resource is fifty percent busy, then it's fifty percent idle. The wait time is fifty percent divided by fifty percent, so one unit of time. Let's call it one hour.\n",
      "\n",
      "So, on average, our task would wait in the queue for one hour before it gets worked.\n",
      "\n",
      "\"On the other hand, if a resource is ninety percent busy, the wait time is 'ninety percent divided by ten percent,' or nine hours. In other words, our task would wait in queue nine times longer than if the resource were fifty percent idle.\"\n",
      "\n",
      "I conclude, \"So...For the Phoenix task, assuming we have seven handoffs, and that each of those resources is busy ninety percent of the time, the tasks would spend in queue a total of nine hours times the seven steps...\"\n",
      "\n",
      "\"What? Sixty-three hours, just in queue time?\" Wes says, incredulously. \"That's impossible!\"\n",
      "\n",
      "Patty says with a smirk, \"Oh, of course. Because it's only thirty seconds of typing, right?\"\n",
      "\n",
      "Bill and team realize that their \"simple thirty-minute task\" actually requires seven handoffs (e.g., server team, networking team, database team, virtualization team, and, of course, Brent, the \"rockstar\" engineer).\n",
      "\n",
      "Assuming that all work centers were 90% busy, the figure shows us that the average wait time at each work center is nine hours—and because the work had to go through seven work centers, the total wait time is seven times that: sixty-three hours.\n",
      "\n",
      "In other words, the total % of _value added time_ (sometimes known as process time) was only 0.16% of the total lead time (thirty minutes divided by sixty-three hours). That means that for 99.8% of our total lead time, the work was simply sitting in queue, waiting to be worked on.\n",
      "\n",
      "## APPENDIX 5 MYTHS OF INDUSTRIAL SAFETY\n",
      "\n",
      "Decades of research into complex systems shows that countermeasures are based on several myths. In \"Some Myths about Industrial Safety,\" by Denis Besnard and Erik Hollnagel, they are summarized as such:\n",
      "\n",
      "  * **Myth 1:** \"Human error is the largest single cause of accidents and incidents.\"\n",
      "  * **Myth 2:** \"Systems will be safe if people comply with the procedures they have been given.\"\n",
      "  * **Myth 3:** \"Safety can be improved by barriers and protection; more layers of protection results in higher safety.\"\n",
      "  * **Myth 4:** \"Accident analysis can identify the root cause (the 'truth') of why the accident happened.\"\n",
      "  * **Myth 5:** \"Accident investigation is the logical and rational identification of causes based on facts.\"\n",
      "  * **Myth 6:** \"Safety always has the highest priority and will never be compromised.\"\n",
      "\n",
      "The differences between what is myth and what is true __are shown below:\n",
      "\n",
      "**Table 5:** Two Stories\n",
      "\n",
      "## APPENDIX 6 THE TOYOTA ANDON CORD\n",
      "\n",
      "Many ask how can any work be completed if the Andon cord is being pulled over five thousand times per day? To be precise, not every Andon cord pull results in stopping the entire assembly line. Rather, when the Andon cord is pulled, the team leader overseeing the specified work center has fifty seconds to resolve the problem. If the problem has not been resolved by the time the fifty seconds is up, the partially assembled vehicle will cross a physically drawn line on the floor, and the assembly line will be stopped.\n",
      "\n",
      "  **Figure 48:** The Toyota Andon cord\n",
      "\n",
      "## APPENDIX 7 COTS SOFTWARE\n",
      "\n",
      "Currently, in order to get complex COTS (commercial off-the-shelf) software (e.g., SAP, IBM WebSphere, Oracle WebLogic) into version control, we may have to eliminate the use of graphical point-and-click vendor installer tools. To do that, we need to discover what the vendor installer is doing, and we may need to do an install on a clean server image, diff the file system, and put those added files into version control. Files that don't vary by environment are put into one place (\"base install\"), while environment-specific files are put into their own directory (\"test\" or \"production\"). By doing this, software install operations become merely a version control operation, enabling better visibility, repeatability, and speed.\n",
      "\n",
      "We may also have to transform any application configuration settings so that they are in version control. For instance, we may transform application configurations that are stored in a database into XML files and vice versa.\n",
      "\n",
      "## APPENDIX 8 POST-MORTEM MEETINGS\n",
      "\n",
      "A sample agenda of the post-mortem meeting is shown below:\n",
      "\n",
      "  * An initial statement will be made by the meeting leader or facilitator to reinforce that this meeting is a blameless post-mortem and that we will not focus on past events or speculate on \"would haves\" or \"could haves.\" Facilitators might read the \"Retrospective Prime Directive\" from the website Retrospective.com.\n",
      "\n",
      "Furthermore, the facilitator will remind everyone that any countermeasures must be assigned to someone, and if the corrective action does not warrant being a top priority when the meeting is over, then it is not a corrective action. (This is to prevent the meeting from generating a list of good ideas that are never implemented.)\n",
      "  * Those at the meeting will reach an agreement on the complete timeline of the incident, including when and who detected the issue, how it was discovered (e.g., automated monitoring, manual detection, customer notified us), when service was satisfactorily restored, and so forth. We will also integrate into the timeline all external communications during the incident.\n",
      "\n",
      "When we use the word \"timeline,\" it may evoke the image of a linear set of steps of how we gained an understanding of the problem and eventually fixed it. In reality, especially in complex systems, there will likely be many events that contributed to the accident, and many troubleshooting paths and actions will have been taken in an effort to fix it. In this activity, we seek to chronicle all of these events and the perspectives of the actors and establish hypotheses concerning cause and effect where possible.\n",
      "  * The team will create a list of all the factors which contributed to the incident, both human and technical. They may then sort them into categories, such as 'design decision,' 'remediation,' 'discovering there was a problem,' and so forth. The team will use techniques such as brainstorming and the 'infinite hows' to drill down on contributing factors they deem particularly important to discover deeper levels of contributing factors. All perspectives should be included and respected—nobody should be permitted to argue with or deny the reality of a contributing factor somebody else has identified. It's important for the post-mortem facilitator to ensure that sufficient time is spent on this activity, and that the team doesn't try and engage in convergent behavior such as trying to identify one or more 'root causes.'\n",
      "  * Those at the meeting will reach an agreement on the list of corrective actions that will be made top priorities after the meeting. Assembling this list will require brainstorming and choosing the best potential actions to either prevent the issue from occurring or enable faster detection or recovery. Other ways to improve the systems may also be included.\n",
      "\n",
      "Our goal is to identify the smallest number of incremental steps to achieve the desired outcomes, as opposed to \"big bang\" changes, which not only take longer to implement, but delay the improvements we need.\n",
      "\n",
      "We will also generate a separate list of lower priority ideas and assign an owner. If similar problems occur in the future, these ideas may serve as the foundation for crafting future countermeasures. \n",
      "  * Those at the meeting will reach an agreement on the incident metrics and their organizational impact. For example, we may choose to measure our incidents by the following metrics:\n",
      "    * ▹ **Event severity:** How severe was this issue? This directly relates to the impact on the service and our customers. \n",
      "    * ▹ **Total downtime:** How long were customers unable to use the service to any degree?\n",
      "    * ▹ **Time to detect:** How long did it take for us or our systems to know there was a problem?\n",
      "    * ▹ **Time to resolve:** How long after we knew there was a problem did it take for us to restore service?\n",
      "\n",
      "Bethany Macri from Etsy observed, \"Blamelessness in a post-mortem does not mean that no one takes responsibility. It means that we want to find out what the circumstances were that allowed the person making the change or who introduced the problem to do this. What was the larger environment....The idea is that by removing blame, you remove fear, and by removing fear, you get honesty.\"\n",
      "\n",
      "## APPENDIX 9 THE SIMIAN ARMY\n",
      "\n",
      "After the 2011 AWS EAST Outage, Netflix had numerous discussions about engineering their systems to automatically deal with failure. These discussions have evolved into a service called \"Chaos Monkey.\"\n",
      "\n",
      "Since then, Chaos Monkey has evolved into a whole family of tools, known internally as the \"Netflix Simian Army,\" to simulate increasingly catastrophic levels of failures:\n",
      "\n",
      "  * **Chaos Gorilla:** simulates the failure of an entire AWS availability zone\n",
      "  * **Chaos Kong:** simulates failure of entire AWS regions, such as North America or Europe\n",
      "\n",
      "Other member of the Simian Army now include:\n",
      "\n",
      "  * **Latency Monkey:** induces artificial delays or downtime in their RESTful client-server communication layer to simulate service degradation and ensure that dependent services respond appropriately \n",
      "  * **Conformity Monkey:** finds and shuts down AWS instances that don't adhere to best-practices (e.g., when instances don't belong to an auto-scaling group or when there is no escalation engineer email address listed in the service catalog)\n",
      "  * **Doctor Monkey:** taps into health checks that run on each instance and finds unhealthy instances and proactively shuts them down if owners don't fix the root cause in time\n",
      "  * **Janitor Monkey:** ensures that their cloud environment is running free of clutter and waste; searches for unused resources and disposes of them\n",
      "  * **Security Monkey:** an extension of Conformity Monkey; finds and terminates instances with security violations or vulnerabilities, such as improperly configured AWS security groups\n",
      "\n",
      "## APPENDIX 10 TRANSPARENT UPTIME\n",
      "\n",
      "Lenny Rachitsky wrote about the benefits of what he called \"transparent uptime\":\n",
      "\n",
      "  1. Your support costs go down as your users are able to self-identify system wide problems without calling or emailing your support department. Users will no longer have to guess whether their issues are local or global, and can more quickly get to the root of the problem before complaining to you.\n",
      "  2. You are better able to communicate with your users during downtime events, taking advantage of the broadcast nature of the Internet versus the one-to-one nature of email and the phone. You spend less time communicating the same thing over and over and more time resolving the issue.\n",
      "  3. You create a single and obvious place for your users to come to when they are experiencing downtime. You save your users' time currently spent searching forums, Twitter, or your blog.\n",
      "  4. Trust is the cornerstone of any successful SaaS adoption. Your customers are betting their business and their livelihoods on your service or platform. Both current and prospective customers require confidence in your service. Both need to know they won't be left in the dark, alone and uninformed, when you run into trouble. Real time insight into unexpected events is the best way to build this trust. Keeping them in the dark and alone is no longer an option.\n",
      "  5. It's only a matter of time before every serious SaaS provider will be offering a public health dashboard. Your users will demand it.\n",
      "\n",
      "# Additional Resources\n",
      "\n",
      "  * Many of the common problems faced by IT organizations are discussed in the first half of the book _The Phoenix Project: A Novel about IT, DevOps, and Helping Your Business Win_ by Gene Kim, Kevin Behr, and George Spafford.\n",
      "  * This video shows a speech Paul O'Neill gave on his tenure as CEO of Alcoa, including the investigation he took part in after a teenage worker was killed at one of Alcoa's plants: <https://www.youtube.com/watch?v=tC2ucDs_XJY> .\n",
      "  * For more on value stream mapping, see _Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation_ by Karen Martin and Mike Osterling.\n",
      "  * For more on ORMs, visit Stack Overflow: <http://stackoverflow.com/questions/1279613/what-is-an-orm-and-where-can-i-learn-more-about-it> . \n",
      "  * An excellent primer on many agile development rituals and how to use them in IT Operations work can be found in a series of posts written on the Agile Admin blog: <http://theagileadmin.com/2011/02/21/scrum-for-operations-what-is-scrum/> . \n",
      "  * For more information on architecting for fast builds, see Daniel Worthington-Bodart's blog post \"Crazy Fast Build Times (or When 10 Seconds Starts to Make You Nervous)\": <http://dan.bodar.com/2012/02/28/crazy-fast-build-times-or-when-10-seconds-starts-to-make-you-nervous/> .\n",
      "  * For more details on performance testing at Facebook, along with some detailed information on Facebook's release process, check out Chuck Rossi's presentation \"The Facebook Release Process\": <http://www.infoq.com/presentations/Facebook-Release-Process> . \n",
      "  * Many more variants of dark launching can be found in chapter 8 of _The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2_ by Thomas A. Limoncelli, Strata R. Chalup, and Christina J. Hogan.\n",
      "  * There is an excellent technical discussion of feature toggles here: <http://martinfowler.com/articles/feature-toggles.html> . \n",
      "  * Releases are discussed in more detail in _The Practice of Cloud System Administration: Designing and Operating Large Distributed Systems, Volume 2_ by Thomas A. Limoncelli, Strata R. Chalup, and Christina J. Hogan; _Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation_ by Jez Humble and David Farley; and _Release It! Design and Deploy Production-Ready Software_ by Michael T. Nygard.\n",
      "  * A description of the circuit breaker pattern can be found here: <http://martinfowler.com/bliki/CircuitBreaker.html> . \n",
      "  * For more on the cost of delay see _The Principles of Product Development Flow: Second Generation Lean Product Development_ by Donald G. Reinertsen.\n",
      "  * A further discussion on staying ahead of failures for the Amazon S3 service can be found here: <https://qconsf.com/sf2010/dl/qcon-sanfran-2009/slides/JasonMcHugh_AmazonS3ArchitectingForResiliencyInTheFaceOfFailures.pdf> .\n",
      "  * For an excellent guide on conducting user research, see _Lean UX: Applying Lean Principles to Improve User Experience_ by Jeff Gothelf and Josh Seiden.\n",
      "  * Which Test Won? is a site that displays hundreds of real-life A/B tests and asks the viewer to guess which variant performed better, reinforcing the key that unless we actually test, we're merely guessing. Visit it here: <http://whichtestwon.com/> .\n",
      "  * A list of architectural patterns can be found in _Release It! Design and Deploy Production-Ready Software_ by Michael T. Nygard.\n",
      "  * An example of published Chef post-mortem meeting notes can be found here: <https://www.chef.io/blog/2014/08/14/cookbook-dependency-api-postmortem/> . A video of the meeting can be found here: <https://www.youtube.com/watch?v=Rmi1Tn5oWfI> .\n",
      "  * A current schedule of upcoming DevOpsDays can be found on the DevOpsDays website: <http://www.devopsdays.org/> . Instructions on organizing a new DevOpsDays can be found on the DevOpsDay Organizing Guide page: <http://www.devopsdays.org/pages/organizing/> .\n",
      "  * More on using tools to manage secrets can be found in Noah Kantrowitz's post \"Secrets Management and Chef\" on his blog: <https://coderanger.net/chef-secrets/> .\n",
      "  * James Wickett and Gareth Rushgrove have put all their examples of secure pipelines on the GitHub website: <https://github.com/secure-pipeline> .\n",
      "  * The National Vulnerability Database website and XML data feeds can be found at: <https://nvd.nist.gov/> . \n",
      "  * A concrete scenario involving integration between Puppet and ThoughtWorks' Go and Mingle (a project management application) can be found in a Puppet Labs blog post by Andrew Cunningham and Andrew Myers and edited by Jez Humble: <https://puppetlabs.com/blog/a-deployment-pipeline-for-infrastructure> .\n",
      "  * Preparing and passing compliance audits is further explored in Jason Chan's 2015 presentation \"SEC310: Splitting the Check on Compliance and Security: Keeping Developers and Auditors Happy in the Cloud\": https://www.youtube.com/watch?v=Io00_K4v12Y&feature=youtu.be .\n",
      "  * The story of how application configuration settings were transformed by Jez Humble and David Farley for Oracle WebLogic was described in the book _Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation_. Mirco Hering described a more generic approach to this process here: <http://notafactoryanymore.com/2015/10/19/devops-for-systems-of-record-a-new-hope-preview-of-does-talk/> .\n",
      "  * A sample list of DevOps operational requirements can be found here: <http://blog.devopsguys.com/2013/12/19/the-top-ten-devops-operational-requirements/> .\n",
      "\n",
      "# Endnotes\n",
      "\n",
      "###  INTRODUCTION\n",
      "\n",
      "_Before the revolution_... Eliyahu M. Goldratt, _Beyond the Goal: Eliyahu Goldratt Speaks on the Theory of Constraints (Your Coach in a Box)_ (Prince Frederick, Maryland: Gildan Media, 2005), Audiobook.\n",
      "\n",
      "_Put even more..._ Jeff Immelt, \"GE CEO Jeff Immelt: Let's Finally End the Debate over Whether We Are in a Tech Bubble,\" _Business Insider_ , December 9, 2015, <http://www.businessinsider.com/ceo-of-ge-lets-finally-end-the-debate-over-whether-we-are-in-a-tech-bubble-2015-12> .\n",
      "\n",
      "_Or as Jeffrey_... \"Weekly Top 10: Your DevOps Flavor,\" _Electric Cloud_ , April 1, 2016, <http://electric-cloud.com/blog/2016/04/weekly-top-10-devops-flavor/> .\n",
      "\n",
      "_Dr. Eliyahu M. Goldratt_... Goldratt, _Beyond the Goal._\n",
      "\n",
      "_As Christopher Little_... Christopher Little, personal correspondence with Gene Kim, 2010.\n",
      "\n",
      "_As Steven J. Spear_... Steven J. Spear, _The High-Velocity Edge: How Market Leaders Leverage Operational Excellence to Beat the Competition_ (New York, NY: McGraw Hill Education), Kindle edition, chap. 3.\n",
      "\n",
      "_In 2013, the_... Chris Skinner, \"Banks have bigger development shops than Microsoft,\" Chris Skinner's Blog, accessed July 28, 2016, <http://thefinanser.com/2011/09/banks-have-bigger-development-shops-than-microsoft.html/> .\n",
      "\n",
      "_Projects are typically_... Nico Stehr and Reiner Grundmann, _Knowledge: Critical Concepts, Volume 3_ (London: Routledge, 2005), 139.\n",
      "\n",
      "_Dr. Vernon Richardson_... A. Masli, V. Richardson, M. Widenmier, and R. Zmud, \"Senior Executive's IT Management Responsibilities: Serious IT Deficiencies and CEO-CFO Turnover,\" _MIS Quaterly_ (published electronically June 21, 2016).\n",
      "\n",
      "_Consider the following_... \"IDC Forecasts Worldwide IT Spending to Grow 6% in 2012, Despite Economic Uncertainty,\" _Business Wire_ , September 10, 2012, <http://www.businesswire.com/news/home/20120910005280/en/IDC-Forecasts-Worldwide-Spending-Grow-6-2012> .\n",
      "\n",
      "_The first surprise_... Nigel Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_ (Portland, OR: Puppet Labs, 2015), https://puppet.com/resources/white-paper/2015-state-of-devops-report?_ga=1.6612658.168869.1464412647&link=blog .\n",
      "\n",
      "_This is highlighted_... Frederick P. Brooks, Jr., _The Mythical Man-Month: Essays on Software Engineering, Anniversary Edition_ (Upper Saddle River, NJ: Addison-Wesley, 1995).\n",
      "\n",
      "_As Randy Shoup_... Gene Kim, Gary Gruver, Randy Shoup, and Andrew Phillips, \"Exploring the Uncharted Territory of Microservices,\" XebiaLabs.com, webinar, February 20, 2015, <https://xebialabs.com/community/webinars/exploring-the-uncharted-territory-of-microservices/> .\n",
      "\n",
      "_The 2015 State_... Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_.\n",
      "\n",
      "__ _Another more extreme_... \"Velocity 2011: Jon Jenkins, 'Velocity Culture',\" YouTube video, 15:13, posted by O'Reilly, June 20, 2011, <https://www.youtube.com/watch?v=dxk8b9rSKOo> ; \"Transforming Software Development,\" YouTube video, 40:57, posted by Amazon Web Service, April 10, 2015, https://www.youtube.com/watch?v=YCrhemssYuI&feature=youtu.be .\n",
      "\n",
      "_Later in his_... Eliyahu M. Goldratt, _Beyond the Goal_.\n",
      "\n",
      "_As with_ The... JGFLL, review of _The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_ , by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 4, 2013, <http://www.amazon.com/review/R1KSSPTEGLWJ23> ; Mark L Townsend, review of _The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_ , by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 2, 2013, http://uedata.amazon.com/gp/customer-reviews/R1097DFODM12VD/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI ; Scott Van Den Elzen, review of _The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_ , by Gene Kim, Kevin Behr, and George Spafford, Amazon.com review, March 13, 2013, http://uedata.amazon.com/gp/customer-reviews/R2K95XEH5OL3Q5/ref=cm_cr_getr_d_rvw_ttl?ie=UTF8&ASIN=B00VATFAMI .\n",
      "\n",
      "###  PART I INTRODUCTION\n",
      "\n",
      "_One key principle_... Kent Beck, et al., \"Twelve Principles of Agile Software,\" AgileManifesto.org, 2001, <http://agilemanifesto.org/principles.html> .\n",
      "\n",
      "_He concluded that_... Mike Rother, _Toyota Kata: Managing People for Improvement, Adaptiveness and Superior Results_ (New York: McGraw Hill, 2010), Kindle edition, Part III.\n",
      "\n",
      "###  CHAPTER 1\n",
      "\n",
      "_Karen Martin and_... Karen Martin and Mike Osterling, _Value Stream Mapping: How to Visualize Work and Align Leadership for Organizational Transformation_ (New York: McGraw Hill, 2013), Kindle edition, chap 1.\n",
      "\n",
      "_In this book_... Ibid., chap. 3.\n",
      "\n",
      "_Karen Martin and_... Ibid.\n",
      "\n",
      "###  CHAPTER 2\n",
      "\n",
      "_Studies have shown_... Joshua S. Rubinstein, David E. Meyer, and Jeffrey E. Evans, \"Executive Control of Cognitive Processes in Task Switching,\" _Journal of Experimental Psychology: Human Perception and Performance_ 27, no. 4 (2001): 763-797, doi: 10.1037//0096-1523.27.4.763, <http://www.umich.edu/~bcalab/documents/RubinsteinMeyerEvans2001.pdf> .\n",
      "\n",
      "_Dominica DeGrandis, one_... \"DOES15—Dominica DeGrandis—The Shape of Uncertainty,\" YouTube video, 22:54, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=Gp05i0d34gg> .\n",
      "\n",
      "_Taiichi Ohno compared_... Sami Bahri, \"Few Patients-In-Process and Less Safety Scheduling; Incoming Supplies are Secondary,\" The Deming Institute Blog, August 22, 2013, <https://blog.deming.org/2013/08/fewer-patients-in-process-and-less-safety-scheduling-incoming-supplies-are-secondary/> .\n",
      "\n",
      "_In other words_... Meeting between David J. Andersen and team at Motorola with Daniel S. Vacanti, February 24, 2004; story retold at USC CSSE Research Review with Barry Boehm in March 2004.\n",
      "\n",
      "_The dramatic differences_... James P. Womack and Daniel T. Jones, _Lean Thinking: Banish Waste and Create Wealth in Your Corporation_ (New York: Free Press, 2010), Kindle edition, chap. 1.\n",
      "\n",
      "_There are many_...  Eric Ries, \"Work in small batches,\" StartupLessonsLearned.com, February 20, 2009, <http://www.startuplessonslearned.com/2009/02/work-in-small-batches.html> .\n",
      "\n",
      "_In_ Beyond the... Goldratt, _Beyond the Goal_.\n",
      "\n",
      "_As a solution_... Eliyahu M. Goldratt, _The Goal: A Process of Ongoing Improvement_ (Great Barrington, MA: North River Press, 2014), Kindle edition, \"Five Focusing Steps.\"\n",
      "\n",
      "_Shigeo Shingo, one_... Shigeo Shingo, _A Study of the Toyota Production System: From an Industrial Engineering Viewpoint_ (London: Productivity Press, 1989); \"The 7 Wastes (Seven forms of Muda),\" BeyondLean.com, accessed July 28, 2016, <http://www.beyondlean.com/7-wastes.html> .\n",
      "\n",
      "_In the book_... Mary Poppendieck and Tom Poppendieck, _Implementing Lean Software: From Concept to Cash_ , (Upper Saddle River, NJ: Addison-Wesley, 2007), 74.\n",
      "\n",
      "_The following categories_... Adapted from Damon Edwards, \"DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,\" Slideshare.net, posted by dev2ops, May 4, 2015, <http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards> .\n",
      "\n",
      "###  CHAPTER 3\n",
      "\n",
      "_Dr. Charles Perrow_... Charles Perrow, _Normal Accidents: Living with High Risk Technologies_ (Princeton, NJ: Princeton University Press, 1999).\n",
      "\n",
      "_Dr. Sidney Dekker_... Dr. Sidney Dekker, _The Field Guide to Understanding Human Error_ (Lund University, Sweden: Ashgate, 2006).\n",
      "\n",
      "_After he decoded_... Spear, _The High-Velocity Edge_ , chap. 8.\n",
      "\n",
      "_Dr. Spear extended_... Ibid.\n",
      "\n",
      "_Dr. Peter Senge_... Peter M. Senge, _The Fifth Discipline: The Art & Practice of the Learning Organization_ (New York: Doubleday, 2006), Kindle edition, chap. 5.\n",
      "\n",
      "_In one well-documented_... \"NUMMI,\" _This American Life_ , March 26, 2010, <http://www.thisamericanlife.org/radio-archives/episode/403/transcript> .\n",
      "\n",
      "_As Elisabeth Hendrickson_... \"DOES15 - Elisabeth Hendrickson - Its All About Feedback,\" YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=r2BFTXBundQ> .\n",
      "\n",
      "_\"In doing so_... Spear, _The High-Velocity Edge_ , chap. 1.\n",
      "\n",
      "_As Dr. Spear_... Ibid., chap. 4.\n",
      "\n",
      "_Examples of ineffective_... Jez Humble, Joanne Molesky, and Barry O'Reilly, _Lean Enterprise: How High Performance Organizations Innovate at Scale_ (Sebastopol, CA: O'Reilly Media, 2015), Kindle edition, Part IV.\n",
      "\n",
      "_In the 1700s_... Dr. Thomas Sowell, _Knowledge and Decisions_ (New York: Basic Books, 1980), 222.\n",
      "\n",
      "_As Gary Gruver_... Gary Gruver, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "###  CHAPTER 4\n",
      "\n",
      "_For instance, in_... Paul Adler, \"Time-and-Motion Regained,\" _Harvard Business Review_ , January-February 1993, <https://hbr.org/1993/01/time-and-motion-regained> .\n",
      "\n",
      "_The \"name, blame_... Dekker, _The Field Guide to Understanding Human Error,_ chap. 1.\n",
      "\n",
      "_Dr. Sidney Dekker_... \"Just Culture: Balancing Safety and Accountability,\" Lund University, Human Factors & System Safety website, November 6, 2015, <http://www.humanfactors.lth.se/sidney-dekker/books/just-culture/> .\n",
      "\n",
      "_He observed that_... Ron Westrum, \"The study of information flow: A personal journey,\" _Proceedings of Safety Science_ 67 (August 2014): 58-63, https://www.researchgate.net/publication/261186680_ The_study_of_information_flow_A_personal_journey .\n",
      "\n",
      "_Just as Dr. Westrum_... Nicole Forsgren Velasquez, Gene Kim, Nigel Kersten, and Jez Humble, _2014 State of DevOps Report_ (Portland, OR: Puppet Labs, IT Revolution Press, and ThoughtWorks, 2014), <http://puppetlabs.com/2014-devops-report> .\n",
      "\n",
      "_As Bethany Macri_... Bethany Macri, \"Morgue: Helping Better Understand Events by Building a Post Mortem Tool - Bethany Macri,\" Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, <http://vimeo.com/77206751> .\n",
      "\n",
      "_Dr. Spear observes_... Spear, _The High-Velocity Edge_ , chap. 1.\n",
      "\n",
      "_In_ The Fifth... Senge, _The Fifth Discipline_ _, chap. 1_.\n",
      "\n",
      "_Mike Rother observed_... Mike Rother, _Toyota Kata_ , 12.\n",
      "\n",
      "_This is why_... Mike Orzen, personal correspondence with Gene Kim, 2012.\n",
      "\n",
      "_Consider the following_... \"Paul O'Neill,\" _Forbes_ , October 11, 2001, <http://www.forbes.com/2001/10/16/poneill.html> .\n",
      "\n",
      "_In 1987, Alcoa_... Spear, _The High-Velocity Edge_ , chap. 4.\n",
      "\n",
      "_As Dr. Spear_... Ibid.\n",
      "\n",
      "_A remarkable example_... Ibid., chap. 5.\n",
      "\n",
      "_This process of_... Nassim Nicholas Taleb, _Antifragile: Things That Gain from Disorder_ (Incerto), (New York: Random House, 2012).\n",
      "\n",
      "_According to Womack_... Jim Womack, Gemba Walks (Cambridge, MA: Lean Enterprise Institute, 2011), Kindle edition, location 4113.\n",
      "\n",
      "_Mike Rother formalized_... Rother, _Toyota Kata_ , Part IV.\n",
      "\n",
      "_Mike Rother observes_... Ibid., Conclusion.\n",
      "\n",
      "###  CHAPTER 5\n",
      "\n",
      "_Therefore, we must_... Michael Rembetsy and Patrick McDonnell, \"Continuously Deploying Culture [at Etsy],\" Slideshare.net, October 4, 2012, posted by Patrick McDonnel.bl, <http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at-etsy-14588485> .\n",
      "\n",
      "_In 2015, Nordstrom_... \"Nordstrom, Inc.,\" company profile on Vault.com, <http://www.vault.com/company-profiles/retail/nordstrom,-inc/company-overview.aspx> .\n",
      "\n",
      "_The stage for_... Courtney Kissler, \"DOES14 - Courtney Kissler - Nordstrom - Transforming to a Culture of Continuous Improvement,\" YouTube video, 29:59, posted by DevOps Enterprise Summit 2014, October 29, 2014, <https://www.youtube.com/watch?v=0ZAcsrZBSlo> .\n",
      "\n",
      "_These organizations were_... Tom Gardner, \"Barnes & Noble, Blockbuster, Borders: The Killer B's Are Dying,\" _The Motley Fool_ , July 21, 2010, <http://www.fool.com/investing/general/2010/07/21/barnes-noble-blockbuster-borders-the-killer-bs-are.aspx> .\n",
      "\n",
      "_As Kissler described_... Kissler, \"DOES14 - Courtney Kissler - Nordstrom.\"\n",
      "\n",
      "_As Kissler said_... Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_As Kissler stated_... Ibid; Alterations to quote made by Courtney Kissler via personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_In 2015, Kissler_... Ibid.\n",
      "\n",
      "_She continued, \"This_... Ibid.\n",
      "\n",
      "_Kissler concluded, \"From_... Ibid.\n",
      "\n",
      "_An example of_... Ernest Mueller, \"Business model driven cloud adoption: what NI Is doing in the cloud,\" Slideshare.net, June 28, 2011, posted by Ernest Mueller, <http://www.slideshare.net/mxyzplk/business-model-driven-cloud-adoption-what-ni-is-doing-in-the-cloud> .\n",
      "\n",
      "_Although many believe_... Unpublished calculation by Gene Kim after the 2014 DevOps Enterprise Summit.\n",
      "\n",
      "_Indeed, one of_... Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_.\n",
      "\n",
      "_CSG (2013): In_... Prugh, \"DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments,\" Slideshare.net, November 14, 2014, posted by DevOps Enterprise Summit, <http://www.slideshare.net/DevOpsEnterpriseSummit/scott-prugh> .\n",
      "\n",
      "_Etsy (2009): In_... Rembetsy and McDonnell, \"Continuously Deploying Culture [at Etsy].\"\n",
      "\n",
      "_The Gartner research_... Bernard Golden, \"What Gartner's Bimodal IT Model Means to Enterprise CIOs,\" _CIO Magazine_ , January 27, 2015, <http://www.cio.com/article/2875803/cio-role/what-gartner-s-bimodal-it-model-means-to-enterprise-cios.html> .\n",
      "\n",
      "_Systems of record_... Ibid.\n",
      "\n",
      "_Systems of engagement_... Ibid.\n",
      "\n",
      "_The data from_... Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_.\n",
      "\n",
      "_Scott Prugh, VP_... Scott Prugh, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Geoffrey A. Moore_... Geoffrey A. Moore and Regis McKenna, _Crossing the Chasm: Marketing and Selling High-Tech Products to Mainstream Customers_ (New York: HarperCollins, 2009), 11.\n",
      "\n",
      "_Big bang, top-down_... Linda Tucci, \"Four Pillars of PayPal's 'Big Bang' Agile Transformation,\" _TechTarget_ , August 2014, <http://searchcio.techtarget.com/feature/Four-pillars-of-PayPals-big-bang-Agile-transformation> .\n",
      "\n",
      "_The following list_... \"Creating High Velocity Organizations,\" description of course by Roberto Fernandez and Steve Spear, MIT Sloan Executive Education website, accessed May 30, 2016, <http://executive.mit.edu/openenrollment/program/organizational-development-high-velocity-organizations> .\n",
      "\n",
      "_But as Ron van Kemenade_... Ron Van Kemande, \"Nothing Beats Engineering Talent: The Agile Transformation at ING,\" presentation at the DevOps Enterprise Summit, London, UK, June 30-July 1, 2016.\n",
      "\n",
      "_Peter Drucker, a_... Leigh Buchanan, \"The Wisdom of Peter Drucker from A to Z,\" _Inc_., November 19, 2009, <http://www.inc.com/articles/2009/11/drucker.html> .\n",
      "\n",
      "###  CHAPTER 6\n",
      "\n",
      "_Over the years_... Kissler, \"DOES14 - Courtney Kissler - Nordstrom.\"\n",
      "\n",
      "_Kissler explained:_... Ross Clanton and Michael Ducy, interview of Courtney Kissler and Jason Josephy, \"Continuous Improvement at Nordstrom,\" _The Goat Farm_ , podcast audio, June 25, 2015, <http://goatcan.do/2015/06/25/the-goat-farm-episode-7-continuous-improvement-at-nordstrom/> .\n",
      "\n",
      "_She said proudly_... Ibid.\n",
      "\n",
      "_Technology executives or_... Brian Maskell, \"What Does This Guy Do? Role of Value Stream Manager,\" _Maskell_ , July 3, 2015, <http://blog.maskell.com/?p=2106http://www.lean.org/common/display/?o=221> .\n",
      "\n",
      "_Damon Edwards observed_... Damon Edwards, \"DevOps Kaizen: Find and Fix What Is Really Behind Your Problems,\" Slideshare.net, posted by dev2ops, May 4, 2015, <http://www.slideshare.net/dev2ops/dev-ops-kaizen-damon-edwards> .\n",
      "\n",
      "_In their book_... Vijay Govindarajan and Chris Trimble, _The Other Side of Innovation: Solving the Execution Challenge_ (Boston, MA: Harvard Business Review, 2010) Kindle edition.\n",
      "\n",
      "_Based on their_... Ibid., Part I.\n",
      "\n",
      "_After the near-death_... Marty Cagan, _Inspired: How to Create Products Customers Love_ (Saratoga, CA: SVPG Press, 2008), 12.\n",
      "\n",
      "_Cagan notes that_... Ibid.\n",
      "\n",
      "_Six months after_... Ashlee Vance, \"LinkedIn: A Story About Silicon Valley's Possibly Unhealthy Need for Speed,\" _Bloomberg_ , April 30, 2013, <http://www.bloomberg.com/bw/articles/2013-04-29/linkedin-a-story-about-silicon-valleys-possibly-unhealthy-need-for-speed> .\n",
      "\n",
      "_LinkedIn was created_... \"LinkedIn started back in 2003 — LinkedIn - A Brief History,\" Slideshare.net, posted by Josh Clemm, November 9, 2015, <http://www.slideshare.net/joshclemm/how-linkedin-scaled-a-brief-history/3-LinkedIn_started_back_in_2003> .\n",
      "\n",
      "_One year later_... Jonas Klit Nielsen, \"8 Years with LinkedIn – Looking at the Growth [Infographic],\" MindJumpers.com, May 10, 2011, <http://www.mindjumpers.com/blog/2011/05/linkedin-growth-infographic/> .\n",
      "\n",
      "_By November 2015_...  \"LinkedIn started back in 2003,\" Slideshare.net.\n",
      "\n",
      "_The problem was_... \"From a Monolith to Microservices + REST: The Evolution of LinkedIn's Architecture,\" Slideshare.net, posted by Karan Parikh, November 6, 2014,<http://www.slideshare.net/parikhk/restli-and-deco> .\n",
      "\n",
      "_ Josh Clemm, a_... \"LinkedIn started back in 2003,\" Slideshare.net.\n",
      "\n",
      "_In 2013, journalist_... Vance, \"LinkedIn: A Story About,\" _Bloomberg_.\n",
      "\n",
      "_Scott launched Operation_... \"How I Structured Engineering Teams at LinkedIn and AdMob for Success,\" _First Round Review_ , 2015,<http://firstround.com/review/how-i-structured-engineering-teams-at-linkedin-and-admob-for-success/> .\n",
      "\n",
      "_Scott described one_... Ashlee Vance, \"Inside Operation InVersion, the Code Freeze that Saved LinkedIn,\" _Bloomberg_ , April 11, 2013, <http://www.bloomberg.com/news/articles/2013-04-10/inside-operation-inversion-the-code-freeze-that-saved-linkedin> .\n",
      "\n",
      "_However, Vance described_... Vance, \"LinkedIn: A Story About,\" _Bloomberg_.\n",
      "\n",
      "_As Josh Clemm_... \"LinkedIn started back in 2003,\" Slideshare.net.\n",
      "\n",
      "_Kevin Scott stated_... \"How I Structured Engineering Teams,\" _First Round Review_.\n",
      "\n",
      "_As Christopher Little_... Christopher Little, personal correspondence with Gene Kim, 2011.\n",
      "\n",
      "_As Ryan Martens_... Ryan Martens, personal correspondence with Gene Kim, 2013.\n",
      "\n",
      "### CHAPTER 7\n",
      "\n",
      "_He observed, \"After_... Dr. Melvin E. Conway, \"How Do Committees Invent?\" MelConway.com, <http://www.melconway.com/research/committees.html> , previously published in _Datamation_ , April 1968.\n",
      "\n",
      "_These observations led_... Ibid.\n",
      "\n",
      "_Eric S. Raymond, author_... Eric S. Raymond, \"Conway's Law,\" catb.org, accessed May 31, 2016, <http://catb.org/~esr/jargon/> .\n",
      "\n",
      "_Etsy's DevOps journey_... Sarah Buhr, \"Etsy Closes Up 86 Percent on First Day of Trading,\" _Tech Crunch_ , April 16, 2015, <http://techcrunch.com/2015/04/16/etsy-stock-surges-86-percent-at-close-of-first-day-of-trading-to-30-per-share/> .\n",
      "\n",
      "_As Ross Snyder_... \"Scaling Etsy: What Went Wrong, What Went Right,\" Slideshare.net, posted by Ross Snyder, October 5, 2011, <http://www.slideshare.net/beamrider9/scaling-etsy-what-went-wrong-what-went-right> .\n",
      "\n",
      "_As Snyder observed_... Ibid.\n",
      "\n",
      "_In other words_... Sean Gallagher, \"When 'Clever' Goes Wrong: How Etsy Overcame Poor Architectural Choices,\" _Arstechnica_ , October 3, 2011, <http://arstechnica.com/business/2011/10/when-clever-goes-wrong-how-etsy-overcame-poor-architectural-choices/> .\n",
      "\n",
      "_ Snyder explained that_... \"Scaling Etsy\" Slideshare.net.\n",
      "\n",
      "_Etsy initially had_... Ibid.\n",
      "\n",
      "_In the spring_... Ibid.\n",
      "\n",
      "_As Snyder described_... Ross Snyder, \"Surge 2011—Scaling Etsy: What Went Wrong, What Went Right,\" YouTube video, posted by Surge Conference, December 23, 2011, <https://www.youtube.com/watch?v=eenrfm50mXw> .\n",
      "\n",
      "_As Snyder said_... Ibid.\n",
      "\n",
      "_Sprouter was one_... \"Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,\" Slideshare.net, posted by Patrick McDonnell, October 4, 2012, <http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at-etsy-14588485> .\n",
      "\n",
      "_They are defined_... \"Creating High Velocity Organizations,\" description of course by Roberto Fernandez and Steven Spear.\n",
      "\n",
      "_Adrian Cockcroft remarked_... Adrian Cockcroft, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_In the Lean_... Spear, _The High-Velocity Edge_ , chap. 8.\n",
      "\n",
      "_As Mike Rother_... Rother, _Toyota Kata_ , 250.\n",
      "\n",
      "_Reflecting on shared_... \"DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,\" YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=USYrDaPEFtM> .\n",
      "\n",
      "_He continued, \"The_... Ibid.\n",
      "\n",
      "_Pedro Canahuati, their_... Pedro Canahuati, \"Growing from the Few to the Many: Scaling the Operations Organization at Facebook,\" _InfoQ_ , December 16, 2013, <http://www.infoq.com/presentations/scaling-operations-facebook> .\n",
      "\n",
      "_When departments over-specialize_... Spear, _The High-Velocity Edge_ , chap. 1.\n",
      "\n",
      "_Scott Prugh writes_... Scott Prugh, \"Continuous Delivery,\" Scaled Agile Framework, updated February 14, 2013, <http://www.scaledagileframework.com/continuous-delivery/> .\n",
      "\n",
      "\" _By cross-training_...  Ibid.\n",
      "\n",
      "_\"Traditional managers will_... Ibid.\n",
      "\n",
      "_Furthermore, as Prugh_... Ibid.\n",
      "\n",
      "_When we value_... Dr. Carol Dweck, \"Carol Dweck Revisits the 'Growth Mindset,'\" _Education Week_ , September 22, 2015, <http://www.edweek.org/ew/articles/2015/09/23/carol-dweck-revisits-the-growth-mindset.html> .\n",
      "\n",
      "_As Jason Cox_... Jason Cox, \"Disney DevOps: To Infinity and Beyond,\" presentation at DevOps Enterprise Summit 2014, San Francisco, CA, October 2014.\n",
      "\n",
      "_As John Lauderbach_... John Lauderbach, personal conversation with Gene Kim, 2001.\n",
      "\n",
      "_These properties are_... Tony Mauro, \"Adopting Microservices at Netflix: Lessons for Architectural Design,\" _NGINX_ , February 19, 2015, <https://www.nginx.com/blog/microservices-at-netflix-architectural-best-practices/> .; Adam Wiggins, \"The Twelve-Factor App,\" 12Factor.net, January 30, 2012, <http://12factor.net/> .\n",
      "\n",
      "_Randy Shoup, former_... \"Exploring the Uncharted Territory of Microservices,\" YouTube video, 56:50, posted by XebiaLabs, Inc., February 20, 2015, <https://www.youtube.com/watch?v=MRa21icSIQk> .\n",
      "\n",
      "_As part of_... Humble, O'Reilly, and Molesky, _Lean Enterprise_ , Part III.\n",
      "\n",
      "_In the Netflix_... Reed Hastings, \"Netflix Culture: Freedom and Responsibility,\" Slideshare.net, August 1, 2009, <http://www.slideshare.net/reed2001/culture-1798664> .\n",
      "\n",
      "_Amazon CTO Werner_... Larry Dignan, \"Little Things Add Up,\" _Baseline_ , October 19, 2005, <http://www.baselinemag.com/c/a/Projects-Management/Profiles-Lessons-From-the-Leaders-in-the-iBaselinei500/3> .\n",
      "\n",
      "_Target is the_... Heather Mickman and Ross Clanton, \"DOES15 - Heather Mickman & Ross Clanton - (Re)building an Engineering Culture: DevOps at Target,\" YouTube video, 33:39, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=7s-VbB1fG5o> .\n",
      "\n",
      "_As Mickman described_... Ibid.\n",
      "\n",
      "_In an attempt_... Ibid.\n",
      "\n",
      "_Because our team_... Ibid.\n",
      "\n",
      "_In the following_... Ibid.\n",
      "\n",
      "_These changes have_... Ibid.\n",
      "\n",
      "_The API Enablement_... Ibid.\n",
      "\n",
      "###  CHAPTER 8\n",
      "\n",
      "_At Big Fish_... \"Big Fish Celebrates 11th Consecutive Year of Record Growth,\" BigFishGames.com, January 28, 2014, <http://pressroom.bigfishgames.com/2014-01-28-Big-Fish-Celebrates-11th-Consecutive-Year-of-Record-Growth> .\n",
      "\n",
      "_He observed that_... Paul Farrall, personal correspondence with Gene Kim, January 2015.\n",
      "\n",
      "_Farrall defined two_... Ibid., 2014.\n",
      "\n",
      "_He concludes, \"The_... Ibid.\n",
      "\n",
      "_Ernest Mueller observed_... Ernest Mueller, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_As Damon Edwards_... Edwards, \"DevOps Kaizen.\"\n",
      "\n",
      "_Dianne Marsh, Director_... \"Dianne Marsh 'Introducing Change while Preserving Engineering Velocity,\" YouTube video, 17:37, posted by Flowcon, November 11, 2014, <https://www.youtube.com/watch?v=eW3ZxY67fnc> .\n",
      "\n",
      "_Jason Cox said_... Jason Cox, \"Disney DevOps.\"\n",
      "\n",
      "_At Etsy, this_... \"devopsdays Minneapolis 2015 - Katherine Daniels - DevOps: The Missing Pieces,\" YouTube video, 33:26, posted by DevOps Minneapolis, July 13, 2015, <https://www.youtube.com/watch?v=LNJkVw93yTU> .\n",
      "\n",
      "_As Ernest Mueller_... Ernest Mueller, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_Scrum is an agile_... Hirotaka Takeuchi and Ikujiro Nonaka, \"New Product Development Game,\" _Harvard Business Review_ (January 1986): 137-146.\n",
      "\n",
      "###  CHAPTER 9\n",
      "\n",
      "_In her presentation_... Em Campbell-Pretty, \"DOES14 - Em Campbell-Pretty - How a Business Exec Led Agile, Lead, CI/CD,\" YouTube video, 29:47, posted by DevOps Enterprise Summit, April 20, 2014, <https://www.youtube.com/watch?v=-4pIMMTbtwE> .\n",
      "\n",
      "_Campbell-Pretty became_... Ibid.\n",
      "\n",
      "_They created a_... Ibid.\n",
      "\n",
      "_Campbell-Pretty observed_... Ibid.\n",
      "\n",
      "_Campbell-Pretty described_... Ibid.\n",
      "\n",
      "_The first version_... \"Version Control History,\" PlasticSCM.com, accessed May 31, 2016, <https://www.plasticscm.com/version-control-history.html> .\n",
      "\n",
      "_A version control_... Jennifer Davis and Katherine Daniels, Effective DevOps: Building a Culture of Collaboration, Affinity, and Tooling at Scale (Sebastopol, CA: O'Reilly Media, 2016), 37.\n",
      "\n",
      "_Bill Baker, a_... Simon Sharwood, \"Are Your Servers PETS or CATTLE?,\" _The Register_ , March 18 2013, <http://www.theregister.co.uk/2013/03/18/servers_pets_or_cattle_cern/> .\n",
      "\n",
      "_At Netflix, the_... Jason Chan, \"OWASP AppSecUSA 2012: Real World Cloud Application Security,\" YouTube video, 37:45, posted by Christiaan008, December 10, 2012, <https://www.youtube.com/watch?v=daNA0jXDvYk> .\n",
      "\n",
      "_The latter pattern_... Chad Fowler, \"Trash Your Servers and Burn Your Code: Immutable Infrastructure and Disposable Components,\" ChadFowler.com, June 23, 2013, <http://chadfowler.com/2013/06/23/immutable-deployments.html> .\n",
      "\n",
      "_The entire application_... John Willis, \"Docker and the Three Ways of DevOps Part 1: The First Way—Systems Thinking,\" _Docker_ , May 26, 2015, <https://blog.docker.com/2015/05/docker-three-ways-devops/> .\n",
      "\n",
      "###  CHAPTER 10\n",
      "\n",
      "_Gary Gruver, former_... Gary Gruver, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_They had problems_... \"DOES15 - Mike Bland - Pain Is Over, If You Want It,\" Slideshare.net, posted by Gene Kim, November 18, 2015, <http://www.slideshare.net/ITRevolution/does15-mike-bland-pain-is-over-if-you-want-it-55236521> .\n",
      "\n",
      "_Bland describes how_... Ibid.\n",
      "\n",
      "_Bland described that_... Ibid.\n",
      "\n",
      "_As Bland describes_... Ibid.\n",
      "\n",
      "_As Bland notes_... Ibid.\n",
      "\n",
      "_Over the next_... Ibid.\n",
      "\n",
      "_Eran Messeri, an_... Eran Messeri, \"What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?,\" presentation at the GOTO Conference, Aarhus, Denmark, October 2, 2013.\n",
      "\n",
      "_Messeri explains, \"There_... Ibid.\n",
      "\n",
      "_All their code_... Ibid.\n",
      "\n",
      "_Some of the_... Ibid.\n",
      "\n",
      "_In Development, continuous_... Jez Humble and David Farley, personal correspondence with Gene Kim, 2012.\n",
      "\n",
      "_The deployment pipeline_... Jez Humble and David Farley, _Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation_ (Upper Saddle River, NJ: Addison-Wesly, 2011), 3.\n",
      "\n",
      "_Humble and Farley_... Ibid., 188.\n",
      "\n",
      "_As Humble and_... Ibid., 258.\n",
      "\n",
      "_Martin Fowler observes_... Martin Fowler, \"Continuous Integration,\" MartinFowler.com, May 1, 2006, <http://www.martinfowler.com/articles/continuousIntegration.html> .\n",
      "\n",
      "_Martin Fowler described_... Martin Fowler, \"TestPyramid,\" MartinFowler.com, May 1, 2012, <http://martinfowler.com/bliki/TestPyramid.html> .\n",
      "\n",
      "_This technique was_ .. Martin Fowler, \"Test Driven Development,\" MartinFowler.com, March 5, 2005, <http://martinfowler.com/bliki/TestDrivenDevelopment.html> .\n",
      "\n",
      "_Nachi Nagappan, E. Michael_... Nachiappan Nagappan, E. Michael Maximilien, Thirumalesh Bhat, and Laurie Williams, \"Realizing quality improvement through test driven development: results and experiences of four industrial teams,\" _Empir Software Engineering_ , 13, (2008): 289-302, <http://research.microsoft.com/en-us/groups/ese/nagappan_tdd.pdf> .\n",
      "\n",
      "_In her 2013_... Elisabeth Hendrickson, \"On the Care and Feeding of Feedback Cycles,\" Slideshare.net, posted by Elisabeth Hendrickson, November 1, 2013, <http://www.slideshare.net/ehendrickson/care-and-feeding-of-feedback-cycles> .\n",
      "\n",
      "_However, merely automating_... \"Decreasing false positives in automated testing,\" Slideshare.net, posted by Sauce Labs, March 24, 2015, <http://www.slideshare.net/saucelabs/decreasing-false-positives-in-automated-testing> .; Martin Fowler, \"Eradicating Non-determinism in Tests,\" MartinFowler.com, April 14, 2011, <http://martinfowler.com/articles/nonDeterminism.html> .\n",
      "\n",
      "_As Gary Gruver_... Gary Gruver, \"DOES14 - Gary Gruver - Macy's - Transforming Traditional Enterprise Software Development Processes,\" YouTube video, 27:24, posted by DevOps Enterprise Summit 2014, October 29, 2014, <https://www.youtube.com/watch?v=-HSSGiYXA7U> .\n",
      "\n",
      "_Randy Shoup, former_... Randy Shoup, \"The Virtuous Cycle of Velocity: What I Learned About Going Fast at eBay and Google by Randy Shoup,\" YouTube video, 30:05, posted by Flowcon, December 26, 2013, <https://www.youtube.com/watch?v=EwLBoRyXTOI> .\n",
      "\n",
      "_This is sometimes_... David West, \"Water scrum-fall is-reality_of_agile_for_most,\" Slideshare.net, posted by harsoft, April 22, 2013, <http://www.slideshare.net/harsoft/water-scrumfall-isrealityofagileformost> .\n",
      "\n",
      "###  CHAPTER 11\n",
      "\n",
      "_The surprising breadth_... Gene Kim, \"The Amazing DevOps Transformation of the HP LaserJet Firmware Team (Gary Gruver),\" ITRevolution.com, 2013, <http://itrevolution.com/the-amazing-devops-transformation-of-the-hp-laserjet-firmware-team-gary-gruver/> .\n",
      "\n",
      "_Gruver described this_... Ibid.\n",
      "\n",
      "_Compile flags (#define_... Ibid.\n",
      "\n",
      "_Gruver admits trunk-based_... Gary Gruver and Tommy Mouser, _Leading the Transformation: Applying Agile and DevOps Principles at Scale_ (Portland, OR: IT Revolution Press), 60.\n",
      "\n",
      "_Gruver observed, \"Without_... Kim, \"The Amazing DevOps Transformation \" ITRevolution.com.\n",
      "\n",
      "_Jeff Atwood, founder_... Jeff Atwood, \"Software Branching and Parallel Universes,\" CodingHorror.com, October 2, 2007, <http://blog.codinghorror.com/software-branching-and-parallel-universes/> .\n",
      "\n",
      "_This is how_... Ward Cunningham, \"Ward Explains Debt Metaphor,\" c2.com, 2011, <http://c2.com/cgi/wiki?WardExplainsDebtMetaphor> .\n",
      "\n",
      "_Ernest Mueller, who_... Ernest Mueller, \"2012: A Release Odyssey,\" Slideshare.net, posted by Ernest Mueller, March 12, 2014, <http://www.slideshare.net/mxyzplk/2012-a-release-odyssey> .\n",
      "\n",
      "_At that time_... \"Bazaarvoice, Inc. Announces Its Financial Results for the Fourth Fiscal Quarter and Fiscal Year Ended April 30, 2012,\" BasaarVoice.com, June 6, 2012, <http://investors.bazaarvoice.com/releasedetail.cfm?ReleaseID=680964> .\n",
      "\n",
      "_Mueller observed, \"It_... Ernest Mueller, \"DOES15 - Ernest Mueller - DevOps Transformations At National Instruments and...,\" YouTube video, 34:14, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=6Ry40h1UAyE> .\n",
      "\n",
      "_\"By running these_... Ibid.\n",
      "\n",
      "_Mueller further described_... Ibid.\n",
      "\n",
      "_However, the data_... Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_.\n",
      "\n",
      "###  CHAPTER 12\n",
      "\n",
      "_In 2012, Rossi_... Chuck Rossi, \"Release engineering and push karma: Chuck Rossi,\" post on Chuck Rossi's Facebook page, April 5, 2012, <https://www.facebook.com/notes/facebook-engineering/release-engineering-and-push-karma-chuck-rossi/10150660826788920> .\n",
      "\n",
      "_Just prior to_... Ryan Paul, \"Exclusive: a behind-the-scenes look at Facebook release engineering,\" _Ars Technica_ , April 5, 2012, <http://arstechnica.com/business/2012/04/exclusive-a-behind-the-scenes-look-at-facebook-release-engineering/1/> .\n",
      "\n",
      "_Rossi continued, \"If_... Chuck Rossi, \"Release engineering and push karma.\"\n",
      "\n",
      "_The Facebook front-end_... Paul, \"Exclusive: a behind-the-scenes look at Facebook release engineering,\" _Ars Technica_.\n",
      "\n",
      "_He explained that_... Chuck Rossi, \"Ship early and ship twice as often,\" post on Chuck Rossi's Facebook page, August 3, 2012, <https://www.facebook.com/notes/facebook-engineering/ship-early-and-ship-twice-as-often/10150985860363920> .\n",
      "\n",
      "_Kent Beck, the_ .. Kent Beck, \"Slow Deployment Causes Meetings,\" post on Kent Beck's Facebook page, November 19, 2015), <https://www.facebook.com/notes/kent-beck/slow-deployment-causes-meetings/1055427371156793?_rdr=p> .\n",
      "\n",
      "_Scott Prugh, their_... Prugh, \"DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.\"\n",
      "\n",
      "_Prugh observed, \"It_... Ibid.\n",
      "\n",
      "_Prugh writes, \"We_... Ibid.\n",
      "\n",
      "_Prugh also observes:_... Ibid.\n",
      "\n",
      "_In their experiments_... Puppet Labs and IT Revolution Press, _2013 State of DevOps Report_ (Portland, OR: Puppet Labs, 2013), <http://www.exin-library.com/Player/eKnowledge/2013-state-of-devops-report.pdf> .\n",
      "\n",
      "_Prugh reported that_... Scott Prugh and Erica Morrison, \"DOES15 - Scott Prugh & Erica Morrison - Conway & Taylor Meet the Strangler (v2.0),\" YouTube video, 29:39, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=tKdIHCL0DUg> .\n",
      "\n",
      "_Consider the following_... Tim Tischler, personal conversation with Gene Kim, FlowCon 2013.\n",
      "\n",
      "_In practice, the_... Puppet Labs and IT Revolution Press, _2013 State of DevOps Report._\n",
      "\n",
      "_In Puppet Labs'_... Velasquez, Kim, Kersten, and Humble, _2014 State of DevOps Report_.\n",
      "\n",
      "_The deployment process_... Chad Dickerson, \"Optimizing for developer happiness,\" CodeAsCraft.com, June 6, 2011, <https://codeascraft.com/2011/06/06/optimizing-for-developer-happiness/> .\n",
      "\n",
      "_As Noah Sussman_... Noah Sussman and Laura Beth Denker, \"Divide and Conquer,\" CodeAsCraft.com, April 20, 2011, <https://codeascraft.com/2011/04/20/divide-and-concur/> .\n",
      "\n",
      "_Sussman writes, \"Through_... Ibid.\n",
      "\n",
      "_If all the tests_... Ibid.\n",
      "\n",
      "_Once it is an_... Erik Kastner, \"Quantum of Deployment,\" CodeAsCraft.com, May 20, 2010, <https://codeascraft.com/2010/05/20/quantum-of-deployment/> .\n",
      "\n",
      "_This technique was_... Timothy Fitz, \"Continuous Deployment at IMVU: Doing the impossible fifty times a day,\" TimothyFitz.com, February 10, 2009, <http://timothyfitz.com/2009/02/10/continuous-deployment-at-imvu-doing-the-impossible-fifty-times-a-day/> .\n",
      "\n",
      "_This pattern is_... Fitz, \"Continuous Deployment,\" TimothyFitz.com.; Michael Hrenko, \"DOES15 - Michael Hrenko - DevOps Insured By Blue Shield of California,\" YouTube video, 42:24, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=NlgrOT24UDw> .\n",
      "\n",
      "_Dan North and Dave_... Humble and Farley, _Continuous Delivery_ , 265.\n",
      "\n",
      "_The cluster immune_... Eric Ries, _The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses_ (New York: Random House, 2011), Audiobook.\n",
      "\n",
      "_One sophisticated example_... Andrew 'Boz' Bosworth, \"Building and testing at Facebook,\" post on Boz Facebook page, August 8, 2012, <https://www.facebook.com/notes/facebook-engineering/building-and-testing-at-facebook/10151004157328920> ; \"Etsy's Feature flagging API used for operational rampups and A/B testing,\" GitHub.com, <https://github.com/etsy/feature> ; \"Library for configuration management API,\" GitHub.com, <https://github.com/Netflix/archaius> .\n",
      "\n",
      "_In 2009, when_... John Allspaw, \"Convincing management that cooperation and collaboration was worth it,\" KitchenSoap.com, January 5, 2012, <http://www.kitchensoap.com/2012/01/05/convincing-management-that-cooperation-and-collaboration-was-worth-it/> .\n",
      "\n",
      "_Similarly, as Chuck_... Rossi, \"Release engineering and push karma.\"\n",
      "\n",
      "_For nearly a decade_... Emil Protalinski, \"Facebook passes 1.55B monthly active users and 1.01B daily active users,\" _Venture Beat_ , November 4, 2015, <http://venturebeat.com/2015/11/04/facebook-passes-1-55b-monthly-active-users-and-1-01-billion-daily-active-users/> .\n",
      "\n",
      "_By 2015, Facebook_... Ibid.\n",
      "\n",
      "_Eugene Letuchy, an_... Eugene Letuchy, \"Facebook Chat,\" post on Eugene Letuchy's Facebook page, May 3, 2008, http://www.facebook.com/note.php?note_id=14218138919&id=944554719 .\n",
      "\n",
      "_Implementing this computationally-intensive_... Ibid.\n",
      "\n",
      "_As Letuchy wrote_... Ibid.\n",
      "\n",
      "_However, in 2015_... Jez Humble, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_His updated definitions_... Ibid.\n",
      "\n",
      "_At Amazon and_... Ibid.\n",
      "\n",
      "###  CHAPTER 13\n",
      "\n",
      "_This is the_... Jez Humble, \"What is Continuous Delivery,\" ContinuousDelivery.com, accessed May 28, 2016, <https://continuousdelivery.com/> .\n",
      "\n",
      "_He observes that_... Kim, Gruver, Shoup, and Phillips, \"Exploring the Uncharted Territory of Microservices.\"\n",
      "\n",
      "_He reflects, \"Looking_... Ibid.\n",
      "\n",
      "_eBay's architecture went_... Shoup, \"From Monolith to Micro-services.\"\n",
      "\n",
      "_Charles Betz, author_... Charles Betz, _Architecture and Patterns for IT Service Management, Resource Planning, and Governance: Making Shoes for the Cobbler's Children_ (Witham, MA: Morgan Kaufmann, 2011), 300.\n",
      "\n",
      "_As Randy Shoup_... Randy Shoup, \"From the Monolith to Micro-services,\" Slideshare.net, posted by Randy Shoup, October 8, 2014, <http://www.slideshare.net/RandyShoup/goto-aarhus2014-enterprisearchitecturemicroservices> .\n",
      "\n",
      "_Shoup notes, \"Organizations_... Ibid.\n",
      "\n",
      "_As Randy Shoup observes_... Ibid.\n",
      "\n",
      "_One of the most_... Werner Vogels, \"A Conversation with Werner Vogels,\" _acmqueque_ 4, no. 4 (2006): 14-22, <http://queue.acm.org/detail.cfm?id=1142065> .\n",
      "\n",
      "_Vogels tells Gray_... Ibid.\n",
      "\n",
      "_Describing the thought_... Ibid.\n",
      "\n",
      "_Vogels notes, \"The_... Ibid.\n",
      "\n",
      "_In 2011, Amazon_... John Jenkins, \"Velocity 2011: Jon Jenkins, \"Velocity Culture,\"\" YouTube video, 15:13, posted by O'Reilly, June 20, 2011, {<https://www.youtube.com/watch?v=dxk8b9rSKOo> .\n",
      "\n",
      "_By 2015, they_... Ken Exner, \"Transforming Software Development,\" YouTube video, 40:57, posted by Amazon Web Services, April 10, 2015, https://www.youtube.com/watch?v=YCrhemssYuI&feature=youtu.be .\n",
      "\n",
      "_The term_ strangler... Martin Fowler, \"StranglerApplication,\" MartinFowler.com, June 29, 2004, <http://www.martinfowler.com/bliki/StranglerApplication.html> .\n",
      "\n",
      "_When we implement_... Boris Lublinsky, \"Versioning in SOA,\" _The Architecture Journal_ , April 2007, <https://msdn.microsoft.com/en-us/library/bb491124.aspx> .\n",
      "\n",
      "_The strangler application_... Paul Hammant, \"Introducing Branch by Abstraction,\" PaulHammant.com, April 26, 2007, <http://paulhammant.com/blog/branch_by_abstraction.html> .\n",
      "\n",
      "_An observation from_... Martin Fowler, \"StranglerApplication,\" MartinFowler.com, June 29, 2004, <http://www.martinfowler.com/bliki/StranglerApplication.html> .\n",
      "\n",
      "_Blackboard Inc., is_... Gregory T. Huang, \"Blackboard CEO Jay Bhatt on the Global Future of Edtech,\" _Xconomy,_ June 2, 2014, <http://www.xconomy.com/boston/2014/06/02/blackboard-ceo-jay-bhatt-on-the-global-future-of-edtech/> .\n",
      "\n",
      "_As David Ashman_... David Ashman, \"DOES14 - David Ashman - Blackboard Learn - Keep Your Head in the Clouds,\" YouTube video, 30:43, posted by DevOps Enterprise Summit 2014, October 28, 2014, <https://www.youtube.com/watch?v=SSmixnMpsI4> .\n",
      "\n",
      "_In 2010, Ashman_... Ibid.\n",
      "\n",
      "_How this started_... David Ashman, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Ashman noted, \"To_... Ibid.\n",
      "\n",
      "_\"In fact,\" Ashman_... Ibid.\n",
      "\n",
      "_Ashman concluded, \"Having_... Ibid.\n",
      "\n",
      "###  CHAPTER 14\n",
      "\n",
      "_In Operations, we_... Kim, Behr, and Spafford, _The Visible Ops Handbook: Implementing ITIL in 4 Practical and Auditable Steps_ (Eugene, OR: IT Process Institute, 2004), Kindle edition, __Introduction.\n",
      "\n",
      "_In contrast, the_... Ibid.\n",
      "\n",
      "_In other words_... Ibid.\n",
      "\n",
      "_To enable this_... \"Telemetry,\" _Wikipedia_ , last modified May 5, 2016, <https://en.wikipedia.org/wiki/Telemetry> .\n",
      "\n",
      "_McDonnell described how_... Michael Rembetsy and Patrick McDonnell, \"Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,\" Slideshare.net, posted by Patrick McDonnell, October 4, 2012, <http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at-etsy-14588485> .\n",
      "\n",
      "_McDonnell explained further_... Ibid.\n",
      "\n",
      "_By 2011, Etsy_... John Allspaw, personal conversation with Gene Kim, 2014.\n",
      "\n",
      "_As Ian Malpass_... Ian Malpass, \"Measure Anything, Measure Everything,\" CodeAsCraft.com, February 15, 2011, <http://codeascraft.com/2011/02/15/measure-anything-measure-everything/> .\n",
      "\n",
      "_One of the findings_... Kersten, IT Revolution, and PwC, _2015 State of DevOps Report_.\n",
      "\n",
      "_The top two_... \"2014 State Of DevOps Findings! Velocity Conference,\" Slideshare.net, posted by Gene Kim, June 30, 2014, <http://www.slideshare.net/realgenekim/2014-state-of-devops-findings-velocity-conference> .\n",
      "\n",
      "_In_ The Art... James Turnbull, _The Art of Monitoring_ (Seattle, WA: Amazon Digital Services, 2016), Kindle edition, Introduction.\n",
      "\n",
      "_The resulting capability_... _\"Monitorama - Please, no more Minutes, Milliseconds, Monoliths or Monitoring Tools,\" Slideshare.net, posted by Adrian Cockcroft, May 5, 2014,_ <http://www.slideshare.net/adriancockcroft/monitorama-please-no-more> .\n",
      "\n",
      "_Scott Prugh, Chief_... Prugh, \"DOES14: Scott Prugh, CSG - DevOps and Lean in Legacy Environments.\"\n",
      "\n",
      "_To support these_... Brice Figureau, \"The 10 Commandments of Logging,\" Mastersen's Blog, January 13, 2013, <http://www.masterzen.fr/2013/01/13/the-10-commandments-of-logging/> .\n",
      "\n",
      "_Choosing the right_... Dan North, personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_To help ensure_... Anton Chuvakin, \"LogLogic/Chuvakin Log Checklist,\" republished with permission, 2008, <http://juliusdavies.ca/logging/llclc.html> .\n",
      "\n",
      "_In 2004, Kim_... Kim, Behr, and Spafford, _The Visible Ops Handbook_ , Introduction.\n",
      "\n",
      "_This was the_... Dan North, \"Ops and Operability,\" SpeakerDeck.com, February 25, 2016, <https://speakerdeck.com/tastapod/ops-and-operability> .\n",
      "\n",
      "_As John Allspaw_... John Allspaw, personal correspondence with Gene Kim, 2011.\n",
      "\n",
      "_This is often_... \"Information Radiators,\" AgileAlliance.com, accessed May 31, 2016, <https://www.agilealliance.org/glossary/incremental-radiators/> .\n",
      "\n",
      "_Although there may_... Ernest Mueller, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Prachi Gupta, Director_... Prachi Gupta, \"Visualizing LinkedIn's Site Performance,\" LinkedIn Engineering blog, June 13, 2011, <https://engineering.linkedin.com/25/visualizing-linkedins-site-performance> .\n",
      "\n",
      "_Thus began Eric_... Eric Wong, \"Eric the Intern: the Origin of InGraphs,\" LinkedIn, June 30, 2011, <http://engineering.linkedin.com/32/eric-intern-origin-ingraphs> .\n",
      "\n",
      "_Wong wrote, \"To_... Ibid.\n",
      "\n",
      "_At the time_... Ibid.\n",
      "\n",
      "_In writing about_... Gupta, \"Visualizing LinkedIn's Site Performance.\"\n",
      "\n",
      "_Ed Blankenship, Senior_... Ed Blankenship, personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_However, increasingly these_... Mike Burrows, \"The Chubby lock service for loosely-coupled distributed systems,\" _OSDI'06: Seventh Symposium on Operating System Design and Implementation_ , November 2006, <http://static.googleusercontent.com/media/research.google.com/en//archive/chubby-osdi06.pdf> .\n",
      "\n",
      "_Consul may be_... Jeff Lindsay, \"Consul Service Discovery with Docker,\" Progrium.com, August 20, 2014, <http://progrium.com/blog/2014/08/20/consul-service-discovery-with-docker> .\n",
      "\n",
      "_As Jody Mulkey_... Jody Mulkey, \"DOES15 - Jody Mulkey - DevOps in the Enterprise: A Transformation Journey,\" YouTube video, 28:22, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=USYrDaPEFtM> .\n",
      "\n",
      "###  CHAPTER 15\n",
      "\n",
      "_In 2015, Netflix_... Netflix Letter to Shareholders _,_ January 19, 2016, <http://files.shareholder.com/downloads/NFLX/2432188684x0x870685/C6213FF9-5498-4084-A0FF-74363CEE35A1/Q4_15_Letter_to_Shareholders_-_COMBINED.pdf> .\n",
      "\n",
      "_Roy Rapoport describes_... Roy Rapoport, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_One of the statistical_... Victoria Hodge and Jim Austin, \"A Survey of Outlier Detection Methodologies,\" _Artificial Intelligence Review_ 22, no. 2 (October 2004): 85-126, http://www.geo.upm.es/postgrado/CarlosLopez/ papers/Hodge+Austin_OutlierDetection_AIRE381.pdf .\n",
      "\n",
      "_Rapoport explains that_... Roy Rapoport, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Rapoport continues, \"We_... Ibid.\n",
      "\n",
      "_Rapoport states that_... Ibid.\n",
      "\n",
      "_As John Vincent_... Toufic Boubez, \"Simple math for anomaly detection toufic boubez - metafor software - monitorama pdx 2014-05-05,\" Slideshare.net, posted by tboubez, May 6, 2014, <http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-boubez-metafor-software-monitorama-pdx-20140505> .\n",
      "\n",
      "_Tom Limoncelli, co-author_... Tom Limoncelli, \"Stop monitoring whether or not your service is up!,\" EverythingSysAdmin.com, November 27, 2013, <http://everythingsysadmin.com/2013/11/stop-monitoring-if-service-is-up.html> .\n",
      "\n",
      "_As Dr. Toufic_... Toufic Boubez, \"Simple math for anomaly detection toufic boubez - metafor software - monitorama pdx 2014-05-05,\" Slideshare.net, posted by tboubez, May 6, 2014, <http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection-toufic-boubez-metafor-software-monitorama-pdx-20140505> .\n",
      "\n",
      "_Dr. Nicole Forsgren_... Dr. Nicole Forsgren, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_Scryer works by_... Daniel Jacobson, Danny Yuan, and Neeraj Joshi, \"Scryer: Netflix's Predictive Auto Scaling Engine,\" _The Netflix Tech Blog_ , November 5, 2013, <http://techblog.netflix.com/2013/11/scryer-netflixs-predictive-auto-scaling.html> .\n",
      "\n",
      "_These techniques are_... Varun Chandola, Arindam Banerjee, and Vipin Kumar, \"Anomaly detection: A survey,\" _ACM Computing Surveys_ 41, no. 3 (July 2009): article no. 15, <http://doi.acm.org/10.1145/1541880.1541882> .\n",
      "\n",
      "_Tarun Reddy, VP_... Tarun Reddy, personal interview with Gene Kim, Rally headquarters, Boulder, CO, 2014.\n",
      "\n",
      "_At Monitorama in_ 2014... \"Kolmogorov-Smirnov Test,\" _Wikipedia_ , last modified May 19, 2016, <http://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test> .\n",
      "\n",
      "_Even saying Kilmogorov-Smirnov_... \"Simple math for anomaly detection toufic boubez - metafor software - monitorama pdx 2014-05-05,\" Slideshare.net, posted by tboubez, May 6, 2014, http://www.slideshare.net/tboubez/simple-math-for-anomaly-detection -toufic-boubez-metafor-software-monitorama-pdx-20140505.\n",
      "\n",
      "###  CHAPTER 16\n",
      "\n",
      "_In 2006, Nick_... Mark Walsh, \"Ad Firms Right Media, AdInterax Sell To Yahoo,\" _MediaPost,_ October 18, 2006, <http://www.mediapost.com/publications/article/49779/ad-firms-right-media-adinterax-sell-to-yahoo.html?edition=> .\n",
      "\n",
      "_Galbreath described the_... Nick Galbreath, personal conversation with Gene, 2013.\n",
      "\n",
      "_However, Galbreath observed_... Nick Galbreath, \"Continuous Deployment - The New #1 Security Feature, from BSildesLA 2012,\" Slideshare.net, posted by Nick Galbreath, Aug 16, 2012, <http://www.slideshare.net/nickgsuperstar/continuous-deployment-the-new-1-security-feature> .\n",
      "\n",
      "_After observing many_... Ibid.\n",
      "\n",
      "_Galbreath observes that_... Ibid.\n",
      "\n",
      "_As Patrick Lightbody_... \"Volocity 2011: Patrick Lightbody, 'From Inception to Acquisition,'\" YouTube video, 15:28, posted by O'Reilly, June 17, 2011, https://www.youtube.com/watch?v=ShmPod8JecQ.\n",
      "\n",
      "_As Arup Chakrabarti_... Arup Chakrabarti, \"Common Ops Mistakes,\" presentation at Heavy Bit Industries, June 3, 2014, http://www .heavybit.com/library/video/common-ops-mistakes/\n",
      "\n",
      "_More recently, Jeff_... \"From Design Thinking to DevOps and Back Again: Unifying Design & Operations,\" Vimeo video, 21:19, posted by William Evans, June 5, 2015, https://vimeo.com/129939230.\n",
      "\n",
      "_As an anonymous_... Anonymous, personal conversation with Gene Kim, 2005.\n",
      "\n",
      "_Launch guidance and_... Tom Limoncelli, \"SRE@Google: Thousands Of DevOps Since 2004,\" YouTube video of USENIX Association Talk, NYC, posted by USENIX, 45:57, posted January 12, 2012, <http://www.youtube.com/watch?v=iIuTnhdTzK> .\n",
      "\n",
      "_As Treynor Sloss has_... Ben Treynor, \"Keys to SRE\" (presentation, Usenix SREcon14, Santa Clara, CA, May 30, 2014), <https://www.usenix.org/conference/srecon14/technical-sessions/presentation/keys-sre> .\n",
      "\n",
      "_Treynor Sloss has resisted_... Ibid.\n",
      "\n",
      "_Even when new_... Limoncelli, \"SRE@Google.\"\n",
      "\n",
      "_Tom Limoncelli noted_... Ibid.\n",
      "\n",
      "_Limoncelli noted, \"In_... Ibid.\n",
      "\n",
      "_Furthermore, Limoncelli observed_... Tom Limoncelli, personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_Limoncelli explained, \"Helping_... Ibid., 2015.\n",
      "\n",
      "###  CHAPTER 17\n",
      "\n",
      "_In general, Jez_... Humble, O'Reilly and Molesky, _Lean Enterprise_ , Part II.\n",
      "\n",
      "_In 2012, they_... Intuit, Inc., \"2012 Annual Report: Form 10-K,\" July 31, 2012, http://s1.q4cdn.com/018592547/files/doc_financials/ 2012/INTU_2012_7_31_10K_r230_at_09_13_12_FINAL_and_Camera_Ready.pdf .\n",
      "\n",
      "_Cook explained that_... Scott Cook, \"Leadership in an Agile Age: An Interview with Scott Cook,\" Intuit.com, April 20, 2011, https://web.archive.org/web/20160205050418/ http://network.intuit.com/2011/04/20/leadership-in-the-agile-age/\n",
      "\n",
      "_He continued, \"By_... Ibid.\n",
      "\n",
      "_In previous eras_... \"Direct Marketing,\" Wikipedia, last modified May 28, 2016, <https://en.wikipedia.org/wiki/Direct_marketing> .\n",
      "\n",
      "_Interestingly, it has_... Freakonomics, \"Fighting Poverty With Actual Evidence: Full Transcript,\" Freakonomics.com, November 27, 2013, <http://freakonomics.com/2013/11/27/fighting-poverty-with-actual-evidence-full-transcript/> .\n",
      "\n",
      "_Ronny Kohavi, Distinguished_... Ron Kohavi, Thomas Crook, and Roger Longbotham, \"Online Experimentation at Microsoft,\" (paper presented at the Fifteenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, Paris, France, 2009), <http://www.exp-platform.com/documents/exp_dmcasestudies.pdf> .\n",
      "\n",
      "_Kohavi goes on_... Ibid.\n",
      "\n",
      "_Jez Humble joked_... Jez Humble, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_In a 2014_... Wang, Kendrick, \"Etsy's Culture Of Continuous Experimentation and A/B Testing Spurs Mobile Innovation,\" Apptimize.com, January 30, 2014, <http://apptimize.com/blog/2014/01/etsy-continuous-innovation-ab-testing/> .\n",
      "\n",
      "_Barry O'Reilly, co-author_... Barry O'Reilly, \"How to Implement Hypothesis-Driven Development,\" BarryOReilly.com, October 21, 2013, <http://barryoreilly.com/2013/10/21/how-to-implement-hypothesis-driven-development/> .\n",
      "\n",
      "_In 2009, Jim_... Gene Kim, \"Organizational Learning and Competitiveness: Revisiting the \"Allspaw/Hammond 10 Deploys Per Day at Flickr\" Story,\" ITRevolution.com, 2015, <http://itrevolution.com/organizational-learning-and-competitiveness-a-different-view-of-the-allspawhammond-10-deploys-per-day-at-flickr-story/> .\n",
      "\n",
      "_Stoneham observes that_... Ibid.\n",
      "\n",
      "_He continues, \"These_... Ibid.\n",
      "\n",
      "_Their astounding achievements_... Ibid.\n",
      "\n",
      "_Stoneham concluded, \"This_... Ibid.\n",
      "\n",
      "###  CHAPTER 18\n",
      "\n",
      "_Once a pull_... Scott Chacon, \"Github Flow,\" ScottChacon.com, August 31, 2011, <http://scottchacon.com/2011/08/31/github-flow.html> .\n",
      "\n",
      "_For example, in_... Jake Douglas, \"Deploying at Github,\" GitHub.com, August 29, 2012, <https://github.com/blog/1241-deploying-at-github> .\n",
      "\n",
      "_A fifteen minute_... John Allspaw, \"Counterfactual Thinking, Rules, and the Knight Capital Accident,\" KitchenSoap.com, October 29, 2013, <http://www.kitchensoap.com/2013/10/29/counterfactuals-knight-capital/> .\n",
      "\n",
      "_One of the core_... Bradley Staats and David M. Upton, \"Lean Knowledge Work,\" _Harvard Business Review_ , October 2011, <https://hbr.org/2011/10/lean-knowledge-work> .\n",
      "\n",
      "_In the 2014_... Velasquez, Kim, Kersten, and Humble, _2014 State of DevOps Report_.\n",
      "\n",
      "_As Randy Shoup_... Randy Shoup, personal interview with Gene Kim, 2015.\n",
      "\n",
      "_As Giary Özil_... Giray Özil, Twitter post, February 27, 2013, 10:42 a.m., <https://twitter.com/girayozil/status/306836785739210752> .\n",
      "\n",
      "_As noted earlier_... Eran Messeri, \"What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?,\" (2013), <http://scribes.tweetscriber.com/realgenekim/206> .\n",
      "\n",
      "_In 2010, there_... John Thomas and Ashish Kumar, \"Welcome to the Google Engineering Tools Blog,\" Google Engineering Tools blog, posted May 3, 2011, <http://google-engtools.blogspot.com/2011/05/welcome-to-google-engineering-tools.html> .\n",
      "\n",
      "_This requires considerable_... Ashish Kumar, \"Development at the Speed and Scale of Google,\" (presentation at QCon, San Francisco, CA, 2010), <https://qconsf.com/sf2010/dl/qcon-sanfran-2010/slides/AshishKumar_DevelopingProductsattheSpeedandScaleofGoogle.pdf> .\n",
      "\n",
      "_He said, \"I_... Randy Shoup, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Jeff Atwood, one_... Jeff Atwood, \"Pair Programming vs. Code Reviews,\" CodingHorror.com, November 18, 2013, <http://blog.codinghorror.com/pair-programming-vs-code-reviews/> .\n",
      "\n",
      "_He continued, \"Most_... Ibid.\n",
      "\n",
      "_Dr. Laurie Williams performed_... \"Pair Programming,\" ALICE Wiki page, last modified April 4, 2014, <http://euler.math.uga.edu/wiki/index.php?title=Pair_programming> .\n",
      "\n",
      "_She argues that_... Elisabeth Hendrickson, \"DOES15 - Elisabeth Hendrickson - Its All About Feedback,\" YouTube video, 34:47, posted by DevOps Enterprise Summit, November 5, 2015, <https://www.youtube.com/watch?v=r2BFTXBundQ> .\n",
      "\n",
      "_In her 2015_... Ibid.\n",
      "\n",
      "_The problem Hendrickson_... Ibid.\n",
      "\n",
      "_Worse, skilled developers_... Ibid.\n",
      "\n",
      "_Hendrickson lamented that_... Ibid.\n",
      "\n",
      "_That was an actual_... Ryan Tomayko and Shawn Davenport, personal interview with Gene Kim, 2013.\n",
      "\n",
      "_It is many_... Ibid.\n",
      "\n",
      "_Reading through the_... Ibid.\n",
      "\n",
      "_Adrian Cockcroft observed_... Adrian Cockcroft, interview by Michael Ducy and Ross Clanton, \"Adrian Cockcroft of Battery Ventures – the Goat Farm – Episode 8,\" _The Goat Farm_ , podcast audio, July 31, 2015, <http://goatcan.do/2015/07/31/adrian-cockcroft-of-battery-ventures-the-goat-farm-episode-8/> .\n",
      "\n",
      "_Similarly, Dr. Tapabrata Pal_... Tapabrata Pal, \"DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,\" YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016, <https://www.youtube.com/watch?v=bbWFCKGhxOs> .\n",
      "\n",
      "_Jason Cox, Senior_... Jason Cox, \"Disney DevOps.\"\n",
      "\n",
      "_At Target in_... Ross Clanton and Heather Mickman, 'DOES14 - Ross Clanton and Heather Mickman - DevOps at Target,\" YouTube video, 29:20, posted by DevOps Enterprise Summit 2014, October 29, 2014, <https://www.youtube.com/watch?v=exrjV9V9vhY> .\n",
      "\n",
      "_\"As we went_... Ibid.\n",
      "\n",
      "_She added, \"I_... Ibid.\n",
      "\n",
      "_Consider a story_... John Allspaw and Jez Humble, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "###  CHAPTER 19\n",
      "\n",
      "_The result is_... Spear, _The High-Velocity Edge_ , chap. 1.\n",
      "\n",
      "_\"For such an_... Ibid., chap. 10.\n",
      "\n",
      "_A striking example_... Julianne Pepitone, \"Amazon EC2 Outage Downs Reddit, Quora,\" _CNN Money_ , April 22, 2011, <http://money.cnn.com/2011/04/21/technology/amazon_server_outage> .\n",
      "\n",
      "_In January 2013_... Timothy Prickett Morgan, \"A Rare Peek Into The Massive Scale of AWS,\" _Enterprise Tech_ , November 14, 2014, <http://www.enterprisetech.com/2014/11/14/rare-peek-massive-scale-aws/> .\n",
      "\n",
      "_However, a_ Netflix... Adrian Cockcroft, Cory Hicks, and Greg Orzell, \"Lessons Netflix Learned from the AWS Outage,\" _The Netflix Tech Blog_ , April 29, 2011, <http://techblog.netflix.com/2011/04/lessons-netflix-learned-from-aws-outage.html> .\n",
      "\n",
      "_They did so_... Ibid.\n",
      "\n",
      "_Dr. Sidney Dekker_... Sidney Dekker, _Just Culture: Balancing Safety and Accountability_ (Lund University, Sweden: Ashgate Publishing Company, 2007), 152.\n",
      "\n",
      "_He asserts that_... \"DevOpsDays Brisbane 2014 - Sidney Decker - System Failure, Human Error: Who's to Blame?\" Vimeo video, 1:07:38, posted by info@devopsdays.org, 2014, <https://vimeo.com/102167635> .\n",
      "\n",
      "_As John Allspaw_... Jenn Webb, interview with John Allspaw, \"Post-Mortems, Sans Finger-Pointing,\" _The O'Reilly Radar Postcast_ , podcast audio, August 21, 2014, <http://radar.oreilly.com/2014/08/postmortems-sans-finger-pointing-the-oreilly-radar-podcast.html> .\n",
      "\n",
      "_Blameless post-mortems, a_... John Allspaw, \"Blameless PostMortems and a Just Culture,\" CodeAsCraft.com, May 22, 2012, <http://codeascraft.com/2012/05/22/blameless-postmortems/> .\n",
      "\n",
      "_Ian Malpass, an_... Ian Malpass, \"DevOpsDays Minneapolis 2014 -- Ian Malpass, Fallible humans,\" YouTube video, 35:48, posted by DevOps Minneapolis, July 20, 2014, <https://www.youtube.com/watch?v=5NY-SrQFrBU> .\n",
      "\n",
      "_Dan Milstein, one_... Dan Milstein, \"Post-Mortems at HubSpot: What I Learned from 250 Whys,\" _HubSpot_ , June 1, 2011, <http://product.hubspot.com/blog/bid/64771/Post-Mortems-at-HubSpot-What-I-Learned-From-250-Whys> .\n",
      "\n",
      "_Randy Shoup, former_... Randy Shoup, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_We may also_... \"Post-Mortem for February 24, 2010 Outage,\" Google App Engine website, March 4, 2010, <https://groups.google.com/forum/#!topic/google-appengine/p2QKJ0OSLc8> ; \"Summary of the Amazon DynamoDB Service Disruption and Related Impacts in the US-East Region,\" Amazon Web Services website, accessed May 28, 2016, <https://aws.amazon.com/message/5467D2/> .\n",
      "\n",
      "_This desire to_... Bethany Macri, \"Morgue: Helping Better Understand Events by Building a Post Mortem Tool - Bethany Macri,\" Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, <http://vimeo.com/77206751> .\n",
      "\n",
      "_For example, as_... Spear, _The High-Velocity Edge_ , chap. 4.\n",
      "\n",
      "_Dr. Amy C. Edmondson_... Amy C. Edmondson, \"Strategies for Learning from Failure,\" _Harvard Business Review_ , April 2011, <https://hbr.org/2011/04/strategies-for-learning-from-failure> .\n",
      "\n",
      "_Dr. Spear summarizes_... Ibid.\n",
      "\n",
      "_We now know_... Ibid., chap. 3.\n",
      "\n",
      "_However, prior to_... Michael Roberto, Richard M.J. Bohmer, and Amy C. Edmondson, \"Facing Ambiguous Threats,\" _Harvard Business Review_ , November 2006, <https://hbr.org/2006/11/facing-ambiguous-threats/ar/1> .\n",
      "\n",
      "_They describe how_... Ibid.\n",
      "\n",
      "_They observe, \"Firms_... Ibid.\n",
      "\n",
      "_The authors conclude_... Ibid.\n",
      "\n",
      "_On failures, Roy_... Roy Rapoport, personal correspondence with Gene Kim, 2012.\n",
      "\n",
      "_He continues, \"I_... Ibid.\n",
      "\n",
      "_He concludes, \"DevOps_... Ibid.\n",
      "\n",
      "_As Michael Nygard_... Michael T. Nygard, _Release It!: Design and Deploy Production-Ready Software_ (Pragmatic Bookshelf: Raleigh, NC, 2007), Kindle edition, Part I.\n",
      "\n",
      "_An even more_... Jeff Barr, \"EC2 Maintenance Update,\" _AWS Blog_ , September 25, 2014, <https://aws.amazon.com/blogs/aws/ec2-maintenance-update/> .\n",
      "\n",
      "_As Christos Kalantzis_... Bruce Wong and Christos Kalantzis, \"A State of Xen - Chaos Monkey & Cassandra,\" _The Netflix Tech Blog_ , October 2, 2014, <http://techblog.netflix.com/2014/10/a-state-of-xen-chaos-monkey-cassandra.html> .\n",
      "\n",
      "_But, Kalantzis continues_... Ibid.\n",
      "\n",
      "_As Kalantzis and_... Ibid.\n",
      "\n",
      "_Even more surprising_... Roy Rapoport, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_Specific architectural patterns_... Adrian Cockcroft, personal correspondence with Gene Kim, 2012.\n",
      "\n",
      "_In this section_... Jesse Robbins, \"GameDay: Creating Resiliency Through Destruction - LISA11,\" Slideshare.net, posted by Jesse Robbins, December 7, 2011, <http://www.slideshare.net/jesserobbins/ameday-creating-resiliency-through-destruction> .\n",
      "\n",
      "_Robbins defines resilience_... Ibid.\n",
      "\n",
      "_Jesse Robbins observes_... Jesse Robbins, Kripa Krishnan, John Allspaw, and Tom Limoncelli, \"Resilience Engineering: Learning to Embrace Failure,\" _amcqueue_ 10, no. 9 (September 13, 2012): <https://queue.acm.org/detail.cfm?id=2371297> .\n",
      "\n",
      "_As Robbins quips_... Ibid.\n",
      "\n",
      "_As Robbins describes_... Ibid.\n",
      "\n",
      "_Robbins explains, \"You_... Ibid.\n",
      "\n",
      "_During that time_... \"Kripa Krishnan: 'Learning Continuously From Failures' at Google,\" YouTube video, 21:35, posted by Flowcon, November 11, 2014, <https://www.youtube.com/watch?v=KqqS3wgQum0> .\n",
      "\n",
      "_Krishnan wrote, \"An_... Kripa Krishnan, \"Weathering the Unexpected,\" _Communications of the ACM 55_ , no. 11 (November 2012): 48-52, <http://cacm.acm.org/magazines/2012/11/156583-weathering-the-unexpected/abstract> .\n",
      "\n",
      "_Some of the learnings_... Ibid.\n",
      "\n",
      "_As Peter Senge_... Widely attributed to Peter Senge.\n",
      "\n",
      "###  CHAPTER 20\n",
      "\n",
      "_As Jesse Newland_... Jesse Newland, \"ChatOps at GitHub,\" SpeakerDeck.com, February 7, 2013, <https://speakerdeck.com/jnewland/chatops-at-github> .\n",
      "\n",
      "_As Mark Imbriaco_... Mark Imbriaco, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_They enabled Hubot_... Newland, \"ChatOps at GitHub.\"\n",
      "\n",
      "_Hubot often performed_... Ibid.\n",
      "\n",
      "_Newland observes that_... Ibid.\n",
      "\n",
      "_Instead of putting_... Leon Osterweil, \"Software processes are software too,\" paper presented at International Conference on Software Engineering, Monterey, CA, 1987, <http://www.cs.unibo.it/cianca/wwwpages/ids/letture/Osterweil.pdf> .\n",
      "\n",
      "_Justin Arbuckle was_... Justin Arbuckle, \"What Is ArchOps: Chef Executive Roundtable\" (2013).\n",
      "\n",
      "_What resulted was_... Ibid.\n",
      "\n",
      "_Arbuckle's conclusion was_... Ibid.\n",
      "\n",
      "_By 2015, Google_... Cade Metz, \"Google Is 2 Billion Lines of Code—and It's All in One Place,\" _Wired_ , September 16, 2015, <http://www.wired.com/2015/09/google-2-billion-lines-codeand-one-place/> .\n",
      "\n",
      "_The Chrome and_... Ibid.\n",
      "\n",
      "_Rachel Potvin, a_... Ibid.\n",
      "\n",
      "_Furthermore, as Eran_... Eran Messeri, \"What Goes Wrong When Thousands of Engineers Share the Same Continuous Build?\" (2013), <http://scribes.tweetscriber.com/realgenekim/206> .\n",
      "\n",
      "_As Randy Shoup_... Randy Shoup, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Tom Limoncelli, co-author_... Tom Limoncelli, \"Yes, you can really work from HEAD,\" EverythingSysAdmin.com, March 15, 2014, <http://everythingsysadmin.com/2014/03/yes-you-really-can-work-from-head.html> .\n",
      "\n",
      "_Tom Limoncelli describes_... Tom Limoncelli, \"Python is better than Perl6,\" EverythingSysAdmin.com, January 10, 2011, <http://everythingsysadmin.com/2011/01/python-is-better-than-perl6.html> .\n",
      "\n",
      "_Google used C++_... \"Which programming languages does Google use internally?,\" Quora.com forum, accessed May 29, 2016, <https://www.quora.com/Which-programming-languages-does-Google-use-internally> .; \"When will Google permit languages other than Python, C++, Java and Go to be used for internal projects?,\" Quora.com forum, accessed May 29, 2016, <https://www.quora.com/When-will-Google-permit-languages-other-than-Python-C-Java-and-Go-to-be-used-for-internal-projects/answer/Neil-Kandalgaonkar> .\n",
      "\n",
      "_In a presentation_... Ralph Loura, Olivier Jacques, and Rafael Garcia, \"DOES15 - Ralph Loura, Olivier Jacques, & Rafael Garcia - Breaking Traditional IT Paradigms to...,\" YouTube video, 31:07, posted by DevOps Enterprise Summit, November 16, 2015, <https://www.youtube.com/watch?v=q9nNqqie_sM> .\n",
      "\n",
      "_In many organizations_... Michael Rembetsy and Patrick McDonnell, \"Continuously Deploying Culture: Scaling Culture at Etsy - Velocity Europe 2012,\" Slideshare.net, posted by Patrick McDonnell, October 4, 2012, <http://www.slideshare.net/mcdonnps/continuously-deploying-culture-scaling-culture-at-etsy-14588485> .\n",
      "\n",
      "_At that time, Etsy_... Ibid.\n",
      "\n",
      "_Over the next_... Ibid.\n",
      "\n",
      "_Similarly, Dan McKinley_... Dan McKinley, \"Why MongoDB Never Worked Out at Etsy,\" McFunley.com, December 26, 2012, <http://mcfunley.com/why-mongodb-never-worked-out-at-etsy> .\n",
      "\n",
      "###  CHAPTER 21\n",
      "\n",
      "_One of the_... \"Kaizen,\" _Wikipedia_ , last modified May 12, 2016, <https://en.wikipedia.org/wiki/Kaizen> .\n",
      "\n",
      "_Dr. Spear explains_... Spear, _The High-Velocity Edge_ , chap. 8.\n",
      "\n",
      "_Spear observes that_... Ibid.\n",
      "\n",
      "_Clanton describes, \"We_... Mickman and Clanton, \"(Re)building an Engineering Culture.\"\n",
      "\n",
      "_Ravi Pandey, a_... Ravi Pandey, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_Clanton expands on_... Mickman and Clanton, \"(Re)building an Engineering Culture.\"\n",
      "\n",
      "_In addition to_... Hal Pomeranz, \"Queue Inversion Week,\" _Righteous IT_ , February 12, 2009, <https://righteousit.wordpress.com/2009/02/12/queue-inversion-week/> .\n",
      "\n",
      "_As Dr. Spear_... Spear, _The High-Velocity Edge_ , chap. 3.\n",
      "\n",
      "_In an interview with Jessica_... Jessica Stillman, \"Hack Days: Not Just for Facebookers,\" _Inc._ , February 3, 2012, <http://www.inc.com/jessica-stillman/hack-days-not-just-for-facebookers.html> .\n",
      "\n",
      "_In 2008, Facebook_... AP, \"Number of active users at Facebook over the years,\" _Yahoo! News_ , May 1, 2013, <https://www.yahoo.com/news/number-active-users-facebook-over-230449748.html?ref=gs> .\n",
      "\n",
      "_During a hack_... Haiping Zhao, \"HipHop for PHP: Move Fast,\" post on Haiping Zhao's Facebook page, February 2, 2010, <https://www.facebook.com/notes/facebook-engineering/hiphop-for-php-move-fast/280583813919> .\n",
      "\n",
      "_In an interview with Cade_... Cade Metz, \"How Three Guys Rebuilt the Foundation of Facebook,\" _Wired_ , June 10, 2013, <http://www.wired.com/wiredenterprise/2013/06/facebook-hhvm-saga/all/> .\n",
      "\n",
      "_Steve Farley, VP_... Steve Farley, personal correspondence with Gene Kim, January 5, 2016.\n",
      "\n",
      "_Karthik Gaekwad, who_... \"Agile 2013 Talk: How DevOps Change Everything,\" Slideshare.net, posted by Karthik Gaekwad, August 7, 2013, http://www.slideshare.net/karthequian/howdevops changeseverythingagile2013karthikgaekwad/ .\n",
      "\n",
      "_As Glenn O'Donnell_... Glenn O'Donnell, \"DOES14 - Glenn O'Donnell - Forrester - Modern Services Demand a DevOps Culture Beyond Apps,\" YouTube video, 12:20, posted by DevOps Enterprise Summit 2014, November 5, 2014, <https://www.youtube.com/watch?v=pvPWKuO4_48> .\n",
      "\n",
      "_As of 2014_... Nationwide, 2014 Annual Report, <https://www.nationwide.com/about-us/nationwide-annual-report-2014.jsp> .\n",
      "\n",
      "_Steve Farley, VP_... Steve Farley, personal correspondence with Gene Kim, 2016.\n",
      "\n",
      "_Capital One, one_... \"DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,\" YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016, <https://www.youtube.com/watch?v=bbWFCKGhxOs> .\n",
      "\n",
      "_Dr. Tapabrata Pal_... Tapabrata Pal, personal correspondence with Gene Kim, 2015.\n",
      "\n",
      "_Target is the_... \"Corporate Fact Sheet,\" Target company website, accessed June 9, 2016, <https://corporate.target.com/press/corporate> .\n",
      "\n",
      "_Incidentally, the first_... Evelijn Van Leeuwen and Kris Buytaert, \"DOES15 - Evelijn Van Leeuwen and Kris Buytaert - Turning Around the Containership,\" YouTube video, 30:28, posted by DevOps Enterprise Summit, December 21, 2015, <https://www.youtube.com/watch?v=0GId4AMKvPc> .\n",
      "\n",
      "_Clanton describes, \"2015_... Mickman and Clanton, \"(Re)building an Engineering Culture.\"\n",
      "\n",
      "_At Capital One_... \"DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,\" YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016, <https://www.youtube.com/watch?v=bbWFCKGhxOs> .\n",
      "\n",
      "_Bland explains that_... Bland, \"DOES15 - Mike Bland - Pain Is Over, If You Want It.\"\n",
      "\n",
      "_Even though they_... Ibid.\n",
      "\n",
      "_They used several_... Ibid.\n",
      "\n",
      "_Bland described, \"The_... Ibid.\n",
      "\n",
      "_Bland continues, \"One_... Ibid.\n",
      "\n",
      "_As Bland describes_... Ibid.\n",
      "\n",
      "_Bland continues, \"It_... Ibid.\n",
      "\n",
      "_He continues, \"The_... Ibid.\n",
      "\n",
      "_Bland describes Fixits_... Mike Bland, \"Fixits, or I Am the Walrus,\" Mike-Bland.com, October 4, 2011, <https://mike-bland.com/2011/10/04/fixits.html> .\n",
      "\n",
      "_These Fixits, as_... Ibid.\n",
      "\n",
      "###  CHAPTER 22\n",
      "\n",
      "_One of the top_... James Wickett, \"Attacking Pipelines--Security meets Continuous Delivery,\" Slideshare.net, posted by James Wickett, June 11, 2014, <http://www.slideshare.net/wickett/attacking-pipelinessecurity-meets-continuous-delivery> .\n",
      "\n",
      "_James Wickett, one_... Ibid.\n",
      "\n",
      "_Similar ideas were_... Tapabrata Pal, \"DOES15 - Tapabrata Pal - Banking on Innovation & DevOps,\" YouTube video, 32:57, posted by DevOps Enterprise Summit, January 4, 2016, <https://www.youtube.com/watch?v=bbWFCKGhxOs> .\n",
      "\n",
      "_Justin Arbuckle, former_... Justin Arbuckle, personal interview with Gene Kim, 2015.\n",
      "\n",
      "_He continues, \"By_... Ibid.\n",
      "\n",
      "_This helped the_... Snehal Antani, \"IBM Innovate DevOps Keynote,\" YouTube video, 47:57, posted by IBM DevOps, June 12, 2014, <https://www.youtube.com/watch?v=s0M1P05-6Io> .\n",
      "\n",
      "_In a presentation_... Nick Galbreath, \"DevOpsSec: Appling DevOps Principles to Security, DevOpsDays Austin 2012,\" Slideshare, posted by Nick Galbreath, April 12, 2012, <http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security> .\n",
      "\n",
      "_Furthermore, he states_... Ibid.\n",
      "\n",
      "_Furthermore, we should_... \"OWASP Cheat Sheet Series,\" OWASP.org, last modified March 2, 2016, <https://www.owasp.org/index.php/OWASP_Cheat_Sheet_Series> .\n",
      "\n",
      "_The scale of_... Justin Collins, Alex Smolen, and Neil Matatall, \"Putting to your Robots to Work V1.1,\" Slideshare.net, posted by Neil Matatall, April 24, 2012, <http://www.slideshare.net/xplodersuv/sf-2013-robots/> .\n",
      "\n",
      "_In early 2009_... \"What Happens to Companies That Get Hacked? FTC Cases,\" Giant Bomb forum, posted by SuicidalSnowman, July 2012, <http://www.giantbomb.com/forums/off-topic-31/what-happens-to-companies-that-get-hacked-ftc-case-540466/> .\n",
      "\n",
      "_In their previously_... Collins, Smolen, and Matatall, \"Putting to your Robots to Work V1.1.\"\n",
      "\n",
      "_The first big_... Twitter Engineering, \"Hack Week @ Twitter,\" Twitter blog, January 25, 2012, <https://blog.twitter.com/2012/hack-week-twitter> .\n",
      "\n",
      "_Josh Corman observed_... Josh Corman and John Willis, \"Immutable Awesomeness - Josh Corman and John Willis at DevOps Enterprise Summit 2015,\" YouTube video, 34:25, posted by Sonatype, October 21, 2015, <https://www.youtube.com/watch?v=-S8-lrm3iV4> .\n",
      "\n",
      "_In the 2014_... Verizon, _\"_ 2014 Data Breach Investigations Report,\" (Verizon Enterprise Solutions, 2014), <https://dti.delaware.gov/pdfs/rp_Verizon-DBIR-2014_en_xg.pdf> .\n",
      "\n",
      "_In 2015, this_... \"2015 State of the Software Supply Chain Report: Hidden Speed Bumps on the Way to 'Continuous,'\" (Fulton, MD: Sonatype, Inc, 2015), http://cdn2.hubspot.net/hubfs/1958393/White_Papers/2015_State_ of_the_Software_Supply_Chain_Report-.pdf?t=1466775053631 .\n",
      "\n",
      "_The last statistic_... Dan Geer and Joshua Corman, \"Almost Too Big to Fail,\" ; _login:: The Usenix Magazine_ , 39, no. 4 (August 2014): 66-68, <https://www.usenix.org/system/files/login/articles/15_geer_0.pdf> .\n",
      "\n",
      "_US Federal Government_... Wyatt Kash, \"New details released on proposed 2016 IT spending,\" _FedScoop,_ February 4, 2015, <http://fedscoop.com/what-top-agencies-would-spend-on-it-projects-in-2016> .\n",
      "\n",
      "_As Mike Bland_... Bland, \"DOES15 - Mike Bland - Pain Is Over, If You Want It.\"\n",
      "\n",
      "_Furthermore, the Cloud.gov_... Mossadeq Zia, Gabriel Ramírez, Noah Kunin, \"Compliance Masonry: Bulding a risk management platform, brick by brick,\" _18F_ , April 15, 2016, <https://18f.gsa.gov/2016/04/15/compliance-masonry-buildling-a-risk-management-platform/> .\n",
      "\n",
      "_Marcus Sachs, one_... Marcus Sachs, personal correspondence with Gene Kim, 2010.\n",
      "\n",
      "_We need to_... \"VPC Best Configuration Practices,\" Flux7 blog, January 23, 2014, <http://blog.flux7.com/blogs/aws/vpc-best-configuration-practices> .\n",
      "\n",
      "_In 2010, Nick_... Nick Galbreath, \"Fraud Engineering, from Merchant Risk Council Annual Meeting 2012,\" Slideshare.net, posted by Nick Galbreath, May 3, 2012, <http://www.slideshare.net/nickgsuperstar/fraud-engineering> .\n",
      "\n",
      "_Of particular concern_... Nick Galbreath, \"DevOpsSec: Appling DevOps Principles to Security, DevOpsDays Austin 2012,\" Slideshare.net, posted by Nick Galbreath, April 12, 2013, <http://www.slideshare.net/nickgsuperstar/devopssec-apply-devops-principles-to-security> .\n",
      "\n",
      "_We were always_... Ibid.\n",
      "\n",
      "_This was a ridiculously_... Ibid.\n",
      "\n",
      "_As Galbreath observed_... Ibid.\n",
      "\n",
      "_Galbreath observed, \"One_... Ibid.\n",
      "\n",
      "_As Jonathan Claudius_... Jonathan Claudius, \"Attacking Cloud Services with Source Code,\" Speakerdeck.com, posted by Jonathan Claudius, April 16, 2013, <https://speakerdeck.com/claudijd/attacking-cloud-services-with-source-code> .\n",
      "\n",
      "###  CHAPTER 23\n",
      "\n",
      "_ITIL defines utility_... _Axelos, ITIL Service Transition_ (ITIL Lifecycle Suite) (Belfast, Ireland: TSO, 2011), 48.\n",
      "\n",
      "_Salesforce was founded_... Reena Matthew and Dave Mangot, \"DOES14 - Reena Mathew and Dave Mangot - Salesforce,\" Slideshare.net, posted by ITRevolution, October 29, 2014, <http://www.slideshare.net/ITRevolution/does14-reena-matthew-and-dave-mangot-salesforce> .\n",
      "\n",
      "_By 2007, the_... Dave Mangot and Karthik Rajan, \"Agile.2013.effecting.a.dev ops.transformation.at.salesforce,\" Slideshare.net, posted by Dave Mangot, August 12, 2013, <http://www.slideshare.net/dmangot/agile2013effectingadev-opstransformationatsalesforce> .\n",
      "\n",
      "_Karthik Rajan, then_... Ibid.\n",
      "\n",
      "_At the 2014_... Matthew and Mangot, \"DOES14 - Salesforce.\"\n",
      "\n",
      "_For Mangot and_... Ibid.\n",
      "\n",
      "_Furthermore, they noted_... Ibid.\n",
      "\n",
      "_Bill Massie is_... Bill Massie, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Because the scope_... \"Glossary,\" PCI Security Standards Council website, accessed May 30, 2016, <https://www.pcisecuritystandards.org/pci_security/glossary> .\n",
      "\n",
      "_Are code review_... PCI Security Standards Council, _Payment Card Industry (PCI) Data Security Stands: Requirements and Security Assessment Procedures, Version 3.1_ (PCI Security Standards Council, 2015), Section 6.3.2. https://webcache.googleusercontent.com/search?q=cache:hpRe2COzzdAJ:https://www.cisecuritystandards.org/documents/PCI_DSS_v3-1_SAQ_D_Merchant_rev1-1.docx+&cd=2&hl=en&ct=clnk&gl=us .\n",
      "\n",
      "_To fulfill this_... Bill Massie, personal correspondence with Gene Kim, 2014.\n",
      "\n",
      "_Massie observes that_... Ibid.\n",
      "\n",
      "_As a result_... Ibid.\n",
      "\n",
      "_As Bill Shinn_... Bill Shinn, \"DOES15 - Bill Shinn - Prove it! The Last Mile for DevOps in Regulated Organizations,\" Slideshare.net, posted by ITRevolution, November 20, 2015, <http://www.slideshare.net/ITRevolution/does15-bill-shinn-prove-it-the-last-mile-for-devops-in-regulated-organizations> .\n",
      "\n",
      "_Helping large enterprise_... Ibid.\n",
      "\n",
      "_Shinn notes, \"One_... Ibid.\n",
      "\n",
      "_\"That was fine_... Ibid.\n",
      "\n",
      "_He explains, \"In_... Ibid.\n",
      "\n",
      "_Shinn states that_... Ibid.\n",
      "\n",
      "_Shinn continues, \"With_... Ibid.\n",
      "\n",
      "_That requires deriving_... Ibid.\n",
      "\n",
      "_Shinn continues, \"How_... Ibid.\n",
      "\n",
      "_Shinn gives an_... Ibid.\n",
      "\n",
      "_To help solve_... James DeLuccia, Jeff Gallimore, Gene Kim, and Byron Miller, _DevOps Audit Defense Toolkit_ (Portland, OR: IT Revolution, 2015), <http://itrevolution.com/devops-and-auditors-the-devops-audit-defense-toolkit> .\n",
      "\n",
      "_She made the_... Mary Smith (a pseudonym), personal correspondence with Gene Kim, 2013\n",
      "\n",
      "_She observed:_... Ibid., 2014.\n",
      "\n",
      "###  CONCLUSION\n",
      "\n",
      "_As Jesse Robbins_... \"Hacking Culture at VelocityConf,\" Slideshare.net, posted by Jesse Robbins, June 28, 2012, <http://www.slideshare.net/jesserobbins/hacking-culture-at-velocityconf> .\n",
      "\n",
      "###  APPENDIX\n",
      "\n",
      "_The Lean movement started_... Ries, _The Lean Startup._\n",
      "\n",
      "_A key principal_... Kent Beck et al., \"Twelve Principles of Agile Software,\" AgileManifesto.org, 2001, <http://agilemanifesto.org/principles.html> .\n",
      "\n",
      "_Building upon the_... Humble and Farley, _Continuous Delivery_.\n",
      "\n",
      "_This idea was_... Fitz, \"Continuous Deployment at IMVU.\"\n",
      "\n",
      "_Toyota Kata describes..._ Rother, _Toyota Kata,_ Introduction.\n",
      "\n",
      "_His conclusion was_... Ibid..\n",
      "\n",
      "_In 2011, Eric_... Ries, _The Lean Startup._\n",
      "\n",
      "_In_ The Phoenix... Kim, Behr, and Spafford, _The Phoenix Project,_ 365.\n",
      "\n",
      "_Myth 1: \"Human_... Denis Besnard and Erik Hollnagel, _Some Myths about Industrial Safety_ (Paris, Centre De Recherche Sur Les Risques Et Les Crises Mines, 2012), 3, <http://gswong.com/?wpfb_dl=31> .\n",
      "\n",
      "_Myth 2: \"Systems_... Ibid., 4.\n",
      "\n",
      "_Myth 3: \"Safety_... Ibid., 6.\n",
      "\n",
      "_Myth 4: \"Accident_... Ibid., 8.\n",
      "\n",
      "_Myth 5: \"Accident_... Ibid., 9.\n",
      "\n",
      "_Myth 6: Safety_... Ibid., 11.\n",
      "\n",
      "_Rather, when the_... John Shook, \"Five Missing Pieces in Your Standardized Work (Part 3 of 3),\" Lean.org, October 27, 2009, <http://www.lean.org/shook/DisplayObject.cfm?o=1321> .\n",
      "\n",
      "_Time to resolve_... \"Post Event Retrospective - Part 1,\" Rally Blogs, accessed May 31, 2016, <https://www.rallydev.com/blog/engineering/post-event-retrospective-part-i> .\n",
      "\n",
      "_Bethany Macri, from_... \"Morgue: Helping Better Understand events by Building a Post Mortem Tool - Bethany Macri,\" Vimeo video, 33:34, posted by info@devopsdays.org, October 18, 2013, <http://vimeo.com/77206751> .\n",
      "\n",
      "_These discussions have_... Cockcroft, Hicks, and Orzell, \"Lessons Netflix Learned.\"\n",
      "\n",
      "_Since then, Chaos_... Ibid.\n",
      "\n",
      "_Lenny Rachitsky wrote_... __Lenny Rachitsky, \"7 Keys to a Successful Public Health Dashboard,\" _Transparent Uptime_ , December 1, 2008, <http://www.transparentuptime.com/2008/11/rules-for-successful-public-health.html> .\n",
      "\n",
      "# Index\n",
      "\n",
      "### Symbols Numbers A B C D E F G H I J K L M N O P Q R S T U V W Y Z\n",
      "\n",
      "Note: Figures are indicated with _f_ ; footnotes are indicated with _n_\n",
      "\n",
      "SYMBOLS\n",
      "\n",
      "%C/A, ,\n",
      "\n",
      "NUMBERS\n",
      "\n",
      "2PT. _See_ two-pizza team\n",
      "\n",
      "18F team, 325–326\n",
      "\n",
      "2013 State of DevOps Report, 159–160\n",
      "\n",
      "A\n",
      "\n",
      "AAS. _See_ Amazon Auto Scaling\n",
      "\n",
      "Adams, Keith,\n",
      "\n",
      "Agile\n",
      "\n",
      "Infrastructure and Velocity Movement,\n",
      "\n",
      "Infrastructure Movement,\n",
      "\n",
      "Manifesto, 4–5\n",
      "\n",
      "Movement,\n",
      "\n",
      "principles, xxii–xxiii\n",
      "\n",
      "Aisin Seiki Global, 43–44\n",
      "\n",
      "Alcoa, 41–42,\n",
      "\n",
      "Algra, Ingrid,  _n_\n",
      "\n",
      "Allspaw, John\n",
      "\n",
      "Agile Infrastructure and Velocity Movement,\n",
      "\n",
      "dark launches, 173–174\n",
      "\n",
      "deployment failures, 251–252\n",
      "\n",
      "learning culture, 273–274\n",
      "\n",
      "production metrics,\n",
      "\n",
      "Velocity Movement,\n",
      "\n",
      "Allstate,\n",
      "\n",
      "Amazon\n",
      "\n",
      "continuous delivery,\n",
      "\n",
      "deploys per day, xxxiv _n_\n",
      "\n",
      "evolutionary architecture, 184–185\n",
      "\n",
      "market-oriented organization,\n",
      "\n",
      "service-oriented architecture, 90–91\n",
      "\n",
      "Amazon Auto Scaling, 221–222\n",
      "\n",
      "Amazon AWS\n",
      "\n",
      "compliance in regulated environments, 342–344\n",
      "\n",
      "resilience, 271–273, 281–282\n",
      "\n",
      "service outage,  _n_\n",
      "\n",
      "Andon cord\n",
      "\n",
      "consequences of not pulling,\n",
      "\n",
      "illustrated,  _f_\n",
      "\n",
      "swarming, ,\n",
      "\n",
      "virtual, 138–140\n",
      "\n",
      "and work stoppages, 360–361\n",
      "\n",
      "Antani, Snehal,\n",
      "\n",
      "APIs\n",
      "\n",
      "enablement, 91–93\n",
      "\n",
      "Feature API,\n",
      "\n",
      "service interactions using,\n",
      "\n",
      "versioned,\n",
      "\n",
      "application logging, 201–203,  _n_\n",
      "\n",
      "application-based release patterns, 171–175\n",
      "\n",
      "Arbuckle, Justin, ,\n",
      "\n",
      "architecture, evolutionary\n",
      "\n",
      "Amazon case study, 184–185\n",
      "\n",
      "architectural archetypes,  _f_\n",
      "\n",
      "Blackboard Learn case study, 186–189,  _f_ ,  _f_\n",
      "\n",
      "code repository,  _f_ ,  _f_\n",
      "\n",
      "decoupling functionality,\n",
      "\n",
      "description of, 179–180\n",
      "\n",
      "immutable services,\n",
      "\n",
      "loosely-coupled architecture, 181–182\n",
      "\n",
      "monoliths vs microservices, 182–185\n",
      "\n",
      "Second Law of Architectural Thermodynamics, 180–181\n",
      "\n",
      "service-oriented architecture,\n",
      "\n",
      "strangler application pattern, , 185–189\n",
      "\n",
      "tightly-coupled architecture, 180–181,\n",
      "\n",
      "versioned APIs,\n",
      "\n",
      "versioned services,\n",
      "\n",
      "architecture, loosely-coupled, 89–93, 181–182, 254–255\n",
      "\n",
      "architecture, monitoring, 198–199\n",
      "\n",
      "architecture, service-oriented, , 90–91,\n",
      "\n",
      "Ashman, David,\n",
      "\n",
      "ATDD. _See_ development, acceptance test-driven\n",
      "\n",
      "Atwood, Jeff, , 259–260\n",
      "\n",
      "Austin, Jim, 215–216\n",
      "\n",
      "automated environment build process\n",
      "\n",
      "assets to check into version control repository,\n",
      "\n",
      "automated configuration systems,\n",
      "\n",
      "benefits of automation, 114–115\n",
      "\n",
      "common build mechanisms,\n",
      "\n",
      "critical role of version control,\n",
      "\n",
      "environment consistency,\n",
      "\n",
      "environment development on demand, 113–115\n",
      "\n",
      "environment re-build vs repair,\n",
      "\n",
      "environments stored in version control, 115–116\n",
      "\n",
      "immutable infrastructure,\n",
      "\n",
      "metadata,\n",
      "\n",
      "new definition of finished development, 119–121\n",
      "\n",
      "quick environment development,\n",
      "\n",
      "shared version control repository, 115–118\n",
      "\n",
      "sprints, 119–120\n",
      "\n",
      "standardization,\n",
      "\n",
      "testing,\n",
      "\n",
      "testing environments,\n",
      "\n",
      "uses of automation,\n",
      "\n",
      "version control as predictor of organizational performance,\n",
      "\n",
      "version control systems, 115–118\n",
      "\n",
      "automated validation test suite\n",
      "\n",
      "acceptance test-driven development, 134–135\n",
      "\n",
      "acceptance tests, ,\n",
      "\n",
      "analysis tools,\n",
      "\n",
      "automating manual tests, 135–136\n",
      "\n",
      "code configuration management tools,\n",
      "\n",
      "environment validation, 137–138\n",
      "\n",
      "error detection, 132–133\n",
      "\n",
      "fast testing, , 133–134\n",
      "\n",
      "feedback,\n",
      "\n",
      "green builds, 129–130\n",
      "\n",
      "ideal vs non-ideal testing,  _f_\n",
      "\n",
      "integration tests, ,\n",
      "\n",
      "non-functional requirements testing, 137–138\n",
      "\n",
      "performance testing, 136–137\n",
      "\n",
      "test types, 130–131\n",
      "\n",
      "test-driven development, 134–135\n",
      "\n",
      "testing in parallel, 133–134,  _f_\n",
      "\n",
      "unit tests, 130–131, 132–133\n",
      "\n",
      "unreliable test,\n",
      "\n",
      "automation. _See_ automated environment build process; deployment process automation; testing, automated\n",
      "\n",
      "B\n",
      "\n",
      "Baker, Bill,\n",
      "\n",
      "Barnes & Noble,\n",
      "\n",
      "batch sizes\n",
      "\n",
      "continuous deployment,\n",
      "\n",
      "error management,\n",
      "\n",
      "large, 19–20\n",
      "\n",
      "single-piece flow, ,\n",
      "\n",
      "small, 18–20\n",
      "\n",
      "small batch strategy,\n",
      "\n",
      "small vs large,  _f_\n",
      "\n",
      "Bazaarvoice,  _n_ , 149–151\n",
      "\n",
      "Beck, Kent, ,\n",
      "\n",
      "Beedle, Mike,  _n_\n",
      "\n",
      "Behr, Kevin, ,  _n_\n",
      "\n",
      "Besnard, Denis, 359–360\n",
      "\n",
      "Betz, Charles, 180–181,  _n_\n",
      "\n",
      "Big Fish Games, 95–97\n",
      "\n",
      "bimodal IT,\n",
      "\n",
      "Blackboard Learn\n",
      "\n",
      "case study,  _f_ ,  _f_\n",
      "\n",
      "Perl,\n",
      "\n",
      "strangler application pattern, 186–189\n",
      "\n",
      "blameless post-mortems\n",
      "\n",
      "countermeasures,\n",
      "\n",
      "goals of, 274–275\n",
      "\n",
      "outcomes of, 275–276\n",
      "\n",
      "publicizing, 277–278\n",
      "\n",
      "sample agenda, 362–364\n",
      "\n",
      "stakeholders present,\n",
      "\n",
      "transparent uptime,  _n_\n",
      "\n",
      "Bland, Mike, 123–126,  _n_ , 306–307,\n",
      "\n",
      "Blank, Steve,\n",
      "\n",
      "Blankenship, Ed,\n",
      "\n",
      "blitz\n",
      "\n",
      "goals,\n",
      "\n",
      "improvement,\n",
      "\n",
      "kaizen,\n",
      "\n",
      "Blockbuster,\n",
      "\n",
      "blue-green deployment pattern\n",
      "\n",
      "deployment, 166–169,  _f_ ,  _n_\n",
      "\n",
      "Farley, David, 168–169\n",
      "\n",
      "low-risk releases, 166–169,  _f_\n",
      "\n",
      "Ruby on Rails,  _n_\n",
      "\n",
      "BMW,\n",
      "\n",
      "Bohmer, Richard M. J.,\n",
      "\n",
      "Booch, Grady,  _n_\n",
      "\n",
      "Borders,\n",
      "\n",
      "Boubez, Toufic, , 224–226\n",
      "\n",
      "Bouwman, Jan-Joost,  _n_\n",
      "\n",
      "Brakeman, ,  _f_\n",
      "\n",
      "brownfield services. _See_ services, brownfield\n",
      "\n",
      "Building Blocks, 188–189\n",
      "\n",
      "build-measure-learn cycle,\n",
      "\n",
      "bureaucratic organizations,\n",
      "\n",
      "Burgess, Mark,  _n_\n",
      "\n",
      "business logic\n",
      "\n",
      "changes to, ,\n",
      "\n",
      "coordinating changes to,\n",
      "\n",
      "moving to application layer,\n",
      "\n",
      "business relationship manager,\n",
      "\n",
      "Buytaert, Kris,  _n_\n",
      "\n",
      "C\n",
      "\n",
      "C++\n",
      "\n",
      "eBay,  _n_ ,\n",
      "\n",
      "Facebook,  _n_ , ,\n",
      "\n",
      "Google,  _n_\n",
      "\n",
      "Google Web Server,\n",
      "\n",
      "Cagan, Marty,\n",
      "\n",
      "Campbell-Pretty, Em, 111–112\n",
      "\n",
      "Canahuati, Pedro,\n",
      "\n",
      "canary release pattern,  _n_ , 169–171,  _f_ ,  _n_\n",
      "\n",
      "canary tests,\n",
      "\n",
      "Capitol One, 304–306\n",
      "\n",
      "case studies\n",
      "\n",
      "Amazon, 184–185\n",
      "\n",
      "Amazon AWS, 271–273, 344–345\n",
      "\n",
      "anomaly detection techniques, 224–226\n",
      "\n",
      "ATM systems, 344–345\n",
      "\n",
      "Bazaarvoice, 149–151\n",
      "\n",
      "Big Fish Games, 95–97\n",
      "\n",
      "Blackboard Learn, 186–189,  _f_ ,  _f_\n",
      "\n",
      "Capitol One, 304–306\n",
      "\n",
      "CSG International, 157–159\n",
      "\n",
      "Dixons Retail, 168–169\n",
      "\n",
      "Etsy, 77–80, 162–164, 297–298, 328–330, 339–341\n",
      "\n",
      "Facebook, 153–155, 174–175\n",
      "\n",
      "Federal Government, 325–326\n",
      "\n",
      "Google, 237–239, 257–258\n",
      "\n",
      "Google Web Server, 123–126\n",
      "\n",
      "HP, 144–146\n",
      "\n",
      "Intuit, 241–248\n",
      "\n",
      "LinkedIn, 71–73, 207–208\n",
      "\n",
      "Nationwide Insurance, 304–306\n",
      "\n",
      "Netflix, 215–216, 221–222, 271–273\n",
      "\n",
      "Nordstrom, 51–55, 61–62\n",
      "\n",
      "Pivotal Labs, 260–261\n",
      "\n",
      "Right Media, 227–229\n",
      "\n",
      "Salesforce.com, 337–338\n",
      "\n",
      "Target, 91–93, 299–300, 304–306\n",
      "\n",
      "Twitter, 320–323\n",
      "\n",
      "Yahoo! Answers, 246–248\n",
      "\n",
      "Chacon, Scott,\n",
      "\n",
      "Chakrabarti, Arup,\n",
      "\n",
      "change approval processes. _See also_ code reviews\n",
      "\n",
      "case study, 249–251\n",
      "\n",
      "change advisory boards,\n",
      "\n",
      "change control failures, 252–253\n",
      "\n",
      "change freezes, 258–259\n",
      "\n",
      "change review lead times,  _f_\n",
      "\n",
      "code reviews, 255–258\n",
      "\n",
      "coordination and scheduling of changes, 254–255\n",
      "\n",
      "counterfactual thinking, ,  _n_\n",
      "\n",
      "cutting bureaucratic processes, 263–264\n",
      "\n",
      "dangers of, 251–252\n",
      "\n",
      "email pass-around,\n",
      "\n",
      "engineer roles,\n",
      "\n",
      "GitHub Flow,\n",
      "\n",
      "Google case study, 257–258\n",
      "\n",
      "guidelines for code reviews,\n",
      "\n",
      "in a loosely-coupled architecture, 254–255\n",
      "\n",
      "manual testing, 258–259\n",
      "\n",
      "over-the-shoulder,\n",
      "\n",
      "pair programming, , 259–263\n",
      "\n",
      "peer reviews, 249–251, ,  _f_ , 255–258\n",
      "\n",
      "Pivotal Labs case study, 260–261\n",
      "\n",
      "pull requests, ,  _f_ , 261–263\n",
      "\n",
      "review steps, 250–251\n",
      "\n",
      "small batch sizes, 255–256\n",
      "\n",
      "test-driven development, 259–260\n",
      "\n",
      "tool-assisted,\n",
      "\n",
      "traditional change controls, 252–254\n",
      "\n",
      "types of code reviews, 256–257\n",
      "\n",
      "Chaos Monkey, 272–273, 281–282,\n",
      "\n",
      "chat rooms,\n",
      "\n",
      "ChatOps, 287–289\n",
      "\n",
      "Chuvakin, Anton A.,\n",
      "\n",
      "CI. _See_ continuous integration\n",
      "\n",
      "Clanton, Ross, , 299–300,\n",
      "\n",
      "Claudius, Jonathan,\n",
      "\n",
      "Clemm, Josh, ,\n",
      "\n",
      "Cloud.gov, 325–326\n",
      "\n",
      "cluster immune system,  _n_\n",
      "\n",
      "cluster immune system release pattern, , 170–171\n",
      "\n",
      "coaching kata,\n",
      "\n",
      "Cockcoft, Adrian,  _n_ , ,\n",
      "\n",
      "code\n",
      "\n",
      "commits,\n",
      "\n",
      "configuration management tools,\n",
      "\n",
      "deployment, , 160–162\n",
      "\n",
      "deployment process automation,\n",
      "\n",
      "deployment process changes,\n",
      "\n",
      "infrastructure as,  _n_\n",
      "\n",
      "merging, 143–144\n",
      "\n",
      "migration,  _n_\n",
      "\n",
      "packaging,\n",
      "\n",
      "repositories,  _f_ ,  _f_ , 290–292, 315–317\n",
      "\n",
      "re-use, 289–290\n",
      "\n",
      "signing, 319–320\n",
      "\n",
      "source code integrity, 319–320\n",
      "\n",
      "code commits,\n",
      "\n",
      "code reviews. _See also_ change approval processes\n",
      "\n",
      "change review lead times,  _f_\n",
      "\n",
      "email pass-around,\n",
      "\n",
      "Google case study, 257–258\n",
      "\n",
      "guidelines for,\n",
      "\n",
      "over-the-shoulder,\n",
      "\n",
      "pair programming,\n",
      "\n",
      "small batch sizes, 255–256\n",
      "\n",
      "tool-assisted,\n",
      "\n",
      "types of, 256–257\n",
      "\n",
      "collective knowledge, 42–43\n",
      "\n",
      "compliance\n",
      "\n",
      "audit and compliance documentation and proof, 341–345\n",
      "\n",
      "regulatory compliance objectives, 235–236\n",
      "\n",
      "security and compliance and change approval processes, 333–335\n",
      "\n",
      "configuration management tools,  _n_\n",
      "\n",
      "constraints\n",
      "\n",
      "bottlenecks,\n",
      "\n",
      "code deployment,\n",
      "\n",
      "environment creation,\n",
      "\n",
      "overly tight architecture,\n",
      "\n",
      "test setup and run, 22–23\n",
      "\n",
      "continual experimentation and learning, 37–46\n",
      "\n",
      "continuous delivery\n",
      "\n",
      "deployment pipeline, 127–129,  _f_\n",
      "\n",
      "Google,\n",
      "\n",
      "low-risk releases, 175–177\n",
      "\n",
      "Continuous Delivery Movement, 5–6,\n",
      "\n",
      "continuous deployment, , , 175–177\n",
      "\n",
      "continuous integration\n",
      "\n",
      "case study, 149–151\n",
      "\n",
      "code merging, 143–144\n",
      "\n",
      "description of, 144–146\n",
      "\n",
      "Dev vs DevOps,  _n_\n",
      "\n",
      "frequent code commits,\n",
      "\n",
      "gated commits,\n",
      "\n",
      "integration problems,\n",
      "\n",
      "large batch development, 147–148\n",
      "\n",
      "and trunk-based development practices, 148–151\n",
      "\n",
      "and version control, 148–149\n",
      "\n",
      "Convergence of DevOps, 353–356\n",
      "\n",
      "Conway, Melvin,\n",
      "\n",
      "Conway's Law, 77–78,\n",
      "\n",
      "Cook, Scott,\n",
      "\n",
      "core chronic conflict, xxiv–xxvi, xxv _n_\n",
      "\n",
      "core conflict cloud, 356–357,  _f_\n",
      "\n",
      "Corman, Josh, ,\n",
      "\n",
      "cost of delay,  _n_\n",
      "\n",
      "COTS software,\n",
      "\n",
      "Cox, Jason, , 99–100,\n",
      "\n",
      "CSG International\n",
      "\n",
      "brownfield services,\n",
      "\n",
      "cross-training, 86–87\n",
      "\n",
      "daily deployments, 157–159\n",
      "\n",
      "Cunningham, Ward,\n",
      "\n",
      "D\n",
      "\n",
      "dark launches, 173–175\n",
      "\n",
      "dashboards,  _n_\n",
      "\n",
      "data sets. _See_ telemetry\n",
      "\n",
      "Debois, Patrick,\n",
      "\n",
      "dedicated release engineer,\n",
      "\n",
      "DeGrandis, Dominica,\n",
      "\n",
      "Dekker, Sidney\n",
      "\n",
      "just culture,\n",
      "\n",
      "safety culture, ,\n",
      "\n",
      "dependency scanning\n",
      "\n",
      "Java,\n",
      "\n",
      "Ruby on Rails,\n",
      "\n",
      "Deployinator, 163–164,  _f_\n",
      "\n",
      "deployment\n",
      "\n",
      "automated self-service, 159–160\n",
      "\n",
      "blue-green pattern, 166–169,  _f_ ,  _n_\n",
      "\n",
      "change,\n",
      "\n",
      "code, , , 160–162\n",
      "\n",
      "consistency,\n",
      "\n",
      "continuous, , 175–177\n",
      "\n",
      "daily, 157–159\n",
      "\n",
      "decoupling from releases, 164–175\n",
      "\n",
      "defined,\n",
      "\n",
      "on demand,\n",
      "\n",
      "fast, ,  _f_\n",
      "\n",
      "flow,\n",
      "\n",
      "issues,\n",
      "\n",
      "lead time, 8–11,  _f_ ,\n",
      "\n",
      "making safer, 229–230\n",
      "\n",
      "overlay of production deployment activities,\n",
      "\n",
      "pace,\n",
      "\n",
      "pipeline requirements, 156–157\n",
      "\n",
      "process automation, 155–164,  _f_\n",
      "\n",
      "self-service developer, 162–164\n",
      "\n",
      "speed and success,\n",
      "\n",
      "tool, 163–164,  _f_\n",
      "\n",
      "deployment lead time\n",
      "\n",
      "design and development,\n",
      "\n",
      "lead time vs processing time, 9–10,  _f_\n",
      "\n",
      "Lean Manufacturing,\n",
      "\n",
      "Lean Product Development,\n",
      "\n",
      "long, ,  _f_ ,\n",
      "\n",
      "short, 10–11,  _f_\n",
      "\n",
      "testing and operations,\n",
      "\n",
      "workflow,\n",
      "\n",
      "deployment pipeline\n",
      "\n",
      "breakdown, 138–140\n",
      "\n",
      "containers in,  _n_\n",
      "\n",
      "continuous delivery, 127–129,  _f_\n",
      "\n",
      "and information security, 330–331\n",
      "\n",
      "deployment pipeline protection\n",
      "\n",
      "Amazon AWS case study, 342–344\n",
      "\n",
      "audit and compliance documentation and proof, 341–345\n",
      "\n",
      "categories of changes, 334–335,  _n_\n",
      "\n",
      "compliance in regulated environments, 341–345\n",
      "\n",
      "destructive testing,\n",
      "\n",
      "Etsy case study, 339–341\n",
      "\n",
      "normal changes, , 336–338\n",
      "\n",
      "production telemetry for ATM systems, 344–345\n",
      "\n",
      "Salesforce case study, 337–338\n",
      "\n",
      "security and compliance and change approval processes, 333–335\n",
      "\n",
      "separation of duties, 338–341\n",
      "\n",
      "standard changes, , 335–336\n",
      "\n",
      "urgent changes, 334–335\n",
      "\n",
      "deployment process automation\n",
      "\n",
      "automated self-service deployments, 159–160\n",
      "\n",
      "automating manual steps, 155–156\n",
      "\n",
      "code deployment as part of deployment pipeline, 160–162\n",
      "\n",
      "code promotion processes,\n",
      "\n",
      "CSG International case study, 157–159\n",
      "\n",
      "deployment consistency,\n",
      "\n",
      "deployment flow,\n",
      "\n",
      "deployment pipeline requirements, 156–157\n",
      "\n",
      "environment consistency, ,\n",
      "\n",
      "Etsy case study, 162–164\n",
      "\n",
      "fast deployments, ,  _f_\n",
      "\n",
      "lead time reduction,\n",
      "\n",
      "MTTR, ,  _f_ ,  _f_\n",
      "\n",
      "process documentation,\n",
      "\n",
      "production incident decrease, ,  _f_\n",
      "\n",
      "self-service developer deployment, 162–164\n",
      "\n",
      "Shared Operations team,\n",
      "\n",
      "smoke testing, ,\n",
      "\n",
      "deployment vs releases, 164–175\n",
      "\n",
      "Dev in production-like environments\n",
      "\n",
      "assets to check into version control repository,\n",
      "\n",
      "automated configuration systems,\n",
      "\n",
      "benefits of automation, 114–115\n",
      "\n",
      "common build mechanisms,\n",
      "\n",
      "critical role of version control,\n",
      "\n",
      "environment consistency,\n",
      "\n",
      "environment development on demand, 113–115\n",
      "\n",
      "environment re-build vs repair,\n",
      "\n",
      "environments stored in version control, 115–116\n",
      "\n",
      "immutable infrastructure,\n",
      "\n",
      "metadata,\n",
      "\n",
      "new definition of finished development, 119–121\n",
      "\n",
      "quick environment development,\n",
      "\n",
      "shared version control repository, 115–118\n",
      "\n",
      "sprints, 119–120\n",
      "\n",
      "standardization,\n",
      "\n",
      "testing,\n",
      "\n",
      "testing environments,\n",
      "\n",
      "uses of automation,\n",
      "\n",
      "version control as predictor of organizational performance,\n",
      "\n",
      "version control systems, 115–118\n",
      "\n",
      "Development. _See_ entries under Dev\n",
      "\n",
      "development, acceptance test-driven, 134–135\n",
      "\n",
      "development, test-driven\n",
      "\n",
      "automated testing, 134–135,\n",
      "\n",
      "handling defect density,  _n_\n",
      "\n",
      "and low-risk releases,\n",
      "\n",
      "and pair programming, 259–260\n",
      "\n",
      "testing before code writing,  _n_\n",
      "\n",
      "development, trunk-based, 143–151\n",
      "\n",
      "development, waterfall,\n",
      "\n",
      "DevOps\n",
      "\n",
      "Agile Infrastructure and Velocity Movement,\n",
      "\n",
      "Agile Manifesto, 4–5\n",
      "\n",
      "business value of, xxxii–xxxiii\n",
      "\n",
      "Continuous Delivery Movement, 5–6\n",
      "\n",
      "defined,\n",
      "\n",
      "DevOpsDays,\n",
      "\n",
      "downward spiral in, xxx–xxxii\n",
      "\n",
      "ethics of, xxix–xxxv\n",
      "\n",
      "history of, 3–6\n",
      "\n",
      "Lean Movement,\n",
      "\n",
      "revolution, xxii\n",
      "\n",
      "team engagement,  _n_\n",
      "\n",
      "The Three Ways, 11–12\n",
      "\n",
      "Toyota Kata movement,\n",
      "\n",
      "DevOps myths\n",
      "\n",
      "LAMP stack, xvi\n",
      "\n",
      "MySQL, xvi\n",
      "\n",
      "PHP, xvi\n",
      "\n",
      "DevOps transformation\n",
      "\n",
      "bimodal IT,\n",
      "\n",
      "chat rooms,\n",
      "\n",
      "expanding DevOps, 58–59\n",
      "\n",
      "greenfield vs brownfield services, 54–56\n",
      "\n",
      "leveraging innovators, 57–58\n",
      "\n",
      "LinkedIn case study, 71–73\n",
      "\n",
      "making work visible,\n",
      "\n",
      "managing technical debt, 69–71\n",
      "\n",
      "phases of initiatives,\n",
      "\n",
      "rapid communication environment,\n",
      "\n",
      "reinforcing desired behavior, 73–74\n",
      "\n",
      "shared tools, 73–74\n",
      "\n",
      "shared work queue, 73–74\n",
      "\n",
      "systems of engagement, 56–57\n",
      "\n",
      "systems of record,\n",
      "\n",
      "technical debt, 69–71,  _f_\n",
      "\n",
      "technology adoption curve,  _f_\n",
      "\n",
      "transformation team, 66–73\n",
      "\n",
      "DevOpsDays, ,  _n_\n",
      "\n",
      "Dignan, Larry,\n",
      "\n",
      "Disney, , 99–100\n",
      "\n",
      "Dixons Retail, 168–169\n",
      "\n",
      "documentation\n",
      "\n",
      "automated tests as,\n",
      "\n",
      "process,\n",
      "\n",
      "downward spiral, xxvi–xxviii, xxx–xxxii, ,  _f_\n",
      "\n",
      "Drucker, Peter,\n",
      "\n",
      "Dweck, Carol,\n",
      "\n",
      "E\n",
      "\n",
      "eBay, , 179–180,  _n_ ,\n",
      "\n",
      "e-commerce sites\n",
      "\n",
      "anomaly detection techniques, 224–226\n",
      "\n",
      "application security,\n",
      "\n",
      "metrics sources,\n",
      "\n",
      "Nordstrom, 51–55\n",
      "\n",
      "Target, 91–93\n",
      "\n",
      "Edmondson, Amy C., ,\n",
      "\n",
      "Edwards, Damon, ,\n",
      "\n",
      "employee Net Promotor Score, xxxiii _n_\n",
      "\n",
      "environment consistency, ,\n",
      "\n",
      "environment security, 324–326,  _n_\n",
      "\n",
      "environments\n",
      "\n",
      "consistency, ,\n",
      "\n",
      "creation restraints,\n",
      "\n",
      "definition of,  _n_\n",
      "\n",
      "rapid communication environment,\n",
      "\n",
      "validation, 137–138\n",
      "\n",
      "environments, automated build\n",
      "\n",
      "assets to check into version control repository,\n",
      "\n",
      "automated configuration systems,\n",
      "\n",
      "benefits of automation, 114–115\n",
      "\n",
      "common build mechanisms,\n",
      "\n",
      "critical role of version control,\n",
      "\n",
      "environment consistency,\n",
      "\n",
      "environment development on demand, 113–115\n",
      "\n",
      "environment re-build vs repair,\n",
      "\n",
      "environments stored in version control, 115–116\n",
      "\n",
      "immutable infrastructure,\n",
      "\n",
      "metadata,\n",
      "\n",
      "new definition of finished development, 119–121\n",
      "\n",
      "quick environment development,\n",
      "\n",
      "shared version control repository, 115–118\n",
      "\n",
      "sprints, 119–120\n",
      "\n",
      "standardization,\n",
      "\n",
      "testing,\n",
      "\n",
      "testing environments,\n",
      "\n",
      "uses of automation,\n",
      "\n",
      "version control as predictor of organizational performance,\n",
      "\n",
      "version control systems, 115–118\n",
      "\n",
      "environments, production-like\n",
      "\n",
      "assets to check into version control repository,\n",
      "\n",
      "automated configuration systems,\n",
      "\n",
      "benefits of automation, 114–115\n",
      "\n",
      "common build mechanisms,\n",
      "\n",
      "critical role of version control,\n",
      "\n",
      "development on demand, 113–115\n",
      "\n",
      "environment consistency,\n",
      "\n",
      "environment development on demand, 113–115\n",
      "\n",
      "environment re-build vs repair,\n",
      "\n",
      "environments stored in version control, 115–116\n",
      "\n",
      "immutable infrastructure,\n",
      "\n",
      "metadata,\n",
      "\n",
      "new definition of finished development, 119–121\n",
      "\n",
      "quick environment development,\n",
      "\n",
      "shared version control repository, 115–118\n",
      "\n",
      "sprints, 119–120\n",
      "\n",
      "standardization,\n",
      "\n",
      "testing,\n",
      "\n",
      "testing environments,\n",
      "\n",
      "uses of automation,\n",
      "\n",
      "version control as predictor of organizational performance,\n",
      "\n",
      "version control systems, 115–118\n",
      "\n",
      "errors\n",
      "\n",
      "detection, ,  _n_ , 132–133\n",
      "\n",
      "management,\n",
      "\n",
      "Etsy\n",
      "\n",
      "brownfield services,\n",
      "\n",
      "case study, 77–80, 162–164\n",
      "\n",
      "code deployment,  _n_\n",
      "\n",
      "designated Ops, 100–101\n",
      "\n",
      "DevOps transformation,\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "LAMP stack at,\n",
      "\n",
      "metrics library, 204–206\n",
      "\n",
      "Morgue, 277–278\n",
      "\n",
      "MySQL at, , 297–298\n",
      "\n",
      "organizational learning,\n",
      "\n",
      "PHP, , ,  _n_\n",
      "\n",
      "production monitoring, 196–198\n",
      "\n",
      "programming languages used,  _n_\n",
      "\n",
      "publicizing post-mortems,\n",
      "\n",
      "Python, ,  _n_\n",
      "\n",
      "security telemetry, 328–330\n",
      "\n",
      "separation of duties, 339–341\n",
      "\n",
      "Sprouter, 78–80,\n",
      "\n",
      "technology stack standardization, 297–298\n",
      "\n",
      "Evans, Eric J.,\n",
      "\n",
      "Evans, Jason,\n",
      "\n",
      "Extreme Programming, ,\n",
      "\n",
      "F\n",
      "\n",
      "Facebook\n",
      "\n",
      "C++,  _n_ , ,\n",
      "\n",
      "canary release pattern,\n",
      "\n",
      "case study, 153–155\n",
      "\n",
      "dark launches, 174–175\n",
      "\n",
      "front-end codebase,  _n_\n",
      "\n",
      "Gatekeeper,  _n_\n",
      "\n",
      "JavaScript,\n",
      "\n",
      "PHP,  _n_ , , ,\n",
      "\n",
      "shared pain,\n",
      "\n",
      "technical debt,\n",
      "\n",
      "Farley, David\n",
      "\n",
      "automated testing, 126–127\n",
      "\n",
      "blue-green deployment pattern, 168–169\n",
      "\n",
      "continuous delivery, 175–176,\n",
      "\n",
      "Continuous Delivery Movement, 5–6\n",
      "\n",
      "continuous integration,  _n_\n",
      "\n",
      "infrastructure as code,  _n_\n",
      "\n",
      "Farley, Steve, ,\n",
      "\n",
      "Farrall, Paul, 95–97\n",
      "\n",
      "fast release cycle experimentation\n",
      "\n",
      "case study, 246–248\n",
      "\n",
      "Feature API,\n",
      "\n",
      "history of,\n",
      "\n",
      "integrating into feature planning, 245–248\n",
      "\n",
      "integrating into feature testing, 244–245\n",
      "\n",
      "integrating into release,\n",
      "\n",
      "outcomes of,\n",
      "\n",
      "user research, 244–245\n",
      "\n",
      "Yahoo! Answers, 246–248\n",
      "\n",
      "fast testing, , 133–134\n",
      "\n",
      "feature toggles, 171–173,  _n_ , , 229–230\n",
      "\n",
      "features\n",
      "\n",
      "consequences of new,\n",
      "\n",
      "extra,\n",
      "\n",
      "planning, 245–248\n",
      "\n",
      "testing, 244–245\n",
      "\n",
      "Federal Government agencies, 325–326\n",
      "\n",
      "feedback\n",
      "\n",
      "automated build, integration, and test processes,\n",
      "\n",
      "automated testing, ,\n",
      "\n",
      "error detection,\n",
      "\n",
      "ineffective quality controls, 32–34\n",
      "\n",
      "mechanisms for production telemetry,\n",
      "\n",
      "Ops and market-oriented outcomes,\n",
      "\n",
      "optimizing for downstream work centers, 34–35\n",
      "\n",
      "problem prevention,\n",
      "\n",
      "problem visibility, 29–30\n",
      "\n",
      "production telemetry,\n",
      "\n",
      "QA automation, 33–34\n",
      "\n",
      "safety in complex systems, 27–29\n",
      "\n",
      "swarming, 30–32\n",
      "\n",
      "feed-forward loops, ,\n",
      "\n",
      "Fernandez, Roberto, ,\n",
      "\n",
      "first stories,  _f_\n",
      "\n",
      "Fitz, Tim\n",
      "\n",
      "continuous delivery,\n",
      "\n",
      "continuous deployment, , 175–177\n",
      "\n",
      "expand/contract pattern,  _n_\n",
      "\n",
      "fix forward,\n",
      "\n",
      "fixed mindset,\n",
      "\n",
      "Flickr, 173–174\n",
      "\n",
      "Forsgren, Nicole,\n",
      "\n",
      "Fowler, Martin, , ,\n",
      "\n",
      "G\n",
      "\n",
      "Gaekwad, Karthik,\n",
      "\n",
      "Galbreath, Nick, 227–229, 328–330\n",
      "\n",
      "Game Days, 282–284\n",
      "\n",
      "Ganglia,\n",
      "\n",
      "Garcia, Rafael,\n",
      "\n",
      "Gatekeeper,  _n_ ,\n",
      "\n",
      "Gauntlt, ,\n",
      "\n",
      "General Motors Fremont plant, , ,\n",
      "\n",
      "generative organizations, 39–40\n",
      "\n",
      "GitHub\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "GitHub Flow,\n",
      "\n",
      "organizational knowledge, 287–289\n",
      "\n",
      "peer reviews, 249–251\n",
      "\n",
      "GitHub Flow,\n",
      "\n",
      "goal setting,\n",
      "\n",
      "Google\n",
      "\n",
      "automated testing,  _n_\n",
      "\n",
      "C++,  _n_\n",
      "\n",
      "code reviews, 257–258\n",
      "\n",
      "continuous delivery,\n",
      "\n",
      "Disaster Recovery Program (DiRT),\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "imposter syndrome,  _n_\n",
      "\n",
      "Java,  _n_\n",
      "\n",
      "JavaScript,  _n_\n",
      "\n",
      "launch and hand-off readiness reviews, 237–239\n",
      "\n",
      "production service,\n",
      "\n",
      "programming languages used,  _n_\n",
      "\n",
      "publicizing post-mortems,\n",
      "\n",
      "Python,\n",
      "\n",
      "service-oriented architecture,\n",
      "\n",
      "source code repository, 291–292\n",
      "\n",
      "Google App Engine,\n",
      "\n",
      "Google Cloud Datastore,  _f_ ,\n",
      "\n",
      "Google Web Server\n",
      "\n",
      "Andon cord,\n",
      "\n",
      "automated testing, 123–126\n",
      "\n",
      "C++,\n",
      "\n",
      "case study, 123–126\n",
      "\n",
      "Fixit Grouplet,\n",
      "\n",
      "Testing Grouplet, 124–126, 306–307\n",
      "\n",
      "Govindarajan, Vijay,\n",
      "\n",
      "Grafana, ,\n",
      "\n",
      "Graphite, , ,\n",
      "\n",
      "Gray, Jim,\n",
      "\n",
      "greenfield services. _See_ services, greenfield\n",
      "\n",
      "growth mindset,\n",
      "\n",
      "Gruver, Gary\n",
      "\n",
      "automated testing, ,\n",
      "\n",
      "continuous integration, 144–146\n",
      "\n",
      "ineffective quality controls,\n",
      "\n",
      "Gupta, Prachi, 207–208\n",
      "\n",
      "GWS. _See_ Google Web Server\n",
      "\n",
      "H\n",
      "\n",
      "Hammant, Paul,  _n_\n",
      "\n",
      "Hammond, Paul, ,\n",
      "\n",
      "Hand-Off Readiness Review, 237–239,  _f_\n",
      "\n",
      "handoffs\n",
      "\n",
      "dangers of, 358–359\n",
      "\n",
      "loss of knowledge,\n",
      "\n",
      "reducing batch size,\n",
      "\n",
      "workflow management,\n",
      "\n",
      "hardening phase,  _n_\n",
      "\n",
      "Hendrickson, Elisabeth, , , 260–261\n",
      "\n",
      "heroics, , ,  _n_\n",
      "\n",
      "high-trust culture, 37–38\n",
      "\n",
      "HipHop compiler,\n",
      "\n",
      "Hodge, Victoria J., 215–216\n",
      "\n",
      "Hollnagel, Erik, 359–360\n",
      "\n",
      "HP, 144–146\n",
      "\n",
      "HRR. _See_ Hand-Off Readiness Review\n",
      "\n",
      "Humble, Jez\n",
      "\n",
      "automated testing, 126–127\n",
      "\n",
      "continuous delivery, 175–176\n",
      "\n",
      "Continuous Delivery Movement, 5–6\n",
      "\n",
      "continuous integration,  _n_\n",
      "\n",
      "dangers of change approval processes,\n",
      "\n",
      "evolutionary architecture,\n",
      "\n",
      "infrastructure as code,  _n_\n",
      "\n",
      "user research,\n",
      "\n",
      "hypothesis-driven development, 241–248\n",
      "\n",
      "I\n",
      "\n",
      "ICHT, 339–341\n",
      "\n",
      "Imbriaco, Mark,\n",
      "\n",
      "imposter syndrome,  _n_\n",
      "\n",
      "improvement blitz\n",
      "\n",
      "organizational learning and improvement,\n",
      "\n",
      "Spear, Steven,\n",
      "\n",
      "Toyota Production System,\n",
      "\n",
      "improvement goal examples,\n",
      "\n",
      "improvement kata, ,\n",
      "\n",
      "industrial safety, 359–360\n",
      "\n",
      "information radiator, 206–208\n",
      "\n",
      "information security\n",
      "\n",
      "18F team, 325–326\n",
      "\n",
      "application security, 318–323\n",
      "\n",
      "automated security testing,  _f_\n",
      "\n",
      "bad paths,\n",
      "\n",
      "Brakeman, ,  _f_\n",
      "\n",
      "build images,\n",
      "\n",
      "Cloud.gov, 325–326\n",
      "\n",
      "code signing, 319–320\n",
      "\n",
      "creating security telemetry, 327–330\n",
      "\n",
      "data breaches, 323–324\n",
      "\n",
      "and defect tracking and post-mortems,\n",
      "\n",
      "dependency scanning,\n",
      "\n",
      "and the deployment pipeline, 317–318, 330–331\n",
      "\n",
      "dynamic analysis,\n",
      "\n",
      "environment security, 324–326\n",
      "\n",
      "Etsy case study, 328–330\n",
      "\n",
      "Federal Government case study, 325–326\n",
      "\n",
      "Gauntlt, ,\n",
      "\n",
      "Graphite,  _f_\n",
      "\n",
      "happy paths,\n",
      "\n",
      "integrating into production telemetry, 326–327\n",
      "\n",
      "inviting InfoSec to product demonstrations,\n",
      "\n",
      "Java,\n",
      "\n",
      "Metasploit,\n",
      "\n",
      "Nmap,\n",
      "\n",
      "preventive security controls, 315–317\n",
      "\n",
      "Ruby on Rails,\n",
      "\n",
      "rugged DevOps,\n",
      "\n",
      "sad paths,\n",
      "\n",
      "security libraries,\n",
      "\n",
      "shared code repositories and services, 315–317\n",
      "\n",
      "software supply chain security, 323–324\n",
      "\n",
      "source code integrity, 319–320\n",
      "\n",
      "SQL injection attempts, ,  _f_\n",
      "\n",
      "static analysis, , 320–323\n",
      "\n",
      "Twitter case study, 320–323\n",
      "\n",
      "value stream,\n",
      "\n",
      "Infosec. _See_ information security\n",
      "\n",
      "infrastructure as code,  _n_\n",
      "\n",
      "infrastructure metrics,  _n_\n",
      "\n",
      "InGraphs,\n",
      "\n",
      "integrated development environment,  _n_\n",
      "\n",
      "integration,  _n_\n",
      "\n",
      "Intuit, 241–248\n",
      "\n",
      "iteration length,\n",
      "\n",
      "ITIL,  _n_ ,  _n_ , , ,  _n_\n",
      "\n",
      "ITIL CMDB, ,  _n_\n",
      "\n",
      "J\n",
      "\n",
      "Jacob, Adam,  _n_\n",
      "\n",
      "Jacques, Olivier,\n",
      "\n",
      "Java\n",
      "\n",
      "automation,\n",
      "\n",
      "Bazaarvoice,\n",
      "\n",
      "dependency scanning,\n",
      "\n",
      "eBay,  _n_\n",
      "\n",
      "Google,  _n_\n",
      "\n",
      "information security,\n",
      "\n",
      "LinkedIn,\n",
      "\n",
      "logging infrastructure,  _n_\n",
      "\n",
      "ORM,  _n_\n",
      "\n",
      "production metrics,\n",
      "\n",
      "threading libraries,\n",
      "\n",
      "JavaScript\n",
      "\n",
      "Facebook,\n",
      "\n",
      "Google,  _n_\n",
      "\n",
      "production telemetry,\n",
      "\n",
      "Jones, Daniel T.,\n",
      "\n",
      "just culture,\n",
      "\n",
      "K\n",
      "\n",
      "Kalantzis, Christos, 281–282\n",
      "\n",
      "kanban boards\n",
      "\n",
      "and the Lean Movement,\n",
      "\n",
      "sharing between Ops and Dev,\n",
      "\n",
      "workflow management, 16–17,  _f_\n",
      "\n",
      "Kanies, Luke,  _n_\n",
      "\n",
      "Kastner, Erik, 163–164\n",
      "\n",
      "Kim, Gene, ,  _n_ , , ,\n",
      "\n",
      "Kissler, Courtney, 51–54, 61–62\n",
      "\n",
      "Knight Capital, 251–252\n",
      "\n",
      "knowledge sharing, 42–43\n",
      "\n",
      "Kohavi, Ronny,\n",
      "\n",
      "Krishnan, Kripa,\n",
      "\n",
      "L\n",
      "\n",
      "LAMP stack\n",
      "\n",
      "DevOps myths, xvi\n",
      "\n",
      "at Etsy,\n",
      "\n",
      "Lauderbach, John,  _n_\n",
      "\n",
      "Launch Readiness Review, 238–239\n",
      "\n",
      "lead time\n",
      "\n",
      "change review,  _f_\n",
      "\n",
      "deployment, 8–11,  _f_ ,\n",
      "\n",
      "and the Lean Movement,\n",
      "\n",
      "reduction,\n",
      "\n",
      "Lean Manufacturing\n",
      "\n",
      "deployment lead time, 8–11\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "manufacturing value stream, 7–8\n",
      "\n",
      "technology value stream, 8–11\n",
      "\n",
      "The Three Ways, 11–12\n",
      "\n",
      "Lean Movement, ,\n",
      "\n",
      "Lean principles, xxii\n",
      "\n",
      "Lean Product Development,\n",
      "\n",
      "Lean Startup Movement,\n",
      "\n",
      "Lean UX Movement,\n",
      "\n",
      "learning culture\n",
      "\n",
      "Amazon AWS case study, 271–273\n",
      "\n",
      "amplifying weak failure signals, 279–280\n",
      "\n",
      "bad apple theory,\n",
      "\n",
      "blameless post-mortems, 274–276\n",
      "\n",
      "Chaos Monkey, 272–273, 281–282\n",
      "\n",
      "Game Days, 282–284\n",
      "\n",
      "injecting faults into production environment, 281–282,  _n_\n",
      "\n",
      "just culture, 273–274\n",
      "\n",
      "leaders and, 44–46\n",
      "\n",
      "Morgue, 277–278\n",
      "\n",
      "Netflix case study, 271–273\n",
      "\n",
      "publicizing post-mortems, 277–278\n",
      "\n",
      "redefining failure, 280–281\n",
      "\n",
      "rehearsing failures, 282–284\n",
      "\n",
      "resilience, 281–282\n",
      "\n",
      "resilience engineering, 282–283\n",
      "\n",
      "The Third Way, 44–46\n",
      "\n",
      "and Toyota Kata,\n",
      "\n",
      "and Toyota Production System,\n",
      "\n",
      "Lesiecki, Nick, 306–307\n",
      "\n",
      "Letuchy, Eugene, 174–175\n",
      "\n",
      "Lightbody, Patrick,\n",
      "\n",
      "Limoncelli, Tom, , 238–239, , 296–297\n",
      "\n",
      "LinkedIn\n",
      "\n",
      "case study, 71–73\n",
      "\n",
      "Java,\n",
      "\n",
      "Operation Inversion,\n",
      "\n",
      "Oracle,\n",
      "\n",
      "self-service metrics, 207–208\n",
      "\n",
      "Little, Christopher, xxvii,\n",
      "\n",
      "logging, 201–203,  _n_\n",
      "\n",
      "logging infrastructure\n",
      "\n",
      "Java,  _n_\n",
      "\n",
      "Ruby on Rails,  _n_\n",
      "\n",
      "Loura, Ralph,\n",
      "\n",
      "Love, Paul,\n",
      "\n",
      "low-risk releases\n",
      "\n",
      "application-based release patterns, 165–166, 171–175\n",
      "\n",
      "automated self-service deployments, 159–160\n",
      "\n",
      "automating manual steps, 155–156\n",
      "\n",
      "blue-green deployment pattern, 166–169,  _f_\n",
      "\n",
      "canary release pattern, 169–171,  _f_\n",
      "\n",
      "canary tests,\n",
      "\n",
      "cluster immune system release pattern, , 170–171\n",
      "\n",
      "code deployment as part of deployment pipeline, 160–162\n",
      "\n",
      "code deployment process changes,\n",
      "\n",
      "code promotion processes,\n",
      "\n",
      "continuous delivery, 175–177\n",
      "\n",
      "continuous deployment, 175–177\n",
      "\n",
      "CSG International case study, 157–159\n",
      "\n",
      "dark launches, 173–175\n",
      "\n",
      "database changes, 167–168,  _n_\n",
      "\n",
      "decoupling deployment from releases, 164–175\n",
      "\n",
      "deployment consistency,\n",
      "\n",
      "deployment defined,\n",
      "\n",
      "deployment flow,\n",
      "\n",
      "deployment lead time,\n",
      "\n",
      "deployment on demand,\n",
      "\n",
      "deployment pace,\n",
      "\n",
      "deployment pipeline requirements, 156–157\n",
      "\n",
      "deployment process automation, 155–164\n",
      "\n",
      "deployment tool, 163–164,  _f_\n",
      "\n",
      "Dixons Retail case study, 168–169\n",
      "\n",
      "environment consistency, ,\n",
      "\n",
      "environment-based release patterns, , 166–171\n",
      "\n",
      "Etsy case study, 162–164\n",
      "\n",
      "evolutionary architecture, 179–189\n",
      "\n",
      "Facebook case study, 174–175\n",
      "\n",
      "fast deployments, ,  _f_\n",
      "\n",
      "feature toggles, 171–173,\n",
      "\n",
      "lead time reduction,\n",
      "\n",
      "MTTR, ,  _f_ ,  _f_\n",
      "\n",
      "performance degradation,\n",
      "\n",
      "point-of-sale systems, 168–169\n",
      "\n",
      "process documentation,\n",
      "\n",
      "production incident decrease, ,  _f_\n",
      "\n",
      "release frequency, ,  _f_\n",
      "\n",
      "release risk,\n",
      "\n",
      "releases defined, 164–165\n",
      "\n",
      "resilience,\n",
      "\n",
      "roll back,\n",
      "\n",
      "self-service developer deployment, 162–164\n",
      "\n",
      "Shared Operations team,\n",
      "\n",
      "smoke testing, ,\n",
      "\n",
      "LRR. _See_ Launch Readiness Review\n",
      "\n",
      "M\n",
      "\n",
      "Macri, Bethany,\n",
      "\n",
      "Macys.com,\n",
      "\n",
      "making work visible, 15–17, ,\n",
      "\n",
      "Malpass, Ian, ,\n",
      "\n",
      "Mangot, Dave, 337–338\n",
      "\n",
      "manufacturing lead time,\n",
      "\n",
      "manufacturing value stream\n",
      "\n",
      "defined,\n",
      "\n",
      "description of, 7–8\n",
      "\n",
      "feedback issues,\n",
      "\n",
      "integrating learning,\n",
      "\n",
      "low-trust environment,\n",
      "\n",
      "workflow, 7–8\n",
      "\n",
      "Marsh, Dianne,\n",
      "\n",
      "Martin, Karen,\n",
      "\n",
      "Massie, Bill, ,\n",
      "\n",
      "Mathew, Reena, 337–338\n",
      "\n",
      "Maximilien, E. Michael,  _n_\n",
      "\n",
      "McDonnell, Patrick,\n",
      "\n",
      "McKinley, Dan,\n",
      "\n",
      "Mediratta, Bharat, , 306–307\n",
      "\n",
      "Messeri, Eran, ,\n",
      "\n",
      "metadata,\n",
      "\n",
      "Metasploit,\n",
      "\n",
      "metrics\n",
      "\n",
      "actionable business,  _f_\n",
      "\n",
      "applications and business, 210–212\n",
      "\n",
      "infrastructure, 212–213\n",
      "\n",
      "libraries, 204–205\n",
      "\n",
      "production, 204–206\n",
      "\n",
      "self-service, 207–208\n",
      "\n",
      "sources,\n",
      "\n",
      "for telemetry improvement, 65–66\n",
      "\n",
      "Metz, Cade,\n",
      "\n",
      "Mickman, Heather, 91–93, ,\n",
      "\n",
      "Microsoft,\n",
      "\n",
      "Microsoft Operations Framework,\n",
      "\n",
      "Milstein, Dan,\n",
      "\n",
      "minimum viable product,\n",
      "\n",
      "MOF. _See_ Microsoft Operations Framework\n",
      "\n",
      "Moore, Geoffrey A.,\n",
      "\n",
      "Morgue, 277–278\n",
      "\n",
      "MTTR\n",
      "\n",
      "deployment process automation, ,  _f_\n",
      "\n",
      "recording,\n",
      "\n",
      "telemetry, ,  _f_\n",
      "\n",
      "Mueller, Ernst,  _n_ , 149–151,\n",
      "\n",
      "Mulkey, Jody, 84–85,\n",
      "\n",
      "MySQL\n",
      "\n",
      "DevOps myths, xvi\n",
      "\n",
      "at Etsy, , 297–298\n",
      "\n",
      "N\n",
      "\n",
      "Nagappan, Nachi,  _n_\n",
      "\n",
      "Nagios,\n",
      "\n",
      "NASA, 279–280\n",
      "\n",
      "National Instruments, 54–55\n",
      "\n",
      "Nationwide Insurance, 304–306\n",
      "\n",
      "Netflix\n",
      "\n",
      "auto-scaling capacity, 221–222\n",
      "\n",
      "market-oriented organization,\n",
      "\n",
      "Netflix AWS,  _n_\n",
      "\n",
      "organizational values,  _n_\n",
      "\n",
      "redefining failure, 280–281\n",
      "\n",
      "resilience, 271–273, 281–282\n",
      "\n",
      "self-service platforms,\n",
      "\n",
      "telemetry analysis, 215–216\n",
      "\n",
      "Newland, Jesse, 287–289\n",
      "\n",
      "Nike,\n",
      "\n",
      "Nmap,\n",
      "\n",
      "non-functional requirements,\n",
      "\n",
      "Nordstrom, 51–55, 61–62\n",
      "\n",
      "North, Dan, 168–169,\n",
      "\n",
      "NR Program, 42–43\n",
      "\n",
      "Nygard, Michael,\n",
      "\n",
      "O\n",
      "\n",
      "Obidos,\n",
      "\n",
      "Object Relational Mapping layer, ,  _n_\n",
      "\n",
      "O'Donnell, Glenn,\n",
      "\n",
      "Ohno, Taiichi,  _n_\n",
      "\n",
      "O'Neill, Paul, ,\n",
      "\n",
      "Open Web Application Security Project, ,  _n_ ,  _n_\n",
      "\n",
      "Operation Desert Shield,  _n_\n",
      "\n",
      "Operation Inversion,\n",
      "\n",
      "Operations. _See_ entries under Ops\n",
      "\n",
      "Ops and market-oriented outcomes\n",
      "\n",
      "creating shared services, 97–99\n",
      "\n",
      "engineers embedded into service teams, 99–100\n",
      "\n",
      "feedback,\n",
      "\n",
      "integration into Dev rituals, 101–104\n",
      "\n",
      "internal shared services teams, 97–99\n",
      "\n",
      "liaisons assigned to service teams, 100–101\n",
      "\n",
      "making work visible,\n",
      "\n",
      "Ops liaisons,\n",
      "\n",
      "participation in Dev retrospectives, 102–103\n",
      "\n",
      "participation in Dev standups,\n",
      "\n",
      "self-service platforms, 97–98\n",
      "\n",
      "silos,\n",
      "\n",
      "tool standardization, 98–99\n",
      "\n",
      "optimizing for downstream work centers\n",
      "\n",
      "customer types,\n",
      "\n",
      "Designing for Manufacturing principles,\n",
      "\n",
      "Oracle\n",
      "\n",
      "application configuration settings,\n",
      "\n",
      "COTS software,\n",
      "\n",
      "eBay,  _n_\n",
      "\n",
      "ERP code migration,  _n_\n",
      "\n",
      "LinkedIn,\n",
      "\n",
      "organizational cultures, ,  _f_\n",
      "\n",
      "organizational knowledge\n",
      "\n",
      "automated tests as documentation,\n",
      "\n",
      "automating standardized processes, 289–290\n",
      "\n",
      "ChatOps, 287–289\n",
      "\n",
      "code re-use, 289–290\n",
      "\n",
      "codified non-functional requirements,\n",
      "\n",
      "communities of practice,\n",
      "\n",
      "Etsy case study, 297–298\n",
      "\n",
      "Hubot, 287–289\n",
      "\n",
      "integrating automation into chat rooms, 287–289\n",
      "\n",
      "resuable Ops user stories,\n",
      "\n",
      "source code repository, 290–292\n",
      "\n",
      "technology choices to achieve organizational goals, 295–298\n",
      "\n",
      "technology stack standardization, 297–298\n",
      "\n",
      "test-driven development,\n",
      "\n",
      "using chat rooms and chat bots, 287–289\n",
      "\n",
      "organizational learning and improvement\n",
      "\n",
      "attending external conferences, 304–306\n",
      "\n",
      "blitz goals,\n",
      "\n",
      "Capitol One case study, 304–306\n",
      "\n",
      "enable learning and teaching, 303–304\n",
      "\n",
      "improvement blitz,\n",
      "\n",
      "internal consulting and coaching, 306–307\n",
      "\n",
      "kaizen blitz,\n",
      "\n",
      "Nationwide Insurance case study, 304–306\n",
      "\n",
      "paying down technical debt, 300–303\n",
      "\n",
      "Target case study, 299–300, 304–306\n",
      "\n",
      "The Third Way, 38–40\n",
      "\n",
      "organizations\n",
      "\n",
      "functional-oriented, ,\n",
      "\n",
      "market-oriented, 80–81\n",
      "\n",
      "matrix-oriented,\n",
      "\n",
      "ORM. _See_ Object Relational Mapping layer\n",
      "\n",
      "Osterling, Mike,\n",
      "\n",
      "Özil, Giray,\n",
      "\n",
      "P\n",
      "\n",
      "pair programming\n",
      "\n",
      "and code reviews,\n",
      "\n",
      "description of, 259–263\n",
      "\n",
      "pair programmed rollback,  _n_\n",
      "\n",
      "pairing hours,  _n_\n",
      "\n",
      "Pivotal Labs case study, 260–261\n",
      "\n",
      "Pal, Tapabrata, , ,\n",
      "\n",
      "Pandey, Ravi,\n",
      "\n",
      "Paroski, Drew,\n",
      "\n",
      "pathological organizations,\n",
      "\n",
      "PayPal,  _n_\n",
      "\n",
      "Perl\n",
      "\n",
      "Blackboard Learn,\n",
      "\n",
      "eBay,  _n_\n",
      "\n",
      "production metrics,\n",
      "\n",
      "Perrow, Charles,\n",
      "\n",
      "PHP\n",
      "\n",
      "Conway's Law,\n",
      "\n",
      "DevOps myths, xvi\n",
      "\n",
      "Etsy, , ,  _n_\n",
      "\n",
      "Facebook,  _n_ , , ,\n",
      "\n",
      "ORM,\n",
      "\n",
      "production telemetry, , ,  _f_\n",
      "\n",
      "Pivotal Labs, 260–261\n",
      "\n",
      "point-of-sale systems, 168–169\n",
      "\n",
      "Poppendieck, Mary,\n",
      "\n",
      "Poppendieck, Tom,\n",
      "\n",
      "post-mortems. _See_ blameless post-mortems\n",
      "\n",
      "problem visibility, 29–30\n",
      "\n",
      "problems\n",
      "\n",
      "fact-based problem solving, 203–204\n",
      "\n",
      "integration,\n",
      "\n",
      "leaders and problem solving,\n",
      "\n",
      "prevention of,\n",
      "\n",
      "problem visibility, 29–30\n",
      "\n",
      "swarming of smaller,\n",
      "\n",
      "production metrics\n",
      "\n",
      "Java,\n",
      "\n",
      "Perl,\n",
      "\n",
      "Python,\n",
      "\n",
      "Ruby on Rails,\n",
      "\n",
      "production monitoring. _See_ telemetry\n",
      "\n",
      "production telemetry. _See also_ telemetry; telemetry analysis\n",
      "\n",
      "contextual inquiry, 232–233\n",
      "\n",
      "feature toggles, 229–230\n",
      "\n",
      "feedback mechanisms,\n",
      "\n",
      "fix forward, ,  _f_\n",
      "\n",
      "function-oriented teams, 231–232\n",
      "\n",
      "Google case study, 237–239\n",
      "\n",
      "Hand-Off Readiness Review, 238–239\n",
      "\n",
      "improving flow,  _n_\n",
      "\n",
      "JavaScript,\n",
      "\n",
      "launch and hand-off readiness reviews, 237–239,  _f_\n",
      "\n",
      "launch guidance, 234–235\n",
      "\n",
      "Launch Readiness Review, 238–239\n",
      "\n",
      "making deployments safer, 229–230\n",
      "\n",
      "market-oriented teams,\n",
      "\n",
      "pager rotation duties, 230–232\n",
      "\n",
      "PHP, , ,  _f_\n",
      "\n",
      "production service, 234–239\n",
      "\n",
      "regulatory compliance objectives, 235–236\n",
      "\n",
      "roll back,\n",
      "\n",
      "service handback mechanism,  _f_ ,\n",
      "\n",
      "site reliability engineers, 237–239\n",
      "\n",
      "UX observation, ,  _n_\n",
      "\n",
      "Prugh, Scott\n",
      "\n",
      "and bimodal IT,\n",
      "\n",
      "cross-training, 86–87\n",
      "\n",
      "daily deployments, 157–158\n",
      "\n",
      "telemetry,\n",
      "\n",
      "Puppet Labs, 159–160\n",
      "\n",
      "Python\n",
      "\n",
      "Etsy, ,  _n_\n",
      "\n",
      "Google,\n",
      "\n",
      "ORM,  _n_\n",
      "\n",
      "production metrics,\n",
      "\n",
      "Q\n",
      "\n",
      "quality controls, 32–34\n",
      "\n",
      "queue time, 358–359,  _f_\n",
      "\n",
      "queues\n",
      "\n",
      "long,\n",
      "\n",
      "queue time, 358–359,  _f_\n",
      "\n",
      "shared work, 73–74\n",
      "\n",
      "size, ,  _n_\n",
      "\n",
      "R\n",
      "\n",
      "Rachitsky, Lenny,\n",
      "\n",
      "Rajan, Karthik,\n",
      "\n",
      "Rapoport, Roy, 215–216, 280–281\n",
      "\n",
      "Rational Unified Process, ,\n",
      "\n",
      "Raymond, Eric A.,\n",
      "\n",
      "Red Hat,\n",
      "\n",
      "Reddy, Tarun, 222–223\n",
      "\n",
      "reinforcing desired behavior, 73–74\n",
      "\n",
      "release patterns\n",
      "\n",
      "application-based, 165–166, 171–175\n",
      "\n",
      "canary, 169–171,  _f_\n",
      "\n",
      "cluster immune system, , 170–171\n",
      "\n",
      "environment-based, , 166–171\n",
      "\n",
      "releases, 164–165\n",
      "\n",
      "Rembetsy, Michael, , ,\n",
      "\n",
      "repositories\n",
      "\n",
      "code,  _f_ ,  _f_ , 290–292, 315–317\n",
      "\n",
      "version control, 115–118\n",
      "\n",
      "resilience, 43–44, , 271–273, 281–282\n",
      "\n",
      "rework,\n",
      "\n",
      "Richardson, Vernon, xxviii _n_\n",
      "\n",
      "Ries, Eric, ,  _n_ ,\n",
      "\n",
      "Right Media, 227–229\n",
      "\n",
      "Robbins, Jesse, ,\n",
      "\n",
      "Roberto, Michael,\n",
      "\n",
      "roll back,  _n_ ,\n",
      "\n",
      "Rossi, Chuck, 153–155,  _n_\n",
      "\n",
      "Rother, Mike\n",
      "\n",
      "coaching kata,\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "improvement kata,\n",
      "\n",
      "Toyota Kata movement,\n",
      "\n",
      "Ruby on Rails\n",
      "\n",
      "automation,\n",
      "\n",
      "blue-green deployment,  _n_\n",
      "\n",
      "dependency scanning,\n",
      "\n",
      "information security,\n",
      "\n",
      "logging infrastructure,  _n_\n",
      "\n",
      "ORM,\n",
      "\n",
      "production metrics,\n",
      "\n",
      "Rugged Computing Movement, 355–356\n",
      "\n",
      "S\n",
      "\n",
      "Sachs, Marcus,\n",
      "\n",
      "safety culture, , 38–40\n",
      "\n",
      "safety in complex systems, 27–29\n",
      "\n",
      "safety in the workplace, 41–42\n",
      "\n",
      "Salesforce.com, 337–338\n",
      "\n",
      "Schafer, Andrew, ,\n",
      "\n",
      "Schwaber, Ken,  _n_\n",
      "\n",
      "Scott, Kevin, 72–73\n",
      "\n",
      "Scrum methodology,  _n_ , 119–120\n",
      "\n",
      "Scryer, 221–222\n",
      "\n",
      "second stories,  _f_\n",
      "\n",
      "self-service platforms, 97–98, 206–208\n",
      "\n",
      "Senge, Peter\n",
      "\n",
      "learning organizations,\n",
      "\n",
      "problem visibility,\n",
      "\n",
      "service handback mechanism,  _f_ , ,  _n_\n",
      "\n",
      "services, brownfield\n",
      "\n",
      "CSG International,\n",
      "\n",
      "defined,\n",
      "\n",
      "DevOps transformation, 54–56\n",
      "\n",
      "Etsy,\n",
      "\n",
      "improving speed and quality,\n",
      "\n",
      "with largest potential business benefit,  _n_\n",
      "\n",
      "transformations of, 55–56\n",
      "\n",
      "value streams, 54–56\n",
      "\n",
      "services, greenfield\n",
      "\n",
      "consequences of new features,\n",
      "\n",
      "defined,\n",
      "\n",
      "types of projects, 54–56\n",
      "\n",
      "services, immutable,\n",
      "\n",
      "services, shared, 97–99\n",
      "\n",
      "services, versioned,\n",
      "\n",
      "Shingo, Shigeo,\n",
      "\n",
      "Shinn, Bill, 342–344\n",
      "\n",
      "Shoup, Randy\n",
      "\n",
      "deployment pipeline breakdowns,\n",
      "\n",
      "evolutionary architecture, ,\n",
      "\n",
      "loosely-coupled architecture,\n",
      "\n",
      "peer reviews of code changes, 255–256\n",
      "\n",
      "publicizing post-mortems,\n",
      "\n",
      "source code repository, 291–292\n",
      "\n",
      "silos\n",
      "\n",
      "Ops and market-oriented outcomes,\n",
      "\n",
      "team organization,\n",
      "\n",
      "Simian Army, 364–365\n",
      "\n",
      "single-piece flow, ,\n",
      "\n",
      "smoke testing, ,\n",
      "\n",
      "smoothing, ,  _f_ ,  _n_\n",
      "\n",
      "SOAs. _See_ architecture, service-oriented\n",
      "\n",
      "Souders, Steve,\n",
      "\n",
      "Spafford, George, ,  _n_ ,\n",
      "\n",
      "Spear, Steven\n",
      "\n",
      "conditions for safety,\n",
      "\n",
      "improvement blitz,\n",
      "\n",
      "IT failures, xxvii\n",
      "\n",
      "organizational learning,\n",
      "\n",
      "paying down technical debt,\n",
      "\n",
      "resilience,\n",
      "\n",
      "workplace safety, 41–42\n",
      "\n",
      "sprint planning boards, 16–17\n",
      "\n",
      "sprints, 119–120\n",
      "\n",
      "Sprouter, 78–80,\n",
      "\n",
      "stabilization phase,  _n_\n",
      "\n",
      "stack engineers,\n",
      "\n",
      "StatsD, 204–205\n",
      "\n",
      "Stillman, Jessica,\n",
      "\n",
      "Stoneham, Him, 246–248\n",
      "\n",
      "strangler application pattern, , 185–189,  _n_\n",
      "\n",
      "Sussman, Noah, 162–163\n",
      "\n",
      "Sussna, Jeff,  _n_\n",
      "\n",
      "swarming\n",
      "\n",
      "Andon cord, ,\n",
      "\n",
      "and common management practice,\n",
      "\n",
      "goal of,\n",
      "\n",
      "reasons for,\n",
      "\n",
      "of smaller problems,\n",
      "\n",
      "systems of engagement\n",
      "\n",
      "defined, 56–57\n",
      "\n",
      "and related brownfield systems of record,\n",
      "\n",
      "systems of record,\n",
      "\n",
      "T\n",
      "\n",
      "Tableau,\n",
      "\n",
      "Target\n",
      "\n",
      "API enablement, 91–93\n",
      "\n",
      "case study, 91–93\n",
      "\n",
      "cutting bureaucratic processes, 263–264\n",
      "\n",
      "DevOps Dojo, 299–300\n",
      "\n",
      "first DevOpsDays,  _n_\n",
      "\n",
      "internal technology conferences, 304–306\n",
      "\n",
      "TDD. _See_ development, test-driven\n",
      "\n",
      "team organization\n",
      "\n",
      "API enablement at Target, 91–93\n",
      "\n",
      "bounded contexts,\n",
      "\n",
      "business logic changes, ,\n",
      "\n",
      "business relationship manager,\n",
      "\n",
      "collaboration,\n",
      "\n",
      "Conway's Law, 77–78,\n",
      "\n",
      "cross-functional and independent teams,\n",
      "\n",
      "cross-training, 85–87\n",
      "\n",
      "database stored procedures changes,\n",
      "\n",
      "decreasing handoffs,\n",
      "\n",
      "dedicated release engineer,\n",
      "\n",
      "deployment issues,\n",
      "\n",
      "deployment speed and success,\n",
      "\n",
      "embedding needed skills, 82–83\n",
      "\n",
      "fixed mindset,\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "funding services and products, 87–88\n",
      "\n",
      "growth mindset,\n",
      "\n",
      "integrating Ops into Dev teams, 95–105\n",
      "\n",
      "internal shared services teams, 97–99\n",
      "\n",
      "long queues,\n",
      "\n",
      "loosely-coupled architecture, 89–93\n",
      "\n",
      "making functional orientation work, 83–84,  _f_\n",
      "\n",
      "market orientations, 80–81, 82–83\n",
      "\n",
      "matrix-orientations,\n",
      "\n",
      "optimizing for cost, 81–82\n",
      "\n",
      "optimizing for speed, 82–83\n",
      "\n",
      "quality as shared goal, 84–85\n",
      "\n",
      "service interactions via APIs,\n",
      "\n",
      "service-oriented architecture,\n",
      "\n",
      "shared pain,\n",
      "\n",
      "silos,\n",
      "\n",
      "specialists vs. generalists, 85–87,  _f_\n",
      "\n",
      "stack engineers,\n",
      "\n",
      "synchronization,\n",
      "\n",
      "team boundaries,\n",
      "\n",
      "team size, 90–91\n",
      "\n",
      "testing and operations,\n",
      "\n",
      "two-pizza team, 90–91\n",
      "\n",
      "team size, 90–91\n",
      "\n",
      "teams\n",
      "\n",
      "18F team, 325–326\n",
      "\n",
      "development,\n",
      "\n",
      "function-oriented, 231–232\n",
      "\n",
      "market-oriented,\n",
      "\n",
      "service,  _n_ , 97–101\n",
      "\n",
      "shared operations, ,  _n_\n",
      "\n",
      "team formation,\n",
      "\n",
      "team size, 90–91\n",
      "\n",
      "technology value stream,\n",
      "\n",
      "testing, 124–126\n",
      "\n",
      "transformation, 66–74\n",
      "\n",
      "two-pizza team, 90–91\n",
      "\n",
      "technical debt\n",
      "\n",
      "description of,\n",
      "\n",
      "managing, 69–71\n",
      "\n",
      "paying down, , 300–303\n",
      "\n",
      "reducing, 69–71,  _f_\n",
      "\n",
      "technology adoption curve,  _f_\n",
      "\n",
      "technology value stream\n",
      "\n",
      "absence of fast feedback, 29–30\n",
      "\n",
      "creating a high-trust culture, 37–38\n",
      "\n",
      "defined,\n",
      "\n",
      "deployment lead time, 8–11\n",
      "\n",
      "inputs,\n",
      "\n",
      "integrating learning, 37–38\n",
      "\n",
      "responses to incidents and accidents, 38–39\n",
      "\n",
      "telemetry. _See also_ production telemetry; telemetry analysis\n",
      "\n",
      "actionable business metrics,  _f_\n",
      "\n",
      "application logging, 201–203\n",
      "\n",
      "applications and business metrics, 210–212\n",
      "\n",
      "centralized infrastructure, 198–200\n",
      "\n",
      "culture of causality,\n",
      "\n",
      "customer acquisition funnel,\n",
      "\n",
      "data collection,\n",
      "\n",
      "DEBUG level,\n",
      "\n",
      "defined,\n",
      "\n",
      "ERROR level,\n",
      "\n",
      "event router,\n",
      "\n",
      "fact-based problem solving, 203–204\n",
      "\n",
      "FATAL level,\n",
      "\n",
      "graphs and dashboards, 204–205\n",
      "\n",
      "identify gaps, 209–213\n",
      "\n",
      "incident resolution time,  _f_\n",
      "\n",
      "INFO level,\n",
      "\n",
      "information radiator, 206–208\n",
      "\n",
      "information security in product telemetry, 326–327\n",
      "\n",
      "infrastructure metrics, 212–213\n",
      "\n",
      "ITIL CMDB,\n",
      "\n",
      "LinkedIn case study, 207–208\n",
      "\n",
      "log centralization,\n",
      "\n",
      "logging entry generation, 202–203\n",
      "\n",
      "logging levels, 201–202\n",
      "\n",
      "making deployments safer, 229–230\n",
      "\n",
      "metrics for improvement, 65–66\n",
      "\n",
      "metrics libraries, 204–205\n",
      "\n",
      "metrics library, 204–206\n",
      "\n",
      "metrics sources,\n",
      "\n",
      "monitoring architecture, 198–199\n",
      "\n",
      "monitoring framework,  _f_\n",
      "\n",
      "MTTR, ,  _f_\n",
      "\n",
      "overlay of production deployment activities,\n",
      "\n",
      "production metrics, 204–206\n",
      "\n",
      "and the Second Way,\n",
      "\n",
      "security telemetry, 327–330\n",
      "\n",
      "self-service metrics, 207–208\n",
      "\n",
      "self-service platforms, 206–208\n",
      "\n",
      "StatsD, 204–205\n",
      "\n",
      "tools,  _n_\n",
      "\n",
      "WARN level,\n",
      "\n",
      "telemetry analysis. _See also_ production telemetry; telemetry\n",
      "\n",
      "3 standard deviation rule, ,  _f_ , ,  _f_\n",
      "\n",
      "alerts for undesired outcomes,\n",
      "\n",
      "analysis tools,  _n_\n",
      "\n",
      "anomaly detection techniques, 222–226\n",
      "\n",
      "automated,  _f_\n",
      "\n",
      "auto-scaling capacity, 221–222,  _f_\n",
      "\n",
      "case study, 215–216\n",
      "\n",
      "filtering techniques,\n",
      "\n",
      "Gaussian distribution, ,  _f_\n",
      "\n",
      "Kolmogorov-Smirnov test, , ,  _f_\n",
      "\n",
      "means, 216–217\n",
      "\n",
      "Netflix case study, 221–222\n",
      "\n",
      "non-Gaussian distribution, 219–222,  _f_\n",
      "\n",
      "non-parametric techniques,\n",
      "\n",
      "outlier detection, 215–216\n",
      "\n",
      "precursors to production incidents,\n",
      "\n",
      "Server Outlier Detection,\n",
      "\n",
      "smoothing, ,  _f_ ,  _n_\n",
      "\n",
      "standard deviations, 216–217,  _f_\n",
      "\n",
      "statistical techniques, 216–217,\n",
      "\n",
      "test environments,  _n_\n",
      "\n",
      "testing\n",
      "\n",
      "automated, 123–127,  _n_ , , 134–135, ,\n",
      "\n",
      "automated validation test suite, , 133–134,  _f_ ,  _f_ , 136–138\n",
      "\n",
      "destructive testing,\n",
      "\n",
      "fast testing, , 133–134\n",
      "\n",
      "ideal vs non-ideal testing,  _f_\n",
      "\n",
      "manual testing, 258–259\n",
      "\n",
      "non-functional requirements testing, 137–138\n",
      "\n",
      "performance testing, 136–137\n",
      "\n",
      "smoke testing, ,\n",
      "\n",
      "testing environments,\n",
      "\n",
      "testing in parallel, 133–134,  _f_\n",
      "\n",
      "testing, A/B\n",
      "\n",
      "case study, 246–248\n",
      "\n",
      "Feature API,\n",
      "\n",
      "history of,\n",
      "\n",
      "integrating into feature planning, 245–248\n",
      "\n",
      "integrating into feature testing, 244–245\n",
      "\n",
      "integrating into release,\n",
      "\n",
      "outcomes of,\n",
      "\n",
      "user research,  _n_ , 244–245\n",
      "\n",
      "Yahoo! Answers, 246–248\n",
      "\n",
      "testing, automated\n",
      "\n",
      "acceptance test-driven development, 134–135\n",
      "\n",
      "acceptance tests, , ,\n",
      "\n",
      "analysis tools,\n",
      "\n",
      "automated build and test processes,\n",
      "\n",
      "automated test suites,\n",
      "\n",
      "automated validation test suite, 129–138\n",
      "\n",
      "automating manual tests, 135–136\n",
      "\n",
      "change deployment,\n",
      "\n",
      "code configuration management tools,\n",
      "\n",
      "code packaging,\n",
      "\n",
      "deployment pipeline, 127–129\n",
      "\n",
      "environment validation, 137–138\n",
      "\n",
      "error detection, 132–133\n",
      "\n",
      "failure indicators,\n",
      "\n",
      "fast testing, , 133–134\n",
      "\n",
      "feedback, ,\n",
      "\n",
      "green builds, 129–130\n",
      "\n",
      "handling input from external integration points,  _n_\n",
      "\n",
      "ideal vs non-ideal testing,  _f_\n",
      "\n",
      "integration tests, ,\n",
      "\n",
      "non-functional requirements testing, 137–138\n",
      "\n",
      "performance testing, 136–137\n",
      "\n",
      "production increases,\n",
      "\n",
      "test types, 130–131\n",
      "\n",
      "test-driven development, 134–135\n",
      "\n",
      "testing in parallel, 133–134,  _f_\n",
      "\n",
      "testing teams, 124–126\n",
      "\n",
      "and trunk-based development, 145–146\n",
      "\n",
      "unit tests, 130–131, 132–133,\n",
      "\n",
      "unreliable test,\n",
      "\n",
      "version control,\n",
      "\n",
      "The First Way\n",
      "\n",
      "batch size comparison,  _f_\n",
      "\n",
      "bottlenecks,\n",
      "\n",
      "constraint identification, 21–23\n",
      "\n",
      "continuous,\n",
      "\n",
      "controlling queue size,\n",
      "\n",
      "description of,\n",
      "\n",
      "error management,\n",
      "\n",
      "handoff reduction,\n",
      "\n",
      "increasing workflow,\n",
      "\n",
      "kanban boards, 16–17\n",
      "\n",
      "large batch sizes, 19–20\n",
      "\n",
      "limiting work in process, 17–18\n",
      "\n",
      "loss of knowledge,\n",
      "\n",
      "making work visible, 15–17\n",
      "\n",
      "multitasking, 17–18\n",
      "\n",
      "reducing batch size, 18–20\n",
      "\n",
      "single piece flow, ,\n",
      "\n",
      "small batch sizes, 18–20\n",
      "\n",
      "small batch strategy,\n",
      "\n",
      "sprint planning boards, 16–17\n",
      "\n",
      "transferring work, 15–16\n",
      "\n",
      "waste elimination, 23–25\n",
      "\n",
      "work interruptions,\n",
      "\n",
      "workflow management, ,\n",
      "\n",
      "workflow visualizations, 16–17\n",
      "\n",
      "The Second Way\n",
      "\n",
      "Andon cord, ,\n",
      "\n",
      "conditions for safety, 28–29\n",
      "\n",
      "description of,\n",
      "\n",
      "error detection,\n",
      "\n",
      "failure,\n",
      "\n",
      "feedback loops, ,\n",
      "\n",
      "feed-forward loops, ,\n",
      "\n",
      "increasing information flow,\n",
      "\n",
      "ineffective quality controls, 32–34\n",
      "\n",
      "optimizing for downstream work centers, 34–35\n",
      "\n",
      "peer reviews, 33–34\n",
      "\n",
      "problem visibility, 29–30\n",
      "\n",
      "QA automation, 33–34\n",
      "\n",
      "safety in complex systems, 27–29\n",
      "\n",
      "swarming, 30–32\n",
      "\n",
      "telemetry,\n",
      "\n",
      "The Third Way\n",
      "\n",
      "blameless post-mortems,\n",
      "\n",
      "collective knowledge, 42–43\n",
      "\n",
      "creating a high-trust culture, 37–38\n",
      "\n",
      "description of, 12–13\n",
      "\n",
      "improvement of daily work, 40–42\n",
      "\n",
      "integrating learning, 37–38\n",
      "\n",
      "knowledge sharing, 42–43\n",
      "\n",
      "leadership,\n",
      "\n",
      "learning culture, 44–46\n",
      "\n",
      "organizational cultures, ,  _f_\n",
      "\n",
      "organizational learning, 38–40\n",
      "\n",
      "resilience, 43–44\n",
      "\n",
      "safety culture, 38–40\n",
      "\n",
      "workarounds,\n",
      "\n",
      "workplace safety, 41–42\n",
      "\n",
      "The Three Ways\n",
      "\n",
      "The First Way,\n",
      "\n",
      "illustrated,  _f_\n",
      "\n",
      "increasing workflow,\n",
      "\n",
      "The Phoenix Project,\n",
      "\n",
      "The Second Way,\n",
      "\n",
      "The Third Way, 12–13\n",
      "\n",
      "Theory of Constraints, 356–357\n",
      "\n",
      "Three Mile Island,\n",
      "\n",
      "threshold-based alerting tools,  _n_\n",
      "\n",
      "Ticketmaster, 84–85\n",
      "\n",
      "Timberland,\n",
      "\n",
      "Tischler, Tim,\n",
      "\n",
      "Tomayko, Ryan, 261–262\n",
      "\n",
      "Total Productive Maintenance, ,\n",
      "\n",
      "Toyota Kata\n",
      "\n",
      "description of,\n",
      "\n",
      "functional-oriented organizations,\n",
      "\n",
      "and the improvement of daily work,\n",
      "\n",
      "and learning culture,\n",
      "\n",
      "movement,\n",
      "\n",
      "Toyota Production System\n",
      "\n",
      "Andon cord,\n",
      "\n",
      "change approval processes,\n",
      "\n",
      "improvement blitz,\n",
      "\n",
      "information radiator,\n",
      "\n",
      "and the Lean Movement, ,\n",
      "\n",
      "and learning culture,\n",
      "\n",
      "and safe systems,\n",
      "\n",
      "Toyota Kata movement,\n",
      "\n",
      "transparent uptime,\n",
      "\n",
      "Treynor, Ben, 237–238\n",
      "\n",
      "Trimble, Chris,\n",
      "\n",
      "Turnbull, James,\n",
      "\n",
      "Twitter, 320–323\n",
      "\n",
      "two-pizza team, 90–91\n",
      "\n",
      "U\n",
      "\n",
      "US Navy,\n",
      "\n",
      "user research, 244–245\n",
      "\n",
      "user stories, , , ,\n",
      "\n",
      "V\n",
      "\n",
      "value stream\n",
      "\n",
      "Development,\n",
      "\n",
      "Infosec,\n",
      "\n",
      "manufacturing, 7–8\n",
      "\n",
      "Operations,\n",
      "\n",
      "product owner,\n",
      "\n",
      "release managers,\n",
      "\n",
      "supporting members,\n",
      "\n",
      "technology, 8–11\n",
      "\n",
      "technology executives,\n",
      "\n",
      "test,\n",
      "\n",
      "value stream manager,\n",
      "\n",
      "value stream mapping, , 61–62,\n",
      "\n",
      "value stream mapping\n",
      "\n",
      "%C/A,\n",
      "\n",
      "areas of focus,\n",
      "\n",
      "creating, 63–66\n",
      "\n",
      "example of,  _f_\n",
      "\n",
      "first pass,\n",
      "\n",
      "future value stream map,\n",
      "\n",
      "and the Lean Movement, ,\n",
      "\n",
      "metrics for improvement, 65–66\n",
      "\n",
      "value stream improvements, 61–62\n",
      "\n",
      "value streams\n",
      "\n",
      "expanding DevOps, 58–59\n",
      "\n",
      "greenfield vs brownfield services, 54–56\n",
      "\n",
      "leveraging innovators, 57–58\n",
      "\n",
      "selecting a stream for DevOps transformation, 51–60\n",
      "\n",
      "systems of engagement, 56–57\n",
      "\n",
      "systems of record,\n",
      "\n",
      "technology adoption curve,  _f_\n",
      "\n",
      "Van Leeuwen, Evelijn,  _n_\n",
      "\n",
      "Vance, Ashlee,\n",
      "\n",
      "Velocity Movement,\n",
      "\n",
      "version control\n",
      "\n",
      "assets to check into version control repository,\n",
      "\n",
      "automated testing,\n",
      "\n",
      "branching,  _n_\n",
      "\n",
      "and continuous integration, 148–149\n",
      "\n",
      "critical role of version control,\n",
      "\n",
      "environments stored in version control, 115–116\n",
      "\n",
      "metadata,\n",
      "\n",
      "shared version control repository, 115–118\n",
      "\n",
      "version control as predictor of organizational performance,\n",
      "\n",
      "version control systems, 115–118\n",
      "\n",
      "Vincent, John,\n",
      "\n",
      "Vogels, Werner, ,\n",
      "\n",
      "W\n",
      "\n",
      "Wall Street Journal,\n",
      "\n",
      "waste and hardship\n",
      "\n",
      "defects,\n",
      "\n",
      "extra features,\n",
      "\n",
      "extra processes,\n",
      "\n",
      "heroics, ,  _n_\n",
      "\n",
      "motion, 24–25\n",
      "\n",
      "nonstandard or manual work,\n",
      "\n",
      "partially done work,\n",
      "\n",
      "task switching,\n",
      "\n",
      "waiting,\n",
      "\n",
      "waste elimination, 23–25\n",
      "\n",
      "water-Scrum-fall anti-pattern,  _n_\n",
      "\n",
      "Westrum, Ron, 39–40\n",
      "\n",
      "Wickett, James,\n",
      "\n",
      "Williams, Laurie,  _n_ ,\n",
      "\n",
      "Willis, John\n",
      "\n",
      "Agile infrastructure,\n",
      "\n",
      "convergence of DevOps,\n",
      "\n",
      "WIP. _See_ work in process\n",
      "\n",
      "Wolaberg, Kirsten,  _n_\n",
      "\n",
      "Womack, James P.\n",
      "\n",
      "batch sizes,\n",
      "\n",
      "leaders and problem solving,\n",
      "\n",
      "Wong, Eric,\n",
      "\n",
      "work in process\n",
      "\n",
      "controlling queue size,\n",
      "\n",
      "interruptions,\n",
      "\n",
      "multitasking, 17–18\n",
      "\n",
      "work visibility, 15–17, ,\n",
      "\n",
      "workflow\n",
      "\n",
      "increasing, ,\n",
      "\n",
      "management,\n",
      "\n",
      "visualizations, 16–17\n",
      "\n",
      "Y\n",
      "\n",
      "Yahoo! Answers, 246–248\n",
      "\n",
      "Z\n",
      "\n",
      "Zenoss, ,\n",
      "\n",
      "Zhao, Haiping,\n",
      "\n",
      "Zuckerberg, Mark, \n",
      "\n",
      "# Acknowledgments\n",
      "\n",
      "#### Jez Humble\n",
      "\n",
      "Creating this book has been a labor of love for Gene in particular. It's an immense privilege and pleasure to have worked with Gene and my other co-authors, John and Pat, along with Todd, Anna, Robyn and the editorial and production team at IT Revolution preparing this work—thank you. I also want to thank Nicole Forsgren whose work with Gene, Alanna Brown, Nigel Kersten and I on the PuppetLabs/DORA _State of DevOps Report_ over the last three years has been instrumental in developing, testing and refining many of the ideas in this book. My wife, Rani, and my two daughters, Amrita and Reshmi, have given me boundless love and support during my work on this book, as in every part of my life. Thank you. I love you. Finally, I feel incredibly lucky to be part of the DevOps community, which almost without exception walks the talk of practicing empathy and growing a culture of respect and learning. Thanks to each and every one of you.\n",
      "\n",
      "#### John Willis\n",
      "\n",
      "First and foremost, I need to acknowledge my saint of a wife for putting up with my crazy career. It would take another book to express how much I learned from my co-authors Patrick, Gene and Jez. Other very important influencers and advisers in my journey are Mark Hinkle, Mark Burgess, Andrew Clay Shafer, and Michael Cote. I also want to give a shout out to Adam Jacob for hiring me at Chef and giving me the freedom to explore, in the early days, this thing we call Devops. Last but definitely not least is my partner in crime, my _Devops Cafe_ cohost, Damon Edwards.\n",
      "\n",
      "#### Patrick Debois\n",
      "\n",
      "I would like to thank those who were on this ride, much gratitude to you all.\n",
      "\n",
      "#### Gene Kim\n",
      "\n",
      "I cannot thank Margueritte, my loving wife of nearly eleven amazing years, enough for putting up with me being in deadline mode for over five years, as well as my sons, Reid, Parker, and Grant. And of course, my parents, Ben and Gail Kim, for helping me become a nerd early in life. I also want to thank my fellow co-authors for everything that I learned from them, as well as Anna Noak, Aly Hoffman, Robyn Crummer-Olsen, Todd Sattersten, and the rest of the IT Revolution team for shepherding this book to its completion.\n",
      "\n",
      "I am so grateful for all the people who taught me so many things, which form the foundation of this book: John Allspaw (Etsy), Alanna Brown (Puppet), Adrian Cockcroft (Battery Ventures), Justin Collins (Brakeman Pro), Josh Corman (Atlantic Council), Jason Cox (The Walt Disney Company), Dominica DeGrandis (LeanKit), Damon Edwards (DTO Solutions), Dr. Nicole Forsgren (Chef), Gary Gruver, Sam Guckenheimer (Microsoft), Elisabeth Hendrickson (Pivotal Software), Nick Galbreath (Signal Sciences), Tom Limoncelli (Stack Exchange), Chris Little, Ryan Martens, Ernest Mueller (AlienVault), Mike Orzen, Scott Prugh (CSG International), Roy Rapoport (Netflix), Tarun Reddy (CA/Rally), Jesse Robbins (Orion Labs), Ben Rockwood (Chef), Andrew Shafer (Pivotal), Randy Shoup (Stitch Fix), James Turnbull (Kickstarter), and James Wickett (Signal Sciences).\n",
      "\n",
      "I also want to thank the many people whose incredible DevOps journeys we studied, including Justin Arbuckle, David Ashman, Charlie Betz, Mike Bland, Dr. Toufic Boubez, Em Campbell-Pretty, Jason Chan, Pete Cheslock, Ross Clanton, Jonathan Claudius, Shawn Davenport, James DeLuccia, Rob England, John Esser, James Fryman, Paul Farrall, Nathen Harvey, Mirco Hering, Adam Jacob, Luke Kanies, Kaimar Karu, Nigel Kersten, Courtney Kissler, Bethany Macri, Simon Morris, Ian Malpass, Dianne Marsh, Norman Marks, Bill Massie, Neil Matatall, Michael Nygard, Patrick McDonnell, Eran Messeri, Heather Mickman, Jody Mulkey, Paul Muller, Jesse Newland, Dan North, Dr. Tapabrata Pal, Michael Rembetsy, Mike Rother, Paul Stack, Gareth Rushgrove, Mark Schwartz, Nathan Shimek, Bill Shinn, JP Schneider, Dr. Steven Spear, Laurence Sweeney, Jim Stoneham, and Ryan Tomayko.\n",
      "\n",
      "And I am so profoundly grateful for the many reviewers who gave us fantastic feedback that shaped this book: Will Albenzi, JT Armstrong, Paul Auclair, Ed Bellis, Daniel Blander, Matt Brender, Alanna Brown, Branden Burton, Ross Clanton, Adrian Cockcroft, Jennifer Davis, Jessica DeVita, Stephen Feldman, Martin Fisher, Stephen Fishman, Jeff Gallimore, Becky Hartman, Matt Hatch, William Hertling, Rob Hirschfeld, Tim Hunter, Stein Inge Morisbak, Mark Klein, Alan Kraft, Bridget Kromhaut, Chris Leavory, Chris Leavoy, Jenny Madorsky, Dave Mangot, Chris McDevitt, Chris McEniry, Mike McGarr, Thomas McGonagle, Sam McLeod, Byron Miller, David Mortman, Chivas Nambiar, Charles Nelles, John Osborne, Matt O'Keefe, Manuel Pais, Gary Pedretti, Dan Piessens, Brian Prince, Dennis Ravenelle, Pete Reid, Markos Rendell, Trevor Roberts, Jr., Frederick Scholl, Matthew Selheimer, David Severski, Samir Shah, Paul Stack, Scott Stockton, Dave Tempero, Todd Varland, Jeremy Voorhis, and Branden Williams.\n",
      "\n",
      "And several people gave me an amazing glimpse of what the future of authoring with modern toolchains looks like, including Andrew Odewahn (O'Reilly Media) who let us use the fantastic Chimera reviewing platform, James Turnbull (Kickstarter) for his help creating my first publishing rendering toolchain, and Scott Chacon (GitHub) for his work on GitHub Flow for authors.\n",
      "\n",
      "# Author Biographies\n",
      "\n",
      "### GENE KIM\n",
      "\n",
      "|  **Gene Kim** is a multiple award-winning CTO, researcher, and author of _The Phoenix Project: A Novel About IT, DevOps, and Helping Your Business Win_ and _The Visible Ops Handbook_. He is founder of IT Revolution and hosts the DevOps Enteprise Summit conferences.  \n",
      "---|---\n",
      "\n",
      "### JEZ HUMBLE\n",
      "\n",
      "|  **Jez Humble** is co-author of _Lean Enterprise_ and the Jolt Award-winning _Continuous Delivery_. He works at 18F, teaches at UC Berkeley, and is CTO and co-founder of DevOps Research and Assessment, LLC.\n",
      "\n",
      "### PATRICK DEBOIS\n",
      "\n",
      "|  **Patrick Debois** is an independent IT consultant who is bridging the gap between projects and operations by using Agile techniques, in development, project management, and system administration.\n",
      "\n",
      "### JOHN WILLIS\n",
      "\n",
      "|  **John Willis** has worked in the IT management industry for more than thirty-five years. He has authored six IBM Redbooks and was the founder and chief architect at Chain Bridge Systems. Currently he is an Evangelist at Docker, Inc.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0][\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agriculture num_rows:100\n",
      "art num_rows:200\n",
      "biography num_rows:180\n",
      "biology num_rows:220\n",
      "cooking num_rows:120\n",
      "cs num_rows:100\n",
      "fiction num_rows:220\n",
      "fin num_rows:345\n",
      "health num_rows:180\n",
      "history num_rows:180\n",
      "legal num_rows:438\n",
      "literature num_rows:180\n",
      "mathematics num_rows:160\n",
      "mix num_rows:130\n",
      "music num_rows:200\n",
      "philosophy num_rows:200\n",
      "physics num_rows:160\n",
      "politics num_rows:180\n",
      "psychology num_rows:200\n",
      "technology num_rows:240\n",
      "Total Examples:3933\n"
     ]
    }
   ],
   "source": [
    "datapath=r\"datasets\\UltraDomain\"\n",
    "file_paths = ['agriculture', 'art', 'biography',\n",
    "              'biology','cooking','cs','fiction',\n",
    "              'fin','health','history','legal',\n",
    "              'literature','mathematics','mix',\n",
    "              'music','philosophy','physics',\n",
    "              'politics','psychology','technology']\n",
    "\n",
    "count=0\n",
    "for file_path in file_paths:\n",
    "    data=load_dataset(\"json\",data_files=f\"{datapath}/{file_path}.jsonl\")\n",
    "    print(f\"{file_path} num_rows:\"+str(data[\"train\"].num_rows))\n",
    "    count+=data[\"train\"].num_rows\n",
    "\n",
    "print(f\"Total Examples:{count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_dataset(\"json\",data_files=f\"{datapath}/cs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "# Table of Contents\n",
      "\n",
      "Machine Learning with Spark\n",
      "\n",
      "Credits\n",
      "\n",
      "About the Author\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "About the Reviewers\n",
      "\n",
      "www.PacktPub.com\n",
      "\n",
      "Support files, eBooks, discount offers, and more\n",
      "\n",
      "Why subscribe?\n",
      "\n",
      "Free access for Packt account holders\n",
      "\n",
      "Preface\n",
      "\n",
      "What this book covers\n",
      "\n",
      "What you need for this book\n",
      "\n",
      "Who this book is for\n",
      "\n",
      "Conventions\n",
      "\n",
      "Reader feedback\n",
      "\n",
      "Customer support\n",
      "\n",
      "Downloading the example code\n",
      "\n",
      "Errata\n",
      "\n",
      "Piracy\n",
      "\n",
      "Questions\n",
      "\n",
      "1. Getting Up and Running with Spark\n",
      "\n",
      "Installing and setting up Spark locally\n",
      "\n",
      "Spark clusters\n",
      "\n",
      "The Spark programming model\n",
      "\n",
      "SparkContext and SparkConf\n",
      "\n",
      "The Spark shell\n",
      "\n",
      "Resilient Distributed Datasets\n",
      "\n",
      "Creating RDDs\n",
      "\n",
      "Spark operations\n",
      "\n",
      "Caching RDDs\n",
      "\n",
      "Broadcast variables and accumulators\n",
      "\n",
      "The first step to a Spark program in Scala\n",
      "\n",
      "The first step to a Spark program in Java\n",
      "\n",
      "The first step to a Spark program in Python\n",
      "\n",
      "Getting Spark running on Amazon EC2\n",
      "\n",
      "Launching an EC2 Spark cluster\n",
      "\n",
      "Summary\n",
      "\n",
      "2. Designing a Machine Learning System\n",
      "\n",
      "Introducing MovieStream\n",
      "\n",
      "Business use cases for a machine learning system\n",
      "\n",
      "Personalization\n",
      "\n",
      "Targeted marketing and customer segmentation\n",
      "\n",
      "Predictive modeling and analytics\n",
      "\n",
      "Types of machine learning models\n",
      "\n",
      "The components of a data-driven machine learning system\n",
      "\n",
      "Data ingestion and storage\n",
      "\n",
      "Data cleansing and transformation\n",
      "\n",
      "Model training and testing loop\n",
      "\n",
      "Model deployment and integration\n",
      "\n",
      "Model monitoring and feedback\n",
      "\n",
      "Batch versus real time\n",
      "\n",
      "An architecture for a machine learning system\n",
      "\n",
      "Practical exercise\n",
      "\n",
      "Summary\n",
      "\n",
      "3. Obtaining, Processing, and Preparing Data with Spark\n",
      "\n",
      "Accessing publicly available datasets\n",
      "\n",
      "The MovieLens 100k dataset\n",
      "\n",
      "Exploring and visualizing your data\n",
      "\n",
      "Exploring the user dataset\n",
      "\n",
      "Exploring the movie dataset\n",
      "\n",
      "Exploring the rating dataset\n",
      "\n",
      "Processing and transforming your data\n",
      "\n",
      "Filling in bad or missing data\n",
      "\n",
      "Extracting useful features from your data\n",
      "\n",
      "Numerical features\n",
      "\n",
      "Categorical features\n",
      "\n",
      "Derived features\n",
      "\n",
      "Transforming timestamps into categorical features\n",
      "\n",
      "Text features\n",
      "\n",
      "Simple text feature extraction\n",
      "\n",
      "Normalizing features\n",
      "\n",
      "Using MLlib for feature normalization\n",
      "\n",
      "Using packages for feature extraction\n",
      "\n",
      "Summary\n",
      "\n",
      "4. Building a Recommendation Engine with Spark\n",
      "\n",
      "Types of recommendation models\n",
      "\n",
      "Content-based filtering\n",
      "\n",
      "Collaborative filtering\n",
      "\n",
      "Matrix factorization\n",
      "\n",
      "Explicit matrix factorization\n",
      "\n",
      "Implicit matrix factorization\n",
      "\n",
      "Alternating least squares\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Extracting features from the MovieLens 100k dataset\n",
      "\n",
      "Training the recommendation model\n",
      "\n",
      "Training a model on the MovieLens 100k dataset\n",
      "\n",
      "Training a model using implicit feedback data\n",
      "\n",
      "Using the recommendation model\n",
      "\n",
      "User recommendations\n",
      "\n",
      "Generating movie recommendations from the MovieLens 100k dataset\n",
      "\n",
      "Inspecting the recommendations\n",
      "\n",
      "Item recommendations\n",
      "\n",
      "Generating similar movies for the MovieLens 100k dataset\n",
      "\n",
      "Inspecting the similar items\n",
      "\n",
      "Evaluating the performance of recommendation models\n",
      "\n",
      "Mean Squared Error\n",
      "\n",
      "Mean average precision at K\n",
      "\n",
      "Using MLlib's built-in evaluation functions\n",
      "\n",
      "RMSE and MSE\n",
      "\n",
      "MAP\n",
      "\n",
      "Summary\n",
      "\n",
      "5. Building a Classification Model with Spark\n",
      "\n",
      "Types of classification models\n",
      "\n",
      "Linear models\n",
      "\n",
      "Logistic regression\n",
      "\n",
      "Linear support vector machines\n",
      "\n",
      "The naïve Bayes model\n",
      "\n",
      "Decision trees\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Extracting features from the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "Training classification models\n",
      "\n",
      "Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "Using classification models\n",
      "\n",
      "Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "Evaluating the performance of classification models\n",
      "\n",
      "Accuracy and prediction error\n",
      "\n",
      "Precision and recall\n",
      "\n",
      "ROC curve and AUC\n",
      "\n",
      "Improving model performance and tuning parameters\n",
      "\n",
      "Feature standardization\n",
      "\n",
      "Additional features\n",
      "\n",
      "Using the correct form of data\n",
      "\n",
      "Tuning model parameters\n",
      "\n",
      "Linear models\n",
      "\n",
      "Iterations\n",
      "\n",
      "Step size\n",
      "\n",
      "Regularization\n",
      "\n",
      "Decision trees\n",
      "\n",
      "Tuning tree depth and impurity\n",
      "\n",
      "The naïve Bayes model\n",
      "\n",
      "Cross-validation\n",
      "\n",
      "Summary\n",
      "\n",
      "6. Building a Regression Model with Spark\n",
      "\n",
      "Types of regression models\n",
      "\n",
      "Least squares regression\n",
      "\n",
      "Decision trees for regression\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Extracting features from the bike sharing dataset\n",
      "\n",
      "Creating feature vectors for the linear model\n",
      "\n",
      "Creating feature vectors for the decision tree\n",
      "\n",
      "Training and using regression models\n",
      "\n",
      "Training a regression model on the bike sharing dataset\n",
      "\n",
      "Evaluating the performance of regression models\n",
      "\n",
      "Mean Squared Error and Root Mean Squared Error\n",
      "\n",
      "Mean Absolute Error\n",
      "\n",
      "Root Mean Squared Log Error\n",
      "\n",
      "The R-squared coefficient\n",
      "\n",
      "Computing performance metrics on the bike sharing dataset\n",
      "\n",
      "Linear model\n",
      "\n",
      "Decision tree\n",
      "\n",
      "Improving model performance and tuning parameters\n",
      "\n",
      "Transforming the target variable\n",
      "\n",
      "Impact of training on log-transformed targets\n",
      "\n",
      "Tuning model parameters\n",
      "\n",
      "Creating training and testing sets to evaluate parameters\n",
      "\n",
      "The impact of parameter settings for linear models\n",
      "\n",
      "Iterations\n",
      "\n",
      "Step size\n",
      "\n",
      "L2 regularization\n",
      "\n",
      "L1 regularization\n",
      "\n",
      "Intercept\n",
      "\n",
      "The impact of parameter settings for the decision tree\n",
      "\n",
      "Tree depth\n",
      "\n",
      "Maximum bins\n",
      "\n",
      "Summary\n",
      "\n",
      "7. Building a Clustering Model with Spark\n",
      "\n",
      "Types of clustering models\n",
      "\n",
      "K-means clustering\n",
      "\n",
      "Initialization methods\n",
      "\n",
      "Variants\n",
      "\n",
      "Mixture models\n",
      "\n",
      "Hierarchical clustering\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Extracting features from the MovieLens dataset\n",
      "\n",
      "Extracting movie genre labels\n",
      "\n",
      "Training the recommendation model\n",
      "\n",
      "Normalization\n",
      "\n",
      "Training a clustering model\n",
      "\n",
      "Training a clustering model on the MovieLens dataset\n",
      "\n",
      "Making predictions using a clustering model\n",
      "\n",
      "Interpreting cluster predictions on the MovieLens dataset\n",
      "\n",
      "Interpreting the movie clusters\n",
      "\n",
      "Evaluating the performance of clustering models\n",
      "\n",
      "Internal evaluation metrics\n",
      "\n",
      "External evaluation metrics\n",
      "\n",
      "Computing performance metrics on the MovieLens dataset\n",
      "\n",
      "Tuning parameters for clustering models\n",
      "\n",
      "Selecting K through cross-validation\n",
      "\n",
      "Summary\n",
      "\n",
      "8. Dimensionality Reduction with Spark\n",
      "\n",
      "Types of dimensionality reduction\n",
      "\n",
      "Principal Components Analysis\n",
      "\n",
      "Singular Value Decomposition\n",
      "\n",
      "Relationship with matrix factorization\n",
      "\n",
      "Clustering as dimensionality reduction\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Extracting features from the LFW dataset\n",
      "\n",
      "Exploring the face data\n",
      "\n",
      "Visualizing the face data\n",
      "\n",
      "Extracting facial images as vectors\n",
      "\n",
      "Loading images\n",
      "\n",
      "Converting to grayscale and resizing the images\n",
      "\n",
      "Extracting feature vectors\n",
      "\n",
      "Normalization\n",
      "\n",
      "Training a dimensionality reduction model\n",
      "\n",
      "Running PCA on the LFW dataset\n",
      "\n",
      "Visualizing the Eigenfaces\n",
      "\n",
      "Interpreting the Eigenfaces\n",
      "\n",
      "Using a dimensionality reduction model\n",
      "\n",
      "Projecting data using PCA on the LFW dataset\n",
      "\n",
      "The relationship between PCA and SVD\n",
      "\n",
      "Evaluating dimensionality reduction models\n",
      "\n",
      "Evaluating k for SVD on the LFW dataset\n",
      "\n",
      "Summary\n",
      "\n",
      "9. Advanced Text Processing with Spark\n",
      "\n",
      "What's so special about text data?\n",
      "\n",
      "Extracting the right features from your data\n",
      "\n",
      "Term weighting schemes\n",
      "\n",
      "Feature hashing\n",
      "\n",
      "Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "\n",
      "Exploring the 20 Newsgroups data\n",
      "\n",
      "Applying basic tokenization\n",
      "\n",
      "Improving our tokenization\n",
      "\n",
      "Removing stop words\n",
      "\n",
      "Excluding terms based on frequency\n",
      "\n",
      "A note about stemming\n",
      "\n",
      "Training a TF-IDF model\n",
      "\n",
      "Analyzing the TF-IDF weightings\n",
      "\n",
      "Using a TF-IDF model\n",
      "\n",
      "Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "\n",
      "Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "\n",
      "Evaluating the impact of text processing\n",
      "\n",
      "Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset\n",
      "\n",
      "Word2Vec models\n",
      "\n",
      "Word2Vec on the 20 Newsgroups dataset\n",
      "\n",
      "Summary\n",
      "\n",
      "10. Real-time Machine Learning with Spark Streaming\n",
      "\n",
      "Online learning\n",
      "\n",
      "Stream processing\n",
      "\n",
      "An introduction to Spark Streaming\n",
      "\n",
      "Input sources\n",
      "\n",
      "Transformations\n",
      "\n",
      "Keeping track of state\n",
      "\n",
      "General transformations\n",
      "\n",
      "Actions\n",
      "\n",
      "Window operators\n",
      "\n",
      "Caching and fault tolerance with Spark Streaming\n",
      "\n",
      "Creating a Spark Streaming application\n",
      "\n",
      "The producer application\n",
      "\n",
      "Creating a basic streaming application\n",
      "\n",
      "Streaming analytics\n",
      "\n",
      "Stateful streaming\n",
      "\n",
      "Online learning with Spark Streaming\n",
      "\n",
      "Streaming regression\n",
      "\n",
      "A simple streaming regression program\n",
      "\n",
      "Creating a streaming data producer\n",
      "\n",
      "Creating a streaming regression model\n",
      "\n",
      "Streaming K-means\n",
      "\n",
      "Online model evaluation\n",
      "\n",
      "Comparing model performance with Spark Streaming\n",
      "\n",
      "Summary\n",
      "\n",
      "Index\n",
      "\n",
      "#  **Machine Learning with Spark**\n",
      "\n",
      "* * *\n",
      "\n",
      "# Machine Learning with Spark\n",
      "\n",
      "Copyright (C) 2015 Packt Publishing\n",
      "\n",
      "All rights reserved. No part of this book may be reproduced, stored in a retrieval system, or transmitted in any form or by any means, without the prior written permission of the publisher, except in the case of brief quotations embedded in critical articles or reviews.\n",
      "\n",
      "Every effort has been made in the preparation of this book to ensure the accuracy of the information presented. However, the information contained in this book is sold without warranty, either express or implied. Neither the author, nor Packt Publishing, and its dealers and distributors will be held liable for any damages caused or alleged to be caused directly or indirectly by this book.\n",
      "\n",
      "Packt Publishing has endeavored to provide trademark information about all of the companies and products mentioned in this book by the appropriate use of capitals. However, Packt Publishing cannot guarantee the accuracy of this information.\n",
      "\n",
      "First published: February 2015\n",
      "\n",
      "Production reference: 1170215\n",
      "\n",
      "Published by Packt Publishing Ltd.\n",
      "\n",
      "Livery Place\n",
      "\n",
      "35 Livery Street\n",
      "\n",
      "Birmingham B3 2PB, UK.\n",
      "\n",
      "ISBN 978-1-78328-851-9\n",
      "\n",
      "www.packtpub.com\n",
      "\n",
      "Cover image by Akshay Paunikar (`<akshaypaunikar4@gmail.com>`)\n",
      "\n",
      "# Credits\n",
      "\n",
      " **Author**\n",
      "\n",
      "Nick Pentreath\n",
      "\n",
      " **Reviewers**\n",
      "\n",
      "Andrea Mostosi\n",
      "\n",
      "Hao Ren\n",
      "\n",
      "Krishna Sankar\n",
      "\n",
      " **Commissioning Editor**\n",
      "\n",
      "Rebecca Youe\n",
      "\n",
      " **Acquisition Editor**\n",
      "\n",
      "Rebecca Youe\n",
      "\n",
      " **Content Development Editor**\n",
      "\n",
      "Susmita Sabat\n",
      "\n",
      " **Technical Editors**\n",
      "\n",
      "Vivek Arora\n",
      "\n",
      "Pankaj Kadam\n",
      "\n",
      " **Copy Editor**\n",
      "\n",
      "Karuna Narayanan\n",
      "\n",
      " **Project Coordinator**\n",
      "\n",
      "Milton Dsouza\n",
      "\n",
      " **Proofreaders**\n",
      "\n",
      "Simran Bhogal\n",
      "\n",
      "Maria Gould\n",
      "\n",
      "Ameesha Green\n",
      "\n",
      "Paul Hindle\n",
      "\n",
      " **Indexer**\n",
      "\n",
      "Priya Sane\n",
      "\n",
      " **Graphics**\n",
      "\n",
      "Sheetal Aute\n",
      "\n",
      "Abhinash Sahu\n",
      "\n",
      " **Production Coordinator**\n",
      "\n",
      "Nitesh Thakur\n",
      "\n",
      " **Cover Work**\n",
      "\n",
      "Nitesh Thakur\n",
      "\n",
      "# About the Author\n",
      "\n",
      " **Nick Pentreath** has a background in financial markets, machine learning, and software development. He has worked at Goldman Sachs Group, Inc.; as a research scientist at the online ad targeting start-up Cognitive Match Limited, London; and led the Data Science and Analytics team at Mxit, Africa's largest social network.\n",
      "\n",
      "He is a cofounder of Graphflow, a big data and machine learning company focused on user-centric recommendations and customer intelligence. He is passionate about combining commercial focus with machine learning and cutting-edge technology to build intelligent systems that learn from data to add value to the bottom line.\n",
      "\n",
      "Nick is a member of the Apache Spark Project Management Committee.\n",
      "\n",
      "# Acknowledgments\n",
      "\n",
      "Writing this book has been quite a rollercoaster ride over the past year, with many ups and downs, late nights, and working weekends. It has also been extremely rewarding to combine my passion for machine learning with my love of the Apache Spark project, and I hope to bring some of this out in this book.\n",
      "\n",
      "I would like to thank the Packt Publishing team for all their assistance throughout the writing and editing process: Rebecca, Susmita, Sudhir, Amey, Neil, Vivek, Pankaj, and everyone who worked on the book.\n",
      "\n",
      "Thanks also go to Debora Donato at StumbleUpon for assistance with data- and legal-related queries.\n",
      "\n",
      "Writing a book like this can be a somewhat lonely process, so it is incredibly helpful to get the feedback of reviewers to understand whether one is headed in the right direction (and what course adjustments need to be made). I'm deeply grateful to Andrea Mostosi, Hao Ren, and Krishna Sankar for taking the time to provide such detailed and critical feedback.\n",
      "\n",
      "I could not have gotten through this project without the unwavering support of all my family and friends, especially my wonderful wife, Tammy, who will be glad to have me back in the evenings and on weekends once again. Thank you all!\n",
      "\n",
      "Finally, thanks to all of you reading this; I hope you find it useful!\n",
      "\n",
      "# About the Reviewers\n",
      "\n",
      " **Andrea Mostosi** is a technology enthusiast. An innovation lover since he was a child, he started a professional job in 2003 and worked on several projects, playing almost every role in the computer science environment. He is currently the CTO at The Fool, a company that tries to make sense of web and social data. During his free time, he likes traveling, running, cooking, biking, and coding.\n",
      "\n",
      " ****\n",
      "\n",
      "I would like to thank my geek friends: Simone M, Daniele V, Luca T, Luigi P, Michele N, Luca O, Luca B, Diego C, and Fabio B. They are the smartest people I know, and comparing myself with them has always pushed me to be better.\n",
      "\n",
      " **Hao Ren** is a software developer who is passionate about Scala, distributed systems, machine learning, and Apache Spark. He was an exchange student at EPFL when he learned about Scala in 2012. He is currently working in Paris as a backend and data engineer for ClaraVista--a company that focuses on high-performance marketing. His work responsibility is to build a Spark-based platform for purchase prediction and a new recommender system.\n",
      "\n",
      "Besides programming, he enjoys running, swimming, and playing basketball and badminton. You can learn more at his blog <http://www.invkrh.me>.\n",
      "\n",
      " **Krishna Sankar** is a chief data scientist at BlackArrow, where he is focusing on enhancing user experience via inference, intelligence, and interfaces. Earlier stints include working as a principal architect and data scientist at Tata America International Corporation, director of data science at a bioinformatics start-up company, and as a distinguished engineer at Cisco Systems, Inc. He has spoken at various conferences about data science (<http://goo.gl/9pyJMH>), machine learning (<http://goo.gl/sSem2Y>), and social media analysis (<http://goo.gl/D9YpVQ>). He has also been a guest lecturer at the Naval Postgraduate School. He has written a few books on Java, wireless LAN security, Web 2.0, and now on Spark. His other passion is LEGO robotics. Earlier in April, he was at the St. Louis FLL World Competition as a robots design judge.\n",
      "\n",
      "# www.PacktPub.com\n",
      "\n",
      "# Support files, eBooks, discount offers, and more\n",
      "\n",
      "For support files and downloads related to your book, please visit www.PacktPub.com.\n",
      "\n",
      "Did you know that Packt offers eBook versions of every book published, with PDF and ePub files available? You can upgrade to the eBook version at www.PacktPub.com and as a print book customer, you are entitled to a discount on the eBook copy. Get in touch with us at `<service@packtpub.com>` for more details.\n",
      "\n",
      "At www.PacktPub.com, you can also read a collection of free technical articles, sign up for a range of free newsletters and receive exclusive discounts and offers on Packt books and eBooks.\n",
      "\n",
      "<https://www2.packtpub.com/books/subscription/packtlib>\n",
      "\n",
      "Do you need instant solutions to your IT questions? PacktLib is Packt's online digital book library. Here, you can search, access, and read Packt's entire library of books.\n",
      "\n",
      "## Why subscribe?\n",
      "\n",
      "  * Fully searchable across every book published by Packt\n",
      "  * Copy and paste, print, and bookmark content\n",
      "  * On demand and accessible via a web browser\n",
      "\n",
      "## Free access for Packt account holders\n",
      "\n",
      "If you have an account with Packt at www.PacktPub.com, you can use this to access PacktLib today and view 9 entirely free books. Simply use your login credentials for immediate access.\n",
      "\n",
      "# Preface\n",
      "\n",
      "In recent years, the volume of data being collected, stored, and analyzed has exploded, in particular in relation to the activity on the Web and mobile devices, as well as data from the physical world collected via sensor networks. While previously large-scale data storage, processing, analysis, and modeling was the domain of the largest institutions such as Google, Yahoo!, Facebook, and Twitter, increasingly, many organizations are being faced with the challenge of how to handle a massive amount of data.\n",
      "\n",
      "When faced with this quantity of data and the common requirement to utilize it in real time, human-powered systems quickly become infeasible. This has led to a rise in the so-called big data and machine learning systems that learn from this data to make automated decisions.\n",
      "\n",
      "In answer to the challenge of dealing with ever larger-scale data without any prohibitive cost, new open source technologies emerged at companies such as Google, Yahoo!, Amazon, and Facebook, which aimed at making it easier to handle massive data volumes by distributing data storage and computation across a cluster of computers.\n",
      "\n",
      "The most widespread of these is Apache Hadoop, which made it significantly easier and cheaper to both store large amounts of data (via the Hadoop Distributed File System, or HDFS) and run computations on this data (via Hadoop MapReduce, a framework to perform computation tasks in parallel across many nodes in a computer cluster).\n",
      "\n",
      "However, MapReduce has some important shortcomings, including high overheads to launch each job and reliance on storing intermediate data and results of the computation to disk, both of which make Hadoop relatively ill-suited for use cases of an iterative or low-latency nature. Apache Spark is a new framework for distributed computing that is designed from the ground up to be optimized for low-latency tasks and to store intermediate data and results in memory, thus addressing some of the major drawbacks of the Hadoop framework. Spark provides a clean, functional, and easy-to-understand API to write applications and is fully compatible with the Hadoop ecosystem.\n",
      "\n",
      "Furthermore, Spark provides native APIs in Scala, Java, and Python. The Scala and Python APIs allow all the benefits of the Scala or Python language, respectively, to be used directly in Spark applications, including using the relevant interpreter for real-time, interactive exploration. Spark itself now provides a toolkit (called MLlib) of distributed machine learning and data mining models that is under heavy development and already contains high-quality, scalable, and efficient algorithms for many common machine learning tasks, some of which we will delve into in this book.\n",
      "\n",
      "Applying machine learning techniques to massive datasets is challenging, primarily because most well-known machine learning algorithms are not designed for parallel architectures. In many cases, designing such algorithms is not an easy task. The nature of machine learning models is generally iterative, hence the strong appeal of Spark for this use case. While there are many competing frameworks for parallel computing, Spark is one of the few that combines speed, scalability, in-memory processing, and fault tolerance with ease of programming and a flexible, expressive, and powerful API design.\n",
      "\n",
      "Throughout this book, we will focus on real-world applications of machine learning technology. While we may briefly delve into some theoretical aspects of machine learning algorithms, the book will generally take a practical, applied approach with a focus on using examples and code to illustrate how to effectively use the features of Spark and MLlib, as well as other well-known and freely available packages for machine learning and data analysis, to create a useful machine learning system.\n",
      "\n",
      "# What this book covers\n",
      "\n",
      "Chapter 1, _Getting Up and Running with Spark_ , shows how to install and set up a local development environment for the Spark framework as well as how to create a Spark cluster in the cloud using Amazon EC2. The Spark programming model and API will be introduced, and a simple Spark application will be created using each of Scala, Java, and Python.\n",
      "\n",
      "Chapter 2, _Designing a Machine Learning System_ , presents an example of a real-world use case for a machine learning system. We will design a high-level architecture for an intelligent system in Spark based on this illustrative use case.\n",
      "\n",
      "Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ , details how to go about obtaining data for use in a machine learning system, in particular from various freely and publicly available sources. We will learn how to process, clean, and transform the raw data into features that may be used in machine learning models, using available tools, libraries, and Spark's functionality.\n",
      "\n",
      "Chapter 4, _Building a Recommendation Engine with Spark_ , deals with creating a recommendation model based on the collaborative filtering approach. This model will be used to recommend items to a given user as well as create lists of items that are similar to a given item. Standard metrics to evaluate the performance of a recommendation model will be covered here.\n",
      "\n",
      "Chapter 5, _Building a Classification Model with Spark_ , details how to create a model for binary classification as well as how to utilize standard performance-evaluation metrics for classification tasks.\n",
      "\n",
      "Chapter 6, _Building a Regression Model with Spark_ , shows how to create a model for regression, extending the classification model created in Chapter 5, _Building a Classification Model with Spark_. Evaluation metrics for the performance of regression models will be detailed here.\n",
      "\n",
      "Chapter 7, _Building a Clustering Model with Spark_ , explores how to create a clustering model as well as how to use related evaluation methodologies. You will learn how to analyze and visualize the clusters generated.\n",
      "\n",
      "Chapter 8, _Dimensionality Reduction with Spark_ , takes us through methods to extract the underlying structure from and reduce the dimensionality of our data. You will learn some common dimensionality-reduction techniques and how to apply and analyze them, as well as how to use the resulting data representation as input to another machine learning model.\n",
      "\n",
      "Chapter 9, _Advanced Text Processing with Spark_ , introduces approaches to deal with large-scale text data, including techniques for feature extraction from text and dealing with the very high-dimensional features typical in text data.\n",
      "\n",
      "Chapter 10, _Real-time Machine Learning with Spark Streaming_ , provides an overview of Spark Streaming and how it fits in with the online and incremental learning approaches to apply machine learning on data streams.\n",
      "\n",
      "# What you need for this book\n",
      "\n",
      "Throughout this book, we assume that you have some basic experience with programming in Scala, Java, or Python and have some basic knowledge of machine learning, statistics, and data analysis.\n",
      "\n",
      "# Who this book is for\n",
      "\n",
      "This book is aimed at entry-level to intermediate data scientists, data analysts, software engineers, and practitioners involved in machine learning or data mining with an interest in large-scale machine learning approaches, but who are not necessarily familiar with Spark. You may have some experience of statistics or machine learning software (perhaps including MATLAB, scikit-learn, Mahout, R, Weka, and so on) or distributed systems (perhaps including some exposure to Hadoop).\n",
      "\n",
      "# Conventions\n",
      "\n",
      "In this book, you will find a number of styles of text that distinguish between different kinds of information. Here are some examples of these styles, and an explanation of their meaning.\n",
      "\n",
      "Code words in text, database table names, folder names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter handles are shown as follows: \"Spark places user scripts to run Spark in the `bin` directory.\"\n",
      "\n",
      "A block of code is set as follows:\n",
      "\n",
      "    val conf = new SparkConf()\n",
      "    .setAppName(\"Test Spark App\")\n",
      "    .setMaster(\"local[4]\")\n",
      "    val sc = new SparkContext(conf)\n",
      "\n",
      "Any command-line input or output is written as follows:\n",
      "\n",
      "    **> tar xfvz spark-1.2.0-bin-hadoop2.4.tgz**\n",
      "    **> cd spark-1.2.0-bin-hadoop2.4**\n",
      "\n",
      "**New terms** and **important words** are shown in bold. Words that you see on the screen, in menus or dialog boxes for example, appear in the text like this: \"These can be obtained from the AWS homepage by clicking **Account** | **Security Credentials** | **Access Credentials**.\"\n",
      "\n",
      "### Note\n",
      "\n",
      "Warnings or important notes appear in a box like this.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Tips and tricks appear like this.\n",
      "\n",
      "# Reader feedback\n",
      "\n",
      "Feedback from our readers is always welcome. Let us know what you think about this book--what you liked or may have disliked. Reader feedback is important for us to develop titles that you really get the most out of.\n",
      "\n",
      "To send us general feedback, simply send an e-mail to `<feedback@packtpub.com>`, and mention the book title through the subject of your message.\n",
      "\n",
      "If there is a topic that you have expertise in and you are interested in either writing or contributing to a book, see our author guide on www.packtpub.com/authors.\n",
      "\n",
      "# Customer support\n",
      "\n",
      "Now that you are the proud owner of a Packt book, we have a number of things to help you to get the most from your purchase.\n",
      "\n",
      "## Downloading the example code\n",
      "\n",
      "You can download the example code files for all Packt books you have purchased from your account at <http://www.packtpub.com>. If you purchased this book elsewhere, you can visit <http://www.packtpub.com/support> and register to have the files e-mailed directly to you.\n",
      "\n",
      "## Errata\n",
      "\n",
      "Although we have taken every care to ensure the accuracy of our content, mistakes do happen. If you find a mistake in one of our books--maybe a mistake in the text or the code--we would be grateful if you would report this to us. By doing so, you can save other readers from frustration and help us improve subsequent versions of this book. If you find any errata, please report them by visiting <http://www.packtpub.com/support>, selecting your book, clicking on the **Errata Submission Form** link, and entering the details of your errata. Once your errata are verified, your submission will be accepted and the errata will be uploaded to our website or added to any list of existing errata under the Errata section of that title.\n",
      "\n",
      "To view the previously submitted errata, go to <https://www.packtpub.com/books/content/support> and enter the name of the book in the search field. The required information will appear under the **Errata** section.\n",
      "\n",
      "## Piracy\n",
      "\n",
      "Piracy of copyright material on the Internet is an ongoing problem across all media. At Packt, we take the protection of our copyright and licenses very seriously. If you come across any illegal copies of our works, in any form, on the Internet, please provide us with the location address or website name immediately so that we can pursue a remedy.\n",
      "\n",
      "Please contact us at `<copyright@packtpub.com>` with a link to the suspected pirated material.\n",
      "\n",
      "We appreciate your help in protecting our authors, and our ability to bring you valuable content.\n",
      "\n",
      "## Questions\n",
      "\n",
      "You can contact us at `<questions@packtpub.com>` if you are having a problem with any aspect of the book, and we will do our best to address it.\n",
      "\n",
      "# Chapter 1. Getting Up and Running with Spark\n",
      "\n",
      "Apache Spark is a framework for distributed computing; this framework aims to make it simpler to write programs that run in parallel across many nodes in a cluster of computers. It tries to abstract the tasks of resource scheduling, job submission, execution, tracking, and communication between nodes, as well as the low-level operations that are inherent in parallel data processing. It also provides a higher level API to work with distributed data. In this way, it is similar to other distributed processing frameworks such as Apache Hadoop; however, the underlying architecture is somewhat different.\n",
      "\n",
      "Spark began as a research project at the University of California, Berkeley. The university was focused on the use case of distributed machine learning algorithms. Hence, it is designed from the ground up for high performance in applications of an iterative nature, where the same data is accessed multiple times. This performance is achieved primarily through caching datasets in memory, combined with low latency and overhead to launch parallel computation tasks. Together with other features such as fault tolerance, flexible distributed-memory data structures, and a powerful functional API, Spark has proved to be broadly useful for a wide range of large-scale data processing tasks, over and above machine learning and iterative analytics.\n",
      "\n",
      "### Note\n",
      "\n",
      "For more background on Spark, including the research papers underlying Spark's development, see the project's history page at <http://spark.apache.org/community.html#history>.\n",
      "\n",
      "Spark runs in four modes:\n",
      "\n",
      "  * The standalone local mode, where all Spark processes are run within the same **Java Virtual Machine** ( **JVM** ) process\n",
      "  * The standalone cluster mode, using Spark's own built-in job-scheduling framework\n",
      "  * Using Mesos, a popular open source cluster-computing framework\n",
      "  * Using YARN (commonly referred to as NextGen MapReduce), a Hadoop-related cluster-computing and resource-scheduling framework\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Download the Spark binaries and set up a development environment that runs in Spark's standalone local mode. This environment will be used throughout the rest of the book to run the example code.\n",
      "  * Explore Spark's programming model and API using Spark's interactive console.\n",
      "  * Write our first Spark program in Scala, Java, and Python.\n",
      "  * Set up a Spark cluster using Amazon's **Elastic Cloud Compute** ( **EC2** ) platform, which can be used for large-sized data and heavier computational requirements, rather than running in the local mode.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Spark can also be run on Amazon's Elastic MapReduce service using custom bootstrap action scripts, but this is beyond the scope of this book. The following article is a good reference guide: <http://aws.amazon.com/articles/Elastic-MapReduce/4926593393724923>.\n",
      "\n",
      "At the time of writing this book, the article covers running Spark Version 1.1.0.\n",
      "\n",
      "If you have previous experience in setting up Spark and are familiar with the basics of writing a Spark program, feel free to skip this chapter.\n",
      "\n",
      "# Installing and setting up Spark locally\n",
      "\n",
      "Spark can be run using the built-in standalone cluster scheduler in the local mode. This means that all the Spark processes are run within the same JVM--effectively, a single, multithreaded instance of Spark. The local mode is very useful for prototyping, development, debugging, and testing. However, this mode can also be useful in real-world scenarios to perform parallel computation across multiple cores on a single computer.\n",
      "\n",
      "As Spark's local mode is fully compatible with the cluster mode, programs written and tested locally can be run on a cluster with just a few additional steps.\n",
      "\n",
      "The first step in setting up Spark locally is to download the latest version (at the time of writing this book, the version is 1.2.0). The download page of the Spark project website, found at <http://spark.apache.org/downloads.html>, contains links to download various versions as well as to obtain the latest source code via GitHub.\n",
      "\n",
      "### Tip\n",
      "\n",
      "The Spark project documentation website at <http://spark.apache.org/docs/latest/> is a comprehensive resource to learn more about Spark. We highly recommend that you explore it!\n",
      "\n",
      "Spark needs to be built against a specific version of Hadoop in order to access **Hadoop Distributed File System** ( **HDFS** ) as well as standard and custom Hadoop input sources. The download page provides prebuilt binary packages for Hadoop 1, CDH4 (Cloudera's Hadoop Distribution), MapR's Hadoop distribution, and Hadoop 2 (YARN). Unless you wish to build Spark against a specific Hadoop version, we recommend that you download the prebuilt Hadoop 2.4 package from an Apache mirror using this link: <http://www.apache.org/dyn/closer.cgi/spark/spark-1.2.0/spark-1.2.0-bin-hadoop2.4.tgz>.\n",
      "\n",
      "Spark requires the Scala programming language (version 2.10.4 at the time of writing this book) in order to run. Fortunately, the prebuilt binary package comes with the Scala runtime packages included, so you don't need to install Scala separately in order to get started. However, you will need to have a **Java Runtime Environment** ( **JRE** ) or **Java Development Kit** ( **JDK** ) installed (see the software and hardware list in this book's code bundle for installation instructions).\n",
      "\n",
      "Once you have downloaded the Spark binary package, unpack the contents of the package and change into the newly created directory by running the following commands:\n",
      "\n",
      "    **> tar xfvz spark-1.2.0-bin-hadoop2.4.tgz**\n",
      "    **> cd spark-1.2.0-bin-hadoop2.4**\n",
      "\n",
      "Spark places user scripts to run Spark in the `bin` directory. You can test whether everything is working correctly by running one of the example programs included in Spark:\n",
      "\n",
      "    **>./bin/run-example org.apache.spark.examples.SparkPi**\n",
      "\n",
      "This will run the example in Spark's local standalone mode. In this mode, all the Spark processes are run within the same JVM, and Spark uses multiple threads for parallel processing. By default, the preceding example uses a number of threads equal to the number of cores available on your system. Once the program is finished running, you should see something similar to the following lines near the end of the output:\n",
      "\n",
      "    **...**\n",
      "    **14/11/27 20:58:47 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 0.723269 s**\n",
      "    **Pi is roughly 3.1465**\n",
      "    **...**\n",
      "\n",
      "To configure the level of parallelism in the local mode, you can pass in a `master` parameter of the `local[N]` form, where `N` is the number of threads to use. For example, to use only two threads, run the following command instead:\n",
      "\n",
      "    **> MASTER=local[2] ./bin/run-example org.apache.spark.examples.SparkPi**\n",
      "\n",
      "# Spark clusters\n",
      "\n",
      "A Spark cluster is made up of two types of processes: a driver program and multiple executors. In the local mode, all these processes are run within the same JVM. In a cluster, these processes are usually run on separate nodes.\n",
      "\n",
      "For example, a typical cluster that runs in Spark's standalone mode (that is, using Spark's built-in cluster-management modules) will have:\n",
      "\n",
      "  * A master node that runs the Spark standalone master process as well as the driver program\n",
      "  * A number of worker nodes, each running an executor process\n",
      "\n",
      "While we will be using Spark's local standalone mode throughout this book to illustrate concepts and examples, the same Spark code that we write can be run on a Spark cluster. In the preceding example, if we run the code on a Spark standalone cluster, we could simply pass in the URL for the master node as follows:\n",
      "\n",
      "    **> MASTER=spark://IP:PORT ./bin/run-example org.apache.spark.examples.SparkPi**\n",
      "\n",
      "Here, `IP` is the IP address, and `PORT` is the port of the Spark master. This tells Spark to run the program on the cluster where the Spark master process is running.\n",
      "\n",
      "A full treatment of Spark's cluster management and deployment is beyond the scope of this book. However, we will briefly teach you how to set up and use an Amazon EC2 cluster later in this chapter.\n",
      "\n",
      "### Note\n",
      "\n",
      "For an overview of the Spark cluster-application deployment, take a look at the following links:\n",
      "\n",
      "  * <http://spark.apache.org/docs/latest/cluster-overview.html>\n",
      "  * <http://spark.apache.org/docs/latest/submitting-applications.html>\n",
      "\n",
      "# The Spark programming model\n",
      "\n",
      "Before we delve into a high-level overview of Spark's design, we will introduce the `SparkContext` object as well as the Spark shell, which we will use to interactively explore the basics of the Spark programming model.\n",
      "\n",
      "### Tip\n",
      "\n",
      "While this section provides a brief overview and examples of using Spark, we recommend that you read the following documentation to get a detailed understanding:\n",
      "\n",
      "  * Spark Quick Start: <http://spark.apache.org/docs/latest/quick-start.html>\n",
      "  * _Spark Programming guide_ , which covers Scala, Java, and Python: <http://spark.apache.org/docs/latest/programming-guide.html>\n",
      "\n",
      "## SparkContext and SparkConf\n",
      "\n",
      "The starting point of writing any Spark program is `SparkContext` (or `JavaSparkContext` in Java). `SparkContext` is initialized with an instance of a `SparkConf` object, which contains various Spark cluster-configuration settings (for example, the URL of the master node).\n",
      "\n",
      "Once initialized, we will use the various methods found in the `SparkContext` object to create and manipulate distributed datasets and shared variables. The Spark shell (in both Scala and Python, which is unfortunately not supported in Java) takes care of this context initialization for us, but the following lines of code show an example of creating a context running in the local mode in Scala:\n",
      "\n",
      "    val conf = new SparkConf()\n",
      "    .setAppName(\"Test Spark App\")\n",
      "    .setMaster(\"local[4]\")\n",
      "    val sc = new SparkContext(conf)\n",
      "\n",
      "This creates a context running in the local mode with four threads, with the name of the application set to `Test Spark App`. If we wish to use default configuration values, we could also call the following simple constructor for our `SparkContext` object, which works in exactly the same way:\n",
      "\n",
      "    val sc = new SparkContext(\"local[4]\", \"Test Spark App\")\n",
      "\n",
      "### Tip\n",
      "\n",
      " **Downloading the example code**\n",
      "\n",
      "You can download the example code files for all Packt books you have purchased from your account at <http://www.packtpub.com>. If you purchased this book elsewhere, you can visit <http://www.packtpub.com/support> and register to have the files e-mailed directly to you.\n",
      "\n",
      "## The Spark shell\n",
      "\n",
      "Spark supports writing programs interactively using either the Scala or Python REPL (that is, the **Read-Eval-Print-Loop** , or interactive shell). The shell provides instant feedback as we enter code, as this code is immediately evaluated. In the Scala shell, the return result and type is also displayed after a piece of code is run.\n",
      "\n",
      "To use the Spark shell with Scala, simply run `./bin/spark-shell` from the Spark base directory. This will launch the Scala shell and initialize `SparkContext`, which is available to us as the Scala value, `sc`. Your console output should look similar to the following screenshot:\n",
      "\n",
      "To use the Python shell with Spark, simply run the `./bin/pyspark` command. Like the Scala shell, the Python `SparkContext` object should be available as the Python variable `sc`. You should see an output similar to the one shown in this screenshot:\n",
      "\n",
      "## Resilient Distributed Datasets\n",
      "\n",
      "The core of Spark is a concept called the **Resilient Distributed Dataset** ( **RDD** ). An RDD is a collection of \"records\" (strictly speaking, objects of some type) that is distributed or partitioned across many nodes in a cluster (for the purposes of the Spark local mode, the single multithreaded process can be thought of in the same way). An RDD in Spark is fault-tolerant; this means that if a given node or task fails (for some reason other than erroneous user code, such as hardware failure, loss of communication, and so on), the RDD can be reconstructed automatically on the remaining nodes and the job will still complete.\n",
      "\n",
      "### Creating RDDs\n",
      "\n",
      "RDDs can be created from existing collections, for example, in the Scala Spark shell that you launched earlier:\n",
      "\n",
      "    val collection = List(\"a\", \"b\", \"c\", \"d\", \"e\")\n",
      "    val rddFromCollection = sc.parallelize(collection)\n",
      "\n",
      "RDDs can also be created from Hadoop-based input sources, including the local filesystem, HDFS, and Amazon S3. A Hadoop-based RDD can utilize any input format that implements the Hadoop `InputFormat` interface, including text files, other standard Hadoop formats, HBase, Cassandra, and many more. The following code is an example of creating an RDD from a text file located on the local filesystem:\n",
      "\n",
      "    val rddFromTextFile = sc.textFile(\"LICENSE\")\n",
      "\n",
      "The preceding `textFile` method returns an RDD where each record is a `String` object that represents one line of the text file.\n",
      "\n",
      "### Spark operations\n",
      "\n",
      "Once we have created an RDD, we have a distributed collection of records that we can manipulate. In Spark's programming model, operations are split into transformations and actions. Generally speaking, a transformation operation applies some function to all the records in the dataset, changing the records in some way. An action typically runs some computation or aggregation operation and returns the result to the driver program where `SparkContext` is running.\n",
      "\n",
      "Spark operations are functional in style. For programmers familiar with functional programming in Scala or Python, these operations should seem natural. For those without experience in functional programming, don't worry; the Spark API is relatively easy to learn.\n",
      "\n",
      "One of the most common transformations that you will use in Spark programs is the `map` operator. This applies a function to each record of an RDD, thus _mapping_ the input to some new output. For example, the following code fragment takes the RDD we created from a local text file and applies the `size` function to each record in the RDD. Remember that we created an RDD of `Strings`. Using `map`, we can transform each string to an integer, thus returning an RDD of `Ints`:\n",
      "\n",
      "    val intsFromStringsRDD = rddFromTextFile.map( **line = > line.size**)\n",
      "\n",
      "You should see output similar to the following line in your shell; this indicates the type of the RDD:\n",
      "\n",
      "    **intsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = MappedRDD[5] at map at <console>:14**\n",
      "\n",
      "In the preceding code, we saw the `=>` syntax used. This is the Scala syntax for an anonymous function, which is a function that is not a named method (that is, one defined using the `def` keyword in Scala or Python, for example).\n",
      "\n",
      "### Note\n",
      "\n",
      "While a detailed treatment of anonymous functions is beyond the scope of this book, they are used extensively in Spark code in Scala and Python, as well as in Java 8 (both in examples and real-world applications), so it is useful to cover a few practicalities.\n",
      "\n",
      "The `line => line.size` syntax means that we are applying a function where the input variable is to the left of the `=>` operator, and the output is the result of the code to the right of the `=>` operator. In this case, the input is `line`, and the output is the result of calling `line.size`. In Scala, this function that maps a string to an integer is expressed as `String => Int`.\n",
      "\n",
      "This syntax saves us from having to separately define functions every time we use methods such as `map`; this is useful when the function is simple and will only be used once, as in this example.\n",
      "\n",
      "Now, we can apply a common action operation, `count`, to return the number of records in our RDD:\n",
      "\n",
      "    intsFromStringsRDD.count\n",
      "\n",
      "The result should look something like the following console output:\n",
      "\n",
      "    **14/01/29 23:28:28 INFO SparkContext: Starting job: count at <console>:17**\n",
      "    **...**\n",
      "    **14/01/29 23:28:28 INFO SparkContext: Job finished: count at <console>:17, took 0.019227 s**\n",
      "    **res4: Long = 398**\n",
      "\n",
      "Perhaps we want to find the average length of each line in this text file. We can first use the `sum` function to add up all the lengths of all the records and then divide the sum by the number of records:\n",
      "\n",
      "    val sumOfRecords = intsFromStringsRDD.sum\n",
      "    val numRecords = intsFromStringsRDD.count\n",
      "    val aveLengthOfRecord = sumOfRecords / numRecords\n",
      "\n",
      "The result will be as follows:\n",
      "\n",
      "    **aveLengthOfRecord: Double = 52.06030150753769**\n",
      "\n",
      "Spark operations, in most cases, return a new RDD, with the exception of most actions, which return the result of a computation (such as `Long` for `count` and `Double` for `sum` in the preceding example). This means that we can naturally chain together operations to make our program flow more concise and expressive. For example, the same result as the one in the preceding line of code can be achieved using the following code:\n",
      "\n",
      "    val aveLengthOfRecordChained = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count\n",
      "\n",
      "An important point to note is that Spark transformations are lazy. That is, invoking a transformation on an RDD does not immediately trigger a computation. Instead, transformations are chained together and are effectively only computed when an action is called. This allows Spark to be more efficient by only returning results to the driver when necessary so that the majority of operations are performed in parallel on the cluster.\n",
      "\n",
      "This means that if your Spark program never uses an action operation, it will never trigger an actual computation, and you will not get any results. For example, the following code will simply return a new RDD that represents the chain of transformations:\n",
      "\n",
      "    val transformedRDD = rddFromTextFile.map(line => line.size).filter(size => size > 10).map(size => size * 2)\n",
      "\n",
      "This returns the following result in the console:\n",
      "\n",
      "    **transformedRDD: org.apache.spark.rdd.RDD[Int] = MappedRDD[8] at map at <console>:14**\n",
      "\n",
      "Notice that no actual computation happens and no result is returned. If we now call an action, such as `sum`, on the resulting RDD, the computation will be triggered:\n",
      "\n",
      "    val computation = transformedRDD.sum\n",
      "\n",
      "You will now see that a Spark job is run, and it results in the following console output:\n",
      "\n",
      "    **...**\n",
      "    **14/11/27 21:48:21 INFO SparkContext: Job finished: sum at <console>:16, took 0.193513 s**\n",
      "    **computation: Double = 60468.0**\n",
      "\n",
      "### Tip\n",
      "\n",
      "The complete list of transformations and actions possible on RDDs as well as a set of more detailed examples are available in the Spark programming guide (located at <http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations>), and the API documentation (the Scala API documentation) is located at <http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD>).\n",
      "\n",
      "### Caching RDDs\n",
      "\n",
      "One of the most powerful features of Spark is the ability to cache data in memory across a cluster. This is achieved through use of the `cache` method on an RDD:\n",
      "\n",
      "    rddFromTextFile.cache\n",
      "\n",
      "Calling `cache` on an RDD tells Spark that the RDD should be kept in memory. The first time an action is called on the RDD that initiates a computation, the data is read from its source and put into memory. Hence, the first time such an operation is called, the time it takes to run the task is partly dependent on the time it takes to read the data from the input source. However, when the data is accessed the next time (for example, in subsequent queries in analytics or iterations in a machine learning model), the data can be read directly from memory, thus avoiding expensive I/O operations and speeding up the computation, in many cases, by a significant factor.\n",
      "\n",
      "If we now call the `count` or `sum` function on our cached RDD, we will see that the RDD is loaded into memory:\n",
      "\n",
      "    val aveLengthOfRecordChained = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count\n",
      "\n",
      "Indeed, in the following output, we see that the dataset was cached in memory on the first call, taking up approximately 62 KB and leaving us with around 270 MB of memory free:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 06:59:27 INFO MemoryStore: ensureFreeSpace(63454) called with curMem=32960, maxMem=311387750**\n",
      "    **14/01/30 06:59:27 INFO MemoryStore: Block rdd_2_0 stored as values to memory (estimated size 62.0 KB, free 296.9 MB)**\n",
      "    **14/01/30 06:59:27 INFO BlockManagerMasterActor$BlockManagerInfo: Added rdd_2_0 in memory on 10.0.0.3:55089 (size: 62.0 KB, free: 296.9 MB)**\n",
      "    **...**\n",
      "\n",
      "Now, we will call the same function again:\n",
      "\n",
      "    val aveLengthOfRecordChainedFromCached = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count\n",
      "\n",
      "We will see from the console output that the cached data is read directly from memory:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 06:59:34 INFO BlockManager: Found block rdd_2_0 locally**\n",
      "    **...**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Spark also allows more fine-grained control over caching behavior. You can use the `persist` method to specify what approach Spark uses to cache data. More information on `RDD` caching can be found here: <http://spark.apache.org/docs/latest/programming-guide.html#rdd-persistence>.\n",
      "\n",
      "## Broadcast variables and accumulators\n",
      "\n",
      "Another core feature of Spark is the ability to create two special types of variables: broadcast variables and accumulators.\n",
      "\n",
      "A **broadcast variable** is a _read-only_ variable that is made available from the driver program that runs the `SparkContext` object to the nodes that will execute the computation. This is very useful in applications that need to make the same data available to the worker nodes in an efficient manner, such as machine learning algorithms. Spark makes creating broadcast variables as simple as calling a method on `SparkContext` as follows:\n",
      "\n",
      "    val broadcastAList = sc.broadcast(List(\"a\", \"b\", \"c\", \"d\", \"e\"))\n",
      "\n",
      "The console output shows that the broadcast variable was stored in memory, taking up approximately 488 bytes, and it also shows that we still have 270 MB available to us:\n",
      "\n",
      "    **14/01/30 07:13:32 INFO MemoryStore: ensureFreeSpace(488) called with curMem=96414, maxMem=311387750**\n",
      "    **14/01/30 07:13:32 INFO MemoryStore: Block broadcast_1 stored as values to memory (estimated size 488.0 B, free 296.9 MB)**\n",
      "    **broadCastAList: org.apache.spark.broadcast.Broadcast[List[String]] = Broadcast(1)**\n",
      "\n",
      "A broadcast variable can be accessed from nodes other than the driver program that created it (that is, the worker nodes) by calling `value` on the variable:\n",
      "\n",
      "    sc.parallelize(List(\"1\", \"2\", \"3\")).map(x => broadcastAList. **value** ++ x).collect\n",
      "\n",
      "This code creates a new RDD with three records from a collection (in this case, a Scala `List`) of `(\"1\", \"2\", \"3\")`. In the `map` function, it returns a new collection with the relevant record from our new RDD appended to the `broadcastAList` that is our broadcast variable.\n",
      "\n",
      "Notice that we used the `collect` method in the preceding code. This is a Spark _action_ that returns the entire RDD to the driver as a Scala (or Python or Java) collection.\n",
      "\n",
      "We will often use `collect` when we wish to apply further processing to our results locally within the driver program.\n",
      "\n",
      "### Note\n",
      "\n",
      "Note that `collect` should generally only be used in cases where we really want to return the full result set to the driver and perform further processing. If we try to call `collect` on a very large dataset, we might run out of memory on the driver and crash our program.\n",
      "\n",
      "It is preferable to perform as much heavy-duty processing on our Spark cluster as possible, preventing the driver from becoming a bottleneck. In many cases, however, collecting results to the driver is necessary, such as during iterations in many machine learning models.\n",
      "\n",
      "On inspecting the result, we will see that for each of the three records in our new RDD, we now have a record that is our original broadcasted `List`, with the new element appended to it (that is, there is now either `\"1\"`, `\"2\"`, or `\"3\"` at the end):\n",
      "\n",
      "    **...**\n",
      "    **14/01/31 10:15:39 INFO SparkContext: Job finished: collect at <console>:15, took 0.025806 s**\n",
      "    **res6: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, c, d, e, 2), List(a, b, c, d, e, 3))**\n",
      "\n",
      "An **accumulator** is also a variable that is broadcasted to the worker nodes. The key difference between a broadcast variable and an accumulator is that while the broadcast variable is read-only, the accumulator can be added to. There are limitations to this, that is, in particular, the addition must be an associative operation so that the global accumulated value can be correctly computed in parallel and returned to the driver program. Each worker node can only access and add to its own local accumulator value, and only the driver program can access the global value. Accumulators are also accessed within the Spark code using the `value` method.\n",
      "\n",
      "### Tip\n",
      "\n",
      "For more details on broadcast variables and accumulators, see the _Shared Variables_ section of the _Spark Programming Guide_ : <http://spark.apache.org/docs/latest/programming-guide.html#shared-variables>.\n",
      "\n",
      "# The first step to a Spark program in Scala\n",
      "\n",
      "We will now use the ideas we introduced in the previous section to write a basic Spark program to manipulate a dataset. We will start with Scala and then write the same program in Java and Python. Our program will be based on exploring some data from an online store, about which users have purchased which products. The data is contained in a **comma-separated-value** ( **CSV** ) file called `UserPurchaseHistory.csv`, and the contents are shown in the following snippet. The first column of the CSV is the username, the second column is the product name, and the final column is the price:\n",
      "\n",
      "    **John,iPhone Cover,9.99**\n",
      "    **John,Headphones,5.49**\n",
      "    **Jack,iPhone Cover,9.99**\n",
      "    **Jill,Samsung Galaxy Cover,8.95**\n",
      "    **Bob,iPad Cover,5.49**\n",
      "\n",
      "For our Scala program, we need to create two files: our Scala code and our project build configuration file, using the build tool **Scala Build Tool** ( **sbt** ). For ease of use, we recommend that you download the sample project code called `scala-spark-app` for this chapter. This code also contains the CSV file under the `data` directory. You will need SBT installed on your system in order to run this example program (we use version 0.13.1 at the time of writing this book).\n",
      "\n",
      "### Tip\n",
      "\n",
      "Setting up SBT is beyond the scope of this book; however, you can find more information at <http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html>.\n",
      "\n",
      "Our SBT configuration file, `build.sbt`, looks like this (note that the empty lines between each line of code are required):\n",
      "\n",
      "    name := \"scala-spark-app\"\n",
      "\n",
      "    version := \"1.0\"\n",
      "\n",
      "    scalaVersion := \"2.10.4\"\n",
      "\n",
      "    libraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"1.2.0 \"\n",
      "\n",
      "The last line adds the dependency on Spark to our project.\n",
      "\n",
      "Our Scala program is contained in the `ScalaApp.scala` file. We will walk through the program piece by piece. First, we need to import the required Spark classes:\n",
      "\n",
      "    import org.apache.spark.SparkContext\n",
      "    import org.apache.spark.SparkContext._\n",
      "\n",
      "    /**\n",
      "     * A simple Spark app in Scala\n",
      "     */\n",
      "    object ScalaApp {\n",
      "\n",
      "In our main method, we need to initialize our `SparkContext` object and use this to access our CSV data file with the `textFile` method. We will then map the raw text by splitting the string on the delimiter character (a comma in this case) and extracting the relevant records for username, product, and price:\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "        val sc = new SparkContext(\"local[2]\", \"First Spark App\")\n",
      "        // we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)\n",
      "        val data = sc.textFile(\"data/UserPurchaseHistory.csv\")\n",
      "          .map(line => line.split(\",\"))\n",
      "          .map(purchaseRecord => (purchaseRecord(0), purchaseRecord(1), purchaseRecord(2)))\n",
      "\n",
      "Now that we have an RDD, where each record is made up of `(user, product, price)`, we can compute various interesting metrics for our store, such as the following ones:\n",
      "\n",
      "  * The total number of purchases\n",
      "  * The number of unique users who purchased\n",
      "  * Our total revenue\n",
      "  * Our most popular product\n",
      "\n",
      "Let's compute the preceding metrics:\n",
      "\n",
      "        // let's count the number of purchases\n",
      "        val numPurchases = data.count()\n",
      "        // let's count how many unique users made purchases\n",
      "        val uniqueUsers = data.map{ case (user, product, price) => user }.distinct().count()\n",
      "        // let's sum up our total revenue\n",
      "        val totalRevenue = data.map{ case (user, product, price) => price.toDouble }.sum()\n",
      "        // let's find our most popular product\n",
      "        val productsByPopularity = data\n",
      "          .map{ case (user, product, price) => (product, 1) }\n",
      "          .reduceByKey(_ + _)\n",
      "          .collect()\n",
      "          .sortBy(-_._2)    \n",
      "        val mostPopular = productsByPopularity(0)\n",
      "\n",
      "This last piece of code to compute the most popular product is an example of the _Map/Reduce_ pattern made popular by Hadoop. First, we mapped our records of `(user, product, price)` to the records of `(product, 1)`. Then, we performed a `reduceByKey` operation, where we summed up the 1s for each unique product.\n",
      "\n",
      "Once we have this transformed RDD, which contains the number of purchases for each product, we will call `collect`, which returns the results of the computation to the driver program as a local Scala collection. We will then sort these counts locally (note that in practice, if the amount of data is large, we will perform the sorting in parallel, usually with a Spark operation such as `sortByKey`).\n",
      "\n",
      "Finally, we will print out the results of our computations to the console:\n",
      "\n",
      "        println(\"Total purchases: \" + numPurchases)\n",
      "        println(\"Unique users: \" + uniqueUsers)\n",
      "        println(\"Total revenue: \" + totalRevenue)\n",
      "        println(\"Most popular product: %s with %d purchases\".format(mostPopular._1, mostPopular._2))\n",
      "      }\n",
      "    }\n",
      "\n",
      "We can run this program by running `sbt run` in the project's base directory or by running the program in your Scala IDE if you are using one. The output should look similar to the following:\n",
      "\n",
      "    **...**\n",
      "    **[info] Compiling 1 Scala source to ...**\n",
      "    **[info] Running ScalaApp**\n",
      "    **...**\n",
      "    **14/01/30 10:54:40 INFO spark.SparkContext: Job finished: collect at ScalaApp.scala:25, took 0.045181 s**\n",
      "    **Total purchases: 5**\n",
      "    **Unique users: 4**\n",
      "    **Total revenue: 39.91**\n",
      "    **Most popular product: iPhone Cover with 2 purchases**\n",
      "\n",
      "We can see that we have five purchases from four different users with a total revenue of 39.91. Our most popular product is an iPhone cover with 2 purchases.\n",
      "\n",
      "# The first step to a Spark program in Java\n",
      "\n",
      "The Java API is very similar in principle to the Scala API. However, while Scala can call the Java code quite easily, in some cases, it is not possible to call the Scala code from Java. This is particularly the case when such Scala code makes use of certain Scala features such as implicit conversions, default parameters, and the Scala reflection API.\n",
      "\n",
      "Spark makes heavy use of these features in general, so it is necessary to have a separate API specifically for Java that includes Java versions of the common classes. Hence, `SparkContext` becomes `JavaSparkContext`, and `RDD` becomes `JavaRDD`.\n",
      "\n",
      "Java versions prior to version 8 do not support anonymous functions and do not have succinct syntax for functional-style programming, so functions in the Spark Java API must implement a `WrappedFunction` interface with the `call` method signature. While it is significantly more verbose, we will often create one-off anonymous classes to pass to our Spark operations, which implement this interface and the `call` method, to achieve much the same effect as anonymous functions in Scala.\n",
      "\n",
      "Spark provides support for Java 8's anonymous function (or _lambda_ ) syntax. Using this syntax makes a Spark program written in Java 8 look very close to the equivalent Scala program.\n",
      "\n",
      "In Scala, an RDD of key/value pairs provides special operators (such as `reduceByKey` and `saveAsSequenceFile`, for example) that are accessed automatically via implicit conversions. In Java, special types of `JavaRDD` classes are required in order to access similar functions. These include `JavaPairRDD` to work with key/value pairs and `JavaDoubleRDD` to work with numerical records.\n",
      "\n",
      "### Tip\n",
      "\n",
      "In this section, we covered the standard Java API syntax. For more details and examples related to working RDDs in Java as well as the Java 8 lambda syntax, see the Java sections of the _Spark Programming Guide_ found at <http://spark.apache.org/docs/latest/programming-guide.html#rdd-operations>.\n",
      "\n",
      "We will see examples of most of these differences in the following Java program, which is included in the example code of this chapter in the directory named `java-spark-app`. The code directory also contains the CSV data file under the `data` subdirectory.\n",
      "\n",
      "We will build and run this project with the Maven build tool, which we assume you have installed on your system.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Installing and setting up Maven is beyond the scope of this book. Usually, Maven can easily be installed using the package manager on your Linux system or HomeBrew or MacPorts on Mac OS X.\n",
      "\n",
      "Detailed installation instructions can be found here: <http://maven.apache.org/download.cgi>.\n",
      "\n",
      "The project contains a Java file called `JavaApp.java`, which contains our program code:\n",
      "\n",
      "    import org.apache.spark.api.java.JavaRDD;\n",
      "    import org.apache.spark.api.java.JavaSparkContext;\n",
      "    import org.apache.spark.api.java.function.DoubleFunction;\n",
      "    import org.apache.spark.api.java.function.Function;\n",
      "    import org.apache.spark.api.java.function.Function2;\n",
      "    import org.apache.spark.api.java.function.PairFunction;\n",
      "    import scala.Tuple2;\n",
      "\n",
      "    import java.util.Collections;\n",
      "    import java.util.Comparator;\n",
      "    import java.util.List;\n",
      "\n",
      "    /**\n",
      "     * A simple Spark app in Java\n",
      "     */\n",
      "    public class JavaApp {\n",
      "\n",
      "      public static void main(String[] args) {\n",
      "\n",
      "As in our Scala example, we first need to initialize our context. Notice that we will use the `JavaSparkContext` class here instead of the `SparkContext` class that we used earlier. We will use the `JavaSparkContext` class in the same way to access our data using `textFile` and then split each row into the required fields. Note how we used an anonymous class to define a split function that performs the string processing, in the highlighted code:\n",
      "\n",
      "        JavaSparkContext sc = new JavaSparkContext(\"local[2]\", \"First Spark App\");\n",
      "        // we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)\n",
      "        JavaRDD<String[]> data = sc.textFile(\"data/UserPurchaseHistory.csv\")\n",
      "        .map( **new Function <String, String[]>() {**\n",
      "    **@Override**\n",
      "    **public String[] call(String s) throws Exception {**\n",
      "    **return s.split(\",\");**\n",
      "    **}**\n",
      "    **}** );\n",
      "\n",
      "Now, we can compute the same metrics as we did in our Scala example. Note how some methods are the same (for example, `distinct` and `count`) for the Java and Scala APIs. Also note the use of anonymous classes that we pass to the `map` function. This code is highlighted here:\n",
      "\n",
      "        // let's count the number of purchases\n",
      "        long numPurchases = data.count();\n",
      "        // let's count how many unique users made purchases\n",
      "        long uniqueUsers = data.map( **new Function <String[], String>() {**\n",
      "    **@Override**\n",
      "    **public String call(String[] strings) throws Exception {**\n",
      "    **return strings[0];**\n",
      "    **}**\n",
      "    **}** ).distinct().count();\n",
      "        // let's sum up our total revenue\n",
      "        double totalRevenue = data.map( **new DoubleFunction <String[]>() {**\n",
      "    **@Override**\n",
      "    **public Double call(String[] strings) throws Exception {**\n",
      "    **return Double.parseDouble(strings[2]);**\n",
      "    **}**\n",
      "    **}** ).sum();\n",
      "\n",
      "In the following lines of code, we can see that the approach to compute the most popular product is the same as that in the Scala example. The extra code might seem complex, but it is mostly related to the Java code required to create the anonymous functions (which we have highlighted here). The actual functionality is the same:\n",
      "\n",
      "        // let's find our most popular product\n",
      "        // first we map the data to records of (product, 1)using a PairFunction\n",
      "        // and the Tuple2 class.\n",
      "        // then we call a reduceByKey operation with a Function2, which is essentially the sum function\n",
      "        List<Tuple2<String, Integer>> pairs = data.map( **new PairFunction <String[], String, Integer>() {**\n",
      "    **@Override**\n",
      "    **public Tuple2 <String, Integer> call(String[] strings)throws Exception {**\n",
      "    **return new Tuple2(strings[1], 1);**\n",
      "    **}**\n",
      "    **}** ).reduceByKey( **new Function2 <Integer, Integer, Integer>() {**\n",
      "    **@Override**\n",
      "    **public Integer call(Integer integer, Integer integer2)throws Exception {**\n",
      "    **return integer + integer2;**\n",
      "    **}**\n",
      "    **}** ).collect();\n",
      "        // finally we sort the result. Note we need to create a Comparator function,\n",
      "        // that reverses the sort order.\n",
      "        Collections.sort(pairs, new Comparator<Tuple2<String,Integer>>() {\n",
      "          @Override\n",
      "          public int compare(Tuple2<String, Integer> o1,Tuple2<String, Integer> o2) {\n",
      "            return -(o1._2() - o2._2());\n",
      "          }\n",
      "        });\n",
      "        String mostPopular = pairs.get(0)._1();\n",
      "        int purchases = pairs.get(0)._2();\n",
      "        System.out.println(\"Total purchases: \" + numPurchases);\n",
      "        System.out.println(\"Unique users: \" + uniqueUsers);\n",
      "        System.out.println(\"Total revenue: \" + totalRevenue);\n",
      "        System.out.println(String.format(\"Most popular product:%s with %d purchases\", mostPopular, purchases));\n",
      "      }\n",
      "    }\n",
      "\n",
      "As can be seen, the general structure is similar to the Scala version, apart from the extra boilerplate code to declare variables and functions via anonymous inner classes. It is a good exercise to work through both examples and compare the same lines of Scala code to those in Java to understand how the same result is achieved in each language.\n",
      "\n",
      "This program can be run with the following command executed from the project's base directory:\n",
      "\n",
      "    **> mvn exec:java -Dexec.mainClass=\"JavaApp\"**\n",
      "\n",
      "You will see output that looks very similar to the Scala version, with the results of the computation identical:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 17:02:43 INFO spark.SparkContext: Job finished: collect at JavaApp.java:46, took 0.039167 s**\n",
      "    **Total purchases: 5**\n",
      "    **Unique users: 4**\n",
      "    **Total revenue: 39.91**\n",
      "    **Most popular product: iPhone Cover with 2 purchases**\n",
      "\n",
      "# The first step to a Spark program in Python\n",
      "\n",
      "Spark's Python API exposes virtually all the functionalities of Spark's Scala API in the Python language. There are some features that are not yet supported (for example, graph processing with GraphX and a few API methods here and there). See the Python section of the _Spark Programming Guide_ (<http://spark.apache.org/docs/latest/programming-guide.html>) for more details.\n",
      "\n",
      "Following on from the preceding examples, we will now write a Python version. We assume that you have Python version 2.6 and higher installed on your system (for example, most Linux and Mac OS X systems come with Python preinstalled).\n",
      "\n",
      "The example program is included in the sample code for this chapter, in the directory named `python-spark-app`, which also contains the CSV data file under the `data` subdirectory. The project contains a script, `pythonapp.py`, provided here:\n",
      "\n",
      "    \"\"\"A simple Spark app in Python\"\"\"\n",
      "    from pyspark import SparkContext\n",
      "\n",
      "    sc = SparkContext(\"local[2]\", \"First Spark App\")\n",
      "    # we take the raw data in CSV format and convert it into a set of records of the form (user, product, price)\n",
      "    data = sc.textFile(\"data/UserPurchaseHistory.csv\").map(lambda line: line.split(\",\")).map(lambda record: (record[0], record[1], record[2]))\n",
      "    # let's count the number of purchases\n",
      "    numPurchases = data.count()\n",
      "    # let's count how many unique users made purchases\n",
      "    uniqueUsers = data.map(lambda record: record[0]).distinct().count()\n",
      "    # let's sum up our total revenue\n",
      "    totalRevenue = data.map(lambda record: float(record[2])).sum()\n",
      "    # let's find our most popular product\n",
      "    **products = data.map(lambda record: (record[1], 1.0)).reduceByKey(lambda a, b: a + b).collect()**\n",
      "    mostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]\n",
      "\n",
      "    print \"Total purchases: %d\" % numPurchases\n",
      "    print \"Unique users: %d\" % uniqueUsers\n",
      "    print \"Total revenue: %2.2f\" % totalRevenue\n",
      "    print \"Most popular product: %s with %d purchases\" % (mostPopular[0], mostPopular[1])\n",
      "\n",
      "If you compare the Scala and Python versions of our program, you will see that generally, the syntax looks very similar. One key difference is how we express anonymous functions (also called `lambda` functions; hence, the use of this keyword for the Python syntax). In Scala, we've seen that an anonymous function mapping an input `x` to an output `y` is expressed as `x => y`, while in Python, it is `lambda x: y`. In the highlighted line in the preceding code, we are applying an anonymous function that maps two inputs, `a` and `b`, generally of the same type, to an output. In this case, the function that we apply is the _plus_ function; hence, `lambda a, b: a + b`.\n",
      "\n",
      "The best way to run the script is to run the following command from the base directory of the sample project:\n",
      "\n",
      "    **> $SPARK_HOME/bin/spark-submit pythonapp.py**\n",
      "\n",
      "Here, the `SPARK_HOME` variable should be replaced with the path of the directory in which you originally unpacked the Spark prebuilt binary package at the start of this chapter.\n",
      "\n",
      "Upon running the script, you should see output similar to that of the Scala and Java examples, with the results of our computation being the same:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 11:43:47 INFO SparkContext: Job finished: collect at pythonapp.py:14, took 0.050251 s**\n",
      "    **Total purchases: 5**\n",
      "    **Unique users: 4**\n",
      "    **Total revenue: 39.91**\n",
      "    **Most popular product: iPhone Cover with 2 purchases**\n",
      "\n",
      "# Getting Spark running on Amazon EC2\n",
      "\n",
      "The Spark project provides scripts to run a Spark cluster in the cloud on Amazon's EC2 service. These scripts are located in the `ec2` directory. You can run the `spark-ec2` script contained in this directory with the following command:\n",
      "\n",
      "    **>./ec2/spark-ec2 **\n",
      "\n",
      "Running it in this way without an argument will show the help output:\n",
      "\n",
      "    **Usage: spark-ec2 [options] <action> <cluster_name>**\n",
      "    **< action> can be: launch, destroy, login, stop, start, get-master**\n",
      "\n",
      "    **Options:**\n",
      "    **...**\n",
      "\n",
      "Before creating a Spark EC2 cluster, you will need to ensure you have an Amazon account.\n",
      "\n",
      "### Tip\n",
      "\n",
      "If you don't have an Amazon Web Services account, you can sign up at <http://aws.amazon.com/>.\n",
      "\n",
      "The AWS console is available at <http://aws.amazon.com/console/>.\n",
      "\n",
      "You will also need to create an Amazon EC2 key pair and retrieve the relevant security credentials. The Spark documentation for EC2 (available at <http://spark.apache.org/docs/latest/ec2-scripts.html>) explains the requirements:\n",
      "\n",
      ">  _Create an Amazon EC2 key pair for yourself. This can be done by logging into your Amazon Web Services account through the AWS console, clicking on_ **Key Pairs** _on the left sidebar, and creating and downloading a key. Make sure that you set the permissions for the private key file to 600 (that is, only you can read and write it) so that`ssh` will work._\n",
      "> \n",
      ">  _Whenever you want to use the_ `spark-ec2` _script, set the environment variables_ `AWS_ACCESS_KEY_ID` _and_ `AWS_SECRET_ACCESS_KEY` _to your Amazon EC2 access key ID and secret access key, respectively. These can be obtained from the AWS homepage by clicking_ **Account** | **Security Credentials** | **Access Credentials**.\n",
      "\n",
      "When creating a key pair, choose a name that is easy to remember. We will simply use `spark` for the key pair name. The key pair file itself will be called `spark.pem`. As mentioned earlier, ensure that the key pair file permissions are set appropriately and that the environment variables for the AWS credentials are exported using the following commands:\n",
      "\n",
      "    **> chmod 600 spark.pem**\n",
      "    **> export AWS_ACCESS_KEY_ID=\"...\"**\n",
      "    **> export AWS_SECRET_ACCESS_KEY=\"...\"**\n",
      "\n",
      "You should also be careful to keep your downloaded key pair file safe and not lose it, as it can only be downloaded once when it is created!\n",
      "\n",
      "Note that launching an Amazon EC2 cluster in the following section will _incur costs_ to your AWS account.\n",
      "\n",
      "## Launching an EC2 Spark cluster\n",
      "\n",
      "We're now ready to launch a small Spark cluster by changing into the `ec2` directory and then running the cluster launch command:\n",
      "\n",
      "    **> cd ec2**\n",
      "    **>./spark-ec2 -k spark -i spark.pem -s 1 --instance-type m3.medium --hadoop-major-version 2 launch test-cluster**\n",
      "\n",
      "This will launch a new Spark cluster called `test-cluster` with one master and one slave node of instance type `m3.medium`. This cluster will be launched with a Spark version built for Hadoop 2. The key pair name we used is `spark`, and the key pair file is `spark.pem` (if you gave the files different names or have an existing AWS key pair, use that name instead).\n",
      "\n",
      "It might take quite a while for the cluster to fully launch and initialize. You should see something like this screenshot immediately after running the launch command:\n",
      "\n",
      "If the cluster has launched successfully, you should eventually see the console output similar to the following screenshot:\n",
      "\n",
      "To test whether we can connect to our new cluster, we can run the following command:\n",
      "\n",
      "    **> ssh -i spark.pem root@ec2-54-227-127-14.compute-1.amazonaws.com**\n",
      "\n",
      "Remember to replace the public domain name of the master node (the address after `root@` in the preceding command) with the correct Amazon EC2 public domain name that will be shown in your console output after launching the cluster.\n",
      "\n",
      "You can also retrieve your cluster's master public domain name by running this line of code:\n",
      "\n",
      "    **>./spark-ec2 -i spark.pem get-master test-cluster**\n",
      "\n",
      "After successfully running the `ssh` command, you will be connected to your Spark master node in EC2, and your terminal output should match the following screenshot:\n",
      "\n",
      "We can test whether our cluster is correctly set up with Spark by changing into the Spark directory and running an example in the local mode:\n",
      "\n",
      "    **> cd spark**\n",
      "    **> MASTER=local[2] ./bin/run-example SparkPi**\n",
      "\n",
      "You should see output similar to running the same command on your local computer:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 20:20:21 INFO SparkContext: Job finished: reduce at SparkPi.scala:35, took 0.864044012 s**\n",
      "    **Pi is roughly 3.14032**\n",
      "    **...**\n",
      "\n",
      "Now that we have an actual cluster with multiple nodes, we can test Spark in the cluster mode. We can run the same example on the cluster, using our 1 slave node, by passing in the master URL instead of the local version:\n",
      "\n",
      "    **> MASTER=spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077 ./bin/run-example SparkPi **\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that you will need to substitute the preceding master domain name with the correct domain name for your specific cluster.\n",
      "\n",
      "Again, the output should be similar to running the example locally; however, the log messages will show that your driver program has connected to the Spark master:\n",
      "\n",
      "    **...**\n",
      "    **14/01/30 20:26:17 INFO client.Client$ClientActor: Connecting to master spark://ec2-54-220-189-136.eu-west-1.compute.amazonaws.com:7077**\n",
      "    **14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Connected to Spark cluster with app ID app-20140130202617-0001**\n",
      "    **14/01/30 20:26:17 INFO client.Client$ClientActor: Executor added: app-20140130202617-0001/0 on worker-20140130201049-ip-10-34-137-45.eu-west-1.compute.internal-57119 (ip-10-34-137-45.eu-west-1.compute.internal:57119) with 1 cores**\n",
      "    **14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20140130202617-0001/0 on hostPort ip-10-34-137-45.eu-west-1.compute.internal:57119 with 1 cores, 2.4 GB RAM**\n",
      "    **14/01/30 20:26:17 INFO client.Client$ClientActor: Executor updated: app-20140130202617-0001/0 is now RUNNING**\n",
      "    **14/01/30 20:26:18 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:39**\n",
      "    **...**\n",
      "\n",
      "Feel free to experiment with your cluster. Try out the interactive console in Scala, for example:\n",
      "\n",
      "    **> ./bin/spark-shell --master spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077**\n",
      "\n",
      "Once you've finished, type `exit` to leave the console. You can also try the PySpark console by running the following command:\n",
      "\n",
      "    **> ./bin/pyspark --master spark://ec2-54-227-127-14.compute-1.amazonaws.com:7077**\n",
      "\n",
      "You can use the Spark Master web interface to see the applications registered with the master. To load the Master Web UI, navigate to `ec2-54-227-127-14.compute-1.amazonaws.com:8080` (again, remember to replace this domain name with your own master domain name). You should see something similar to the following screenshot showing the example you ran as well as the two console applications you launched:\n",
      "\n",
      "Remember that _you will be charged by Amazon_ for usage of the cluster. Don't forget to stop or terminate this test cluster once you're done with it. To do this, you can first exit the `ssh` session by typing `exit` to return to your own local system and then, run the following command:\n",
      "\n",
      "    **>./ec2/spark-ec2 -k spark -i spark.pem destroy test-cluster**\n",
      "\n",
      "You should see the following output:\n",
      "\n",
      "    **Are you sure you want to destroy the cluster test-cluster?**\n",
      "    **The following instances will be terminated:**\n",
      "    **Searching for existing cluster test-cluster...**\n",
      "    **Found 1 master(s), 1 slaves**\n",
      "    **> ec2-54-227-127-14.compute-1.amazonaws.com**\n",
      "    **> ec2-54-91-61-225.compute-1.amazonaws.com**\n",
      "    **ALL DATA ON ALL NODES WILL BE LOST!!**\n",
      "    **Destroy cluster test-cluster (y/N): y**\n",
      "    **Terminating master...**\n",
      "    **Terminating slaves...**\n",
      "\n",
      "Hit _Y_ and then _Enter_ to destroy the cluster.\n",
      "\n",
      "Congratulations! You've just set up a Spark cluster in the cloud, run a fully parallel example program on this cluster, and terminated it. If you would like to try out any of the example code in the subsequent chapters (or your own Spark programs) on a cluster, feel free to experiment with the Spark EC2 scripts and launch a cluster of your chosen size and instance profile (just be mindful of the costs and remember to shut it down when you're done!).\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we covered how to set up Spark locally on our own computer as well as in the cloud as a cluster running on Amazon EC2. You learned the basics of Spark's programming model and API using the interactive Scala console, and we wrote the same basic Spark program in Scala, Java, and Python.\n",
      "\n",
      "In the next chapter, we will consider how to go about using Spark to create a machine learning system.\n",
      "\n",
      "# Chapter 2. Designing a Machine Learning System\n",
      "\n",
      "In this chapter, we will design a high-level architecture for an intelligent, distributed machine learning system that uses Spark as its core computation engine. The problem we will focus on will be taking the existing architecture for a web-based business and redesigning it to use automated machine learning systems to power key areas of the business. In this chapter, we will:\n",
      "\n",
      "  * Introduce our hypothetical business scenario\n",
      "  * Provide an overview of the current architecture\n",
      "  * Explore various ways in which machine learning systems can enhance or replace certain business functions\n",
      "  * Provide a new architecture based on these ideas\n",
      "\n",
      "A modern large-scale data environment includes the following requirements:\n",
      "\n",
      "  * It must integrate with other components of the system, especially with data collection and storage systems, analytics and reporting, and frontend applications.\n",
      "  * It should be easily scalable and independent of the rest of the architecture. Ideally, this should be in the form of horizontal as well as vertical scalability.\n",
      "  * It should allow efficient computation in respect of the type of workload in mind, that is machine learning and iterative analytics applications.\n",
      "  * If possible, it should support both batch and real-time workloads.\n",
      "\n",
      "As a framework, Spark meets these criteria. However, we must ensure that the machine learning systems designed on Spark also meet these criteria. There is no good in implementing an algorithm that ends up having bottlenecks that cause our system to fail in terms of one or more of these requirements.\n",
      "\n",
      "# Introducing MovieStream\n",
      "\n",
      "To better illustrate the design of our architecture, we will introduce a practical scenario. Let's assume that we have just been appointed to head the data science team of MovieStream, a fictitious Internet business that streams movies and television shows to its users.\n",
      "\n",
      "MovieStream is growing rapidly, adding both users and titles to its catalogue. The current MovieStream system is outlined in the following diagram:\n",
      "\n",
      "MovieStream's current architecture\n",
      "\n",
      "As we can see in the preceding diagram, currently, MovieStream's content editorial team is responsible for deciding which movies and shows are promoted and shown on the various parts of the site. They are also responsible for creating the content for MovieStream's bulk marketing campaigns, which include e-mail and other direct marketing channels. Currently, MovieStream collects basic data on what titles are viewed by users on an aggregate basis and has access to some demographic data collected from users when they sign up to the service. In addition, they have access to some basic metadata about the titles in their catalogue.\n",
      "\n",
      "The MovieStream team is stretched thin due to their rapid growth, and they can't keep up with the number of new releases and the growing activity of their users. The CEO of MovieStream has heard a lot about big data, machine learning, and artificial intelligence, and would like us to build a machine learning system for MovieStream that can handle many of the functions currently handled by the content team in an automated manner.\n",
      "\n",
      "# Business use cases for a machine learning system\n",
      "\n",
      "Perhaps the first question we should answer is, \"Why use machine learning at all?\" Why doesn't MovieStream simply continue with human-driven decisions? There are many reasons to use machine learning (and certainly some reasons not to), but the most important ones are mentioned here:\n",
      "\n",
      "  * The scale of data involved means that full human involvement quickly becomes infeasible as MovieStream grows\n",
      "  * Model-driven approaches such as machine learning and statistics can often benefit from uncovering patterns that cannot be seen by humans (due to the size and complexity of the datasets)\n",
      "  * Model-driven approaches can avoid human and emotional biases (as long as the correct processes are carefully applied)\n",
      "\n",
      "However, there is no reason why both model-driven and human-driven processes and decision making cannot coexist. For example, many machine learning systems rely on receiving labeled data in order to train models. Often, labeling such data is costly, time consuming, and requires human input. A good example of this is classifying textual data into categories or assigning a sentiment indicator to the text. Many real-world systems use some form of human-driven system to generate labels for such data (or at least part of it) to provide training data to models. These models are then used to make predictions in the live system at a larger scale.\n",
      "\n",
      "In the context of MovieStream, we need not fear that our machine learning system will make the content team redundant. Indeed, we will see that our aim is to lift the burden of time-consuming tasks where machine learning might be able to perform better while providing tools to allow the team to better understand the users and content. This might, for example, help them in selecting which new content to acquire for the catalogue (which involves a significant amount of cost and is therefore a critical aspect of the business).\n",
      "\n",
      "## Personalization\n",
      "\n",
      "Perhaps one of the most important potential applications of machine learning in MovieStream's business is personalization. Generally speaking, personalization refers to adapting the experience of a user and the content presented to them based on various factors, which might include user behavior data as well as external factors.\n",
      "\n",
      " **Recommendations** are essentially a subset of personalization. Recommendation generally refers to presenting a user with a list of items that we hope the user will be interested in. Recommendations might be used in web pages (for example, recommending related products), via e-mails or other direct marketing channels, via mobile apps, and so on.\n",
      "\n",
      "Personalization is very similar to recommendations, but while recommendations are usually focused on an _explicit_ presentation of products or content to the user, personalization is more generic and, often, more _implicit_. For example, applying personalization to search on the MovieStream site might allow us to adapt the search results for a given user, based on the data available about that user. This might include recommendation-based data (in the case of a search for products or content) but might also include various other factors such as geolocation and past search history. It might not be apparent to the user that the search results are adapted to their specific profile; this is why personalization tends to be more implicit.\n",
      "\n",
      "## Targeted marketing and customer segmentation\n",
      "\n",
      "In a manner similar to recommendations, targeted marketing uses a model to select what to target at users. While generally recommendations and personalization are focused on a one-to-one situation, segmentation approaches might try to assign users into groups based on characteristics and, possibly, behavioral data. The approach might be fairly simple or might involve a machine learning model such as clustering. Either way, the result is a set of segment assignments that might allow us to understand the broad characteristics of each group of users, what makes them similar to each other within a group, and what makes them different from others in different groups.\n",
      "\n",
      "This could help MovieStream to better understand the drivers of user behavior and might also allow a broader targeting approach where groups are targeted as opposed to (or more likely, in addition to) direct one-to-one targeting with personalization.\n",
      "\n",
      "These methods can also help when we don't necessarily have labeled data available (as is the case with certain user and content profile data) but we still wish to perform more focused targeting than a complete _one-size-fits-all_ approach.\n",
      "\n",
      "## Predictive modeling and analytics\n",
      "\n",
      "A third area where machine learning can be applied is in predictive analytics. This is a very broad term, and in some ways, it encompasses recommendations, personalization, and targeting too. In this context, since recommendations and segmentation are somewhat distinct, we use the term **predictive modeling** to refer to other models that seek to make predictions. An example of this can be a model to predict the potential viewing activity and revenue of new titles before any data is available on how popular the title might be. MovieStream can use past activity and revenue data, together with content attributes, to create a **regression model** that can be used to make predictions for brand new titles.\n",
      "\n",
      "As another example, we can use a **classification model** to automatically assign tags, keywords, or categories to new titles for which we only have partial data.\n",
      "\n",
      "# Types of machine learning models\n",
      "\n",
      "While we have highlighted a few use cases for machine learning in the context of the preceding MovieStream example, there are many other examples, some of which we will touch on in the relevant chapters when we introduce each machine learning task.\n",
      "\n",
      "However, we can broadly divide the preceding use cases and methods into two categories of machine learning:\n",
      "\n",
      "  *  **Supervised learning** : These types of models use _labeled_ data to learn. Recommendation engines, regression, and classification are examples of supervised learning methods. The labels in these models can be user-movie ratings (for recommendation), movie tags (in the case of the preceding classification example), or revenue figures (for regression). We will cover supervised learning models in Chapter 4, _Building a Recommendation Engine with Spark_ , Chapter 5, _Building a Classification Model with Spark_ , and Chapter 6, _Building a Regression Model with Spark_.\n",
      "  *  **Unsupervised learning** : When a model does not require labeled data, we refer to unsupervised learning. These types of models try to learn or extract some underlying structure in the data or reduce the data down to its most important features. Clustering, dimensionality reduction, and some forms of feature extraction, such as text processing, are all unsupervised techniques and will be dealt with in Chapter 7, _Building a Clustering Model with Spark_ , Chapter 8, _Dimensionality Reduction with Spark_ , and Chapter 9, _Advanced Text Processing with Spark_.\n",
      "\n",
      "# The components of a data-driven machine learning system\n",
      "\n",
      "The high-level components of our machine learning system are outlined in the following diagram. This diagram illustrates the machine learning pipeline from which we obtain data and in which we store data. We then transform it into a form that is usable as input to a machine learning model; train, test, and refine our model; and then, deploy the final model to our production system. The process is then repeated as new data is generated.\n",
      "\n",
      "A general machine learning pipeline\n",
      "\n",
      "## Data ingestion and storage\n",
      "\n",
      "The first step in our machine learning pipeline will be taking in the data that we require for training our models. Like many other businesses, MovieStream's data is typically generated by user activity, other systems (this is commonly referred to as machine-generated data), and external sources (for example, the time of day and weather during a particular user's visit to the site).\n",
      "\n",
      "This data can be ingested in various ways, for example, gathering user activity data from browser and mobile application event logs or accessing external web APIs to collect data on geolocation or weather.\n",
      "\n",
      "Once the collection mechanisms are in place, the data usually needs to be stored. This includes the raw data, data resulting from intermediate processing, and final model results to be used in production.\n",
      "\n",
      "Data storage can be complex and involve a wide variety of systems, including HDFS, Amazon S3, and other filesystems; SQL databases such as MySQL or PostgreSQL; distributed NoSQL data stores such as HBase, Cassandra, and DynamoDB; and search engines such as Solr or Elasticsearch to stream data systems such as Kafka, Flume, or Amazon Kinesis.\n",
      "\n",
      "For the purposes of this book, we will assume that the relevant data is available to us, so we will focus on the processing and modeling steps in the following pipeline.\n",
      "\n",
      "## Data cleansing and transformation\n",
      "\n",
      "The majority of machine learning models operate on features, which are typically numerical representations of the input variables that will be used for the model.\n",
      "\n",
      "While we might want to spend the majority of our time exploring machine learning models, data collected via various systems and sources in the preceding ingestion step is, in most cases, in a raw form. For example, we might log user events such as details of when a user views the information page for a movie, when they watch a movie, or when they provide some other feedback. We might also collect external information such as the location of the user (as provided through their IP address, for example). These event logs will typically contain some combination of textual and numeric information about the event (and also, perhaps, other forms of data such as images or audio).\n",
      "\n",
      "In order to use this raw data in our models, in almost all cases, we need to perform preprocessing, which might include:\n",
      "\n",
      "  *  **Filtering data** : Let's assume that we want to create a model from a subset of the raw data, such as only the most recent few months of activity data or only events that match certain criteria.\n",
      "  *  **Dealing with missing, incomplete, or corrupted data** : Many real-world datasets are incomplete in some way. This might include data that is missing (for example, due to a missing user input) or data that is incorrect or flawed (for example, due to an error in data ingestion or storage, technical issues or bugs, or software or hardware failure). We might need to filter out bad data or alternatively decide a method to fill in missing data points (such as using the average value from the dataset for missing points, for example).\n",
      "  *  **Dealing with potential anomalies, errors, and outliers** : Erroneous or outlier data might skew the results of model training, so we might wish to filter these cases out or use techniques that are able to deal with outliers.\n",
      "  *  **Joining together disparate data sources** : For example, we might need to match up the event data for each user with different internal data sources, such as user profiles, as well as external data, such as geolocation, weather, and economic data.\n",
      "  *  **Aggregating data** : Certain models might require input data that is aggregated in some way, such as computing the sum of a number of different event types per user.\n",
      "\n",
      "Once we have performed initial preprocessing on our data, we often need to transform the data into a representation that is suitable for machine learning models. For many model types, this representation will take the form of a vector or matrix structure that contains numerical data. Common challenges during data transformation and feature extraction include:\n",
      "\n",
      "  * Taking categorical data (such as country for geolocation or category for a movie) and encoding it in a numerical representation.\n",
      "  * Extracting useful features from text data.\n",
      "  * Dealing with image or audio data.\n",
      "  * We often convert numerical data into categorical data to reduce the number of values a variable can take on. An example of this is converting a variable for age into buckets (such as 25-35, 45-55, and so on).\n",
      "  * Transforming numerical features; for example, applying a log transformation to a numerical variable can help deal with variables that take on a very large range of values.\n",
      "  * Normalizing and standardizing numerical features ensures that all the different input variables for a model have a consistent scale. Many machine learning models require standardized input to work properly.\n",
      "  * Feature engineering is the process of combining or transforming the existing variables to create new features. For example, we can create a new variable that is the average of some other data, such as the average number of times a user watches a movie.\n",
      "\n",
      "We will cover all of these techniques through the examples in this book.\n",
      "\n",
      "These data-cleansing, exploration, aggregation, and transformation steps can be carried out using both Spark's core API functions as well as the SparkSQL engine, not to mention other external Scala, Java, or Python libraries. We can take advantage of Spark's Hadoop compatibility to read data from and write data to the various different storage systems mentioned earlier.\n",
      "\n",
      "## Model training and testing loop\n",
      "\n",
      "Once we have our training data in a form that is suitable for our model, we can proceed with the model's training and testing phase. During this phase, we are primarily concerned with  **model selection**. This can refer to choosing the best modeling approach for our task, or the best parameter settings for a given model. In fact, the term model selection often refers to both of these processes, as, in many cases, we might wish to try out various models and select the best performing model (with the best performing parameter settings for each model). It is also common to explore the application of combinations of different models (known as **ensemble methods** ) in this phase.\n",
      "\n",
      "This is typically a fairly straightforward process of running our chosen model on our training dataset and testing its performance on a test dataset (that is, a set of data that is held out for the evaluation of the model that the model has not seen in the training phase). This process is referred to as  **cross-validation**.\n",
      "\n",
      "However, due to the large scale of data we are typically working with, it is often useful to carry out this initial train-test loop on a smaller representative sample of our full dataset or perform model selection using parallel methods where possible.\n",
      "\n",
      "For this part of the pipeline, Spark's built-in machine learning library, MLlib, is a perfect fit. We will focus most of our attention in this book on the model training, evaluation, and cross-validation steps for various machine learning techniques, using MLlib and Spark's core features.\n",
      "\n",
      "## Model deployment and integration\n",
      "\n",
      "Once we have found the optimal model based on the train-test loop, we might still face the task of deploying the model to a production system so that it can be used to make actionable predictions.\n",
      "\n",
      "Usually, this process involves exporting the trained model to a central data store from where the production-serving system can obtain the latest version. Thus, the live system _refreshes_ the model periodically as a new model is trained.\n",
      "\n",
      "## Model monitoring and feedback\n",
      "\n",
      "It is critically important to monitor the performance of our machine learning system in production. Once we deploy our optimal trained model, we wish to understand how it is doing in the \"wild\". Is it performing as we expect on new, unseen data? Is its accuracy good enough? The reality is regardless of how much model selection and tuning we try to do in the earlier phases; the only way to measure true performance is to observe what happens in our production system.\n",
      "\n",
      "Also, bear in mind that model accuracy and predictive performance is only one aspect of a real-world system. Usually, we are concerned with other metrics related to business performance (for example, revenue and profitability) or user experience (such as the time spent on our site and how active our users are overall). In most cases, we cannot easily map model-predictive performance to these business metrics. The accuracy of a recommendation or targeting system might be important, but it relates only indirectly to the true metrics we are concerned about, namely whether we are improving user experience, activity, and ultimately, revenue.\n",
      "\n",
      "So, in real-world systems, we should monitor both model-accuracy metrics as well as business metrics. If possible, we should be able to experiment with different models running in production to allow us to optimize against these business metrics by making changes to the models. This is often done using live split tests. However, doing this correctly is not an easy task, and live testing and experimentation is expensive, in the sense that mistakes, poor performance, and using baseline models (they provide a control against which we test out production models) can negatively impact user experience and revenue.\n",
      "\n",
      "Another important aspect of this phase is  **model feedback**. This is the process where the predictions of our model feed through into user behavior; this, in turn, feeds through into our model. In a real-world system, our models are essentially influencing their own future training data by impacting decision-making and potential user behavior.\n",
      "\n",
      "For example, if we have deployed a recommendation system, then, by making recommendations, we might be influencing user behavior because we are only allowing users a limited selection of choices. We hope that this selection is relevant due to our model; however, this feedback loop, in turn, can influence our model's training data. This, in turn, feeds back into real-world performance. It is possible to get into an ever-narrowing feedback loop; ultimately, this can negatively affect both model accuracy and our important business metrics.\n",
      "\n",
      "Fortunately, there are mechanisms by which we can try to limit the potential negative impact of this feedback loop. These include providing some unbiased training data by having a small portion of data coming from users who are not exposed to our models or by being principled in the way we balance exploration, to learn more about our data, and exploitation, to use what we have learned to improve our system's performance.\n",
      "\n",
      "We will briefly cover some aspects of real-time monitoring and model updates in Chapter 10, _Real-time Machine Learning with Spark Streaming_.\n",
      "\n",
      "## Batch versus real time\n",
      "\n",
      "In the previous sections, we outlined the common batch processing approach, where the model is retrained using all data or a subset of all data, periodically. As the preceding pipeline takes some time to complete, it might not be possible to use this approach to update models immediately as new data arrives.\n",
      "\n",
      "While we will be mostly covering batch machine learning approaches in this book, there is a class of machine learning algorithms known as  **online learning** ; they update immediately as new data is fed into the model, thus enabling a real-time system. A common example is an online-optimization algorithm for a linear model, such as stochastic gradient descent. We can learn this algorithm using examples. The advantages of these methods are that the system can react very quickly to new information and also that the system can adapt to changes in the underlying behavior (that is, if the characteristics and distribution of the input data are changing over time, which is almost always the case in real-world situations).\n",
      "\n",
      "However, online-learning models come with their own unique challenges in a production context. For example, it might be difficult to ingest and transform data in real time. It can also be complex to properly perform model selection in a purely online setting. Latency of the online training and the model selection and deployment phases might be too high for true real-time requirements (for example, in online advertising, latency requirements are measured in single-digit milliseconds). Finally, batch-oriented frameworks might make it awkward to handle real-time processes of a streaming nature.\n",
      "\n",
      "Fortunately, Spark's real-time stream processing component, **Spark Streaming** , is a good potential fit for real-time machine learning workflows. We will explore Spark Streaming and online learning in Chapter 10, _Real-time Machine Learning with Spark Streaming_.\n",
      "\n",
      "Due to the complexities inherent in a true real-time machine learning system, in practice, many systems target near real-time operations. This is essentially a hybrid approach where models are not necessarily updated immediately as new data arrives; instead, the new data is collected into mini-batches of a small set of training data. These mini-batches can be fed to an online-learning algorithm. In many cases, this approach is combined with a periodic batch process that might recompute the model on the entire data set and perform more complex processing and model selection. This can help ensure that the real-time model does not degrade over time.\n",
      "\n",
      "Another similar approach involves making approximate updates to a more complex model as new data arrives while recomputing the entire model in a batch process periodically. In this way, the model can learn from new data, with a short delay (usually measured in seconds or, perhaps, a few minutes), but will become more and more inaccurate over time due to the approximation applied. The periodic recomputation takes care of this by retraining the model on all available data.\n",
      "\n",
      "# An architecture for a machine learning system\n",
      "\n",
      "Now that we have explored how our machine learning system might work in the context of MovieStream, we can outline a possible architecture for our system:\n",
      "\n",
      "MovieStream's future architecture\n",
      "\n",
      "As we can see, our system incorporates the machine learning pipeline outlined in the preceding diagram; this system also includes:\n",
      "\n",
      "  * Collecting data about users, their behavior, and our content titles\n",
      "  * Transforming this data into features\n",
      "  * Training our models, including our training-testing and model-selection phases\n",
      "  * Deploying the trained models to both our live model-serving system as well as using these models for offline processes\n",
      "  * Feeding back the model results into the MovieStream website through recommendation and targeting pages\n",
      "  * Feeding back the model results into MovieStream's personalized marketing channels\n",
      "  * Using the offline models to provide tools to MovieStream's various teams to better understand user behavior, characteristics of the content catalogue, and drivers of revenue for the business\n",
      "\n",
      "## Practical exercise\n",
      "\n",
      "Imagine that you now need to provide input to the frontend and infrastructure engineering team about the data that your machine learning system will need. Consider a brief for them on how they should structure the data-collection mechanisms. Write down some examples of what the raw data might look like (for example, web logs, event logs, and so on) and how it should flow through the system. Take into account the following aspects:\n",
      "\n",
      "  * What data sources will be required\n",
      "  * What format should the data be in\n",
      "  * How often should data be collected, processed, potentially aggregated, and stored\n",
      "  * What data storage will you use to ensure scalability\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, you learned about the components inherent in a data-driven, automated machine learning system. We also outlined how a possible high-level architecture for such a system might look in a real-world situation.\n",
      "\n",
      "In the next chapter, we will discuss how to obtain publicly-available datasets for common machine learning tasks. We will also explore general concepts related to processing, cleaning, and transforming data so that they can be used to train a machine learning model.\n",
      "\n",
      "# Chapter 3. Obtaining, Processing, and Preparing Data with Spark\n",
      "\n",
      "Machine learning is an extremely broad field, and these days, applications can be found across areas that include web and mobile applications, Internet of Things and sensor networks, financial services, healthcare, and various scientific fields, to name just a few.\n",
      "\n",
      "Therefore, the range of data available for potential use in machine learning is enormous. In this book, we will focus mostly on business applications. In this context, the data available often consists of data internal to an organization (such as transactional data for a financial services company) as well as external data sources (such as financial asset price data for the same financial services company).\n",
      "\n",
      "For example, recall from Chapter 2, _Designing a Machine Learning System_ , that the main internal source of data for our hypothetical Internet business, MovieStream, consists of data on the movies available on the site, the users of the service, and their behavior. This includes data about movies and other content (for example, title, categories, description, images, actors, and directors), user information (for example, demographics, location, and so on), and user activity data (for example, web page views, title previews and views, ratings, reviews, and social data such as _likes_ , _shares_ , and social network profiles on services including Facebook and Twitter).\n",
      "\n",
      "External data sources in this example might include weather and geolocation services, third-party movie ratings and review sites such as _IMDB_ and _Rotten Tomatoes_ , and so on.\n",
      "\n",
      "Generally speaking, it is quite difficult to obtain data of an internal nature for real-world services and businesses, as it is commercially sensitive (in particular, data on purchasing activity, user or customer behavior, and revenue) and of great potential value to the organization concerned. This is why it is also often the most useful and interesting data on which to apply machine learning--a good machine learning model that can make accurate predictions can be highly valuable (witness the success of machine learning competitions such as the _Netflix Prize_ and _Kaggle_ ).\n",
      "\n",
      "In this book, we will make use of datasets that are publicly available to illustrate concepts around data processing and training of machine learning models.\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Briefly cover the types of data typically used in machine learning.\n",
      "  * Provide examples of where to obtain interesting datasets, often publicly available on the Internet. We will use some of these datasets throughout the book to illustrate the use of the models we introduce.\n",
      "  * Discover how to process, clean, explore, and visualize our data.\n",
      "  * Introduce various techniques to transform our raw data into features that can be used as input to machine learning algorithms.\n",
      "  * Learn how to normalize input features using external libraries as well as Spark's built-in functionality.\n",
      "\n",
      "# Accessing publicly available datasets\n",
      "\n",
      "Fortunately, while commercially-sensitive data can be hard to come by, there are still a number of useful datasets available publicly. Many of these are often used as benchmark datasets for specific types of machine learning problems. Examples of common data sources include:\n",
      "\n",
      "  *  **UCI Machine Learning Repository** : This is a collection of almost 300 datasets of various types and sizes for tasks including classification, regression, clustering, and recommender systems. The list is available at <http://archive.ics.uci.edu/ml/>.\n",
      "  *  **Amazon AWS public datasets** : This is a set of often very large datasets that can be accessed via Amazon S3. These datasets include the Human Genome Project, the Common Crawl web corpus, Wikipedia data, and Google Books Ngrams. Information on these datasets can be found at <http://aws.amazon.com/publicdatasets/>.\n",
      "  *  **Kaggle** : This is a collection of datasets used in machine learning competitions run by Kaggle. Areas include classification, regression, ranking, recommender systems, and image analysis. These datasets can be found under the _Competitions_ section at <http://www.kaggle.com/competitions>.\n",
      "  *  **KDnuggets** : This has a detailed list of public datasets, including some of those mentioned earlier. The list is available at <http://www.kdnuggets.com/datasets/index.html>.\n",
      "\n",
      "### Tip\n",
      "\n",
      "There are many other resources to find public datasets depending on the specific domain and machine learning task. Hopefully, you might also have exposure to some interesting academic or commercial data of your own!\n",
      "\n",
      "To illustrate a few key concepts related to data processing, transformation, and feature extraction in Spark, we will download a commonly-used dataset for movie recommendations; this dataset is known as the **MovieLens** dataset. As it is applicable to recommender systems as well as potentially other machine learning tasks, it serves as a useful example dataset.\n",
      "\n",
      "### Note\n",
      "\n",
      "Spark's machine learning library, MLlib, has been under heavy development since its inception, and unlike the Spark core, it is still not in a fully stable state with regard to its overall API and design.\n",
      "\n",
      "As of Spark Version 1.2.0, a new, experimental API for MLlib has been released under the `ml` package (whereas the current library resides under the `mllib` package). This new API aims to enhance the APIs and interfaces for models as well as feature extraction and transformation so as to make it easier to build pipelines that chain together steps that include feature extraction, normalization, dataset transformations, model training, and cross-validation.\n",
      "\n",
      "In the upcoming chapters, we will only cover the existing, more developed MLlib API, since the new API is still experimental and may be subject to major changes in the next few Spark releases. Over time, the various feature-processing techniques and models that we will cover will simply be ported to the new API; however, the core concepts and most underlying code will remain largely unchanged.\n",
      "\n",
      "## The MovieLens 100k dataset\n",
      "\n",
      "The MovieLens 100k dataset is a set of 100,000 data points related to ratings given by a set of users to a set of movies. It also contains movie metadata and user profiles. While it is a small dataset, you can quickly download it and run Spark code on it. This makes it ideal for illustrative purposes.\n",
      "\n",
      "You can download the dataset from <http://files.grouplens.org/datasets/movielens/ml-100k.zip>.\n",
      "\n",
      "Once you have downloaded the data, unzip it using your terminal:\n",
      "\n",
      "    **> unzip ml-100k.zip**\n",
      "    **inflating: ml-100k/allbut.pl**\n",
      "    **inflating: ml-100k/mku.sh**\n",
      "    **inflating: ml-100k/README**\n",
      "    **...**\n",
      "    **inflating: ml-100k/ub.base**\n",
      "    **inflating: ml-100k/ub.test**\n",
      "\n",
      "This will create a directory called `ml-100k`. Change into this directory and examine the contents. The important files are `u.user` (user profiles), `u.item` (movie metadata), and `u.data` (the ratings given by users to movies):\n",
      "\n",
      "    **> cd ml-100k**\n",
      "\n",
      "The `README` file contains more information on the dataset, including the variables present in each data file. We can use the `head` command to examine the contents of the various files.\n",
      "\n",
      "For example, we can see that the `u.user` file contains the `user id`, `age`, `gender`, `occupation`, and `ZIP code` fields, separated by a pipe (`|` character):\n",
      "\n",
      "    **> head -5 u.user**\n",
      "    **1|24|M|technician|85711**\n",
      "    **2|53|F|other|94043**\n",
      "    **3|23|M|writer|32067**\n",
      "    **4|24|M|technician|43537**\n",
      "    **5|33|F|other|15213**\n",
      "\n",
      "The `u.item` file contains the `movie id`, `title`, `release data`, and `IMDB link` fields and a set of fields related to movie category data. It is also separated by a `|` character:\n",
      "\n",
      "    **> head -5 u.item**\n",
      "    **1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0**\n",
      "    **2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0**\n",
      "    **3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0**\n",
      "    **4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0**\n",
      "    **5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0**\n",
      "\n",
      "Finally, the `u.data` file contains the `user id`, `movie id`, `rating (1-5 scale)`, and `timestamp` fields and is separated by a tab (the `\\t` character):\n",
      "\n",
      "    **> head -5 u.data**\n",
      "    **196\t242\t3\t881250949**\n",
      "    **186\t302\t3\t891717742**\n",
      "    **22\t377\t1\t878887116**\n",
      "    **244\t51\t2\t880606923**\n",
      "    **166\t346\t1\t886397596**\n",
      "\n",
      "# Exploring and visualizing your data\n",
      "\n",
      "Now that we have our data available, let's fire up an interactive Spark console and explore it! For this section, we will use Python and the PySpark shell, as we are going to use the IPython interactive console and the matplotlib plotting library to process and visualize our data.\n",
      "\n",
      "### Note\n",
      "\n",
      "IPython is an advanced, interactive shell for Python. It includes a useful set of features called pylab, which includes NumPy and SciPy for numerical computing and matplotlib for interactive plotting and visualization.\n",
      "\n",
      "We recommend that you use the latest version of IPython (2.3.1 at the time of writing this book). To install IPython for your platform, follow the instructions available at <http://ipython.org/install.html>. If this is the first time you are using IPython, you can find a tutorial at <http://ipython.org/ipython-doc/stable/interactive/tutorial.html>.\n",
      "\n",
      "You will need to install all the packages listed earlier in order to work through the code in this chapter. Instructions to install the packages can be found in the code bundle. If you are starting out with Python or are unfamiliar with the process of installing these packages, we strongly recommend that you use a prebuilt scientific Python installation such as Anaconda (available at <http://continuum.io/downloads>) or Enthought (available at <https://store.enthought.com/downloads/>). These make the installation process much easier and include everything you will need to follow the example code.\n",
      "\n",
      "The PySpark console allows the option of setting which Python executable needs to be used to run the shell. We can choose to use IPython, as opposed to the standard Python shell, when launching our PySpark console. We can also pass in additional options to IPython, including telling it to launch with the pylab functionality enabled.\n",
      "\n",
      "We can do this by running the following command from the Spark home directory (that is, the same directory that we used previously to explore the Spark interactive console):\n",
      "\n",
      "    **> IPYTHON=1 IPYTHON_OPTS=\"--pylab\" ./bin/pyspark**\n",
      "\n",
      "You will see the PySpark console start up, showing output similar to the following screenshot:\n",
      "\n",
      "The PySpark console using IPython\n",
      "\n",
      "### Tip\n",
      "\n",
      "Notice the `IPython 2.3.1 -- An enhanced Interactive Python` and `Using matplotlib backend: MacOSX` lines; they indicate that both the IPython and pylab functionalities are being used by the PySpark shell.\n",
      "\n",
      "You might see a slightly different output, depending on your operating system and software versions.\n",
      "\n",
      "Now that we have our IPython console open, we can start to explore the MovieLens dataset and do some basic analysis.\n",
      "\n",
      "### Note\n",
      "\n",
      "You can follow along with this chapter by entering the code examples into your IPython console. IPython also provides an HTML-enabled Notebook application. It provides some enhanced functionality over the standard IPython console, such as inline graphics for plotting, the HTML markup functionality, as well as the ability to run cells of code independently.\n",
      "\n",
      "The images used in this chapter were generated using the IPython Notebook, so don't worry if yours look a little bit different in style, as long as they contain the same content! You can also use the Notebook for the code in this chapter, if you prefer. In addition to the Python code for this chapter, we have provided a version saved in the IPython Notebook format, which you can load into your own IPython Notebook.\n",
      "\n",
      "Check out the instructions on how to use the IPython Notebook at <http://ipython.org/ipython-doc/stable/interactive/notebook.html>.\n",
      "\n",
      "## Exploring the user dataset\n",
      "\n",
      "First, we will analyze the characteristics of MovieLens users. Enter the following lines into your console (where `PATH` refers to the base directory in which you performed the `unzip` command to unzip the preceding MovieLens 100k dataset):\n",
      "\n",
      "    user_data = sc.textFile(\"/ **PATH** /ml-100k/u.user\")\n",
      "    user_data.first()\n",
      "\n",
      "You should see output similar to this:\n",
      "\n",
      "    **u'1|24|M|technician|85711'**\n",
      "\n",
      "As we can see, this is the first line of our user data file, separated by the `\"|\"` character.\n",
      "\n",
      "### Tip\n",
      "\n",
      "The `first` function is similar to `collect`, but it only returns the first element of the RDD to the driver. We can also use `take(k)` to collect only the first _k_ elements of the RDD to the driver.\n",
      "\n",
      "Let's transform the data by splitting each line, around the `\"|\"` character. This will give us an RDD where each record is a Python list that contains the user ID, age, gender, occupation, and ZIP code fields.\n",
      "\n",
      "We will then count the number of users, genders, occupations, and ZIP codes. We can achieve this by running the following code in the console, line by line. Note that we do not cache the data, as it is unnecessary for this small size:\n",
      "\n",
      "    user_fields = user_data.map(lambda line: line.split(\"|\"))\n",
      "    num_users = user_fields.map(lambda fields: fields[0]).count()\n",
      "    num_genders = user_fields.map(lambda fields:fields[2]).distinct().count()\n",
      "    num_occupations = user_fields.map(lambda fields:fields[3]).distinct().count()\n",
      "    num_zipcodes = user_fields.map(lambda fields:fields[4]).distinct().count()\n",
      "    print \"Users: %d, genders: %d, occupations: %d, ZIP codes: %d\" % (num_users, num_genders, num_occupations, num_zipcodes)\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **Users: 943, genders: 2, occupations: 21, ZIP codes: 795**\n",
      "\n",
      "Next, we will create a histogram to analyze the distribution of user ages, using matplotlib's `hist` function:\n",
      "\n",
      "    ages = user_fields.map(lambda x: int(x[1])).collect()\n",
      "    hist(ages, bins=20, color='lightblue', normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "We passed in the `ages` array, together with the number of `bins` for our histogram (`20` in this case), to the `hist` function. Using the `normed=True` argument, we also specified that we want the histogram to be normalized so that each bucket represents the percentage of the overall data that falls into that bucket.\n",
      "\n",
      "You will see an image containing the histogram chart, which looks something like the one shown here. As we can see, the ages of MovieLens users are somewhat skewed towards younger viewers. A large number of users are between the ages of about 15 and 35.\n",
      "\n",
      "Distribution of user ages\n",
      "\n",
      "We might also want to explore the relative frequencies of the various occupations of our users. We can do this using the following code snippet. First, we will use the MapReduce approach introduced previously to count the occurrences of each occupation in the dataset. Then, we will use `matplotlib` to display a bar chart of occupation counts, using the `bar` function.\n",
      "\n",
      "Since part of our data is the descriptions of textual occupation, we will need to manipulate it a little to get it to work with the `bar` function:\n",
      "\n",
      "    count_by_occupation = user_fields.map(lambda fields: (fields[3], 1)).reduceByKey(lambda x, y: x + y).collect()\n",
      "    x_axis1 = np.array([c[0] for c in count_by_occupation])\n",
      "    y_axis1 = np.array([c[1] for c in count_by_occupation])\n",
      "\n",
      "Once we have collected the `RDD` of counts per occupation, we will convert it into two arrays for the _x_ axis (the occupations) and the _y_ axis (the counts) of our chart. The `collect` function returns the count data to us in no particular order. We need to sort the count data so that our bar chart is ordered from the lowest to the highest count.\n",
      "\n",
      "We will achieve this by first creating two `numpy` arrays and then using the `argsort` method of `numpy` to select the elements from each array, ordered by the count data in an ascending fashion. Notice that here, we will sort both the _x_ and _y_ axis arrays by the _y_ axis (that is, by the counts):\n",
      "\n",
      "    x_axis = x_axis1[np.argsort(y_axis1)]\n",
      "    y_axis = y_axis1[np.argsort(y_axis1)]\n",
      "\n",
      "Once we have the _x_ and _y_ axis data for our chart, we will create the bar chart with the occupations as labels on the _x_ axis and the counts as the values on the _y_ axis. We will also add a few lines, such as the `plt.xticks(rotation=30)` code, to display a better-looking chart:\n",
      "\n",
      "    pos = np.arange(len(x_axis))\n",
      "    width = 1.0\n",
      "\n",
      "    ax = plt.axes()\n",
      "    ax.set_xticks(pos + (width / 2))\n",
      "    ax.set_xticklabels(x_axis)\n",
      "\n",
      "    plt.bar(pos, y_axis, width, color='lightblue')\n",
      "    plt.xticks(rotation=30)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "The image you have generated should look like the one here. It appears that the most prevalent occupations are **student** , **other** , **educator** , **administrator** , **engineer** , and **programmer**.\n",
      "\n",
      "Distribution of user occupations\n",
      "\n",
      "Spark provides a convenience method on RDDs called `countByValue`; this method counts the occurrences of each unique value in the RDD and returns it to the driver as a Python `dict` method (or a Scala or Java `Map` method). We can create the `count_by_occupation` variable using this method:\n",
      "\n",
      "    count_by_occupation2 = user_fields.map(lambda fields: fields[3]).countByValue()\n",
      "    print \"Map-reduce approach:\"\n",
      "    print dict(count_by_occupation2)\n",
      "    print \"\"\n",
      "    print \"countByValue approach:\"\n",
      "    print dict(count_by_occupation)\n",
      "\n",
      "You should see that the results are the same for each approach.\n",
      "\n",
      "## Exploring the movie dataset\n",
      "\n",
      "Next, we will investigate a few properties of the movie catalogue. We can inspect a row of the movie data file, as we did for the user data earlier, and then count the number of movies:\n",
      "\n",
      "    movie_data = sc.textFile(\"/ **PATH** /ml-100k/u.item\")\n",
      "    print movie_data.first()\n",
      "    num_movies = movie_data.count()\n",
      "    print \"Movies: %d\" % num_movies\n",
      "\n",
      "You will see the following output on your console:\n",
      "\n",
      "    **1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0**\n",
      "    **Movies: 1682**\n",
      "\n",
      "In the same manner as we did for user ages and occupations earlier, we can plot the distribution of movie age, that is, the year of release relative to the current date (note that for this dataset, the current year is 1998).\n",
      "\n",
      "In the following code block, we can see that we need a small function called `convert_year` to handle errors in the parsing of the `release date` field. This is due to some bad data in one line of the movie data:\n",
      "\n",
      "    def convert_year(x):\n",
      "      try:\n",
      "        return int(x[-4:])\n",
      "      except:\n",
      "        return 1900 # there is a 'bad' data point with a blank year,\n",
      "        which we set to 1900 and will filter out later\n",
      "\n",
      "Once we have our utility function to parse the year of release, we can apply it to the movie data using a `map` transformation and collect the results:\n",
      "\n",
      "    movie_fields = movie_data.map(lambda lines: lines.split(\"|\"))\n",
      "    years = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x))\n",
      "\n",
      "Since we have assigned the value `1900` to any error in parsing, we can filter these bad values out of the resulting data using Spark's `filter` transformation:\n",
      "\n",
      "    years_filtered = years.filter(lambda x: x != 1900)\n",
      "\n",
      "This is a good example of how real-world datasets can often be messy and require a more in-depth approach to parsing data. In fact, this also illustrates why data exploration is so important, as many of these issues in data integrity and quality are picked up during this phase.\n",
      "\n",
      "After filtering out bad data, we will transform the list of movie release years into movie ages by subtracting the current year, use `countByValue` to compute the counts for each movie age, and finally, plot our histogram of movie ages (again, using the `hist` function, where the `values` variable are the values of the result from `countByValue`, and the `bins` variable are the keys):\n",
      "\n",
      "    movie_ages = years_filtered.map(lambda yr: 1998-yr).countByValue()\n",
      "    values = movie_ages.values()\n",
      "    bins = movie_ages.keys()\n",
      "    hist(values, bins=bins, color='lightblue', normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16,10)\n",
      "\n",
      "You will see an image similar to the one here; it illustrates that most of the movies were released in the last few years before 1998:\n",
      "\n",
      "Distribution of movie ages\n",
      "\n",
      "## Exploring the rating dataset\n",
      "\n",
      "Let's now take a look at the ratings data:\n",
      "\n",
      "    rating_data = sc.textFile(\"/ **PATH** /ml-100k/u.data\")\n",
      "    print rating_data.first()\n",
      "    num_ratings = rating_data.count()\n",
      "    print \"Ratings: %d\" % num_ratings\n",
      "\n",
      "This gives us the following result:\n",
      "\n",
      "    **196\t242\t3\t881250949**\n",
      "    **Ratings: 100000**\n",
      "\n",
      "There are 100,000 ratings, and unlike the user and movie datasets, these records are split with a tab character (`\"\\t\"`). As you might have guessed, we'd probably want to compute some basic summary statistics and frequency histograms for the rating values. Let's do this now:\n",
      "\n",
      "    rating_data = rating_data_raw.map(lambda line: line.split(\"\\t\"))\n",
      "    ratings = rating_data.map(lambda fields: int(fields[2]))\n",
      "    max_rating = ratings.reduce(lambda x, y: max(x, y))\n",
      "    min_rating = ratings.reduce(lambda x, y: min(x, y))\n",
      "    mean_rating = ratings.reduce(lambda x, y: x + y) / num_ratings\n",
      "    median_rating = np.median(ratings.collect())\n",
      "    ratings_per_user = num_ratings / num_users\n",
      "    ratings_per_movie = num_ratings / num_movies\n",
      "    print \"Min rating: %d\" % min_rating\n",
      "    print \"Max rating: %d\" % max_rating\n",
      "    print \"Average rating: %2.2f\" % mean_rating\n",
      "    print \"Median rating: %d\" % median_rating\n",
      "    print \"Average # of ratings per user: %2.2f\" % ratings_per_user\n",
      "    print \"Average # of ratings per movie: %2.2f\" % ratings_per_movie\n",
      "\n",
      "After running these lines on your console, you will see output similar to the following result:\n",
      "\n",
      "    **Min rating: 1**\n",
      "    **Max rating: 5**\n",
      "    **Average rating: 3.53**\n",
      "    **Median rating: 4**\n",
      "    **Average # of ratings per user: 106.00**\n",
      "    **Average # of ratings per movie: 59.00**\n",
      "\n",
      "We can see that the minimum rating is 1, while the maximum rating is 5. This is in line with what we expect, since the ratings are on a scale of 1 to 5.\n",
      "\n",
      "Spark also provides a `stats` function for RDDs; this function contains a numeric variable (such as `ratings` in this case) to compute similar summary statistics:\n",
      "\n",
      "    ratings.stats()\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **(count: 100000, mean: 3.52986, stdev: 1.12566797076, max: 5.0, min: 1.0)**\n",
      "\n",
      "Looking at the results, the average rating given by a user to a movie is around 3.5 and the median rating is 4, so we might expect that the distribution of ratings will be skewed towards slightly higher ratings. Let's see whether this is true by creating a bar chart of rating values using a similar procedure as we did for occupations:\n",
      "\n",
      "    count_by_rating = ratings.countByValue()\n",
      "    x_axis = np.array(count_by_rating.keys())\n",
      "    y_axis = np.array([float(c) for c in count_by_rating.values()])\n",
      "    # we normalize the y-axis here to percentages\n",
      "    y_axis_normed = y_axis / y_axis.sum()\n",
      "    pos = np.arange(len(x_axis))\n",
      "    width = 1.0\n",
      "\n",
      "    ax = plt.axes()\n",
      "    ax.set_xticks(pos + (width / 2))\n",
      "    ax.set_xticklabels(x_axis)\n",
      "\n",
      "    plt.bar(pos, y_axis_normed, width, color='lightblue')\n",
      "    plt.xticks(rotation=30)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "The preceding code should produce the following chart:\n",
      "\n",
      "Distribution of rating values\n",
      "\n",
      "In line with what we might have expected after seeing some summary statistics, it is clear that the distribution of ratings is skewed towards average to high ratings.\n",
      "\n",
      "We can also look at the distribution of the number of ratings made by each user. Recall that we previously computed the `rating_data` RDD used in the preceding code by splitting the ratings with the tab character. We will now use the `rating_data` variable again in the following code.\n",
      "\n",
      "To compute the distribution of ratings per user, we will first extract the user ID as key and rating as value from `rating_data` RDD. We will then group the ratings by user ID using Spark's `groupByKey` function:\n",
      "\n",
      "    user_ratings_grouped = rating_data.map(lambda fields: (int(fields[0]), int(fields[2]))).\\\n",
      "        groupByKey()\n",
      "\n",
      "Next, for each key (user ID), we will find the size of the set of ratings; this will give us the number of ratings for that user:\n",
      "\n",
      "    user_ratings_byuser = user_ratings_grouped.map(lambda (k, v): (k, len(v)))\n",
      "    user_ratings_byuser.take(5)\n",
      "\n",
      "We can inspect the resulting RDD by taking a few records from it; this should give us an RDD of the (user ID, number of ratings) pairs:\n",
      "\n",
      "    **[(1, 272), (2, 62), (3, 54), (4, 24), (5, 175)]**\n",
      "\n",
      "Finally, we will plot the histogram of number of ratings per user using our favorite `hist` function:\n",
      "\n",
      "    user_ratings_byuser_local = user_ratings_byuser.map(lambda (k, v):v).collect()\n",
      "    hist(user_ratings_byuser_local, bins=200, color='lightblue',normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16,10)\n",
      "\n",
      "Your chart should look similar to the following screenshot. We can see that most of the users give fewer than 100 ratings. The distribution of the ratings shows, however, that there are fairly large number of users that provide hundreds of ratings.\n",
      "\n",
      "Distribution of ratings per user\n",
      "\n",
      "We leave it to you to perform a similar analysis to create a histogram plot for the number of ratings given to each movie. Perhaps, if you're feeling adventurous, you could also extract a dataset of movie ratings by date (taken from the timestamps in the last column of the rating dataset) and chart a time series of the total number of ratings, number of unique users who gave a rating, and the number of unique movies rated, for each day.\n",
      "\n",
      "# Processing and transforming your data\n",
      "\n",
      "Now that we have done some initial exploratory analysis of our dataset and we know a little more about the characteristics of our users and movies, what do we do next?\n",
      "\n",
      "In order to make the raw data usable in a machine learning algorithm, we first need to clean it up and possibly transform it in various ways before extracting useful features from the transformed data. The transformation and feature extraction steps are closely linked, and in some cases, certain transformations are themselves a case of feature extraction.\n",
      "\n",
      "We have already seen an example of the need to clean data in the movie dataset. Generally, real-world datasets contain bad data, missing data points, and outliers. Ideally, we would correct bad data; however, this is often not possible, as many datasets derive from some form of collection process that cannot be repeated (this is the case, for example, in web activity data and sensor data). Missing values and outliers are also common and can be dealt with in a manner similar to bad data. Overall, the broad options are as follows:\n",
      "\n",
      "  *  **Filter out or remove records with bad or missing values** : This is sometimes unavoidable; however, this means losing the good part of a bad or missing record.\n",
      "  *  **Fill in bad or missing data** : We can try to assign a value to bad or missing data based on the rest of the data we have available. Approaches can include assigning a zero value, assigning the global mean or median, interpolating nearby or similar data points (usually, in a time-series dataset), and so on. Deciding on the correct approach is often a tricky task and depends on the data, situation, and one's own experience.\n",
      "  *  **Apply robust techniques to outliers** : The main issue with outliers is that they might be correct values, even though they are extreme. They might also be errors. It is often very difficult to know which case you are dealing with. Outliers can also be removed or filled in, although fortunately, there are statistical techniques (such as robust regression) to handle outliers and extreme values.\n",
      "  *  **Apply transformations to potential outliers** : Another approach for outliers or extreme values is to apply transformations, such as a logarithmic or Gaussian kernel transformation, to features that have potential outliers, or display large ranges of potential values. These types of transformations have the effect of dampening the impact of large changes in the scale of a variable and turning a nonlinear relationship into one that is linear.\n",
      "\n",
      "## Filling in bad or missing data\n",
      "\n",
      "We have already seen an example of filtering out bad data. Following on from the preceding code, the following code snippet applies the fill-in approach to the bad release date record by assigning a value to the data point that is equal to the median year of release:\n",
      "\n",
      "    years_pre_processed = movie_fields.map(lambda fields: fields[2]).map(lambda x: convert_year(x)).collect()\n",
      "    years_pre_processed_array = np.array(years_pre_processed)\n",
      "\n",
      "First, we will compute the mean and median year of release after selecting all the year of release data, _except_ the bad data point. We will then use the `numpy` function, `where`, to find the index of the bad value in `years_pre_processed_array` (recall that we assigned the value `1900` to this data point). Finally, we will use this index to assign the median release year to the bad value:\n",
      "\n",
      "    mean_year = np.mean(years_pre_processed_array[years_pre_processed_array!=1900])\n",
      "    median_year = np.median(years_pre_processed_array[years_pre_processed_array!=1900])\n",
      "    index_bad_data = np.where(years_pre_processed_array==1900)[0][0]\n",
      "    years_pre_processed_array[index_bad_data] = median_year\n",
      "    print \"Mean year of release: %d\" % mean_year\n",
      "    print \"Median year of release: %d\" % median_year\n",
      "    print \"Index of '1900' after assigning median: %s\" % np.where(years_pre_processed_array == 1900)[0]\n",
      "\n",
      "You should expect to see the following output:\n",
      "\n",
      "    **Mean year of release: 1989**\n",
      "    **Median year of release: 1995**\n",
      "    **Index of '1900' after assigning median: []**\n",
      "\n",
      "We computed both the mean and the median year of release here. As can be seen from the output, the median release year is quite higher because of the skewed distribution of the years. While it is not always straightforward to decide on precisely which fill-in value to use for a given situation, in this case, it is certainly feasible to use the median due to this skew.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that the preceding code example is, strictly speaking, not very scalable, as it requires collecting all the data to the driver. We can use Spark's `mean` function for numeric RDDs to compute the mean, but there is no median function available currently. We can solve this by creating our own or by computing the median on a sample of the dataset created using the `sample` function (we will see more of this in the upcoming chapters).\n",
      "\n",
      "# Extracting useful features from your data\n",
      "\n",
      "Once we have completed the initial exploration, processing, and cleaning of our data, we are ready to get down to the business of extracting actual features from the data, with which our machine learning model can be trained.\n",
      "\n",
      " **Features** refer to the variables that we use to train our model. Each row of data contains various information that we would like to extract into a training example. Almost all machine learning models ultimately work on numerical representations in the form of a **vector** ; hence, we need to convert raw data into numbers.\n",
      "\n",
      "Features broadly fall into a few categories, which are as follows:\n",
      "\n",
      "  *  **Numerical features** : These features are typically real or integer numbers, for example, the user age that we used in an example earlier.\n",
      "  *  **Categorical features** : These features refer to variables that can take one of a set of possible states at any given time. Examples from our dataset might include a user's gender or occupation or movie categories.\n",
      "  *  **Text features** : These are features derived from the text content in the data, for example, movie titles, descriptions, or reviews.\n",
      "  *  **Other features** : Most other types of features are ultimately represented numerically. For example, images, video, and audio can be represented as sets of numerical data. Geographical locations can be represented as latitude and longitude or geohash data.\n",
      "\n",
      "Here we will cover numerical, categorical, and text features.\n",
      "\n",
      "## Numerical features\n",
      "\n",
      "What is the difference between any old number and a numerical feature? Well, in reality, any numerical data can be used as an input variable. However, in a machine learning model, we learn about a vector of weights for each feature. The weights play a role in mapping feature values to an outcome or target variable (in the case of supervised learning models).\n",
      "\n",
      "Thus, we want to use features that make sense, that is, where the model can learn the relationship between feature values and the target variable. For example, age might be a reasonable feature. Perhaps there is a direct relationship between increasing age and a certain outcome. Similarly, height is a good example of a numerical feature that can be used directly.\n",
      "\n",
      "We will often see that numerical features are less useful in their raw form, but can be turned into representations that are more useful. Location is an example of such a case. Using raw locations (say, latitude and longitude) might not be that useful unless our data is very dense indeed, since our model might not be able to learn about a useful relationship between the raw location and an outcome. However, a relationship might exist between some aggregated or binned representation of the location (for example, a city or country) and the outcome.\n",
      "\n",
      "## Categorical features\n",
      "\n",
      "Categorical features cannot be used as input in their raw form, as they are not numbers; instead, they are members of a set of possible values that the variable can take. In the example mentioned earlier, user occupation is a categorical variable that can take the value of student, programmer, and so on.\n",
      "\n",
      "Such categorical variables are also known as **nominal** variables where there is no concept of order between the values of the variable. By contrast, when there is a concept of order between variables (such as the ratings mentioned earlier, where a rating of 5 is conceptually higher or better than a rating of 1), we refer to **ordinal** variables.\n",
      "\n",
      "To transform categorical variables into a numerical representation, we can use a common approach known as **1-of-k** encoding. An approach such as 1-of-k encoding is required to represent nominal variables in a way that makes sense for machine learning tasks. Ordinal variables might be used in their raw form but are often encoded in the same way as nominal variables.\n",
      "\n",
      "Assume that there are k possible values that the variable can take. If we assign each possible value an index from the set of 1 to k, then we can represent a given state of the variable using a binary vector of length k; here, all entries are zero, except the entry at the index that corresponds to the given state of the variable. This entry is set to one.\n",
      "\n",
      "For example, we can collect all the possible states of the `occupation` variable:\n",
      "\n",
      "    all_occupations = user_fields.map(lambda fields: fields[3]).distinct().collect()\n",
      "    all_occupations.sort()\n",
      "\n",
      "We can then assign index values to each possible occupation in turn (note that we start from zero, since Python, Scala, and Java arrays all use zero-based indices):\n",
      "\n",
      "    idx = 0\n",
      "    all_occupations_dict = {}\n",
      "    for o in all_occupations:\n",
      "        all_occupations_dict[o] = idx\n",
      "        idx +=1\n",
      "    # try a few examples to see what \"1-of-k\" encoding is assigned\n",
      "    print \"Encoding of 'doctor': %d\" % all_occupations_dict['doctor']\n",
      "    print \"Encoding of 'programmer': %d\" % all_occupations_dict['programmer']\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **Encoding of 'doctor': 2**\n",
      "    **Encoding of 'programmer': 14**\n",
      "\n",
      "Finally, we can encode the value of `programmer`. We will start by creating a `numpy` array of a length that is equal to the number of possible occupations (k in this case) and filling it with zeros. We will use the `zeros` function of `numpy` to create this array.\n",
      "\n",
      "We will then extract the index of the word `programmer` and assign a value of `1` to the array value at this index:\n",
      "\n",
      "    K = len(all_occupations_dict)\n",
      "    binary_x = np.zeros(K)\n",
      "    k_programmer = all_occupations_dict['programmer']\n",
      "    binary_x[k_programmer] = 1\n",
      "    print \"Binary feature vector: %s\" % binary_x\n",
      "    print \"Length of binary vector: %d\" % K\n",
      "\n",
      "This will give us the resulting binary feature vector of length `21`:\n",
      "\n",
      "    **Binary feature vector: [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0. 0.  0.  0.]**\n",
      "    **Length of binary vector: 21**\n",
      "\n",
      "## Derived features\n",
      "\n",
      "As we mentioned earlier, it is often useful to compute a derived feature from one or more available variables. We hope that the derived feature can add more information than only using the variable in its raw form.\n",
      "\n",
      "For instance, we can compute the average rating given by each user to all the movies they rated. This would be a feature that could provide a _user-specific_ intercept in our model (in fact, this is a commonly used approach in recommendation models). We have taken the raw rating data and created a new feature that can allow us to learn a better model.\n",
      "\n",
      "Examples of features derived from raw data include computing average values, median values, variances, sums, differences, maximums or minimums, and counts. We have already seen a case of this when we created a new `movie age` feature from the year of release of the movie and the current year. Often, the idea behind using these transformations is to summarize the numerical data in some way that might make it easier for a model to learn.\n",
      "\n",
      "It is also common to transform numerical features into categorical features, for example, by binning features. Common examples of this include variables such as age, geolocation, and time.\n",
      "\n",
      "### Transforming timestamps into categorical features\n",
      "\n",
      "To illustrate how to derive categorical features from numerical data, we will use the times of the ratings given by users to movies. These are in the form of Unix timestamps. We can use Python's `datetime` module to extract the date and time from the timestamp and, in turn, extract the `hour` of the day. This will result in an RDD of the hour of the day for each rating.\n",
      "\n",
      "We will need a function to extract a `datetime` representation of the rating timestamp (in seconds); we will create this function now:\n",
      "\n",
      "    def extract_datetime(ts):\n",
      "        import datetime\n",
      "        return datetime.datetime.fromtimestamp(ts)\n",
      "\n",
      "We will again use the `rating_data` RDD that we computed in the earlier examples as our starting point.\n",
      "\n",
      "First, we will use a `map` transformation to extract the timestamp field, converting it to a Python `int` datatype. We will then apply our `extract_datetime` function to each timestamp and extract the hour from the resulting `datetime` object:\n",
      "\n",
      "    timestamps = rating_data.map(lambda fields: int(fields[3]))\n",
      "    hour_of_day = timestamps.map(lambda ts: extract_datetime(ts). **hour** )\n",
      "    hour_of_day.take(5)\n",
      "\n",
      "If we take the first five records of the resulting RDD, we will see the following output:\n",
      "\n",
      "    **[17, 21, 9, 7, 7]**\n",
      "\n",
      "We have transformed the raw time data into a categorical feature that represents the hour of the day in which the rating was given.\n",
      "\n",
      "Now, say that we decide this is too coarse a representation. Perhaps we want to further refine the transformation. We can assign each hour-of-the-day value into a defined bucket that represents a time of day.\n",
      "\n",
      "For example, we can say that morning is from 7 a.m. to 11 a.m., while lunch is from 11 a.m. to 1 a.m., and so on. Using these buckets, we can create a function to assign a time of day, given the hour of the day as input:\n",
      "\n",
      "    def assign_tod(hr):\n",
      "      times_of_day = {\n",
      "        'morning' : range(7, 12),\n",
      "        'lunch' : range(12, 14),\n",
      "        'afternoon' : range(14, 18),\n",
      "        'evening' : range(18, 23),\n",
      "        'night' : range(23, 7)\n",
      "      }\n",
      "      for k, v in times_of_day.iteritems():\n",
      "        if hr in v: \n",
      "          return k\n",
      "\n",
      "Now, we will apply the `assign_tod` function to the hour of each rating event contained in the `hour_of_day` RDD:\n",
      "\n",
      "    time_of_day = hour_of_day.map(lambda hr: assign_tod(hr))\n",
      "    time_of_day.take(5)\n",
      "\n",
      "If we again take the first five records of this new RDD, we will see the following transformed values:\n",
      "\n",
      "    **['afternoon', 'evening', 'morning', 'morning', 'morning']**\n",
      "\n",
      "We have now transformed the timestamp variable (which can take on thousands of values and is probably not useful to a model in its raw form) into hours (taking on 24 values) and then into a time of day (taking on five possible values). Now that we have a categorical feature, we can use the same 1-of-k encoding method outlined earlier to generate a binary feature vector.\n",
      "\n",
      "## Text features\n",
      "\n",
      "In some ways, text features are a form of categorical and derived features. Let's take the example of the description for a movie (which we do not have in our dataset). Here, the raw text could not be used directly, even as a categorical feature, since there are virtually unlimited possible combinations of words that could occur if each piece of text was a possible value. Our model would almost never see two occurrences of the same feature and would not be able to learn effectively. Therefore, we would like to turn raw text into a form that is more amenable to machine learning.\n",
      "\n",
      "There are numerous ways of dealing with text, and the field of natural language processing is dedicated to processing, representing, and modeling textual content. A full treatment is beyond the scope of this book, but we will introduce a simple and standard approach for text-feature extraction; this approach is known as the **bag-of-words** representation.\n",
      "\n",
      "The bag-of-words approach treats a piece of text content as a set of the words, and possibly numbers, in the text (these are often referred to as terms). The process of the bag-of-words approach is as follows:\n",
      "\n",
      "  *  **Tokenization** : First, some form of tokenization is applied to the text to split it into a set of tokens (generally words, numbers, and so on). An example of this is simple whitespace tokenization, which splits the text on each space and might remove punctuation and other characters that are not alphabetical or numerical.\n",
      "  *  **Stop word removal** : Next, it is usual to remove very common words such as \"the\", \"and\", and \"but\" (these are known as **stop words** ).\n",
      "  *  **Stemming** : The next step can include stemming, which refers to taking a term and reducing it to its base form or stem. A common example is plural terms becoming singular (for example, dogs becomes dog and so on). There are many approaches to stemming, and text-processing libraries often contain various stemming algorithms.\n",
      "  *  **Vectorization** : The final step is turning the processed terms into a vector representation. The simplest form is, perhaps, a binary vector representation, where we assign a value of one if a term exists in the text and zero if it does not. This is essentially identical to the categorical 1-of-k encoding we encountered earlier. Like 1-of-k encoding, this requires a dictionary of terms mapping a given term to an index number. As you might gather, there are potentially millions of individual possible terms (even after stop word removal and stemming). Hence, it becomes critical to use a sparse vector representation where only the fact that a term is present is stored, to save memory and disk space as well as compute time.\n",
      "\n",
      "### Note\n",
      "\n",
      "In Chapter 9, _Advanced Text Processing with Spark_ , we will cover more complex text processing and feature extraction, including methods to weight terms; these methods go beyond the basic binary encoding we saw earlier.\n",
      "\n",
      "### Simple text feature extraction\n",
      "\n",
      "To show an example of extracting textual features in the binary vector representation, we can use the movie titles that we have available.\n",
      "\n",
      "First, we will create a function to strip away the year of release for each movie, if the year is present, leaving only the title of the movie.\n",
      "\n",
      "We will use Python's regular expression module, `re`, to search for the year between parentheses in the movie titles. If we find a match with this regular expression, we will extract only the title up to the index of the first match (that is, the index in the title string of the opening parenthesis). This is done with the following `raw[:grps.start()]` code snippet:\n",
      "\n",
      "    def extract_title(raw):\n",
      "      import re\n",
      "      # this regular expression finds the non-word (numbers) betweenparentheses\n",
      "      grps = re.search(\"\\((\\w+)\\)\", raw)\n",
      "      if grps:\n",
      "        # we take only the title part, and strip the trailing whitespace from the remaining text, below\n",
      "        return raw[:grps.start()].strip() \n",
      "      else:\n",
      "        return raw\n",
      "\n",
      "Next, we will extract the raw movie titles from the `movie_fields` RDD:\n",
      "\n",
      "    raw_titles = movie_fields.map(lambda fields: fields[1])\n",
      "\n",
      "We can test out our `extract_title` function on the first five raw titles as follows:\n",
      "\n",
      "    for raw_title in raw_titles.take(5):\n",
      "      print extract_title(raw_title)\n",
      "\n",
      "We can verify that our function works by inspecting the results, which should look like this:\n",
      "\n",
      "    **Toy Story**\n",
      "    **GoldenEye**\n",
      "    **Four Rooms**\n",
      "    **Get Shorty**\n",
      "    **Copycat**\n",
      "\n",
      "We would then like to apply our function to the raw titles and apply a tokenization scheme to the extracted titles to convert them to terms. We will use the simple whitespace tokenization we covered earlier:\n",
      "\n",
      "    movie_titles = raw_titles.map(lambda m: extract_title(m))\n",
      "    # next we tokenize the titles into terms. We'll use simple whitespace tokenization\n",
      "    title_terms = movie_titles.map(lambda t: t.split(\" \"))\n",
      "    print title_terms.take(5)\n",
      "\n",
      "Applying this simple tokenization gives the following result:\n",
      "\n",
      "    **[[u'Toy', u'Story'], [u'GoldenEye'], [u'Four', u'Rooms'], [u'Get', u'Shorty'], [u'Copycat']]**\n",
      "\n",
      "We can see that we have split each title on spaces so that each word becomes a token.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Here, we do not cover details such as converting text to lowercase, removing non-word or non-numerical characters such as punctuation and special characters, removing stop words, and stemming. These steps will be important in a real-world application. We will cover many of these topics in Chapter 9, _Advanced Text Processing with Spark_.\n",
      "\n",
      "This additional processing can be done fairly simply using string functions, regular expressions, and the Spark API (apart from stemming). Perhaps you would like to give it a try!\n",
      "\n",
      "In order to assign each term to an index in our vector, we need to create the term dictionary, which maps each term to an integer index.\n",
      "\n",
      "First, we will use Spark's `flatMap` function (highlighted in the following code snippet) to expand the list of strings in each record of the `title_terms` RDD into a new RDD of strings where each record is a term called `all_terms`.\n",
      "\n",
      "We can then collect all the unique terms and assign indexes in exactly the same way that we did for the 1-of-k encoding of user occupations earlier:\n",
      "\n",
      "    # next we would like to collect all the possible terms, in order to build out dictionary of term <-> index mappings\n",
      "    all_terms = title_terms. **flatMap** (lambda x: x).distinct().collect()\n",
      "    # create a new dictionary to hold the terms, and assign the \"1-of-k\" indexes\n",
      "    idx = 0\n",
      "    all_terms_dict = {}\n",
      "    for term in all_terms:\n",
      "      all_terms_dict[term] = idx\n",
      "      idx +=1\n",
      "\n",
      "We can print out the total number of unique terms and test out our term mapping on a few terms:\n",
      "\n",
      "    print \"Total number of terms: %d\" % len(all_terms_dict)\n",
      "    print \"Index of term 'Dead': %d\" % all_terms_dict['Dead']\n",
      "    print \"Index of term 'Rooms': %d\" % all_terms_dict['Rooms']\n",
      "\n",
      "This will result in the following output:\n",
      "\n",
      "    **Total number of terms: 2645**\n",
      "    **Index of term 'Dead': 147**\n",
      "    **Index of term 'Rooms': 1963**\n",
      "\n",
      "We can also achieve the same result more efficiently using Spark's `zipWithIndex` function. This function takes an RDD of values and merges them together with an index to create a new RDD of key-value pairs, where the key will be the term and the value will be the index in the term dictionary. We will use `collectAsMap` to collect the key-value RDD to the driver as a Python `dict` method:\n",
      "\n",
      "    all_terms_dict2 = title_terms.flatMap(lambda x: x).distinct().zipWithIndex().collectAsMap()\n",
      "    print \"Index of term 'Dead': %d\" % all_terms_dict2['Dead']\n",
      "    print \"Index of term 'Rooms': %d\" % all_terms_dict2['Rooms']\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **Index of term 'Dead': 147**\n",
      "    **Index of term 'Rooms': 1963**\n",
      "\n",
      "The final step is to create a function that converts a set of terms into a sparse vector representation. To do this, we will create an empty sparse matrix with one row and a number of columns equal to the total number of terms in our dictionary. We will then step through each term in the input list of terms and check whether this term is in our term dictionary. If it is, we assign a value of 1 to the vector at the index that corresponds to the term in our dictionary mapping:\n",
      "\n",
      "    # this function takes a list of terms and encodes it as a scipy sparse vector using an approach \n",
      "    # similar to the 1-of-k encoding\n",
      "    def create_vector(terms, term_dict):\n",
      "      from scipy import sparse as sp\n",
      "        num_terms = len(term_dict)\n",
      "        x = sp.csc_matrix((1, num_terms))  \n",
      "        for t in terms:\n",
      "          if t in term_dict:\n",
      "            idx = term_dict[t]\n",
      "            x[0, idx] = 1\n",
      "      return x\n",
      "\n",
      "Once we have our function, we will apply it to each record in our RDD of extracted terms:\n",
      "\n",
      "    all_terms_bcast = sc. **broadcast** (all_terms_dict)\n",
      "    term_vectors = title_terms.map(lambda terms: create_vector(terms, all_terms_bcast.value))\n",
      "    term_vectors.take(5)\n",
      "\n",
      "We can then inspect the first few records of our new RDD of sparse vectors:\n",
      "\n",
      "    **[ <1x2645 sparse matrix of type '<type 'numpy.float64'>'**\n",
      "    **with 2 stored elements in Compressed Sparse Column format >,**\n",
      "    ** <1x2645 sparse matrix of type '<type 'numpy.float64'>'**\n",
      "    **with 1 stored elements in Compressed Sparse Column format >,**\n",
      "    ** <1x2645 sparse matrix of type '<type 'numpy.float64'>'**\n",
      "    **with 2 stored elements in Compressed Sparse Column format >,**\n",
      "    ** <1x2645 sparse matrix of type '<type 'numpy.float64'>'**\n",
      "    **with 2 stored elements in Compressed Sparse Column format >,**\n",
      "    ** <1x2645 sparse matrix of type '<type 'numpy.float64'>'**\n",
      "    **with 1 stored elements in Compressed Sparse Column format >]**\n",
      "\n",
      "We can see that each movie title has now been transformed into a sparse vector. We can see that the titles where we extracted two terms have two non-zero entries in the vector, titles where we extracted only one term have one non-zero entry, and so on.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note the use of Spark's `broadcast` method in the preceding example code to create a broadcast variable that contains the term dictionary. In real-world applications, such term dictionaries can be extremely large, so using a broadcast variable is advisable.\n",
      "\n",
      "## Normalizing features\n",
      "\n",
      "Once the features have been extracted into the form of a vector, a common preprocessing step is to normalize the numerical data. The idea behind this is to transform each numerical feature in a way that scales it to a standard size. We can perform different kinds of normalization, which are as follows:\n",
      "\n",
      "  *  **Normalize a feature** : This is usually a transformation applied to an individual feature across the dataset, for example, subtracting the mean ( _centering_ the feature) or applying the standard normal transformation (such that the feature has a mean of zero and a standard deviation of 1).\n",
      "  *  **Normalize a feature vector** : This is usually a transformation applied to all features in a given row of the dataset such that the resulting feature vector has a normalized length. That is, we will ensure that each feature in the vector is scaled such that the vector has a norm of 1 (typically, on an L1 or L2 norm).\n",
      "\n",
      "We will use the second case as an example. We can use the `norm` function of `numpy` to achieve the vector normalization by first computing the L2 norm of a random vector and then dividing each element in the vector by this norm to create our normalized vector:\n",
      "\n",
      "    np.random.seed(42)\n",
      "    x = np.random.randn(10)\n",
      "    norm_x_2 = np.linalg.norm(x)\n",
      "    normalized_x = x / norm_x_2\n",
      "    print \"x:\\n%s\" % x\n",
      "    print \"2-Norm of x: %2.4f\" % norm_x_2\n",
      "    print \"Normalized x:\\n%s\" % normalized_x\n",
      "    print \"2-Norm of normalized_x: %2.4f\" % np.linalg.norm(normalized_x)\n",
      "\n",
      "This should give the following result (note that in the preceding code snippet, we set the random seed equal to 42 so that the result will always be the same):\n",
      "\n",
      "    **x: [ 0.49671415 -0.1382643 0.64768854 1.52302986 -0.23415337 -0.23413696 1.57921282 0.76743473 -0.46947439 0.54256004]**\n",
      "    **2-Norm of x: 2.5908**\n",
      "    **Normalized x: [ 0.19172213 -0.05336737 0.24999534 0.58786029 -0.09037871 -0.09037237 0.60954584 0.29621508 -0.1812081 0.20941776]**\n",
      "    **2-Norm of normalized_x: 1.0000**\n",
      "\n",
      "### Using MLlib for feature normalization\n",
      "\n",
      "Spark provides some built-in functions for feature scaling and standardization in its MLlib machine learning library. These include `StandardScaler`, which applies the standard normal transformation, and `Normalizer`, which applies the same feature vector normalization we showed you in our preceding example code.\n",
      "\n",
      "We will explore the use of these methods in the upcoming chapters, but for now, let's simply compare the results of using MLlib's `Normalizer` to our own results:\n",
      "\n",
      "    from pyspark.mllib.feature import Normalizer\n",
      "    normalizer = Normalizer()\n",
      "    vector = sc.parallelize([x])\n",
      "\n",
      "After importing the required class, we will instantiate `Normalizer` (by default, it will use the L2 norm as we did earlier). Note that as in most situations in Spark, we need to provide `Normalizer` with an RDD as input (it contains `numpy` arrays or MLlib vectors); hence, we will create a single-element RDD from our vector `x` for illustrative purposes.\n",
      "\n",
      "We will then use the `transform` function of `Normalizer` on our RDD. Since the RDD only has one vector in it, we will return our vector to the driver by calling `first` and finally by calling the `toArray` function to convert the vector back into a `numpy` array:\n",
      "\n",
      "    normalized_x_mllib = normalizer.transform(vector).first().toArray()\n",
      "\n",
      "Finally, we can print out the same details as we did previously, comparing the results:\n",
      "\n",
      "    print \"x:\\n%s\" % x\n",
      "    print \"2-Norm of x: %2.4f\" % norm_x_2\n",
      "    print \"Normalized x MLlib:\\n%s\" % normalized_x_mllib\n",
      "    print \"2-Norm of normalized_x_mllib: %2.4f\" % np.linalg.norm(normalized_x_mllib)\n",
      "\n",
      "You will end up with exactly the same normalized vector as we did with our own code. However, using MLlib's built-in methods is certainly more convenient and efficient than writing our own functions!\n",
      "\n",
      "## Using packages for feature extraction\n",
      "\n",
      "While we have covered many different approaches to feature extraction, it will be rather painful to have to create the code to perform these common tasks each and every time. Certainly, we can create our own reusable code libraries for this purpose; however, fortunately, we can rely on the existing tools and packages.\n",
      "\n",
      "Since Spark supports Scala, Java, and Python bindings, we can use packages available in these languages that provide sophisticated tools to process and extract features and represent them as vectors. A few examples of packages for feature extraction include scikit-learn, gensim, scikit-image, matplotlib, and NLTK in Python; OpenNLP in Java; and Breeze and Chalk in Scala. In fact, Breeze has been part of Spark MLlib since version 1.0, and we will see how to use some Breeze functionality for linear algebra in the later chapters.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we saw how to find common, publicly-available datasets that can be used to test various machine learning models. You learned how to load, process, and clean data, as well as how to apply common techniques to transform raw data into feature vectors that can be used as training examples for our models.\n",
      "\n",
      "In the next chapter, you will learn the basics of recommender systems and explore how to create a recommendation model, use the model to make predictions, and evaluate the model.\n",
      "\n",
      "# Chapter 4. Building a Recommendation Engine with Spark\n",
      "\n",
      "Now that you have learned the basics of data processing and feature extraction, we will move on to explore individual machine learning models in detail, starting with recommendation engines.\n",
      "\n",
      "Recommendation engines are probably among the best types of machine learning model known to the general public. Even if people do not know exactly what a recommendation engine is, they have most likely experienced one through the use of popular websites such as Amazon, Netflix, YouTube, Twitter, LinkedIn, and Facebook. Recommendations are a core part of all these businesses, and in some cases, they drive significant percentages of their revenue.\n",
      "\n",
      "The idea behind recommendation engines is to predict what people might like and to uncover relationships between items to aid in the discovery process (in this way, it is similar and, in fact, often complementary to search engines, which also play a role in discovery). However, unlike search engines, recommendation engines try to present people with relevant content that they did not necessarily search for or that they might not even have heard of.\n",
      "\n",
      "Typically, a recommendation engine tries to model the connections between users and some type of item. In our MovieStream scenario from Chapter 2, _Designing a Machine Learning System_ , for example, we could use a recommendation engine to show our users movies that they might enjoy. If we can do this well, we could keep our users engaged using our service, which is good for both our users and us. Similarly, if we can do a good job of showing our users movies related to a given movie, we could aid in discovery and navigation on our site, again improving our users' experience, engagement, and the relevance of our content to them.\n",
      "\n",
      "However, recommendation engines are not limited to movies, books, or products. The techniques we will explore in this chapter can be applied to just about any user-to-item relationship as well as user-to-user connections, such as those found on social networks, allowing us to make recommendations such as people you may know or who to follow.\n",
      "\n",
      "Recommendation engines are most effective in two general scenarios (which are not mutually exclusive). They are explained here:\n",
      "\n",
      "  *  **Large number of available options for users** : When there are a very large number of available items, it becomes increasingly difficult for the user to find something they want. Searching can help when the user knows what they are looking for, but often, the right item might be something previously unknown to them. In this case, being recommended relevant items, that the user may not already know about, can help them discover new items.\n",
      "  *  **A significant degree of personal taste involved** : When personal taste plays a large role in selection, recommendation models, which often utilize a wisdom of the crowd approach, can be helpful in discovering items based on the behavior of others that have similar taste profiles.\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Introduce the various types of recommendation engines\n",
      "  * Build a recommendation model using data about user preferences\n",
      "  * Use the trained model to compute recommendations for a given user as well compute similar items for a given item (that is, related items)\n",
      "  * Apply standard evaluation metrics to the model that we created to measure how well it performs in terms of predictive capability\n",
      "\n",
      "# Types of recommendation models\n",
      "\n",
      "Recommender systems are widely studied, and there are many approaches used, but there are two that are probably most prevalent: content-based filtering and collaborative filtering. Recently, other approaches such as ranking models have also gained in popularity. In practice, many approaches are hybrids, incorporating elements of many different methods into a model or combination of models.\n",
      "\n",
      "## Content-based filtering\n",
      "\n",
      "Content-based methods try to use the content or attributes of an item, together with some notion of similarity between two pieces of content, to generate items similar to a given item. These attributes are often textual content (such as titles, names, tags, and other metadata attached to an item), or in the case of media, they could include other features of the item, such as attributes extracted from audio and video content.\n",
      "\n",
      "In a similar manner, user recommendations can be generated based on attributes of users or user profiles, which are then matched to item attributes using the same measure of similarity. For example, a user can be represented by the combined attributes of the items they have interacted with. This becomes their user profile, which is then compared to item attributes to find items that match the user profile.\n",
      "\n",
      "## Collaborative filtering\n",
      "\n",
      "Collaborative filtering is a form of wisdom of the crowd approach where the set of preferences of many users with respect to items is used to generate estimated preferences of users for items with which they have not yet interacted. The idea behind this is the notion of similarity.\n",
      "\n",
      "In a user-based approach, if two users have exhibited similar preferences (that is, patterns of interacting with the same items in broadly the same way), then we would assume that they are similar to each other in terms of taste. To generate recommendations for unknown items for a given user, we can use the known preferences of other users that exhibit similar behavior. We can do this by selecting a set of similar users and computing some form of combined score based on the items they have shown a preference for. The overall logic is that if others have tastes similar to a set of items, these items would tend to be good candidates for recommendation.\n",
      "\n",
      "We can also take an item-based approach that computes some measure of similarity between items. This is usually based on the existing user-item preferences or ratings. Items that tend to be rated the same by similar users will be classed as similar under this approach. Once we have these similarities, we can represent a user in terms of the items they have interacted with and find items that are similar to these known items, which we can then recommend to the user. Again, a set of items similar to the known items is used to generate a combined score to estimate for an unknown item.\n",
      "\n",
      "The user- and item-based approaches are usually referred to as nearest-neighbor models, since the estimated scores are computed based on the set of most similar users or items (that is, their neighbors).\n",
      "\n",
      "Finally, there are many model-based methods that attempt to model the user-item preferences themselves so that new preferences can be estimated directly by applying the model to unknown user-item combinations.\n",
      "\n",
      "### Matrix factorization\n",
      "\n",
      "Since Spark's recommendation models currently only include an implementation of matrix factorization, we will focus our attention on this class of models. This focus is with good reason; however, these types of models have consistently been shown to perform extremely well in collaborative filtering and were among the best models in well-known competitions such as the Netflix prize.\n",
      "\n",
      "### Note\n",
      "\n",
      "For more information on and a brief overview of the performance of the best algorithms for the Netflix prize, see <http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html>.\n",
      "\n",
      "#### Explicit matrix factorization\n",
      "\n",
      "When we deal with data that consists of preferences of users that are provided by the users themselves, we refer to explicit preference data. This includes, for example, ratings, thumbs up, likes, and so on that are given by users to items.\n",
      "\n",
      "We can take these ratings and form a two-dimensional matrix with users as rows and items as columns. Each entry represents a rating given by a user to a certain item. Since in most cases, each user has only interacted with a relatively small set of items, this matrix has only a few non-zero entries (that is, it is very sparse).\n",
      "\n",
      "As a simple example, let's assume that we have the following user ratings for a set of movies:\n",
      "\n",
      "    **Tom, Star Wars, 5**\n",
      "    **Jane, Titanic, 4**\n",
      "    **Bill, Batman, 3**\n",
      "    **Jane, Star Wars, 2**\n",
      "    **Bill, Titanic, 3**\n",
      "\n",
      "We will form the following ratings matrix:\n",
      "\n",
      "A simple movie-rating matrix\n",
      "\n",
      "Matrix factorization (or matrix completion) attempts to directly model this user-item matrix by representing it as a product of two smaller matrices of lower dimension. Thus, it is a dimensionality-reduction technique. If we have **U** users and **I** items, then our user-item matrix is of dimension U x I and might look something like the one shown in the following diagram:\n",
      "\n",
      "A sparse ratings matrix\n",
      "\n",
      "If we want to find a lower dimension (low-rank) approximation to our user-item matrix with the dimension **k** , we would end up with two matrices: one for users of size U x k and one for items of size I x k. These are known as factor matrices. If we multiply these two factor matrices, we would reconstruct an approximate version of the original ratings matrix. Note that while the original ratings matrix is typically very sparse, each factor matrix is dense, as shown in the following diagram:\n",
      "\n",
      "The user- and item-factor matrices\n",
      "\n",
      "These models are often also called latent feature models, as we are trying to discover some form of hidden features (which are represented by the factor matrices) that account for the structure of behavior inherent in the user-item rating matrix. While the latent features or factors are not directly interpretable, they might, perhaps, represent things such as the tendency of a user to like movies from a certain director, genre, style, or group of actors, for example.\n",
      "\n",
      "As we are directly modeling the user-item matrix, the prediction in these models is relatively straightforward: to compute a predicted rating for a user and item, we compute the vector dot product between the relevant row of the user-factor matrix (that is, the user's factor vector) and the relevant row of the item-factor matrix (that is, the item's factor vector).\n",
      "\n",
      "This is illustrated with the highlighted vectors in the following diagram:\n",
      "\n",
      "Computing recommendations from user- and item-factor vectors\n",
      "\n",
      "To find out the similarity between two items, we can use the same measures of similarity as we would use in the nearest-neighbor models, except that we can use the factor vectors directly by computing the similarity between two item-factor vectors, as illustrated in the following diagram:\n",
      "\n",
      "Computing similarity with item-factor vectors\n",
      "\n",
      "The benefit of factorization models is the relative ease of computing recommendations once the model is created. However, for very large user and itemsets, this can become a challenge as it requires storage and computation across potentially many millions of user- and item-factor vectors. Another advantage, as mentioned earlier, is that they tend to offer very good performance.\n",
      "\n",
      "### Note\n",
      "\n",
      "Projects such as Oryx (<https://github.com/OryxProject/oryx>) and Prediction.io (<https://github.com/PredictionIO/PredictionIO>) focus on model serving for large-scale models, including recommenders based on matrix factorization.\n",
      "\n",
      "On the down side, factorization models are relatively more complex to understand and interpret compared to nearest-neighbor models and are often more computationally intensive during the model's training phase.\n",
      "\n",
      "#### Implicit matrix factorization\n",
      "\n",
      "So far, we have dealt with explicit preferences such as ratings. However, much of the preference data that we might be able to collect is implicit feedback, where the preferences between a user and item are not given to us, but are, instead, implied from the interactions they might have with an item. Examples include binary data (such as whether a user viewed a movie, whether they purchased a product, and so on) as well as count data (such as the number of times a user watched a movie).\n",
      "\n",
      "There are many different approaches to deal with implicit data. MLlib implements a particular approach that treats the input rating matrix as two matrices: a binary preference matrix, **P** , and a matrix of confidence weights, **C**.\n",
      "\n",
      "For example, let's assume that the user-movie ratings we saw previously were, in fact, the number of times each user had viewed that movie. The two matrices would look something like ones shown in the following screenshot. Here, the matrix **P** informs us that a movie was viewed by a user, and the matrix **C** represents the confidence weighting, in the form of the view counts--generally, the more a user has watched a movie, the higher the confidence that they actually like it.\n",
      "\n",
      "Representation of an implicit preference and confidence matrix\n",
      "\n",
      "The implicit model still creates a user- and item-factor matrix. In this case, however, the matrix that the model is attempting to approximate is not the overall ratings matrix but the preference matrix P. If we compute a recommendation by calculating the dot product of a user- and item-factor vector, the score will not be an estimate of a rating directly. It will rather be an estimate of the preference of a user for an item (though not strictly between 0 and 1, these scores will generally be fairly close to a scale of 0 to 1).\n",
      "\n",
      "#### Alternating least squares\n",
      "\n",
      " **Alternating Least Squares** ( **ALS** ) is an optimization technique to solve matrix factorization problems; this technique is powerful, achieves good performance, and has proven to be relatively easy to implement in a parallel fashion. Hence, it is well suited for platforms such as Spark. At the time of writing this book, it is the only recommendation model implemented in MLlib.\n",
      "\n",
      "ALS works by iteratively solving a series of least squares regression problems. In each iteration, one of the user- or item-factor matrices is treated as fixed, while the other one is updated using the fixed factor and the rating data. Then, the factor matrix that was solved for is, in turn, treated as fixed, while the other one is updated. This process continues until the model has converged (or for a fixed number of iterations).\n",
      "\n",
      "### Note\n",
      "\n",
      "Spark's documentation for collaborative filtering contains references to the papers that underlie the ALS algorithms implemented each component of explicit and implicit data. You can view the documentation at <http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html>.\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "In this section, we will use explicit rating data, without additional user or item metadata or other information related to the user-item interactions. Hence, the features that we need as inputs are simply the user IDs, movie IDs, and the ratings assigned to each user and movie pair.\n",
      "\n",
      "## Extracting features from the MovieLens 100k dataset\n",
      "\n",
      "Start the Spark shell in the Spark base directory, ensuring that you provide enough memory via the `-driver-memory` option:\n",
      "\n",
      "    **>./bin/spark-shell -driver-memory 4g**\n",
      "\n",
      "In this example, we will use the same MovieLens dataset that we used in the previous chapter. Use the directory in which you placed the MovieLens 100k dataset as the input path in the following code.\n",
      "\n",
      "First, let's inspect the raw ratings dataset:\n",
      "\n",
      "    val rawData = sc.textFile(\"/ **PATH** /ml-100k/u.data\")\n",
      "    rawData.first()\n",
      "\n",
      "You will see output similar to these lines of code:\n",
      "\n",
      "    **14/03/30 11:42:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable**\n",
      "    **14/03/30 11:42:41 WARN LoadSnappy: Snappy native library not loaded**\n",
      "    **14/03/30 11:42:41 INFO FileInputFormat: Total input paths to process : 1**\n",
      "    **14/03/30 11:42:41 INFO SparkContext: Starting job: first at <console>:15**\n",
      "    **14/03/30 11:42:41 INFO DAGScheduler: Got job 0 (first at <console>:15) with 1 output partitions (allowLocal=true)**\n",
      "    **14/03/30 11:42:41 INFO DAGScheduler: Final stage: Stage 0 (first at <console>:15)**\n",
      "    **14/03/30 11:42:41 INFO DAGScheduler: Parents of final stage: List()**\n",
      "    **14/03/30 11:42:41 INFO DAGScheduler: Missing parents: List()**\n",
      "    **14/03/30 11:42:41 INFO DAGScheduler: Computing the requested partition locally**\n",
      "    **14/03/30 11:42:41 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173**\n",
      "    **14/03/30 11:42:41 INFO SparkContext: Job finished: first at <console>:15, took 0.030533 s**\n",
      "    **res0: String = 196  242  3  881250949**\n",
      "\n",
      "Recall that this dataset consisted of the `user id`, `movie id`, `rating`, `timestamp` fields separated by a tab (`\"\\t\"`) character. We don't need the time when the rating was made to train our model, so let's simply extract the first three fields:\n",
      "\n",
      "    val rawRatings = rawData.map(_.split(\"\\t\"). **take** (3))\n",
      "\n",
      "We will first split each record on the `\"\\t\"` character, which gives us an `Array[String]` array. We will then use Scala's `take` function to keep only the first `3` elements of the array, which correspond to `user id`, `movie id`, and `rating`, respectively.\n",
      "\n",
      "We can inspect the first record of our new RDD by calling `rawRatings.first()`, which collects just the first record of the RDD back to the driver program. This will result in the following output:\n",
      "\n",
      "    **14/03/30 12:24:00 INFO SparkContext: Starting job: first at <console>:21**\n",
      "    **14/03/30 12:24:00 INFO DAGScheduler: Got job 1 (first at <console>:21) with 1 output partitions (allowLocal=true)**\n",
      "    **14/03/30 12:24:00 INFO DAGScheduler: Final stage: Stage 1 (first at <console>:21)**\n",
      "    **14/03/30 12:24:00 INFO DAGScheduler: Parents of final stage: List()**\n",
      "    **14/03/30 12:24:00 INFO DAGScheduler: Missing parents: List()**\n",
      "    **14/03/30 12:24:00 INFO DAGScheduler: Computing the requested partition locally**\n",
      "    **14/03/30 12:24:00 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173**\n",
      "    **14/03/30 12:24:00 INFO SparkContext: Job finished: first at <console>:21, took 0.00391 s**\n",
      "    **res6: Array[String] = Array(196, 242, 3)**\n",
      "\n",
      "We will use Spark's MLlib library to train our model. Let's take a look at what methods are available for us to use and what input is required. First, import the `ALS` model from MLlib:\n",
      "\n",
      "    import org.apache.spark.mllib.recommendation.ALS\n",
      "\n",
      "On the console, we can inspect the available methods on the ALS object using tab completion. Type in `ALS.` (note the dot) and then press the _Tab_ key. You should see the autocompletion of the methods:\n",
      "\n",
      "    **ALS.**\n",
      "    **asInstanceOf    isInstanceOf    main            toString        train           trainImplicit**\n",
      "\n",
      "The method we want to use is `train`. If we type `ALS.train` and hit _Enter_ , we will get an error. However, this error will tell us what the method signature looks like:\n",
      "\n",
      "    **ALS.train**\n",
      "    **< console>:12: error: ambiguous reference to overloaded definition,**\n",
      "    **both method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int**\n",
      "    **, iterations: Int)org.apache.spark.mllib.recommendation.MatrixFactorizationModel**\n",
      "    **and  method train in object ALS of type (ratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating], rank: Int, iterations: Int, lambda: Double)org.apache.spark.mllib.recommendation.MatrixFactorizationModel**\n",
      "    **match expected type ?**\n",
      "    **ALS.train**\n",
      "    **^**\n",
      "\n",
      "So, we can see that at a minimum, we need to provide the input arguments, `ratings`, `rank`, and `iterations`. The second method also requires an argument called `lambda`. We'll cover these three shortly, but let's take a look at the `ratings` argument. First, let's import the `Rating` class that it references and use a similar approach to find out what an instance of `Rating` requires, by typing in `Rating()` and hitting _Enter_ :\n",
      "\n",
      "    **import org.apache.spark.mllib.recommendation.Rating**\n",
      "    **Rating()**\n",
      "    **< console>:13: error: not enough arguments for method apply: (user: Int, product: Int, rating: Double)org.apache.spark.mllib.recommendation.Rating in object Rating.**\n",
      "    **Unspecified value parameters user, product, rating.**\n",
      "    **Rating()**\n",
      "    **^**\n",
      "\n",
      "As we can see from the preceding output, we need to provide the `ALS` model with an RDD that consists of `Rating` records. A `Rating` class, in turn, is just a wrapper around `user id`, `movie id` (called `product` here), and the actual `rating` arguments. We'll create our rating dataset using the `map` method and transforming the array of IDs and ratings into a `Rating` object:\n",
      "\n",
      "    val ratings = rawRatings.map { case Array(user, movie, rating) => Rating(user. **toInt** , movie. **toInt** , rating. **toDouble** ) }\n",
      "\n",
      "### Note\n",
      "\n",
      "Notice that we need to use `toInt` or `toDouble` to convert the raw rating data (which was extracted as `Strings` from the text file) to `Int` or `Double` numeric inputs. Also, note the use of a `case` statement that allows us to extract the relevant variable names and use them directly (this saves us from having to use something like `val user = ratings(0)`).\n",
      "\n",
      "For more on Scala case statements and pattern matching as used here, take a look at <http://docs.scala-lang.org/tutorials/tour/pattern-matching.html>.\n",
      "\n",
      "We now have an `RDD[Rating]` that we can verify by calling:\n",
      "\n",
      "    **ratings.first()**\n",
      "    **14/03/30 12:32:48 INFO SparkContext: Starting job: first at <console>:24**\n",
      "    **14/03/30 12:32:48 INFO DAGScheduler: Got job 2 (first at <console>:24) with 1 output partitions (allowLocal=true)**\n",
      "    **14/03/30 12:32:48 INFO DAGScheduler: Final stage: Stage 2 (first at <console>:24)**\n",
      "    **14/03/30 12:32:48 INFO DAGScheduler: Parents of final stage: List()**\n",
      "    **14/03/30 12:32:48 INFO DAGScheduler: Missing parents: List()**\n",
      "    **14/03/30 12:32:48 INFO DAGScheduler: Computing the requested partition locally**\n",
      "    **14/03/30 12:32:48 INFO HadoopRDD: Input split: file:/Users/Nick/workspace/datasets/ml-100k/u.data:0+1979173**\n",
      "    **14/03/30 12:32:48 INFO SparkContext: Job finished: first at <console>:24, took 0.003752 s**\n",
      "    **res8: org.apache.spark.mllib.recommendation.Rating = Rating(196,242,3.0)**\n",
      "\n",
      "# Training the recommendation model\n",
      "\n",
      "Once we have extracted these simple features from our raw data, we are ready to proceed with model training; MLlib takes care of this for us. All we have to do is provide the correctly-parsed input RDD we just created as well as our chosen model parameters.\n",
      "\n",
      "## Training a model on the MovieLens 100k dataset\n",
      "\n",
      "We're now ready to train our model! The other inputs required for our model are as follows:\n",
      "\n",
      "  * `rank`: This refers to the number of factors in our ALS model, that is, the number of hidden features in our low-rank approximation matrices. Generally, the greater the number of factors, the better, but this has a direct impact on memory usage, both for computation and to store models for serving, particularly for large number of users or items. Hence, this is often a trade-off in real-world use cases. A rank in the range of 10 to 200 is usually reasonable.\n",
      "  * `iterations`: This refers to the number of iterations to run. While each iteration in `ALS` is guaranteed to decrease the reconstruction error of the ratings matrix, `ALS` models will converge to a reasonably good solution after relatively few iterations. So, we don't need to run for too many iterations in most cases (around 10 is often a good default).\n",
      "  * `lambda`: This parameter controls the regularization of our model. Thus, `lambda` controls over fitting. The higher the value of `lambda`, the more is the regularization applied. What constitutes a sensible value is very dependent on the size, nature, and sparsity of the underlying data, and as with almost all machine learning models, the regularization parameter is something that should be tuned using out-of-sample test data and cross-validation approaches.\n",
      "\n",
      "We'll use `rank` of `50`, `10` iterations, and a lambda parameter of `0.01` to illustrate how to train our model:\n",
      "\n",
      "    val model = ALS.train(ratings, 50, 10, 0.01)\n",
      "\n",
      "This returns a `MatrixFactorizationModel` object, which contains the user and item factors in the form of an RDD of `(id, factor)` pairs. These are called `userFeatures` and `productFeatures`, respectively. For example:\n",
      "\n",
      "    model.userFeatures\n",
      "\n",
      "You will see the output as:\n",
      "\n",
      "    **res14: org.apache.spark.rdd.RDD[(Int, Array[Double])] = FlatMappedRDD[659] at flatMap at ALS.scala:231**\n",
      "\n",
      "We can see that the factors are in the form of an `Array[Double]`.\n",
      "\n",
      "Note that the operations used in MLlib's `ALS` implementation are lazy transformations, so the actual computation will only be performed once we call some sort of action on the resulting `RDDs` of the user and item factors. We can force the computation using a Spark action such as `count`:\n",
      "\n",
      "    **model.userFeatures.count**\n",
      "\n",
      "This will trigger the computation, and we will see a quite a bit of output text similar to the following lines of code:\n",
      "\n",
      "    **14/03/30 13:10:40 INFO SparkContext: Starting job: count at <console>:26**\n",
      "    **14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 665 (map at ALS.scala:147)**\n",
      "    **14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 664 (map at ALS.scala:146)**\n",
      "    **14/03/30 13:10:40 INFO DAGScheduler: Registering RDD 674 (mapPartitionsWithIndex at ALS.scala:164)**\n",
      "    **...**\n",
      "    **14/03/30 13:10:45 INFO SparkContext: Job finished: count at <console>:26, took 5.068255 s**\n",
      "    **res16: Long = 943**\n",
      "\n",
      "If we call `count` for the movie factors, we will see the following output:\n",
      "\n",
      "    **model.productFeatures.count**\n",
      "    **14/03/30 13:15:21 INFO SparkContext: Starting job: count at <console>:26**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Got job 10 (count at <console>:26) with 1 output partitions (allowLocal=false)**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Final stage: Stage 165 (count at <console>:26)**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Parents of final stage: List(Stage 169, Stage 166)**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Missing parents: List()**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Submitting Stage 165 (FlatMappedRDD[883] at flatMap at ALS.scala:231), which has no missing parents**\n",
      "    **14/03/30 13:15:21 INFO DAGScheduler: Submitting 1 missing tasks from Stage 165 (FlatMappedRDD[883] at flatMap at ALS.scala:231)**\n",
      "    **...**\n",
      "    **14/03/30 13:15:21 INFO SparkContext: Job finished: count at <console>:26, took 0.030044 s**\n",
      "    **res21: Long = 1682**\n",
      "\n",
      "As expected, we have a factor array for each user (`943` factors) and movie (`1682` factors).\n",
      "\n",
      "### Training a model using implicit feedback data\n",
      "\n",
      "The standard matrix factorization approach in MLlib deals with explicit ratings. To work with implicit data, you can use the `trainImplicit` method. It is called in a manner similar to the standard `train` method. There is an additional parameter, `alpha`, that can be set (and in the same way, the regularization parameter, `lambda`, should be selected via testing and cross-validation methods).\n",
      "\n",
      "The `alpha` parameter controls the baseline level of confidence weighting applied. A higher level of `alpha` tends to make the model more confident about the fact that missing data equates to no preference for the relevant user-item pair.\n",
      "\n",
      "### Note\n",
      "\n",
      "As an exercise, try to take the existing MovieLens dataset and convert it into an implicit dataset. One possible approach is to convert it to binary feedback (0s and 1s) by applying a threshold on the ratings at some level.\n",
      "\n",
      "Another approach could be to convert the ratings' values into confidence weights (for example, perhaps, low ratings could imply zero weights, or even negative weights, which are supported by MLlib's implementation).\n",
      "\n",
      "Train a model on this dataset and compare the results of the following section with those generated by your implicit model.\n",
      "\n",
      "# Using the recommendation model\n",
      "\n",
      "Now that we have our trained model, we're ready to use it to make predictions. These predictions typically take one of two forms: recommendations for a given user and related or similar items for a given item.\n",
      "\n",
      "## User recommendations\n",
      "\n",
      "In this case, we would like to generate recommended items for a given user. This usually takes the form of a _top-K_ list, that is, the _K_ items that our model predicts will have the highest probability of the user liking them. This is done by computing the predicted score for each item and ranking the list based on this score.\n",
      "\n",
      "The exact method to perform this computation depends on the model involved. For example, in user-based approaches, the ratings of similar users on items are used to compute the recommendations for a user, while in an item-based approach, the computation is based on the similarity of items the user has rated to the candidate items.\n",
      "\n",
      "In matrix factorization, because we are modeling the ratings matrix directly, the predicted score can be computed as the vector dot product between a user-factor vector and an item-factor vector.\n",
      "\n",
      "### Generating movie recommendations from the MovieLens 100k dataset\n",
      "\n",
      "As MLlib's recommendation model is based on matrix factorization, we can use the factor matrices computed by our model to compute predicted scores (or ratings) for a user. We will focus on the explicit rating case using MovieLens data; however, the approach is the same when using the implicit model.\n",
      "\n",
      "The `MatrixFactorizationModel` class has a convenient `predict` method that will compute a predicted score for a given user and item combination:\n",
      "\n",
      "    val predictedRating = model.predict(789, 123)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **14/03/30 16:10:10 INFO SparkContext: Starting job: lookup at MatrixFactorizationModel.scala:45**\n",
      "    **14/03/30 16:10:10 INFO DAGScheduler: Got job 30 (lookup at MatrixFactorizationModel.scala:45) with 1 output partitions (allowLocal=false)**\n",
      "    **...**\n",
      "    **14/03/30 16:10:10 INFO SparkContext: Job finished: lookup at MatrixFactorizationModel.scala:46, took 0.023077 s**\n",
      "    **predictedRating: Double = 3.128545693368485**\n",
      "\n",
      "As we can see, this model predicts a rating of `3.12` for user `789` and movie `123`.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that you might see different results than those shown in this section because the `ALS` model is initialized randomly. So, different runs of the model will lead to different solutions.\n",
      "\n",
      "The `predict` method can also take an RDD of `(user, item)` IDs as the input and will generate predictions for each of these. We can use this method to make predictions for many users and items at the same time.\n",
      "\n",
      "To generate the _top-K_ recommended items for a user, `MatrixFactorizationModel` provides a convenience method called `recommendProducts`. This takes two arguments: `user` and `num`, where `user` is the user ID, and `num` is the number of items to recommend.\n",
      "\n",
      "It returns the top `num` items ranked in the order of the predicted score. Here, the scores are computed as the dot product between the user-factor vector and each item-factor vector.\n",
      "\n",
      "Let's generate the top `10` recommended items for user `789`:\n",
      "\n",
      "    val userId = 789\n",
      "    val K = 10\n",
      "    val topKRecs = model.recommendProducts(userId, K)\n",
      "\n",
      "We now have a set of predicted ratings for each movie for user `789`. If we print this out, we could inspect the top 10 recommendations for this user:\n",
      "\n",
      "    println(topKRecs.mkString(\"\\n\"))\n",
      "\n",
      "You should see the following output on your console:\n",
      "\n",
      "    **Rating(789,715,5.931851273771102)**\n",
      "    **Rating(789,12,5.582301095666215)**\n",
      "    **Rating(789,959,5.516272981542168)**\n",
      "    **Rating(789,42,5.458065302395629)**\n",
      "    **Rating(789,584,5.449949837103569)**\n",
      "    **Rating(789,750,5.348768847643657)**\n",
      "    **Rating(789,663,5.30832117499004)**\n",
      "    **Rating(789,134,5.278933936827717)**\n",
      "    **Rating(789,156,5.250959077906759)**\n",
      "    **Rating(789,432,5.169863417126231)**\n",
      "\n",
      "#### Inspecting the recommendations\n",
      "\n",
      "We can give these recommendations a sense check by taking a quick look at the titles of the movies a user has rated and the recommended movies. First, we need to load the movie data (which is the one of the datasets we explored in the previous chapter). We'll collect this data as a `Map[Int, String]` method mapping the movie ID to the title:\n",
      "\n",
      "    val movies = sc.textFile(\"/PATH/ml-100k/u.item\")\n",
      "    val titles = movies.map(line => line.split(\"\\\\|\").take(2)).map(array => (array(0).toInt,array(1))).collectAsMap()\n",
      "    titles(123)\n",
      "\n",
      "The preceding code will produce the output as:\n",
      "\n",
      "    **res68: String = Frighteners, The (1996)**\n",
      "\n",
      "For our user `789`, we can find out what movies they have rated, take the `10` movies with the highest rating, and then check the titles. We will do this now by first using the `keyBy` Spark function to create an RDD of key-value pairs from our `ratings` RDD, where the key will be the user ID. We will then use the `lookup` function to return just the ratings for this key (that is, that particular user ID) to the driver:\n",
      "\n",
      "    val moviesForUser = ratings.keyBy(_.user).lookup(789)\n",
      "\n",
      "Let's see how many movies this user has rated. This will be the `size` of the `moviesForUser` collection:\n",
      "\n",
      "    println(moviesForUser.size)\n",
      "\n",
      "We will see that this user has rated `33` movies.\n",
      "\n",
      "Next, we will take the 10 movies with the highest ratings by sorting the `moviesForUser` collection using the `rating` field of the `Rating` object. We will then extract the movie title for the relevant product ID attached to the `Rating` class from our mapping of movie titles and print out the top `10` titles with their ratings:\n",
      "\n",
      "    moviesForUser.sortBy(-_.rating).take(10).map(rating => (titles(rating.product), rating.rating)).foreach(println)\n",
      "\n",
      "You will see the following output displayed:\n",
      "\n",
      "    **(Godfather, The (1972),5.0)**\n",
      "    **(Trainspotting (1996),5.0)**\n",
      "    **(Dead Man Walking (1995),5.0)**\n",
      "    **(Star Wars (1977),5.0)**\n",
      "    **(Swingers (1996),5.0)**\n",
      "    **(Leaving Las Vegas (1995),5.0)**\n",
      "    **(Bound (1996),5.0)**\n",
      "    **(Fargo (1996),5.0)**\n",
      "    **(Last Supper, The (1995),5.0)**\n",
      "    **(Private Parts (1997),4.0)**\n",
      "\n",
      "Now, let's take a look at the top 10 recommendations for this user and see what the titles are using the same approach as the one we used earlier (note that the recommendations are already sorted):\n",
      "\n",
      "    topKRecs.map(rating => (titles(rating.product), rating.rating)).foreach(println)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **(To Die For (1995),5.931851273771102)**\n",
      "    **(Usual Suspects, The (1995),5.582301095666215)**\n",
      "    **(Dazed and Confused (1993),5.516272981542168)**\n",
      "    **(Clerks (1994),5.458065302395629)**\n",
      "    **(Secret Garden, The (1993),5.449949837103569)**\n",
      "    **(Amistad (1997),5.348768847643657)**\n",
      "    **(Being There (1979),5.30832117499004)**\n",
      "    **(Citizen Kane (1941),5.278933936827717)**\n",
      "    **(Reservoir Dogs (1992),5.250959077906759)**\n",
      "    **(Fantasia (1940),5.169863417126231)**\n",
      "\n",
      "We leave it to you to decide whether these recommendations make sense.\n",
      "\n",
      "## Item recommendations\n",
      "\n",
      "Item recommendations are about answering the following question: for a certain item, what are the items most similar to it? Here, the precise definition of similarity is dependent on the model involved. In most cases, similarity is computed by comparing the vector representation of two items using some similarity measure. Common similarity measures include Pearson correlation and cosine similarity for real-valued vectors and Jaccard similarity for binary vectors.\n",
      "\n",
      "### Generating similar movies for the MovieLens 100k dataset\n",
      "\n",
      "The current `MatrixFactorizationModel` API does not directly support item-to-item similarity computations. Therefore, we will need to create our own code to do this.\n",
      "\n",
      "We will use the cosine similarity metric, and we will use the jblas linear algebra library (a dependency of MLlib) to compute the required vector dot products. This is similar to how the existing `predict` and `recommendProducts` methods work, except that we will use cosine similarity as opposed to just the dot product.\n",
      "\n",
      "We would like to compare the factor vector of our chosen item with each of the other items, using our similarity metric. In order to perform linear algebra computations, we will first need to create a vector object out of the factor vectors, which are in the form of an `Array[Double]`. The `JBLAS` class, `DoubleMatrix`, takes an `Array[Double]` as the constructor argument as follows:\n",
      "\n",
      "    import org.jblas.DoubleMatrix\n",
      "    val aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))\n",
      "\n",
      "Here is the output of the preceding code:\n",
      "\n",
      "    **aMatrix: org.jblas.DoubleMatrix = [1.000000; 2.000000; 3.000000]**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that using jblas, vectors are represented as a one-dimensional `DoubleMatrix` class, while matrices are a two-dimensional `DoubleMatrix` class.\n",
      "\n",
      "We will need a method to compute the cosine similarity between two vectors. Cosine similarity is a measure of the angle between two vectors in an _n_ -dimensional space. It is computed by first calculating the dot product between the vectors and then dividing the result by a denominator, which is the norm (or length) of each vector multiplied together (specifically, the L2-norm is used in cosine similarity). In this way, cosine similarity is a normalized dot product.\n",
      "\n",
      "The cosine similarity measure takes on values between -1 and 1. A value of 1 implies completely similar, while a value of 0 implies independence (that is, no similarity). This measure is useful because it also captures negative similarity, that is, a value of -1 implies that not only are the vectors not similar, but they are also completely dissimilar.\n",
      "\n",
      "Let's create our `cosineSimilarity` function here:\n",
      "\n",
      "    def cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = {\n",
      "      vec1.dot(vec2) / (vec1.norm2() * vec2.norm2())\n",
      "    }\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that we defined a return type for this function of `Double`. We are not required to do this, since Scala features type inference. However, it can often be useful to document return types for Scala functions.\n",
      "\n",
      "Let's try it out on one of our item factors for item `567`. We will need to collect an item factor from our model; we will do this using the `lookup` method in a similar way that we did earlier to collect the ratings for a specific user. In the following lines of code, we also use the `head` function, since `lookup` returns an array of values, and we only need the first value (in fact, there will only be one value, which is the factor vector for this item).\n",
      "\n",
      "Since this will be an `Array[Double]`, we will then need to create a `DoubleMatrix` object from it and compute the cosine similarity with itself:\n",
      "\n",
      "    val itemId = 567\n",
      "    val itemFactor = model.productFeatures.lookup(itemId).head\n",
      "    val itemVector = new DoubleMatrix(itemFactor)\n",
      "    cosineSimilarity(itemVector, itemVector)\n",
      "\n",
      "A similarity metric should measure how close, in some sense, two vectors are to each other. Here, we can see that our cosine similarity metric tells us that this item vector is identical to itself, which is what we would expect:\n",
      "\n",
      "    **res113: Double = 1.0**\n",
      "\n",
      "Now, we are ready to apply our similarity metric to each item:\n",
      "\n",
      "    val sims = model.productFeatures.map{ case (id, factor) => \n",
      "      val factorVector = new DoubleMatrix(factor)\n",
      "      val sim = cosineSimilarity(factorVector, itemVector)\n",
      "      (id, sim)\n",
      "    }\n",
      "\n",
      "Next, we can compute the top 10 most similar items by sorting out the similarity score for each item:\n",
      "\n",
      "    // recall we defined K = 10 earlier\n",
      "    val sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
      "\n",
      "In the preceding code snippet, we used Spark's `top` function, which is an efficient way to compute _top-K_ results in a distributed fashion, instead of using `collect` to return all the data to the driver and sorting it locally (remember that we could be dealing with millions of users and items in the case of recommendation models).\n",
      "\n",
      "We need to tell Spark how to sort the `(item id, similarity score)` pairs in the `sims` RDD. To do this, we will pass an extra argument to `top`, which is a Scala `Ordering` object that tells Spark that it should sort by the value in the key-value pair (that is, sort by `similarity`).\n",
      "\n",
      "Finally, we can print the 10 items with the highest computed similarity metric to our given item:\n",
      "\n",
      "    println(sortedSims.take(10).mkString(\"\\n\"))\n",
      "\n",
      "You will see output like the following one:\n",
      "\n",
      "    **(567,1.0000000000000002)**\n",
      "    **(1471,0.6932331537649621)**\n",
      "    **(670,0.6898690594544726)**\n",
      "    **(201,0.6897964975027041)**\n",
      "    **(343,0.6891221044611473)**\n",
      "    **(563,0.6864214133620066)**\n",
      "    **(294,0.6812075443259535)**\n",
      "    **(413,0.6754663844488256)**\n",
      "    **(184,0.6702643811753909)**\n",
      "    **(109,0.6594872765176396)**\n",
      "\n",
      "Not surprisingly, we can see that the top-ranked similar item is our item. The rest are the other items in our set of items, ranked in order of our similarity metric.\n",
      "\n",
      "#### Inspecting the similar items\n",
      "\n",
      "Let's see what the title of our chosen movie is:\n",
      "\n",
      "    println(titles(itemId))\n",
      "\n",
      "The preceding code will print the following output:\n",
      "\n",
      "    **Wes Craven's New Nightmare (1994)**\n",
      "\n",
      "As we did for user recommendations, we can sense check our item-to-item similarity computations and take a look at the titles of the most similar movies. This time, we will take the top 11 so that we can exclude our given movie. So, we will take the numbers 1 to 11 in the list:\n",
      "\n",
      "    val sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), Double] { case (id, similarity) => similarity })\n",
      "    sortedSims2.slice(1, 11).map{ case (id, sim) => (titles(id), sim) }.mkString(\"\\n\")\n",
      "\n",
      "You will see the movie titles and scores displayed similar to this output:\n",
      "\n",
      "    **(Hideaway (1995),0.6932331537649621)**\n",
      "    **(Body Snatchers (1993),0.6898690594544726)**\n",
      "    **(Evil Dead II (1987),0.6897964975027041)**\n",
      "    **(Alien: Resurrection (1997),0.6891221044611473)**\n",
      "    **(Stephen King's The Langoliers (1995),0.6864214133620066)**\n",
      "    **(Liar Liar (1997),0.6812075443259535)**\n",
      "    **(Tales from the Crypt Presents: Bordello of Blood (1996),0.6754663844488256)**\n",
      "    **(Army of Darkness (1993),0.6702643811753909)**\n",
      "    **(Mystery Science Theater 3000: The Movie (1996),0.6594872765176396)**\n",
      "    **(Scream (1996),0.6538249646863378)**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Once again note that you might see quite different results due to random model initialization.\n",
      "\n",
      "Now that you have computed similar items using cosine similarity, see if you can do the same with the user-factor vectors to compute similar users for a given user.\n",
      "\n",
      "# Evaluating the performance of recommendation models\n",
      "\n",
      "How do we know whether the model we have trained is a good model? We need to be able to evaluate its predictive performance in some way. **Evaluation metrics** are measures of a model's predictive capability or accuracy. Some are direct measures of how well a model predicts the model's target variable (such as Mean Squared Error), while others are concerned with how well the model performs at predicting things that might not be directly optimized in the model but are often closer to what we care about in the real world (such as Mean average precision).\n",
      "\n",
      "Evaluation metrics provide a standardized way of comparing the performance of the same model with different parameter settings and of comparing performance across different models. Using these metrics, we can perform model selection to choose the best-performing model from the set of models we wish to evaluate.\n",
      "\n",
      "Here, we will show you how to calculate two common evaluation metrics used in recommender systems and collaborative filtering models: Mean Squared Error and Mean average precision at K.\n",
      "\n",
      "## Mean Squared Error\n",
      "\n",
      "The **Mean Squared Error** ( **MSE** ) is a direct measure of the reconstruction error of the user-item rating matrix. It is also the objective function being minimized in certain models, specifically many matrix-factorization techniques, including `ALS`. As such, it is commonly used in explicit ratings settings.\n",
      "\n",
      "It is defined as the sum of the squared errors divided by the number of observations. The squared error, in turn, is the square of the difference between the predicted rating for a given user-item pair and the actual rating.\n",
      "\n",
      "We will use our user `789` as an example. Let's take the first rating for this user from the `moviesForUser` set of `Ratings` that we previously computed:\n",
      "\n",
      "    val actualRating = moviesForUser.take(1)(0)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **actualRating: org.apache.spark.mllib.recommendation.Rating = Rating(789,1012,4.0)**\n",
      "\n",
      "We will see that the rating for this user-item combination is 4. Next, we will compute the model's predicted rating:\n",
      "\n",
      "    val predictedRating = model.predict(789, actualRating.product)\n",
      "\n",
      "The output of the model's predicted rating is as follows:\n",
      "\n",
      "    **...**\n",
      "    **14/04/13 13:01:15 INFO SparkContext: Job finished: lookup at MatrixFactorizationModel.scala:46, took 0.025404 s**\n",
      "    **predictedRating: Double = 4.001005374200248**\n",
      "\n",
      "We will see that the predicted rating is about 4, very close to the actual rating. Finally, we will compute the squared error between the actual rating and the predicted rating:\n",
      "\n",
      "    val squaredError = math.pow(predictedRating - actualRating.rating, 2.0)\n",
      "\n",
      "The preceding code will output the squared error:\n",
      "\n",
      "    **squaredError: Double = 1.010777282523947E-6**\n",
      "\n",
      "So, in order to compute the overall MSE for the dataset, we need to compute this squared error for each `(user, movie, actual rating, predicted rating)` entry, sum them up, and divide them by the number of ratings. We will do this in the following code snippet.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note the following code is adapted from the Apache Spark programming guide for ALS at <http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html>.\n",
      "\n",
      "First, we will extract the user and product IDs from the `ratings` RDD and make predictions for each user-item pair using `model.predict`. We will use the user-item pair as the key and the predicted rating as the value:\n",
      "\n",
      "    val usersProducts = ratings.map{ case Rating(user, product, rating)  => (user, product)}\n",
      "    val predictions = model.predict(usersProducts).map{\n",
      "        case Rating(user, product, rating) => ((user, product), rating)\n",
      "    }\n",
      "\n",
      "Next, we extract the actual ratings and also map the `ratings` RDD so that the user-item pair is the key and the actual rating is the value. Now that we have two RDDs with the same form of key, we can join them together to create a new RDD with the actual and predicted ratings for each user-item combination:\n",
      "\n",
      "    val ratingsAndPredictions = ratings.map{\n",
      "      case Rating(user, product, rating) => ((user, product), rating)\n",
      "    }.join(predictions)\n",
      "\n",
      "Finally, we will compute the MSE by summing up the squared errors using `reduce` and dividing by the `count` method of the number of records:\n",
      "\n",
      "    val MSE = ratingsAndPredictions.map{\n",
      "        case ((user, product), (actual, predicted)) =>  math.pow((actual - predicted), 2)\n",
      "    }.reduce(_ + _) / ratingsAndPredictions.count\n",
      "    println(\"Mean Squared Error = \" + MSE)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **Mean Squared Error = 0.08231947642632852**\n",
      "\n",
      "It is common to use the **Root Mean Squared Error** ( **RMSE** ), which is just the square root of the MSE metric. This is somewhat more interpretable, as it is in the same units as the underlying data (that is, the ratings in this case). It is equivalent to the standard deviation of the differences between the predicted and actual ratings. We can compute it simply as follows:\n",
      "\n",
      "    val RMSE = math.sqrt(MSE)\n",
      "    println(\"Root Mean Squared Error = \" + RMSE)\n",
      "\n",
      "The preceding code will print the Root Mean Squared Error:\n",
      "\n",
      "    **Root Mean Squared Error = 0.2869137090247319**\n",
      "\n",
      "## Mean average precision at K\n",
      "\n",
      " **Mean average precision at K** ( **MAPK** ) is the mean of the **average precision at K** ( **APK** ) metric across all instances in the dataset. APK is a metric commonly used in information retrieval. APK is a measure of the average relevance scores of a set of the _top-K_ documents presented in response to a query. For each query instance, we will compare the set of _top-K_ results with the set of actual relevant documents (that is, a ground truth set of relevant documents for the query).\n",
      "\n",
      "In the APK metric, the order of the result set matters, in that, the APK score would be higher if the result documents are both relevant and the relevant documents are presented higher in the results. It is, thus, a good metric for recommender systems in that typically we would compute the _top-K_ recommended items for each user and present these to the user. Of course, we prefer models where the items with the highest predicted scores (which are presented at the top of the list of recommendations) are, in fact, the most relevant items for the user. APK and other ranking-based metrics are also more appropriate evaluation measures for implicit datasets; here, MSE makes less sense.\n",
      "\n",
      "In order to evaluate our model, we can use APK, where each user is the equivalent of a query, and the set of _top-K_ recommended items is the document result set. The relevant documents (that is, the ground truth) in this case, is the set of items that a user interacted with. Hence, APK attempts to measure how good our model is at predicting items that a user will find relevant and choose to interact with.\n",
      "\n",
      "### Note\n",
      "\n",
      "The code for the following average precision computation is based on <https://github.com/benhamner/Metrics>.\n",
      "\n",
      "More information on MAPK can be found at <https://www.kaggle.com/wiki/MeanAveragePrecision>.\n",
      "\n",
      "Our function to compute the APK is shown here:\n",
      "\n",
      "    def avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): Double = {\n",
      "      val predK = predicted.take(k)\n",
      "      var score = 0.0\n",
      "      var numHits = 0.0\n",
      "      for ((p, i) <- predK.zipWithIndex) {\n",
      "        if (actual.contains(p)) {\n",
      "          numHits += 1.0\n",
      "          score += numHits / (i.toDouble + 1.0)\n",
      "        }\n",
      "      }\n",
      "      if (actual.isEmpty) {\n",
      "        1.0\n",
      "      } else {\n",
      "        score / scala.math.min(actual.size, k).toDouble\n",
      "      }\n",
      "    }\n",
      "\n",
      "As you can see, this takes as input a list of `actual` item IDs that are associated with the user and another list of `predicted` ids so that our estimate will be relevant for the user.\n",
      "\n",
      "We can compute the APK metric for our example user `789` as follows. First, we will extract the actual movie IDs for the user:\n",
      "\n",
      "    val actualMovies = moviesForUser.map(_.product)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **actualMovies: Seq[Int] = ArrayBuffer(1012, 127, 475, 93, 1161, 286, 293, 9, 50, 294, 181, 1, 1008, 508, 284, 1017, 137, 111, 742, 248, 249, 1007, 591, 150, 276, 151, 129, 100, 741, 288, 762, 628, 124)**\n",
      "\n",
      "We will then use the movie recommendations we made previously to compute the APK score using `K = 10`:\n",
      "\n",
      "    val predictedMovies = topKRecs.map(_.product)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **predictedMovies: Array[Int] = Array(27, 497, 633, 827, 602, 849, 401, 584, 1035, 1014)**\n",
      "\n",
      "The following code will produce the average precision:\n",
      "\n",
      "    val apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)\n",
      "\n",
      "The preceding code will print:\n",
      "\n",
      "    **apk10: Double = 0.0**\n",
      "\n",
      "In this case, we can see that our model is not doing a very good job of predicting relevant movies for this user as the APK score is 0.\n",
      "\n",
      "In order to compute the APK for each user and average them to compute the overall MAPK, we will need to generate the list of recommendations for each user in our dataset. While this can be fairly intensive on a large scale, we can distribute the computation using our Spark functionality. However, one limitation is that each worker must have the full item-factor matrix available so that it can compute the dot product between the relevant user vector and all item vectors. This can be a problem when the number of items is extremely high as the item matrix must fit in the memory of one machine.\n",
      "\n",
      "### Tip\n",
      "\n",
      "There is actually no easy way around this limitation. One possible approach is to only compute recommendations for a subset of items from the total item set, using approximate techniques such as Locality Sensitive Hashing (<http://en.wikipedia.org/wiki/Locality-sensitive_hashing>).\n",
      "\n",
      "We will now see how to go about this. First, we will collect the item factors and form a `DoubleMatrix` object from them:\n",
      "\n",
      "    val itemFactors = model.productFeatures.map { case (id, factor) => factor }.collect()\n",
      "    val itemMatrix = new DoubleMatrix(itemFactors)\n",
      "    println(itemMatrix.rows, itemMatrix.columns)\n",
      "\n",
      "The output of the preceding code is as follows:\n",
      "\n",
      "    **(1682,50)**\n",
      "\n",
      "This gives us a matrix with `1682` rows and `50` columns, as we would expect from `1682` movies with a factor dimension of `50`. Next, we will distribute the item matrix as a broadcast variable so that it is available on each worker node:\n",
      "\n",
      "    val imBroadcast = sc.broadcast(itemMatrix)\n",
      "\n",
      "You will see the output as follows:\n",
      "\n",
      "    **14/04/13 21:02:01 INFO MemoryStore: ensureFreeSpace(672960) called with curMem=4006896, maxMem=311387750**\n",
      "    **14/04/13 21:02:01 INFO MemoryStore: Block broadcast_21 stored as values to memory (estimated size 657.2 KB, free 292.5 MB)**\n",
      "    **imBroadcast: org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = Broadcast(21)**\n",
      "\n",
      "Now we are ready to compute the recommendations for each user. We will do this by applying a `map` function to each user factor within which we will perform a matrix multiplication between the user-factor vector and the movie-factor matrix. The result is a vector (of length `1682`, that is, the number of movies we have) with the predicted rating for each movie. We will then sort these predictions by the predicted rating:\n",
      "\n",
      "    val allRecs = model.userFeatures.map{ case (userId, array) => \n",
      "      val userVector = new DoubleMatrix(array)\n",
      "      val scores = imBroadcast.value.mmul(userVector)\n",
      "      val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1)\n",
      "      val recommendedIds = sortedWithId.map **(_._2 + 1** ).toSeq\n",
      "      (userId, recommendedIds)\n",
      "    }\n",
      "\n",
      "You will see the following on the screen:\n",
      "\n",
      "    **allRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MappedRDD[269] at map at <console>:29**\n",
      "\n",
      "As we can see, we now have an RDD that contains a list of movie IDs for each user ID. These movie IDs are sorted in order of the estimated rating.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that we needed to add 1 to the returned movie ids (as highlighted in the preceding code snippet), as the item-factor matrix is 0-indexed, while our movie IDs start at `1`.\n",
      "\n",
      "We also need the list of movie IDs for each user to pass into our APK function as the `actual` argument. We already have the `ratings` RDD ready, so we can extract just the user and movie IDs from it.\n",
      "\n",
      "If we use Spark's `groupBy` operator, we will get an RDD that contains a list of `(userid, movieid)` pairs for each user ID (as the user ID is the key on which we perform the `groupBy` operation):\n",
      "\n",
      "    val userMovies = ratings.map{ case Rating(user, product, rating) => (user, product) }.groupBy(_._1)\n",
      "\n",
      "The output of the preceding code is as follows:\n",
      "\n",
      "    **userMovies: org.apache.spark.rdd.RDD[(Int, Seq[(Int, Int)])] = MapPartitionsRDD[277] at groupBy at <console>:21**\n",
      "\n",
      "Finally, we can use Spark's `join` operator to join these two RDDs together on the user ID key. Then, for each user, we have the list of actual and predicted movie IDs that we can pass to our APK function. In a manner similar to how we computed MSE, we will sum each of these APK scores using a `reduce` action and divide by the number of users (that is, the count of the `allRecs` RDD):\n",
      "\n",
      "    val K = 10\n",
      "    val MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
      "      val actual = actualWithIds.map(_._2).toSeq\n",
      "      avgPrecisionK(actual, predicted, K)\n",
      "    }.reduce(_ + _) / allRecs.count\n",
      "    println(\"Mean Average Precision at K = \" + MAPK)\n",
      "\n",
      "The preceding code will print the mean average precision at K as follows:\n",
      "\n",
      "    **Mean Average Precision at K = 0.030486963254725705**\n",
      "\n",
      "Our model achieves a fairly low MAPK. However, note that typical values for recommendation tasks are usually relatively low, especially if the item set is extremely large.\n",
      "\n",
      "Try out a few parameter settings for `lambda` and `rank `(and `alpha` if you are using the implicit version of ALS) and see whether you can find a model that performs better based on the RMSE and MAPK evaluation metrics.\n",
      "\n",
      "## Using MLlib's built-in evaluation functions\n",
      "\n",
      "While we have computed MSE, RMSE, and MAPK from scratch, and it a useful learning exercise to do so, MLlib provides convenience functions to do this for us in the `RegressionMetrics` and `RankingMetrics` classes.\n",
      "\n",
      "### RMSE and MSE\n",
      "\n",
      "First, we will compute the MSE and RMSE metrics using `RegressionMetrics`. We will instantiate a `RegressionMetrics` instance by passing in an RDD of key-value pairs that represent the predicted and true values for each data point, as shown in the following code snippet. Here, we will again use the `ratingsAndPredictions` RDD we computed in our earlier example:\n",
      "\n",
      "    import org.apache.spark.mllib.evaluation.RegressionMetrics\n",
      "    val predictedAndTrue = ratingsAndPredictions.map { case ((user, product), (predicted, actual)) => (predicted, actual) }\n",
      "    val regressionMetrics = new RegressionMetrics(predictedAndTrue)\n",
      "\n",
      "We can then access various metrics, including MSE and RMSE. We will print out these metrics here:\n",
      "\n",
      "    println(\"Mean Squared Error = \" + regressionMetrics.meanSquaredError)\n",
      "    println(\"Root Mean Squared Error = \" + regressionMetrics.rootMeanSquaredError)\n",
      "\n",
      "You will see that the output for MSE and RMSE is exactly the same as the metrics we computed earlier:\n",
      "\n",
      "    **Mean Squared Error = 0.08231947642632852**\n",
      "    **Root Mean Squared Error = 0.2869137090247319**\n",
      "\n",
      "### MAP\n",
      "\n",
      "As we did for MSE and RMSE, we can compute ranking-based evaluation metrics using MLlib's `RankingMetrics` class. Similarly, to our own average precision function, we need to pass in an RDD of key-value pairs, where the key is an `Array` of predicted item IDs for a user, while the value is an array of actual item IDs.\n",
      "\n",
      "The implementation of the average precision at the K function in `RankingMetrics` is slightly different from ours, so we will get different results. However, the computation of the overall mean average precision (MAP, which does not use a threshold at K) is the same as our function if we select `K` to be very high (say, at least as high as the number of items in our item set):\n",
      "\n",
      "First, we will calculate MAP using `RankingMetrics`:\n",
      "\n",
      "    import org.apache.spark.mllib.evaluation.RankingMetrics\n",
      "    val predictedAndTrueForRanking = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
      "      val actual = actualWithIds.map(_._2)\n",
      "      (predicted.toArray, actual.toArray)\n",
      "    }\n",
      "    val rankingMetrics = new RankingMetrics(predictedAndTrueForRanking)\n",
      "    println(\"Mean Average Precision = \" + rankingMetrics.meanAveragePrecision)\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **Mean Average Precision = 0.07171412913757183**\n",
      "\n",
      "Next, we will use our function to compute the MAP in exactly the same way as we did previously, except that we set `K` to a very high value, say `2000`:\n",
      "\n",
      "    val MAPK2000 = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n",
      "      val actual = actualWithIds.map(_._2).toSeq\n",
      "      avgPrecisionK(actual, predicted, 2000)\n",
      "    }.reduce(_ + _) / allRecs.count\n",
      "    println(\"Mean Average Precision = \" + MAPK2000)\n",
      "\n",
      "You will see that the MAP from our own function is the same as the one computed using `RankingMetrics`:\n",
      "\n",
      "    **Mean Average Precision = 0.07171412913757186**\n",
      "\n",
      "### Note\n",
      "\n",
      "We will not cover cross-validation in this chapter, as we will provide a detailed treatment in the next few chapters. However, note that the same techniques for cross-validation that are explored in the upcoming chapters can be used to evaluate recommendation models, using the performance metrics such as MSE, RMSE, and MAP, which we covered in this section.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we used Spark's MLlib library to train a collaborative filtering recommendation model, and you learned how to use this model to make predictions for the items that a given user might have a preference for. We also used our model to find items that are similar or related to a given item. Finally, we explored common metrics to evaluate the predictive capability of our recommendation model.\n",
      "\n",
      "In the next chapter, you will learn how to use Spark to train a model to classify your data and to use standard evaluation mechanisms to gauge the performance of your model.\n",
      "\n",
      "# Chapter 5. Building a Classification Model with Spark\n",
      "\n",
      "In this chapter, you will learn the basics of classification models and how they can be used in a variety of contexts. Classification generically refers to classifying things into distinct categories or classes. In the case of a classification model, we typically wish to assign classes based on a set of features. The features might represent variables related to an item or object, an event or context, or some combination of these.\n",
      "\n",
      "The simplest form of classification is when we have two classes; this is referred to as binary classification. One of the classes is usually labeled as the positive class (assigned a label of 1), while the other is labeled as the negative class (assigned a label of -1 or, sometimes, 0).\n",
      "\n",
      "A simple example with two classes is shown in the following figure. The input features in this case have two dimensions, and the feature values are represented on the _x_ and _y_ axes in the figure.\n",
      "\n",
      "Our task is to train a model that can classify new data points in this two-dimensional space as either one class (red) or the other (blue).\n",
      "\n",
      "A simple binary classification problem\n",
      "\n",
      "If we have more than two classes, we would refer to multiclass classification, and classes are typically labeled using integer numbers starting at 0 (for example, five different classes would range from label 0 to 4). An example is shown in the following figure. Again, the input features are assumed to be two-dimensional for ease of illustration.\n",
      "\n",
      "A simple multiclass classification problem\n",
      "\n",
      "Classification is a form of supervised learning where we train a model with training examples that include known targets or outcomes of interest (that is, the model is supervised with these example outcomes). Classification models can be used in many situations, but a few common examples include:\n",
      "\n",
      "  * Predicting the probability of Internet users clicking on an online advert; here, the classes are binary in nature (that is, click or no click)\n",
      "  * Detecting fraud; again, in this case, the classes are commonly binary (fraud or no fraud)\n",
      "  * Predicting defaults on loans (binary)\n",
      "  * Classifying images, video, or sounds (most often multiclass, with potentially very many different classes)\n",
      "  * Assigning categories or tags to news articles, web pages, or other content (multiclass)\n",
      "  * Discovering e-mail and web spam, network intrusions, and other malicious behavior (binary or multiclass)\n",
      "  * Detecting failure situations, for example in computer systems or networks\n",
      "  * Ranking customers or users in order of probability that they might purchase a product or use a service (this can be framed as classification by predicting probabilities and then ranking in the descending order)\n",
      "  * Predicting customers or users who might stop using a product, service, or provider (called churn)\n",
      "\n",
      "These are just a few possible use cases. In fact, it is probably safe to say that classification is one of the most widely used machine learning and statistical techniques in modern businesses and especially online businesses.\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Discuss the types of classification models available in MLlib\n",
      "  * Use Spark to extract the appropriate features from raw input data\n",
      "  * Train a number of classification models using MLlib\n",
      "  * Make predictions with our classification models\n",
      "  * Apply a number of standard evaluation techniques to assess the predictive performance of our models\n",
      "  * Illustrate how to improve model performance using some of the feature-extraction approaches from Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_\n",
      "  * Explore the impact of parameter tuning on model performance and learn how to use cross-validation to select the most optimal model parameters\n",
      "\n",
      "# Types of classification models\n",
      "\n",
      "We will explore three common classification models available in Spark: linear models, decision trees, and naive Bayes models. Linear models, while less complex, are relatively easier to scale to very large datasets. Decision tree is a powerful nonlinear technique that can be a little more difficult to scale up (fortunately, MLlib takes care of this for us!) and more computationally intensive to train, but delivers leading performance in many situations. Naive Bayes models are more simple but are easy to train efficiently and parallelize (in fact, they require only one pass over the dataset). They can also give reasonable performance in many cases when appropriate feature engineering is used. A naive Bayes model also provides a good baseline model against which we can measure the performance of other models.\n",
      "\n",
      "Currently, Spark's MLlib library supports binary classification for linear models, decision trees, and naive Bayes models and multiclass classification for decision trees and naive Bayes models. In this book, for simplicity in illustrating the examples, we will focus on the binary case.\n",
      "\n",
      "## Linear models\n",
      "\n",
      "The core idea of linear models (or generalized linear models) is that we model the predicted outcome of interest (often called the target or dependent variable) as a function of a simple linear predictor applied to the input variables (also referred to as features or independent variables).\n",
      "\n",
      "    y = f(wTx)\n",
      "\n",
      "Here, _y_ is the target variable, _w_ is the vector of parameters (known as the weight vector), and _x_ is the vector of input features.\n",
      "\n",
      " _w Tx_ is the linear predictor (or vector dot product) of the weight vector _w_ and feature vector _x_. To this linear predictor, we applied a function _f_ (called the link function).\n",
      "\n",
      "Linear models can, in fact, be used for both classification and regression, simply by changing the link function. Standard linear regression (covered in the next chapter) uses an identity link (that is, _y = w Tx_ directly), while binary classification uses alternative link functions as discussed here.\n",
      "\n",
      "Let's take a look at the example of online advertising. In this case, the target variable would be 0 (often assigned the class label of -1 in mathematical treatments) if no click was observed for a given advert displayed on a web page (called an impression). The target variable would be 1 if a click occurred. The feature vector for each impression would consist of variables related to the impression event (such as features relating to the user, web page, advert and advertiser, and various other factors relating to the context of the event, such as the type of device used, time of the day, and geolocation).\n",
      "\n",
      "Thus, we would like to find a model that maps a given input feature vector (advert impression) to a predicted outcome (click or not). To make a prediction for a new data point, we will take the new feature vector (which is unseen, and hence, we do not know what the target variable is) and compute the dot product with our weight vector. We will then apply the relevant link function, and the result is our predicted outcome (after applying a threshold to the prediction, in the case of some models).\n",
      "\n",
      "Given a set of input data in the form of feature vectors and target variables, we would like to find the weight vector that is the best fit for the data, in the sense that we minimize some error between what our model predicts and the actual outcomes observed. This process is called **model** **fitting** , **training** , or **optimization**.\n",
      "\n",
      "More formally, we seek to find the weight vector that minimizes the sum, over all the training examples, of the loss (or error) computed from some loss function. The loss function takes the weight vector, feature vector, and the actual outcome for a given training example as input and outputs the loss. In fact, the loss function itself is effectively specified by the link function; hence, for a given type of classification or regression (that is, a given link function), there is a corresponding loss function.\n",
      "\n",
      "### Tip\n",
      "\n",
      "For further details on linear models and loss functions, see the linear methods section related to binary classification in the _Spark Programming Guide_ at <http://spark.apache.org/docs/latest/mllib-linear-methods.html#binary-classification>.Also, see the Wikipedia entry for generalized linear models at <http://en.wikipedia.org/wiki/Generalized_linear_model>.\n",
      "\n",
      "While a detailed treatment of linear models and loss functions is beyond the scope of this book, MLlib provides two loss functions suitable to binary classification (you can learn more about them from the Spark documentation). The first one is logistic loss, which equates to a model known as **logistic regression** , while the second one is the hinge loss, which is equivalent to a linear **Support Vector Machine** ( **SVM** ). Note that the SVM does not strictly fall into the statistical framework of generalized linear models but can be used in the same way as it essentially specifies a loss and link function.\n",
      "\n",
      "In the following image, we show the logistic loss and hinge loss relative to the actual zero-one loss. The zero-one loss is the true loss for binary classification--it is either zero if the model predicts correctly or one if the model predicts incorrectly. The reason it is not actually used is that it is not a differentiable loss function, so it is not possible to easily compute a gradient and, thus, very difficult to optimize.\n",
      "\n",
      "The other loss functions are approximations to the zero-one loss that make optimization possible.\n",
      "\n",
      "The logistic, hinge and zero-one loss functions\n",
      "\n",
      "### Note\n",
      "\n",
      "The preceding loss diagram is adapted from the scikit-learn example at <http://scikit-learn.org/stable/auto_examples/linear_model/plot_sgd_loss_functions.html>.\n",
      "\n",
      "### Logistic regression\n",
      "\n",
      "Logistic regression is a probabilistic model--that is, its predictions are bounded between 0 and 1, and for binary classification equate to the model's estimate of the probability of the data point belonging to the positive class. Logistic regression is one of the most widely used linear classification models.\n",
      "\n",
      "As mentioned earlier, the link function used in logistic regression is the logit link:\n",
      "\n",
      "    1 / (1 + exp(-wTx))\n",
      "\n",
      "The related loss function for logistic regression is the logistic loss:\n",
      "\n",
      "    log(1 + exp(-ywTx))\n",
      "\n",
      "Here, _y_ is the actual target variable (either _1_ for the positive class or _-1_ for the negative class).\n",
      "\n",
      "### Linear support vector machines\n",
      "\n",
      "SVM is a powerful and popular technique for regression and classification. Unlike logistic regression, it is not a probabilistic model but predicts classes based on whether the model evaluation is positive or negative.\n",
      "\n",
      "The SVM link function is the identity link, so the predicted outcome is:\n",
      "\n",
      "    y = wTx\n",
      "\n",
      "Hence, if the evaluation of _w Tx_ is greater than or equal to a threshold of 0, the SVM will assign the data point to class 1; otherwise, the SVM will assign it to class 0 (this threshold is a model parameter of SVM and can be adjusted).\n",
      "\n",
      "The loss function for SVM is known as the **hinge loss** and is defined as:\n",
      "\n",
      "    max(0, 1 - ywTx)\n",
      "\n",
      "SVM is a maximum margin classifier--it tries to find a weight vector such that the classes are separated as much as possible. It has been shown to perform well on many classification tasks, and the linear variant can scale to very large datasets.\n",
      "\n",
      "### Note\n",
      "\n",
      "SVMs have a large amount of theory behind them, which is beyond the scope of this book, but you can visit <http://en.wikipedia.org/wiki/Support_vector_machine> and <http://www.support-vector-machines.org/> for more details.\n",
      "\n",
      "In the following image, we have plotted the different decision functions for logistic regression (the blue line) and linear SVM (the red line), based on the simple binary classification example explained earlier.\n",
      "\n",
      "You can see that the SVM effectively focuses on the points that lie closest to the decision function (the margin lines are shown with red dashes):\n",
      "\n",
      "Decision functions for logistic regression and linear SVM for binary classification\n",
      "\n",
      "## The naive Bayes model\n",
      "\n",
      "Naive Bayes is a probabilistic model that makes predictions by computing the probability of a data point that belongs to a given class. A naive Bayes model assumes that each feature makes an independent contribution to the probability assigned to a class (it assumes conditional independence between features).\n",
      "\n",
      "Due to this assumption, the probability of each class becomes a function of the product of the probability of a feature occurring, given the class, as well as the probability of this class. This makes training the model tractable and relatively straightforward. The class prior probabilities and feature conditional probabilities are all estimated from the frequencies present in the dataset. Classification is performed by selecting the most probable class, given the features and class probabilities.\n",
      "\n",
      "An assumption is also made about the feature distributions (the parameters of which are estimated from the data). MLlib implements multinomial naive Bayes that assumes that the feature distribution is a multinomial distribution that represents non-negative frequency counts of the features.\n",
      "\n",
      "It is suitable for binary features (for example, _1-of-k_ encoded categorical features) and is commonly used for text and document classification (where, as we have seen in Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ , the bag-of-words vector is a typical feature representation).\n",
      "\n",
      "### Note\n",
      "\n",
      "Take a look at the _MLlib - Naive Bayes_ section in the Spark documentation at <http://spark.apache.org/docs/latest/mllib-naive-bayes.html> for more information.\n",
      "\n",
      "The Wikipedia page at <http://en.wikipedia.org/wiki/Naive_Bayes_classifier> has a more detailed explanation of the mathematical formulation.\n",
      "\n",
      "Here, we have shown the decision function of naive Bayes on our simple binary classification example:\n",
      "\n",
      "Decision function of naive Bayes for binary classification\n",
      "\n",
      "## Decision trees\n",
      "\n",
      "Decision tree model is a powerful, nonprobabilistic technique that can capture more complex nonlinear patterns and feature interactions. They have been shown to perform well on many tasks, are relatively easy to understand and interpret, can handle categorical and numerical features, and do not require input data to be scaled or standardized. They are well suited to be included in ensemble methods (for example, ensembles of decision tree models, which are called decision forests).\n",
      "\n",
      "The decision tree model constructs a tree where the leaves represent a class assignment to class 0 or 1, and the branches are a set of features. In the following figure, we show a simple decision tree where the binary outcome is **Stay at home** or **Go to the beach**. The features are the weather outside.\n",
      "\n",
      "A simple decision tree\n",
      "\n",
      "The decision tree algorithm is a top-down approach that begins at a root node (or feature), and then selects a feature at each step that gives the best split of the dataset, as measured by the information gain of this split. The information gain is computed from the node impurity (which is the extent to which the labels at the node are similar, or homogenous) minus the weighted sum of the impurities for the two child nodes that would be created by the split. For classification tasks, there are two measures that can be used to select the best split. These are Gini impurity and entropy.\n",
      "\n",
      "### Note\n",
      "\n",
      "See the _MLlib - Decision Tree_ section in the _Spark Programming Guide_ at <http://spark.apache.org/docs/latest/mllib-decision-tree.html> for further details on the decision tree algorithm and impurity measures for classification.\n",
      "\n",
      "In the following screenshot, we have plotted the decision boundary for the decision tree model, as we did for the other models earlier. We can see that the decision tree is able to fit complex, nonlinear models.\n",
      "\n",
      "Decision function for a decision tree for binary classification\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "You might recall from Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ that the majority of machine learning models operate on numerical data in the form of feature vectors. In addition, for supervised learning methods such as classification and regression, we need to provide the target variable (or variables in the case of multiclass situations) together with the feature vector.\n",
      "\n",
      "Classification models in MLlib operate on instances of `LabeledPoint`, which is a wrapper around the target variable (called the **label** ) and the **feature vector** :\n",
      "\n",
      "    case class LabeledPoint(label: Double, features: Vector)\n",
      "\n",
      "While in most examples of using classification, you will come across existing datasets that are already in the vector format, in practice, you will usually start with raw data that needs to be transformed into features. As we have already seen, this can involve preprocessing and transformation, such as binning numerical features, scaling and normalizing features, and using _1-of-k_ encodings for categorical features.\n",
      "\n",
      "## Extracting features from the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "In this chapter, we will use a different dataset from the one we used for our recommendation model, as the MovieLens data doesn't have much for us to work with in terms of a classification problem. We will use a dataset from a competition on Kaggle. The dataset was provided by StumbleUpon, and the problem relates to classifying whether a given web page is ephemeral (that is, short lived and will cease being popular soon) or evergreen (that is, persistently popular) on their web content recommendation pages.\n",
      "\n",
      "### Note\n",
      "\n",
      "The dataset used here can be downloaded from <http://www.kaggle.com/c/stumbleupon/data>.\n",
      "\n",
      "Download the training data (`train.tsv`)--you will need to accept the terms and conditions before downloading the dataset.\n",
      "\n",
      "You can find more information about the competition at <http://www.kaggle.com/c/stumbleupon>.\n",
      "\n",
      "Before we begin, it will be easier for us to work with the data in Spark if we remove the column name header from the first line of the file. Change to the directory in which you downloaded the data (referred to as `PATH` here) and run the following command to remove the first line and pipe the result to a new file called `train_noheader.tsv`:\n",
      "\n",
      "    **> sed 1d train.tsv > train_noheader.tsv**\n",
      "\n",
      "Now, we are ready to start up our Spark shell (remember to run this command from your Spark installation directory):\n",
      "\n",
      "    **>./bin/spark-shell --driver-memory 4g**\n",
      "\n",
      "You can type in the code that follows for the remainder of this chapter directly into your Spark shell.\n",
      "\n",
      "In a manner similar to what we did in the earlier chapters, we will load the raw training data into an RDD and inspect it:\n",
      "\n",
      "    val rawData = sc.textFile(\"/ **PATH** /train_noheader.tsv\")\n",
      "    val records = rawData.map(line => line.split(\"\\t\"))\n",
      "    records.first()\n",
      "\n",
      "You will the following on the screen:\n",
      "\n",
      "    **Array[String] = Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", ...**\n",
      "\n",
      "You can check the fields that are available by reading through the overview on the dataset page above. The first two columns contain the URL and ID of the page. The next column contains some raw textual content. The next column contains the category assigned to the page. The next 22 columns contain numeric or categorical features of various kinds. The final column contains the target--1 is evergreen, while 0 is non-evergreen.\n",
      "\n",
      "We'll start off with a simple approach of using only the available numeric features directly. As each categorical variable is binary, we already have a _1-of-k_ encoding for these variables, so we don't need to do any further feature extraction.\n",
      "\n",
      "Due to the way the data is formatted, we will have to do a bit of data cleaning during our initial processing by trimming out the extra quotation characters (`\"`). There are also missing values in the dataset; they are denoted by the `\"?\"` character. In this case, we will simply assign a zero value to these missing values:\n",
      "\n",
      "    import org.apache.spark.mllib.regression.LabeledPoint\n",
      "    import org.apache.spark.mllib.linalg.Vectors\n",
      "    val data = records.map { r =>\n",
      "      val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
      "      val label = trimmed(r.size - 1).toInt\n",
      "      val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
      "      LabeledPoint(label, Vectors.dense(features))\n",
      "    }\n",
      "\n",
      "In the preceding code, we extracted the label variable from the last column and an array of features for columns 5 to 25 after cleaning and dealing with missing values. We converted the label to an `Int` value and the features to an `Array[Double]`. Finally, we wrapped the label and features in a `LabeledPoint` instance, converting the features into an MLlib `Vector`.\n",
      "\n",
      "We will also cache the data and count the number of data points:\n",
      "\n",
      "    data.cache\n",
      "    val numData = data.count\n",
      "\n",
      "You will see that the value of `numData` is 7395.\n",
      "\n",
      "We will explore the dataset in more detail a little later, but we will tell you now that there are some negative feature values in the numeric data. As we saw earlier, the naive Bayes model requires non-negative features and will throw an error if it encounters negative values. So, for now, we will create a version of our input feature vectors for the naive Bayes model by setting any negative feature values to zero:\n",
      "\n",
      "    val nbData = records.map { r =>\n",
      "      val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
      "      val label = trimmed(r.size - 1).toInt\n",
      "      val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d)\n",
      "      LabeledPoint(label, Vectors.dense(features))\n",
      "    }\n",
      "\n",
      "# Training classification models\n",
      "\n",
      "Now that we have extracted some basic features from our dataset and created our input RDD, we are ready to train a number of models. To compare the performance and use of different models, we will train a model using logistic regression, SVM, naive Bayes, and a decision tree. You will notice that training each model looks nearly identical, although each has its own specific model parameters that can be set. MLlib sets sensible defaults in most cases, but in practice, the best parameter setting should be selected using evaluation techniques, which we will cover later in this chapter.\n",
      "\n",
      "## Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "We can now apply the models from MLlib to our input data. First, we need to import the required classes and set up some minimal input parameters for each model. For logistic regression and SVM, this is the number of iterations, while for the decision tree model, it is the maximum tree depth:\n",
      "\n",
      "    import org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n",
      "    import org.apache.spark.mllib.classification.SVMWithSGD\n",
      "    import org.apache.spark.mllib.classification.NaiveBayes\n",
      "    import org.apache.spark.mllib.tree.DecisionTree\n",
      "    import org.apache.spark.mllib.tree.configuration.Algo\n",
      "    import org.apache.spark.mllib.tree.impurity.Entropy \n",
      "    val numIterations = 10\n",
      "    val maxTreeDepth = 5\n",
      "\n",
      "Now, train each model in turn. First, we will train logistic regression:\n",
      "\n",
      "    val lrModel = LogisticRegressionWithSGD.train(data, numIterations)\n",
      "\n",
      "    **...**\n",
      "    **14/12/06 13:41:47 INFO DAGScheduler: Job 81 finished: reduce at RDDFunctions.scala:112, took 0.011968 s**\n",
      "    **14/12/06 13:41:47 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 0.6931471805599474, 1196521.395699124, Infinity, 1861127.002201189, Infinity, 2639638.049627607, Infinity, Infinity, Infinity, Infinity**\n",
      "    **lrModel: org.apache.spark.mllib.classification.LogisticRegressionModel = (weights=[-0.11372778986947886,-0.511619752777837,**\n",
      "    **...**\n",
      "\n",
      "Next up, we will train an SVM model:\n",
      "\n",
      "    val svmModel = SVMWithSGD.train(data, numIterations)\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **...**\n",
      "    **14/12/06 13:43:08 INFO DAGScheduler: Job 94 finished: reduce at RDDFunctions.scala:112, took 0.007192 s**\n",
      "    **14/12/06 13:43:08 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 1.0, 2398226.619666797, 2196192.9647478117, 3057987.2024311484, 271452.9038284356, 3158131.191895948, 1041799.350498323, 1507522.941537049, 1754560.9909073508, 136866.76745605646**\n",
      "    **svmModel: org.apache.spark.mllib.classification.SVMModel = (weights=[-0.12218838697834929,-0.5275107581589767,**\n",
      "    **...**\n",
      "\n",
      "Then, we will train the naive Bayes model; remember to use your special non-negative feature dataset:\n",
      "\n",
      "    val nbModel = NaiveBayes.train(nbData)\n",
      "\n",
      "The following is the output:\n",
      "\n",
      "    **...**\n",
      "    **14/12/06 13:44:48 INFO DAGScheduler: Job 95 finished: collect at NaiveBayes.scala:120, took 0.441273 s**\n",
      "    **nbModel: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@666ac612**\n",
      "    **...**\n",
      "\n",
      "Finally, we will train our decision tree:\n",
      "\n",
      "    val dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **...**\n",
      "    **14/12/06 13:46:03 INFO DAGScheduler: Job 104 finished: collectAsMap at DecisionTree.scala:653, took 0.031338 s**\n",
      "    **...**\n",
      "    **total: 0.343024**\n",
      "    **findSplitsBins: 0.119499**\n",
      "    **findBestSplits: 0.200352**\n",
      "    **chooseSplits: 0.199705**\n",
      "    **dtModel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 5 with 61 nodes**\n",
      "    **...**\n",
      "\n",
      "Notice that we set the mode, or `Algo`, of the decision tree to `Classification`, and we used the `Entropy` impurity measure.\n",
      "\n",
      "# Using classification models\n",
      "\n",
      "We now have four models trained on our input labels and features. We will now see how to use these models to make predictions on our dataset. For now, we will use the same training data to illustrate the `predict` method of each model.\n",
      "\n",
      "## Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "\n",
      "We will use our logistic regression model as an example (the other models are used in the same way):\n",
      "\n",
      "    val dataPoint = data.first\n",
      "    val prediction = lrModel.predict(dataPoint.features)\n",
      "\n",
      "The following is the output:\n",
      "\n",
      "    **prediction: Double = 1.0**\n",
      "\n",
      "We saw that for the first data point in our training dataset, the model predicted a label of `1` (that is, evergreen). Let's examine the true label for this data point:\n",
      "\n",
      "    val trueLabel = dataPoint.label\n",
      "\n",
      "You can see the following output:\n",
      "\n",
      "    **trueLabel: Double = 0.0**\n",
      "\n",
      "So, in this case, our model got it wrong!\n",
      "\n",
      "We can also make predictions in bulk by passing in an `RDD[Vector]` as input:\n",
      "\n",
      "    val predictions = lrModel.predict(data.map(lp => lp.features))\n",
      "    predictions.take(5)\n",
      "\n",
      "The following is the output:\n",
      "\n",
      "    **Array[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0)**\n",
      "\n",
      "# Evaluating the performance of classification models\n",
      "\n",
      "When we make predictions using our model, as we did earlier, how do we know whether the predictions are good or not? We need to be able to evaluate how well our model performs. Evaluation metrics commonly used in binary classification include prediction accuracy and error, precision and recall, and area under the precision-recall curve, the **receiver operating characteristic** ( **ROC** ) curve, **area under ROC curve** ( **AUC** ), and F-measure.\n",
      "\n",
      "## Accuracy and prediction error\n",
      "\n",
      "The prediction error for binary classification is possibly the simplest measure available. It is the number of training examples that are misclassified, divided by the total number of examples. Similarly, accuracy is the number of correctly classified examples divided by the total examples.\n",
      "\n",
      "We can calculate the accuracy of our models in our training data by making predictions on each input feature and comparing them to the true label. We will sum up the number of correctly classified instances and divide this by the total number of data points to get the average classification accuracy:\n",
      "\n",
      "    val lrTotalCorrect = data.map { point =>\n",
      "      if (lrModel.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum \n",
      "    val lrAccuracy = lrTotalCorrect / data.count\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **lrAccuracy: Double = 0.5146720757268425**\n",
      "\n",
      "This gives us 51.5 percent accuracy, which doesn't look particularly impressive! Our model got only half of the training examples correct, which seems to be about as good as a random chance.\n",
      "\n",
      "### Note\n",
      "\n",
      "Note that the predictions made by the model are not naturally exactly 1 or 0. The output is usually a real number that must be turned into a class prediction. This is done through use of a threshold in the classifier's decision or scoring function.\n",
      "\n",
      "For example, binary logistic regression is a probabilistic model that returns the estimated probability of class 1 in its scoring function. Thus, a decision threshold of 0.5 is typical. That is, if the estimated probability of being in class 1 is higher than 50 percent, the model decides to classify the point as class 1; otherwise, it will be classified as class 0.\n",
      "\n",
      "Note that the threshold itself is effectively a model parameter that can be tuned in some models. It also plays a role in evaluation measures, as we will see now.\n",
      "\n",
      "What about the other models? Let's compute the accuracy for the other three:\n",
      "\n",
      "    val svmTotalCorrect = data.map { point =>\n",
      "      if (svmModel.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum\n",
      "    val nbTotalCorrect = nbData.map { point =>\n",
      "      if (nbModel.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum\n",
      "\n",
      "Note that the decision tree prediction threshold needs to be specified explicitly, as highlighted here:\n",
      "\n",
      "    val dtTotalCorrect = data.map { point =>\n",
      "      val score = dtModel.predict(point.features)\n",
      "      val predicted = if ( **score > 0.5**) 1 else 0 \n",
      "      if (predicted == point.label) 1 else 0\n",
      "    }.sum\n",
      "\n",
      "We can now inspect the accuracy for the other three models.\n",
      "\n",
      "First, the SVM model:\n",
      "\n",
      "    val svmAccuracy = svmTotalCorrect / numData\n",
      "\n",
      "Here is the output for the SVM model:\n",
      "\n",
      "    **svmAccuracy: Double = 0.5146720757268425**\n",
      "\n",
      "Next, our naive Bayes model:\n",
      "\n",
      "    val nbAccuracy = nbTotalCorrect / numData\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **nbAccuracy: Double = 0.5803921568627451**\n",
      "\n",
      "Finally, we compute the accuracy for the decision tree:\n",
      "\n",
      "    val dtAccuracy = dtTotalCorrect / numData\n",
      "\n",
      "And, the output is:\n",
      "\n",
      "    **dtAccuracy: Double = 0.6482758620689655**\n",
      "\n",
      "We can see that both SVM and naive Bayes also performed quite poorly. The decision tree model is better with 65 percent accuracy, but this is still not particularly high.\n",
      "\n",
      "## Precision and recall\n",
      "\n",
      "In information retrieval, precision is a commonly used measure of the quality of the results, while recall is a measure of the completeness of the results.\n",
      "\n",
      "In the binary classification context, precision is defined as the number of true positives (that is, the number of examples correctly predicted as class 1) divided by the sum of true positives and false positives (that is, the number of examples that were incorrectly predicted as class 1). Thus, we can see that a precision of 1.0 (or 100 percent) is achieved if every example predicted by the classifier to be class 1 is, in fact, in class 1 (that is, there are no false positives).\n",
      "\n",
      "Recall is defined as the number of true positives divided by the sum of true positives and false negatives (that is, the number of examples that were in class 1, but were predicted as class 0 by the model). We can see that a recall of 1.0 (or 100 percent) is achieved if the model doesn't miss any examples that were in class 1 (that is, there are no false negatives).\n",
      "\n",
      "Generally, precision and recall are inversely related; often, higher precision is related to lower recall and vice versa. To illustrate this, assume that we built a model that always predicted class 1. In this case, the model predictions would have no false negatives because the model always predicts 1; it will not miss any of class 1. Thus, the recall will be 1.0 for this model. On the other hand, the false positive rate could be very high, meaning precision would be low (this depends on the exact distribution of the classes in the dataset).\n",
      "\n",
      "Precision and recall are not particularly useful as standalone metrics, but are typically used together to form an aggregate or averaged metric. Precision and recall are also dependent on the threshold selected for the model.\n",
      "\n",
      "Intuitively, below some threshold level, a model will always predict class 1. Hence, it will have a recall of 1, but most likely, it will have low precision. At a high enough threshold, the model will always predict class 0. The model will then have a recall of 0, since it cannot achieve any true positives and will likely have many false negatives. Furthermore, its precision score will be undefined, as it will achieve zero true positives and zero false positives.\n",
      "\n",
      "The **precision-recall** ( **PR** ) curve shown in the following figure plots precision against recall outcomes for a given model, as the decision threshold of the classifier is changed. The area under this PR curve is referred to as the average precision. Intuitively, an area under the PR curve of 1.0 will equate to a perfect classifier that will achieve 100 percent in both precision and recall.\n",
      "\n",
      "Precision-recall curve\n",
      "\n",
      "### Tip\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Precision_and_recall> and <http://en.wikipedia.org/wiki/Average_precision#Average_precision> for more details on precision, recall, and area under the PR curve.\n",
      "\n",
      "## ROC curve and AUC\n",
      "\n",
      "The **ROC** curve is a concept similar to the PR curve. It is a graphical illustration of the true positive rate against the false positive rate for a classifier.\n",
      "\n",
      "The **true positive rate** ( **TPR** ) is the number of true positives divided by the sum of true positives and false negatives. In other words, it is the ratio of true positives to all positive examples. This is the same as the recall we saw earlier and is also commonly referred to as sensitivity.\n",
      "\n",
      "The **false positive rate** ( **FPR** ) is the number of false positives divided by the sum of false positives and **true negatives** (that is, the number of examples correctly predicted as class 0). In other words, it is the ratio of false positives to all negative examples.\n",
      "\n",
      "In a manner similar to precision and recall, the ROC curve (plotted in the following figure) represents the classifier's performance tradeoff of TPR against FPR, for different decision thresholds. Each point on the curve represents a different threshold in the decision function for the classifier.\n",
      "\n",
      "The ROC curve\n",
      "\n",
      "The area under the ROC curve (commonly referred to as AUC) represents an average value. Again, an AUC of 1.0 will represent a perfect classifier. An area of 0.5 is referred to as the random score. Thus, a model that achieves an AUC of 0.5 is no better than randomly guessing.\n",
      "\n",
      "### Note\n",
      "\n",
      "As both the area under the PR curve and the area under the ROC curve are effectively normalized (with a minimum of 0 and maximum of 1), we can use these measures to compare models with differing parameter settings and even compare completely different models. Thus, these metrics are popular for model evaluation and selection purposes.\n",
      "\n",
      "MLlib comes with a set of built-in routines to compute the area under the PR and ROC curves for binary classification. Here, we will compute these metrics for each of our models:\n",
      "\n",
      "    import org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\n",
      "    val metrics = Seq(lrModel, svmModel).map { model => \n",
      "      val scoreAndLabels = data.map { point =>\n",
      "        (model.predict(point.features), point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
      "    }\n",
      "\n",
      "As we did previously to train the naive Bayes model and computing accuracy, we need to use the special `nbData` version of the dataset that we created to compute the classification metrics:\n",
      "\n",
      "    val nbMetrics = Seq(nbModel).map{ model =>\n",
      "      val scoreAndLabels = nbData.map { point =>\n",
      "        val score = model.predict(point.features)\n",
      "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
      "    }\n",
      "\n",
      "Note that because the `DecisionTreeModel` model does not implement the `ClassificationModel` interface that is implemented by the other three models, we need to compute the results separately for this model in the following code:\n",
      "\n",
      "    val dtMetrics = Seq(dtModel).map{ model =>\n",
      "      val scoreAndLabels = data.map { point =>\n",
      "        val score = model.predict(point.features)\n",
      "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC)\n",
      "    }\n",
      "    val allMetrics = metrics ++ nbMetrics ++ dtMetrics\n",
      "    allMetrics.foreach{ case (m, pr, roc) => \n",
      "      println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under ROC: ${roc * 100.0}%2.4f%%\") \n",
      "    }\n",
      "\n",
      "Your output will look similar to the one here:\n",
      "\n",
      "    **LogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%**\n",
      "    **SVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%**\n",
      "    **NaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%**\n",
      "    **DecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%**\n",
      "\n",
      "We can see that all models achieve broadly similar results for the average precision metric.\n",
      "\n",
      "Logistic regression and SVM achieve results of around 0.5 for AUC. This indicates that they do no better than random chance! Our naive Bayes and decision tree models fare a little better, achieving an AUC of 0.58 and 0.65, respectively. Still, this is not a very good result in terms of binary classification performance.\n",
      "\n",
      "### Note\n",
      "\n",
      "While we don't cover multiclass classification here, MLlib provides a similar evaluation class called `MulticlassMetrics`, which provides averaged versions of many common metrics.\n",
      "\n",
      "# Improving model performance and tuning parameters\n",
      "\n",
      "So, what went wrong? Why have our sophisticated models achieved nothing better than random chance? Is there a problem with our models?\n",
      "\n",
      "Recall that we started out by just throwing the data at our model. In fact, we didn't even throw all our data at the model, just the numeric columns that were easy to use. Furthermore, we didn't do a lot of analysis on these numeric features.\n",
      "\n",
      "## Feature standardization\n",
      "\n",
      "Many models that we employ make inherent assumptions about the distribution or scale of input data. One of the most common forms of assumption is about normally-distributed features. Let's take a deeper look at the distribution of our features.\n",
      "\n",
      "To do this, we can represent the feature vectors as a distributed matrix in MLlib, using the `RowMatrix` class. `RowMatrix` is an RDD made up of vector, where each vector is a row of our matrix.\n",
      "\n",
      "The `RowMatrix` class comes with some useful methods to operate on the matrix, one of which is a utility to compute statistics on the columns of the matrix:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "    val vectors = data.map(lp => lp.features)\n",
      "    val matrix = new RowMatrix(vectors)\n",
      "    val matrixSummary = matrix.computeColumnSummaryStatistics()\n",
      "\n",
      "The following code statement will print the mean of the matrix:\n",
      "\n",
      "    println(matrixSummary.mean)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **[0.41225805299526636,2.761823191986623,0.46823047328614004, ...**\n",
      "\n",
      "The following code statement will print the minimum value of the matrix:\n",
      "\n",
      "    println(matrixSummary.min)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0, ...**\n",
      "\n",
      "The following code statement will print the maximum value of the matrix:\n",
      "\n",
      "    println(matrixSummary.max)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444, ...**\n",
      "\n",
      "The following code statement will print the variance of the matrix:\n",
      "\n",
      "    println(matrixSummary.variance)\n",
      "\n",
      "The output of the variance is:\n",
      "\n",
      "    **[0.1097424416755897,74.30082476809638,0.04126316989120246, ...**\n",
      "\n",
      "The following code statement will print the nonzero number of the matrix:\n",
      "\n",
      "    println(matrixSummary.numNonzeros)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0, ...**\n",
      "\n",
      "The `computeColumnSummaryStatistics` method computes a number of statistics over each column of features, including the mean and variance, storing each of these in a `Vector` with one entry per column (that is, one entry per feature in our case).\n",
      "\n",
      "Looking at the preceding output for mean and variance, we can see quite clearly that the second feature has a much higher mean and variance than some of the other features (you will find a few other features that are similar and a few others that are more extreme). So, our data definitely does not conform to a standard Gaussian distribution in its raw form. To get the data in a more suitable form for our models, we can standardize each feature such that it has zero mean and unit standard deviation. We can do this by subtracting the column mean from each feature value and then scaling it by dividing it by the column standard deviation for the feature:\n",
      "\n",
      "    (x - μ) / sqrt(variance)\n",
      "\n",
      "Practically, for each feature vector in our input dataset, we can simply perform an element-wise subtraction of the preceding mean vector from the feature vector and then perform an element-wise division of the feature vector by the vector of feature standard deviations. The standard deviation vector itself can be obtained by performing an element-wise square root operation on the variance vector.\n",
      "\n",
      "As we mentioned in Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ , we fortunately have access to a convenience method from Spark's `StandardScaler` to accomplish this.\n",
      "\n",
      "`StandardScaler` works in much the same way as the `Normalizer` feature we used in that chapter. We will instantiate it by passing in two arguments that tell it whether to subtract the mean from the data and whether to apply standard deviation scaling. We will then fit `StandardScaler` on our input `vectors`. Finally, we will pass in an input vector to the `transform` function, which will then return a normalized vector. We will do this within the following `map` function to preserve the `label` from our dataset:\n",
      "\n",
      "    import org.apache.spark.mllib.feature.StandardScaler\n",
      "    val scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors)\n",
      "    val scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features)))\n",
      "\n",
      "Our data should now be standardized. Let's inspect the first row of the original and standardized features:\n",
      "\n",
      "    println(data.first.features)\n",
      "\n",
      "The output of the preceding line of code is as follows:\n",
      "\n",
      "    **[0.789131,2.055555556,0.676470588,0.205882353,**\n",
      "\n",
      "The following code will the first row of the standardized features:\n",
      "\n",
      "    println(scaledData.first.features)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **[1.1376439023494747,-0.08193556218743517,1.025134766284205,-0.0558631837375738,**\n",
      "\n",
      "As we can see, the first feature has been transformed by applying the standardization formula. We can check this by subtracting the mean (which we computed earlier) from the first feature and dividing the result by the square root of the variance (which we computed earlier):\n",
      "\n",
      "    println((0.789131 - 0.41225805299526636)/ math.sqrt(0.1097424416755897))\n",
      "\n",
      "The result should be equal to the first element of our scaled vector:\n",
      "\n",
      "    **1.137647336497682**\n",
      "\n",
      "We can now retrain our model using the standardized data. We will use only the logistic regression model to illustrate the impact of feature standardization (since the decision tree and naive Bayes are not impacted by this):\n",
      "\n",
      "    val lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations)\n",
      "    val lrTotalCorrectScaled = scaledData.map { point =>\n",
      "      if (lrModelScaled.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum\n",
      "    val lrAccuracyScaled = lrTotalCorrectScaled / numData\n",
      "    val lrPredictionsVsTrue = scaledData.map { point => \n",
      "      (lrModelScaled.predict(point.features), point.label) \n",
      "    }\n",
      "    val lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue)\n",
      "    val lrPr = lrMetricsScaled.areaUnderPR\n",
      "    val lrRoc = lrMetricsScaled.areaUnderROC\n",
      "    println(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")\n",
      "\n",
      "The result should look similar to this:\n",
      "\n",
      "    **LogisticRegressionModel**\n",
      "    **Accuracy: 62.0419%**\n",
      "    **Area under PR: 72.7254%**\n",
      "    **Area under ROC: 61.9663%**\n",
      "\n",
      "Simply through standardizing our features, we have improved the logistic regression performance for accuracy and AUC from 50 percent, no better than random, to 62 percent.\n",
      "\n",
      "## Additional features\n",
      "\n",
      "We have seen that we need to be careful about standardizing and potentially normalizing our features, and the impact on model performance can be serious. In this case, we used only a portion of the features available. For example, we completely ignored the category variable and the textual content in the boilerplate variable column.\n",
      "\n",
      "This was done for ease of illustration, but let's assess the impact of adding an additional feature such as the category feature.\n",
      "\n",
      "First, we will inspect the categories and form a mapping of index to category, which you might recognize as the basis for a _1-of-k_ encoding of this categorical feature:\n",
      "\n",
      "    val categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap\n",
      "    val numCategories = categories.size\n",
      "    println(categories)\n",
      "\n",
      "The output of the different categories is as follows:\n",
      "\n",
      "    **Map(\"weather\" - > 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)**\n",
      "\n",
      "The following code will print the number of categories:\n",
      "\n",
      "    println(numCategories)\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **14**\n",
      "\n",
      "So, we will need to create a vector of length 14 to represent this feature and assign a value of 1 for the index of the relevant category for each data point. We can then prepend this new feature vector to the vector of other numerical features:\n",
      "\n",
      "    val dataCategories = records.map { r =>\n",
      "      val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
      "      val label = trimmed(r.size - 1).toInt\n",
      "      val categoryIdx = categories(r(3))\n",
      "      val categoryFeatures = Array.ofDim[Double](numCategories)\n",
      "      categoryFeatures(categoryIdx) = 1.0\n",
      "      val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble)\n",
      "      val features = categoryFeatures ++ otherFeatures\n",
      "      LabeledPoint(label, Vectors.dense(features))\n",
      "    }\n",
      "    println(dataCategories.first)\n",
      "\n",
      "You should see output similar to what is shown here. You can see that the first part of our feature vector is now a vector of length 14 with one nonzero entry at the relevant category index:\n",
      "\n",
      "    **LabeledPoint(0.0, [0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])**\n",
      "\n",
      "Again, since our raw features are not standardized, we should perform this transformation using the same `StandardScaler` approach that we used earlier before training a new model on this expanded dataset:\n",
      "\n",
      "    val scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features))\n",
      "    val scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features)))\n",
      "\n",
      "We can inspect the features before and after scaling as we did earlier:\n",
      "\n",
      "    println(dataCategories.first.features)\n",
      "\n",
      "The output is as follows:\n",
      "\n",
      "    **0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556 ...**\n",
      "\n",
      "The following code will print the features after scaling:\n",
      "\n",
      "    println(scaledDataCats.first.features)\n",
      "\n",
      "You will see the following on the screen:\n",
      "\n",
      "    **[-0.023261105535492967,2.720728254208072,-0.4464200056407091,-0.2205258360869135, ...**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that while the original raw features were sparse (that is, there are many entries that are zero), if we subtract the mean from each entry, we would end up with a non-sparse (dense) representation, as can be seen in the preceding example.\n",
      "\n",
      "This is not a problem in this case as the data size is small, but often large-scale real-world problems have extremely sparse input data with many features (online advertising and text classification are good examples). In this case, it is not advisable to lose this sparsity, as the memory and processing requirements for the equivalent dense representation can quickly explode with many millions of features. We can use StandardScaler and set `withMean` to `false` to avoid this.\n",
      "\n",
      "We're now ready to train a new logistic regression model with our expanded feature set, and then we will evaluate the performance:\n",
      "\n",
      "    val lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations)\n",
      "    val lrTotalCorrectScaledCats = scaledDataCats.map { point =>\n",
      "      if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum\n",
      "    val lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData\n",
      "    val lrPredictionsVsTrueCats = scaledDataCats.map { point => \n",
      "      (lrModelScaledCats.predict(point.features), point.label) \n",
      "    }\n",
      "    val lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats)\n",
      "    val lrPrCats = lrMetricsScaledCats.areaUnderPR\n",
      "    val lrRocCats = lrMetricsScaledCats.areaUnderROC\n",
      "    println(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")\n",
      "\n",
      "You should see output similar to this one:\n",
      "\n",
      "    **LogisticRegressionModel**\n",
      "    **Accuracy: 66.5720%**\n",
      "    **Area under PR: 75.7964%**\n",
      "    **Area under ROC: 66.5483%**\n",
      "\n",
      "By applying a feature standardization transformation to our data, we improved both the accuracy and AUC measures from 50 percent to 62 percent, and then, we achieved a further boost to 66 percent by adding the category feature into our model (remember to apply the standardization to our new feature set).\n",
      "\n",
      "### Note\n",
      "\n",
      "Note that the best model performance in the competition was an AUC of 0.88906 (see <http://www.kaggle.com/c/stumbleupon/leaderboard/private>).\n",
      "\n",
      "One approach to achieving performance almost as high is outlined at <http://www.kaggle.com/c/stumbleupon/forums/t/5680/beating-the-benchmark-leaderboard-auc-0-878>.\n",
      "\n",
      "Notice that there are still features that we have not yet used; most notably, the text features in the boilerplate variable. The leading competition submissions predominantly use the boilerplate features and features based on the raw textual content to achieve their performance. As we saw earlier, while adding category-improved performance, it appears that most of the variables are not very useful as predictors, while the textual content turned out to be highly predictive.\n",
      "\n",
      "Going through some of the best performing approaches for these competitions can give you a good idea as to how feature extraction and engineering play a critical role in model performance.\n",
      "\n",
      "## Using the correct form of data\n",
      "\n",
      "Another critical aspect of model performance is using the correct form of data for each model. Previously, we saw that applying a naive Bayes model to our numerical features resulted in very poor performance. Is this because the model itself is deficient?\n",
      "\n",
      "In this case, recall that MLlib implements a multinomial model. This model works on input in the form of non-zero count data. This can include a binary representation of categorical features (such as the _1-of-k_ encoding covered previously) or frequency data (such as the frequency of occurrences of words in a document). The numerical features we used initially do not conform to this assumed input distribution, so it is probably unsurprising that the model did so poorly.\n",
      "\n",
      "To illustrate this, we'll use only the category feature, which, when _1-of-k_ encoded, is of the correct form for the model. We will create a new dataset as follows:\n",
      "\n",
      "    val dataNB = records.map { r =>\n",
      "      val trimmed = r.map(_.replaceAll(\"\\\"\", \"\"))\n",
      "      val label = trimmed(r.size - 1).toInt\n",
      "      val categoryIdx = categories(r(3))\n",
      "      val categoryFeatures = Array.ofDim[Double](numCategories)\n",
      "      categoryFeatures(categoryIdx) = 1.0\n",
      "      LabeledPoint(label, Vectors.dense(categoryFeatures))\n",
      "    }\n",
      "\n",
      "Next, we will train a new naive Bayes model and evaluate its performance:\n",
      "\n",
      "    val nbModelCats = NaiveBayes.train(dataNB)\n",
      "    val nbTotalCorrectCats = dataNB.map { point =>\n",
      "      if (nbModelCats.predict(point.features) == point.label) 1 else 0\n",
      "    }.sum\n",
      "    val nbAccuracyCats = nbTotalCorrectCats / numData\n",
      "    val nbPredictionsVsTrueCats = dataNB.map { point => \n",
      "      (nbModelCats.predict(point.features), point.label) \n",
      "    }\n",
      "    val nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats)\n",
      "    val nbPrCats = nbMetricsCats.areaUnderPR\n",
      "    val nbRocCats = nbMetricsCats.areaUnderROC\n",
      "    println(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\")\n",
      "\n",
      "You should see the following output:\n",
      "\n",
      "    **NaiveBayesModel**\n",
      "    **Accuracy: 60.9601%**\n",
      "    **Area under PR: 74.0522%**\n",
      "    **Area under ROC: 60.5138%**\n",
      "\n",
      "So, by ensuring that we use the correct form of input, we have improved the performance of the naive Bayes model slightly from 58 percent to 60 percent.\n",
      "\n",
      "## Tuning model parameters\n",
      "\n",
      "The previous section showed the impact on model performance of feature extraction and selection, as well as the form of input data and a model's assumptions around data distributions. So far, we have discussed model parameters only in passing, but they also play a significant role in model performance.\n",
      "\n",
      "MLlib's default `train` methods use default values for the parameters of each model. Let's take a deeper look at them.\n",
      "\n",
      "### Linear models\n",
      "\n",
      "Both logistic regression and SVM share the same parameters, because they use the same underlying optimization technique of **stochastic gradient descent** ( **SGD** ). They differ only in the loss function applied. If we take a look at the class definition for logistic regression in MLlib, we will see the following definition:\n",
      "\n",
      "    class LogisticRegressionWithSGD private (\n",
      "      private var stepSize: Double,\n",
      "      private var numIterations: Int,\n",
      "      private var regParam: Double,\n",
      "      private var miniBatchFraction: Double)\n",
      "      extends GeneralizedLinearAlgorithm[LogisticRegressionModel] ...\n",
      "\n",
      "We can see that the arguments that can be passed to the constructor are `stepSize`, `numIterations`, `regParam`, and `miniBatchFraction`. Of these, all except `regParam` are related to the underlying optimization technique.\n",
      "\n",
      "The instantiation code for logistic regression initializes the `Gradient`, `Updater`, and `Optimizer` and sets the relevant arguments for `Optimizer` (`GradientDescent` in this case):\n",
      "\n",
      "      private val gradient = new LogisticGradient()\n",
      "      private val updater = new SimpleUpdater()\n",
      "      override val optimizer = new GradientDescent(gradient, updater)\n",
      "        .setStepSize(stepSize)\n",
      "        .setNumIterations(numIterations)\n",
      "        .setRegParam(regParam)\n",
      "        .setMiniBatchFraction(miniBatchFraction)\n",
      "\n",
      "`LogisticGradient` sets up the logistic loss function that defines our logistic regression model.\n",
      "\n",
      "### Tip\n",
      "\n",
      "While a detailed treatment of optimization techniques is beyond the scope of this book, MLlib provides two optimizers for linear models: SGD and L-BFGS. L-BFGS is often more accurate and has fewer parameters to tune.\n",
      "\n",
      "SGD is the default, while L-BGFS can currently only be used directly for logistic regression via `LogisticRegressionWithLBFGS`. Try it out yourself and compare the results to those found with SGD.\n",
      "\n",
      "See <http://spark.apache.org/docs/latest/mllib-optimization.html> for further details.\n",
      "\n",
      "To investigate the impact of the remaining parameter settings, we will create a helper function that will train a logistic regression model, given a set of parameter inputs. First, we will import the required classes:\n",
      "\n",
      "    import org.apache.spark.rdd.RDD\n",
      "    import org.apache.spark.mllib.optimization.Updater\n",
      "    import org.apache.spark.mllib.optimization.SimpleUpdater\n",
      "    import org.apache.spark.mllib.optimization.L1Updater\n",
      "    import org.apache.spark.mllib.optimization.SquaredL2Updater\n",
      "    import org.apache.spark.mllib.classification.ClassificationModel\n",
      "\n",
      "Next, we will define our helper function to train a mode given a set of inputs:\n",
      "\n",
      "    def trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = {\n",
      "      val lr = new LogisticRegressionWithSGD\n",
      "      lr.optimizer.setNumIterations(numIterations). setUpdater(updater).setRegParam(regParam).setStepSize(stepSize)\n",
      "      lr.run(input)\n",
      "    }\n",
      "\n",
      "Finally, we will create a second helper function to take the input data and a classification model and generate the relevant AUC metrics:\n",
      "\n",
      "    def createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = {\n",
      "      val scoreAndLabels = data.map { point =>\n",
      "        (model.predict(point.features), point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (label, metrics.areaUnderROC)\n",
      "    }\n",
      "\n",
      "We will also cache our scaled dataset, including categories, to speed up the multiple model training runs that we will be using to explore these different parameter settings:\n",
      "\n",
      "    scaledDataCats.cache\n",
      "\n",
      "#### Iterations\n",
      "\n",
      "Many machine learning methods are iterative in nature, converging to a solution (the optimal weight vector that minimizes the chosen loss function) over a number of iteration steps. SGD typically requires relatively few iterations to converge to a reasonable solution but can be run for more iterations to improve the solution. We can see this by trying a few different settings for the `numIterations` parameter and comparing the AUC results:\n",
      "\n",
      "    val iterResults = Seq(1, 5, 10, 50).map { param =>\n",
      "      val model = trainWithParams(scaledDataCats, 0.0, param, new SimpleUpdater, 1.0)\n",
      "      createMetrics(s\"$param iterations\", scaledDataCats, model)\n",
      "    }\n",
      "    iterResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
      "\n",
      "Your output should look like this:\n",
      "\n",
      "    **1 iterations, AUC = 64.97%**\n",
      "    **5 iterations, AUC = 66.62%**\n",
      "    **10 iterations, AUC = 66.55%**\n",
      "    **50 iterations, AUC = 66.81%**\n",
      "\n",
      "So, we can see that the number of iterations has minor impact on the results once a certain number of iterations have been completed.\n",
      "\n",
      "#### Step size\n",
      "\n",
      "In SGD, the step size parameter controls how far in the direction of the steepest gradient the algorithm takes a step when updating the model weight vector after each training example. A larger step size might speed up convergence, but a step size that is too large might cause problems with convergence as good solutions are overshot.\n",
      "\n",
      "We can see the impact of changing the step size here:\n",
      "\n",
      "    val stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
      "      val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param)\n",
      "      createMetrics(s\"$param step size\", scaledDataCats, model)\n",
      "    }\n",
      "    stepResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
      "\n",
      "This will give us the following results, which show that increasing the step size too much can begin to negatively impact performance.\n",
      "\n",
      "    **0.001 step size, AUC = 64.95%**\n",
      "    **0.01 step size, AUC = 65.00%**\n",
      "    **0.1 step size, AUC = 65.52%**\n",
      "    **1.0 step size, AUC = 66.55%**\n",
      "    **10.0 step size, AUC = 61.92%**\n",
      "\n",
      "#### Regularization\n",
      "\n",
      "We briefly touched on the `Updater` class in the preceding logistic regression code. An `Updater` class in MLlib implements regularization. Regularization can help avoid over-fitting of a model to training data by effectively penalizing model complexity. This can be done by adding a term to the loss function that acts to increase the loss as a function of the model weight vector.\n",
      "\n",
      "Regularization is almost always required in real use cases, but is of particular importance when the feature dimension is very high (that is, the effective number of variable weights that can be learned is high) relative to the number of training examples.\n",
      "\n",
      "When regularization is absent or low, models can tend to over-fit. Without regularization, most models will over-fit on a training dataset. This is a key reason behind the use of cross-validation techniques for model fitting (which we will cover now).\n",
      "\n",
      "Conversely, since applying regularization encourages simpler models, model performance can suffer when regularization is high through under-fitting the data.\n",
      "\n",
      "The forms of regularization available in MLlib are:\n",
      "\n",
      "  * `SimpleUpdater`: This equates to no regularization and is the default for logistic regression\n",
      "  * `SquaredL2Updater`: This implements a regularizer based on the squared L2-norm of the weight vector; this is the default for SVM models\n",
      "  * `L1Updater`: This applies a regularizer based on the L1-norm of the weight vector; this can lead to sparse solutions in the weight vector (as less important weights are pulled towards zero)\n",
      "\n",
      "### Note\n",
      "\n",
      "Regularization and its relation to optimization is a broad and heavily researched area. Some more information is available from the following links:\n",
      "\n",
      "  * General regularization overview: <http://en.wikipedia.org/wiki/Regularization_(mathematics)>\n",
      "  * L2 regularization: <http://en.wikipedia.org/wiki/Tikhonov_regularization>\n",
      "  * Over-fitting and under-fitting: <http://en.wikipedia.org/wiki/Overfitting>\n",
      "  * Detailed overview of over-fitting and L1 versus L2 regularization: http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.9860&rep=rep1&type=pdf\n",
      "\n",
      "Let's explore the impact of a range of regularization parameters using `SquaredL2Updater`:\n",
      "\n",
      "    val regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
      "      val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0)\n",
      "      createMetrics(s\"$param L2 regularization parameter\", scaledDataCats, model)\n",
      "    }\n",
      "    regResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
      "\n",
      "Your output should look like this:\n",
      "\n",
      "    **0.001 L2 regularization parameter, AUC = 66.55%**\n",
      "    **0.01 L2 regularization parameter, AUC = 66.55%**\n",
      "    **0.1 L2 regularization parameter, AUC = 66.63%**\n",
      "    **1.0 L2 regularization parameter, AUC = 66.04%**\n",
      "    **10.0 L2 regularization parameter, AUC = 35.33%**\n",
      "\n",
      "As we can see, at low levels of regularization, there is not much impact in model performance. However, as we increase regularization, we can see the impact of under-fitting on our model evaluation.\n",
      "\n",
      "### Tip\n",
      "\n",
      "You will find similar results when using the L1 regularization. Give it a try by performing the same evaluation of regularization parameter against the AUC measure for `L1Updater`.\n",
      "\n",
      "### Decision trees\n",
      "\n",
      "The decision tree model we trained earlier was the best performer on the raw data that we first used. We set a parameter called `maxDepth`, which controls the maximum depth of the tree and, thus, the complexity of the model. Deeper trees result in more complex models that will be able to fit the data better.\n",
      "\n",
      "For classification problems, we can also select between two measures of impurity: `Gini` and `Entropy`.\n",
      "\n",
      "#### Tuning tree depth and impurity\n",
      "\n",
      "We will illustrate the impact of tree depth in a similar manner as we did for our logistic regression model.\n",
      "\n",
      "First, we will need to create another helper function in the Spark shell:\n",
      "\n",
      "    import org.apache.spark.mllib.tree.impurity.Impurity\n",
      "    import org.apache.spark.mllib.tree.impurity.Entropy\n",
      "    import org.apache.spark.mllib.tree.impurity.Gini\n",
      "\n",
      "    def trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = {\n",
      "      DecisionTree.train(input, Algo.Classification, impurity, maxDepth)\n",
      "    }\n",
      "\n",
      "Now, we're ready to compute our AUC metric for different settings of tree depth. We will simply use our original dataset in this example since we do not need the data to be standardized.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that decision tree models generally do not require features to be standardized or normalized, nor do they require categorical features to be binary-encoded.\n",
      "\n",
      "First, train the model using the `Entropy` impurity measure and varying tree depths:\n",
      "\n",
      "    val dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param =>\n",
      "      val model = trainDTWithParams(data, param, Entropy)\n",
      "      val scoreAndLabels = data.map { point =>\n",
      "        val score = model.predict(point.features)\n",
      "        (if (score > 0.5) 1.0 else 0.0, point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (s\"$param tree depth\", metrics.areaUnderROC)\n",
      "    }\n",
      "    dtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") }\n",
      "\n",
      "This should output the results shown here:\n",
      "\n",
      "    **1 tree depth, AUC = 59.33%**\n",
      "    **2 tree depth, AUC = 61.68%**\n",
      "    **3 tree depth, AUC = 62.61%**\n",
      "    **4 tree depth, AUC = 63.63%**\n",
      "    **5 tree depth, AUC = 64.88%**\n",
      "    **10 tree depth, AUC = 76.26%**\n",
      "    **20 tree depth, AUC = 98.45%**\n",
      "\n",
      "Next, we will perform the same computation using the `Gini` impurity measure (we omitted the code as it is very similar, but it can be found in the code bundle). Your results should look something like this:\n",
      "\n",
      "    **1 tree depth, AUC = 59.33%**\n",
      "    **2 tree depth, AUC = 61.68%**\n",
      "    **3 tree depth, AUC = 62.61%**\n",
      "    **4 tree depth, AUC = 63.63%**\n",
      "    **5 tree depth, AUC = 64.89%**\n",
      "    **10 tree depth, AUC = 78.37%**\n",
      "    **20 tree depth, AUC = 98.87%**\n",
      "\n",
      "As you can see from the preceding results, increasing the tree depth parameter results in a more accurate model (as expected since the model is allowed to get more complex with greater tree depth). It is very likely that at higher tree depths, the model will over-fit the dataset significantly.\n",
      "\n",
      "There is very little difference in performance between the two impurity measures.\n",
      "\n",
      "### The naive Bayes model\n",
      "\n",
      "Finally, let's see the impact of changing the `lambda` parameter for naive Bayes. This parameter controls additive smoothing, which handles the case when a class and feature value do not occur together in the dataset.\n",
      "\n",
      "### Tip\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Additive_smoothing> for more details on additive smoothing.\n",
      "\n",
      "We will take the same approach as we did earlier, first creating a convenience training function and training the model with varying levels of `lambda`:\n",
      "\n",
      "    def trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = {\n",
      "      val nb = new NaiveBayes\n",
      "      nb.setLambda(lambda)\n",
      "      nb.run(input)\n",
      "    }\n",
      "    val nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param =>\n",
      "      val model = trainNBWithParams(dataNB, param)\n",
      "      val scoreAndLabels = dataNB.map { point =>\n",
      "        (model.predict(point.features), point.label)\n",
      "      }\n",
      "      val metrics = new BinaryClassificationMetrics(scoreAndLabels)\n",
      "      (s\"$param lambda\", metrics.areaUnderROC)\n",
      "    }\n",
      "    nbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") \n",
      "    }\n",
      "\n",
      "The results of the training are as follows:\n",
      "\n",
      "    **0.001 lambda, AUC = 60.51%**\n",
      "    **0.01 lambda, AUC = 60.51%**\n",
      "    **0.1 lambda, AUC = 60.51%**\n",
      "    **1.0 lambda, AUC = 60.51%**\n",
      "    **10.0 lambda, AUC = 60.51%**\n",
      "\n",
      "We can see that `lambda` has no impact in this case, since it will not be a problem if the combination of feature and class label not occurring together in the dataset.\n",
      "\n",
      "## Cross-validation\n",
      "\n",
      "So far in this book, we have only briefly mentioned the idea of cross-validation and out-of-sample testing. Cross-validation is a critical part of real-world machine learning and is central to many model selection and parameter tuning pipelines.\n",
      "\n",
      "The general idea behind cross-validation is that we want to know how our model will perform on unseen data. Evaluating this on real, live data (for example, in a production system) is risky, because we don't really know whether the trained model is the best in the sense of being able to make accurate predictions on new data. As we saw previously with regard to regularization, our model might have over-fit the training data and be poor at making predictions on data it has not been trained on.\n",
      "\n",
      "Cross-validation provides a mechanism where we use part of our available dataset to train our model and another part to evaluate the performance of this model. As the model is tested on data that it has not seen during the training phase, its performance, when evaluated on this part of the dataset, gives us an estimate as to how well our model generalizes for the new data points.\n",
      "\n",
      "Here, we will implement a simple cross-validation evaluation approach using a train-test split. We will divide our dataset into two non-overlapping parts. The first dataset is used to train our model and is called the training set. The second dataset, called the test set or hold-out set, is used to evaluate the performance of our model using our chosen evaluation measure. Common splits used in practice include 50/50, 60/40, and 80/20 splits, but you can use any split as long as the training set is not too small for the model to learn (generally, at least 50 percent is a practical minimum).\n",
      "\n",
      "In many cases, three sets are created: a training set, an evaluation set (which is used like the above test set to tune the model parameters such as lambda and step size), and a test set (which is never used to train a model or tune any parameters, but is only used to generate an estimated true performance on completely unseen data).\n",
      "\n",
      "### Note\n",
      "\n",
      "Here, we will explore a simple train-test split approach. There are many cross-validation techniques that are more exhaustive and complex.\n",
      "\n",
      "One popular example is K-fold cross-validation, where the dataset is split into K non-overlapping folds. The model is trained on K-1 folds of data and tested on the remaining, held-out fold. This is repeated K times, and the results are averaged to give the cross-validation score. The train-test split is effectively like two-fold cross-validation.\n",
      "\n",
      "Other approaches include leave-one-out cross-validation and random sampling. See the article at <http://en.wikipedia.org/wiki/Cross-validation_(statistics)> for further details.\n",
      "\n",
      "First, we will split our dataset into a 60 percent training set and a 40 percent test set (we will use a constant random seed of 123 here to ensure that we get the same results for ease of illustration):\n",
      "\n",
      "    val trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123)\n",
      "    val train = trainTestSplit(0)\n",
      "    val test = trainTestSplit(1)\n",
      "\n",
      "Next, we will compute the evaluation metric of interest (again, we will use AUC) for a range of regularization parameter settings. Note that here we will use a finer-grained step size between the evaluated regularization parameters to better illustrate the differences in AUC, which are very small in this case:\n",
      "\n",
      "    val regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param =>\n",
      "      val model = trainWithParams( **train** , param, numIterations, new SquaredL2Updater, 1.0)\n",
      "      createMetrics(s\"$param L2 regularization parameter\", test, model)\n",
      "    }\n",
      "    regResultsTest.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\") \n",
      "    }\n",
      "\n",
      "This will compute the results of training on the training set and the results of evaluating on the test set, as shown here:\n",
      "\n",
      "    **0.0 L2 regularization parameter, AUC = 66.480874%**\n",
      "    **0.001 L2 regularization parameter, AUC = 66.480874%**\n",
      "    **0.0025 L2 regularization parameter, AUC = 66.515027%**\n",
      "    **0.005 L2 regularization parameter, AUC = 66.515027%**\n",
      "    **0.01 L2 regularization parameter, AUC = 66.549180%**\n",
      "\n",
      "Now, let's compare this to the results of training and testing on the training set (this is what we were doing previously by training and testing on all data). Again, we will omit the code as it is very similar (but it is available in the code bundle):\n",
      "\n",
      "    **0.0 L2 regularization parameter, AUC = 66.260311%**\n",
      "    **0.001 L2 regularization parameter, AUC = 66.260311%**\n",
      "    **0.0025 L2 regularization parameter, AUC = 66.260311%**\n",
      "    **0.005 L2 regularization parameter, AUC = 66.238294%**\n",
      "    **0.01 L2 regularization parameter, AUC = 66.238294%**\n",
      "\n",
      "So, we can see that when we train and evaluate our model on the same dataset, we generally achieve the highest performance when regularization is lower. This is because our model has seen all the data points, and with low levels of regularization, it can over-fit the data set and achieve higher performance.\n",
      "\n",
      "In contrast, when we train on one dataset and test on another, we see that generally a slightly higher level of regularization results in better test set performance.\n",
      "\n",
      "In cross-validation, we would typically find the parameter settings (including regularization as well as the various other parameters such as step size and so on) that result in the best test set performance. We would then use these parameter settings to retrain the model on all of our data in order to use it to make predictions on new data.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Recall from Chapter 4, _Building a Recommendation Engine with Spark_ , that we did not cover cross-validation. You can apply the same techniques we used earlier to split the ratings dataset from that chapter into a training and test dataset. You can then try out different parameter settings on the training set while evaluating the MSE and MAP performance metrics on the test set in a manner similar to what we did earlier. Give it a try!\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we covered the various classification models available in Spark MLlib, and we saw how to train models on input data and how to evaluate their performance using standard metrics and measures. We also explored how to apply some of the techniques previously introduced to transform our features. Finally, we investigated the impact of using the correct input data format or distribution on model performance, and we also saw the impact of adding more data to our model, tuning model parameters, and implementing cross-validation.\n",
      "\n",
      "In the next chapter, we will take a similar approach to delve into MLlib's regression models.\n",
      "\n",
      "# Chapter 6. Building a Regression Model with Spark\n",
      "\n",
      "In this chapter, we will build on what we covered in Chapter 5, _Building a Classification Model with Spark_. While classification models deal with outcomes that represent discrete classes, regression models are concerned with target variables that can take any real value. The underlying principle is very similar--we wish to find a model that maps input features to predicted target variables. Like classification, regression is also a form of supervised learning.\n",
      "\n",
      "Regression models can be used to predict just about any variable of interest. A few examples include the following:\n",
      "\n",
      "  * Predicting stock returns and other economic variables\n",
      "  * Predicting loss amounts for loan defaults (this can be combined with a classification model that predicts the probability of default, while the regression model predicts the amount in the case of a default)\n",
      "  * Recommendations (the Alternating Least Squares factorization model from Chapter 4, _Building a Recommendation Engine with Spark_ , uses linear regression in each iteration)\n",
      "  * Predicting **customer lifetime value** ( **CLTV** ) in a retail, mobile, or other business, based on user behavior and spending patterns\n",
      "\n",
      "In the following sections, we will:\n",
      "\n",
      "  * Introduce the various types of regression models available in MLlib\n",
      "  * Explore feature extraction and target variable transformation for regression models\n",
      "  * Train a number of regression models using MLlib\n",
      "  * See how to make predictions using the trained models\n",
      "  * Investigate the impact on performance of various parameter settings for regression using cross-validation\n",
      "\n",
      "# Types of regression models\n",
      "\n",
      "Spark's MLlib library offers two broad classes of regression models: linear models and decision tree regression models.\n",
      "\n",
      "Linear models are essentially the same as their classification counterparts, the only difference is that linear regression models use a different loss function, related link function, and decision function. MLlib provides a standard least squares regression model (although other types of generalized linear models for regression are planned).\n",
      "\n",
      "Decision trees can also be used for regression by changing the impurity measure.\n",
      "\n",
      "## Least squares regression\n",
      "\n",
      "You might recall from Chapter 5, _Building a Classification Model with Spark_ , that there are a variety of loss functions that can be applied to generalized linear models. The loss function used for least squares is the squared loss, which is defined as follows:\n",
      "\n",
      "    ½ (wTx - y)2\n",
      "\n",
      "Here, as for the classification setting, _y_ is the target variable (this time, real valued), _w_ is the weight vector, and _x_ is the feature vector.\n",
      "\n",
      "The related link function is the identity link, and the decision function is also the identity function, as generally, no thresholding is applied in regression. So, the model's prediction is simply _y = w Tx_.\n",
      "\n",
      "The standard least squares regression in MLlib does not use regularization. Looking at the squared loss function, we can see that the loss applied to incorrectly predicted points will be magnified since the loss is squared. This means that least squares regression is susceptible to outliers in the dataset and also to over-fitting. Generally, as for classification, we should apply some level of regularization in practice.\n",
      "\n",
      "Linear regression with L2 regularization is commonly referred to as ridge regression, while applying L1 regularization is called the **lasso**.\n",
      "\n",
      "### Tip\n",
      "\n",
      "See the section on linear least squares in the Spark MLlib documentation at <http://spark.apache.org/docs/latest/mllib-linear-methods.html#linear-least-squares-lasso-and-ridge-regression> for further information.\n",
      "\n",
      "## Decision trees for regression\n",
      "\n",
      "Just like using linear models for regression tasks involves changing the loss function used, using decision trees for regression involves changing the measure of the node impurity used. The impurity metric is called **variance** and is defined in the same way as the squared loss for least squares linear regression.\n",
      "\n",
      "### Note\n",
      "\n",
      "See the _MLlib - Decision Tree_ section in the Spark documentation at <http://spark.apache.org/docs/latest/mllib-decision-tree.html> for further details on the decision tree algorithm and impurity measure for regression.\n",
      "\n",
      "Now, we will plot a simple example of a regression problem with only one input variable shown on the _x_ axis and the target variable on the _y_ axis. The linear model prediction function is shown by a red dashed line, while the decision tree prediction function is shown by a green dashed line. We can see that the decision tree allows a more complex, nonlinear model to be fitted to the data.\n",
      "\n",
      "Linear model and decision tree prediction functions for regression\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "As the underlying models for regression are the same as those for the classification case, we can use the same approach to create input features. The only practical difference is that the target is now a real-valued variable, as opposed to a categorical one. The `LabeledPoint` class in MLlib already takes this into account, as the `label` field is of the `Double` type, so it can handle both cases.\n",
      "\n",
      "## Extracting features from the bike sharing dataset\n",
      "\n",
      "To illustrate the concepts in this chapter, we will be using the bike sharing dataset. This dataset contains hourly records of the number of bicycle rentals in the capital bike sharing system. It also contains variables related to date and time, weather, and seasonal and holiday information.\n",
      "\n",
      "### Note\n",
      "\n",
      "The dataset is available at <http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset>.\n",
      "\n",
      "Click on the **Data Folder** link and then download the `Bike-Sharing-Dataset.zip` file.\n",
      "\n",
      "The bike sharing data was enriched with weather and seasonal data by Hadi Fanaee-T at the University of Porto and used in the following paper:\n",
      "\n",
      "Fanaee-T, Hadi and Gama Joao, Event labeling combining ensemble detectors and background knowledge, _Progress in Artificial Intelligence_ , pp. 1-15, Springer Berlin Heidelberg, 2013.\n",
      "\n",
      "The paper is available at <http://link.springer.com/article/10.1007%2Fs13748-013-0040-3>.\n",
      "\n",
      "Once you have downloaded the `Bike-Sharing-Dataset.zip` file, unzip it. This will create a directory called `Bike-Sharing-Dataset`, which contains the `day.csv`, `hour.csv`, and the `Readme.txt` files.\n",
      "\n",
      "The `Readme.txt` file contains information on the dataset, including the variable names and descriptions. Take a look at the file, and you will see that we have the following variables available:\n",
      "\n",
      "  * `instant`: This is the record ID\n",
      "  * `dteday`: This is the raw date\n",
      "  * `season`: This is different seasons such as spring, summer, winter, and fall\n",
      "  * `yr`: This is the year (2011 or 2012)\n",
      "  * `mnth`: This is the month of the year\n",
      "  * `hr`: This is the hour of the day\n",
      "  * `holiday`: This is whether the day was a holiday or not\n",
      "  * `weekday`: This is the day of the week\n",
      "  * `workingday`: This is whether the day was a working day or not\n",
      "  * `weathersit`: This is a categorical variable that describes the weather at a particular time\n",
      "  * `temp`: This is the normalized temperature\n",
      "  * `atemp`: This is the normalized apparent temperature\n",
      "  * `hum`: This is the normalized humidity\n",
      "  * `windspeed`: This is the normalized wind speed\n",
      "  * `cnt`: This is the target variable, that is, the count of bike rentals for that hour\n",
      "\n",
      "We will work with the hourly data contained in `hour.csv`. If you look at the first line of the dataset, you will see that it contains the column names as a header. You can do this by running the following command:\n",
      "\n",
      "    **> head -1 hour.csv**\n",
      "\n",
      "This should output the following result:\n",
      "\n",
      "    **instant,dteday,season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt**\n",
      "\n",
      "Before we work with the data in Spark, we will again remove the header from the first line of the file using the same `sed` command that we used previously to create a new file called `hour_noheader.csv`:\n",
      "\n",
      "    **> sed 1d hour.csv > hour_noheader.csv**\n",
      "\n",
      "Since we will be doing some plotting of our dataset later on, we will use the Python shell for this chapter. This also serves to illustrate how to use MLlib's linear model and decision tree functionality from PySpark.\n",
      "\n",
      "Start up your PySpark shell from your Spark installation directory. If you want to use IPython, which we highly recommend, remember to include the `IPYTHON=1` environment variable together with the `pylab` functionality:\n",
      "\n",
      "    **> IPYTHON=1 IPYTHON_OPTS=\"--pylab\" ./bin/pyspark**\n",
      "\n",
      "If you prefer to use IPython Notebook, you can start it with the following command:\n",
      "\n",
      "    **> IPYTHON=1 IPYTHON_OPTS=notebook ./bin/pyspark**\n",
      "\n",
      "You can type all the code that follows for the remainder of this chapter directly into your PySpark shell (or into IPython Notebook if you wish to use it).\n",
      "\n",
      "### Tip\n",
      "\n",
      "Recall that we used the IPython shell in Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_. Take a look at that chapter and the code bundle for instructions to install IPython.\n",
      "\n",
      "We'll start as usual by loading the dataset and inspecting it:\n",
      "\n",
      "    path = \"/ **PATH** /hour_noheader.csv\"\n",
      "    raw_data = sc.textFile(path)\n",
      "    num_data = raw_data.count()\n",
      "    records = raw_data.map(lambda x: x.split(\",\"))\n",
      "    first = records.first()\n",
      "    print first\n",
      "    print num_data\n",
      "\n",
      "You should see the following output:\n",
      "\n",
      "    **[u'1', u'2011-01-01', u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']**\n",
      "    **17379**\n",
      "\n",
      "So, we have `17,379` hourly records in our dataset. We have inspected the column names already. We will ignore the record ID and raw date columns. We will also ignore the `casual` and `registered` count target variables and focus on the overall count variable, `cnt` (which is the sum of the other two counts). We are left with 12 variables. The first eight are categorical, while the last 4 are normalized real-valued variables.\n",
      "\n",
      "To deal with the eight categorical variables, we will use the binary encoding approach with which you should be quite familiar by now. The four real-valued variables will be left as is.\n",
      "\n",
      "We will first cache our dataset, since we will be reading from it many times:\n",
      "\n",
      "    records.cache()\n",
      "\n",
      "In order to extract each categorical feature into a binary vector form, we will need to know the feature mapping of each feature value to the index of the nonzero value in our binary vector. Let's define a function that will extract this mapping from our dataset for a given column:\n",
      "\n",
      "    def get_mapping(rdd, idx):\n",
      "        return rdd.map(lambda fields: fields[idx]).distinct().zipWithIndex().collectAsMap()\n",
      "\n",
      "Our function first maps the field to its unique values and then uses the `zipWithIndex` transformation to zip the value up with a unique index such that a key-value RDD is formed, where the key is the variable and the value is the index. This index will be the index of the nonzero entry in the binary vector representation of the feature. We will finally collect this RDD back to the driver as a Python dictionary.\n",
      "\n",
      "We can test our function on the third variable column (index 2):\n",
      "\n",
      "    print \"Mapping of first categorical feasture column: %s\" % get_mapping(records, 2)\n",
      "\n",
      "The preceding line of code will give us the following output:\n",
      "\n",
      "    **Mapping of first categorical feasture column: {u'1': 0, u'3': 2, u'2': 1, u'4': 3}**\n",
      "\n",
      "Now, we can apply this function to each categorical column (that is, for variable indices 2 to 9):\n",
      "\n",
      "    mappings = [get_mapping(records, i) for i in range(2,10)]\n",
      "    cat_len = sum(map(len, mappings))\n",
      "    num_len = len(records.first()[11:15])\n",
      "    total_len = num_len + cat_len\n",
      "\n",
      "We now have the mappings for each variable, and we can see how many values in total we need for our binary vector representation:\n",
      "\n",
      "    print \"Feature vector length for categorical features: %d\" % cat_len \n",
      "    print \"Feature vector length for numerical features: %d\" % num_len\n",
      "    print \"Total feature vector length: %d\" % total_len\n",
      "\n",
      "The output of the preceding code is as follows:\n",
      "\n",
      "    **Feature vector length for categorical features: 57**\n",
      "    **Feature vector length for numerical features: 4**\n",
      "    **Total feature vector length: 61**\n",
      "\n",
      "### Creating feature vectors for the linear model\n",
      "\n",
      "The next step is to use our extracted mappings to convert the categorical features to binary-encoded features. Again, it will be helpful to create a function that we can apply to each record in our dataset for this purpose. We will also create a function to extract the target variable from each record. We will need to import `numpy` for linear algebra utilities and MLlib's `LabeledPoint` class to wrap our feature vectors and target variables:\n",
      "\n",
      "    from pyspark.mllib.regression import LabeledPoint\n",
      "    import numpy as np\n",
      "\n",
      "    def extract_features(record):\n",
      "      cat_vec = np.zeros(cat_len)\n",
      "      i = 0\n",
      "      step = 0\n",
      "      for field in record[2:9]:\n",
      "        m = mappings[i]\n",
      "        idx = m[field]\n",
      "        cat_vec[idx + step] = 1\n",
      "        i = i + 1\n",
      "        step = step + len(m)\n",
      "      num_vec = np.array([float(field) for field in record[10:14]])\n",
      "      return np.concatenate((cat_vec, num_vec))\n",
      "\n",
      "    def extract_label(record):\n",
      "      return float(record[-1])\n",
      "\n",
      "In the preceding `extract_features` function, we ran through each column in the row of data. We extracted the binary encoding for each variable in turn from the mappings we created previously. The `step` variable ensures that the nonzero feature index in the full feature vector is correct (and is somewhat more efficient than, say, creating many smaller binary vectors and concatenating them). The numeric vector is created directly by first converting the data to floating point numbers and wrapping these in a `numpy` array. The resulting two vectors are then concatenated. The `extract_label` function simply converts the last column variable (the count) into a float.\n",
      "\n",
      "With our utility functions defined, we can proceed with extracting feature vectors and labels from our data records:\n",
      "\n",
      "    data = records.map(lambda r: LabeledPoint(extract_label(r), extract_features(r)))\n",
      "\n",
      "Let's inspect the first record in the extracted feature RDD:\n",
      "\n",
      "    first_point = data.first()\n",
      "    print \"Raw data: \" + str(first[2:])\n",
      "    print \"Label: \" + str(first_point.label)\n",
      "    print \"Linear Model feature vector:\\n\" + str(first_point.features)\n",
      "    print \"Linear Model feature vector length: \" + str(len(first_point.features))\n",
      "\n",
      "You should see output similar to the following:\n",
      "\n",
      "    **Raw data: [u'1', u'0', u'1', u'0', u'0', u'6', u'0', u'1', u'0.24', u'0.2879', u'0.81', u'0', u'3', u'13', u'16']**\n",
      "    **Label: 16.0**\n",
      "    **Linear Model feature vector: [1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.24,0.2879,0.81,0.0]**\n",
      "    **Linear Model feature vector length: 61**\n",
      "\n",
      "As we can see, we converted the raw data into a feature vector made up of the binary categorical and real numeric features, and we indeed have a total vector length of `61`.\n",
      "\n",
      "### Creating feature vectors for the decision tree\n",
      "\n",
      "As we have seen, decision tree models typically work on raw features (that is, it is not required to convert categorical features into a binary vector encoding; they can, instead, be used directly). Therefore, we will create a separate function to extract the decision tree feature vector, which simply converts all the values to floats and wraps them in a `numpy` array:\n",
      "\n",
      "    def extract_features_dt(record):\n",
      "      return np.array(map(float, record[2:14]))\n",
      "    data_dt = records.map(lambda r: LabeledPoint(extract_label(r), extract_features_dt(r)))\n",
      "    first_point_dt = data_dt.first()\n",
      "    print \"Decision Tree feature vector: \" + str(first_point_dt.features)\n",
      "    print \"Decision Tree feature vector length: \" + str(len(first_point_dt.features))\n",
      "\n",
      "The following output shows the extracted feature vector, and we can see that we have a vector length of `12`, which matches the number of raw variables we are using:\n",
      "\n",
      "    **Decision Tree feature vector: [1.0,0.0,1.0,0.0,0.0,6.0,0.0,1.0,0.24,0.2879,0.81,0.0]**\n",
      "    **Decision Tree feature vector length: 12**\n",
      "\n",
      "# Training and using regression models\n",
      "\n",
      "Training for regression models using decision trees and linear models follows the same procedure as for classification models. We simply pass the training data contained in a `[LabeledPoint]` RDD to the relevant `train` method. Note that in Scala, if we wanted to customize the various model parameters (such as regularization and step size for the SGD optimizer), we are required to instantiate a new model instance and use the `optimizer` field to access these available parameter setters.\n",
      "\n",
      "In Python, we are provided with a convenience method that gives us access to all the available model arguments, so we only have to use this one entry point for training. We can see the details of these convenience functions by importing the relevant modules and then calling the `help` function on the `train` methods:\n",
      "\n",
      "    from pyspark.mllib.regression import LinearRegressionWithSGD\n",
      "    from pyspark.mllib.tree import DecisionTree\n",
      "    help(LinearRegressionWithSGD.train)\n",
      "\n",
      "Doing this for the linear model outputs the following documentation:\n",
      "\n",
      "Linear regression help documentation\n",
      "\n",
      "We can see from the linear regression documentation that we need to pass in the training data at a minimum, but we can set any of the other model parameters using this `train` method.\n",
      "\n",
      "Similarly, for the decision tree model, which has a `trainRegressor` method (in addition to a `trainClassifier` method for classification models):\n",
      "\n",
      "    help(DecisionTree.trainRegressor)\n",
      "\n",
      "The preceding code would display the following documentation:\n",
      "\n",
      "Decision tree regression help documentation\n",
      "\n",
      "## Training a regression model on the bike sharing dataset\n",
      "\n",
      "We're ready to use the features we have extracted to train our models on the bike sharing data. First, we'll train the linear regression model and take a look at the first few predictions that the model makes on the data:\n",
      "\n",
      "    linear_model = LinearRegressionWithSGD.train(data, iterations=10, step=0.1, intercept=False)\n",
      "    true_vs_predicted = data.map(lambda p: (p.label, linear_model.predict(p.features)))\n",
      "    print \"Linear Model predictions: \" + str(true_vs_predicted.take(5))\n",
      "\n",
      "Note that we have not used the default settings for `iterations` and `step` here. We've changed the number of iterations so that the model does not take too long to train. As for the step size, you will see why this has been changed from the default a little later. You will see the following output:\n",
      "\n",
      "    **Linear Model predictions: [(16.0, 119.30920003093595), (40.0, 116.95463511937379), (32.0, 116.57294610647752), (13.0, 116.43535423855654), (1.0, 116.221247828503)]**\n",
      "\n",
      "Next, we will train the decision tree model simply using the default arguments to the `trainRegressor` method (which equates to using a tree depth of 5). Note that we need to pass in the other form of the dataset, `data_dt`, that we created from the raw feature values (as opposed to the binary encoded features that we used for the preceding linear model).\n",
      "\n",
      "We also need to pass in an argument for `categoricalFeaturesInfo`. This is a dictionary that maps the categorical feature index to the number of categories for the feature. If a feature is not in this mapping, it will be treated as continuous. For our purposes, we will leave this as is, passing in an empty mapping:\n",
      "\n",
      "    dt_model = DecisionTree.trainRegressor(data_dt,{})\n",
      "    preds = dt_model.predict(data_dt.map(lambda p: p.features))\n",
      "    actual = data.map(lambda p: p.label)\n",
      "    true_vs_predicted_dt = actual.zip(preds)\n",
      "    print \"Decision Tree predictions: \" + str(true_vs_predicted_dt.take(5))\n",
      "    print \"Decision Tree depth: \" + str(dt_model.depth())\n",
      "    print \"Decision Tree number of nodes: \" + str(dt_model.numNodes())\n",
      "\n",
      "This should output these predictions:\n",
      "\n",
      "    **Decision Tree predictions: [(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945), (13.0, 14.284023668639053), (1.0, 14.284023668639053)]**\n",
      "    **Decision Tree depth: 5**\n",
      "    **Decision Tree number of nodes: 63**\n",
      "\n",
      "### Note\n",
      "\n",
      "This is not as bad as it sounds. While we do not cover it here, the Python code included with this chapter's code bundle includes an example of using `categoricalFeaturesInfo`. It does not make a large difference to performance in this case.\n",
      "\n",
      "From a quick glance at these predictions, it appears that the decision tree might do better, as the linear model is quite a way off in its predictions. However, we will apply more stringent evaluation methods to find out.\n",
      "\n",
      "# Evaluating the performance of regression models\n",
      "\n",
      "We saw in Chapter 5, _Building a Classification Model with Spark_ , that evaluation methods for classification models typically focus on measurements related to predicted class memberships relative to the actual class memberships. These are binary outcomes (either the predicted class is correct or incorrect), and it is less important whether the model just barely predicted correctly or not; what we care most about is the number of correct and incorrect predictions.\n",
      "\n",
      "When dealing with regression models, it is very unlikely that our model will precisely predict the target variable, because the target variable can take on any real value. However, we would naturally like to understand how far away our predicted values are from the true values, so will we utilize a metric that takes into account the overall deviation.\n",
      "\n",
      "Some of the standard evaluation metrics used to measure the performance of regression models include the **Mean Squared Error** ( **MSE** ) and **Root Mean Squared Error** ( **RMSE** ), the **Mean Absolute Error** ( **MAE** ), the R-squared coefficient, and many others.\n",
      "\n",
      "## Mean Squared Error and Root Mean Squared Error\n",
      "\n",
      "MSE is the average of the squared error that is used as the loss function for least squares regression:\n",
      "\n",
      "It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points.\n",
      "\n",
      "RMSE is the square root of MSE. MSE is measured in units that are the square of the target variable, while RMSE is measured in the same units as the target variable. Due to its formulation, MSE, just like the squared loss function that it derives from, effectively penalizes larger errors more severely.\n",
      "\n",
      "In order to evaluate our predictions based on the mean of an error metric, we will first make predictions for each input feature vector in an RDD of `LabeledPoint` instances by computing the error for each record using a function that takes the prediction and true target value as inputs. This will return a `[Double]` RDD that contains the error values. We can then find the average using the `mean` method of RDDs that contain `Double` values.\n",
      "\n",
      "Let's define our squared error function as follows:\n",
      "\n",
      "    def squared_error(actual, pred):\n",
      "        return (pred - actual)**2\n",
      "\n",
      "## Mean Absolute Error\n",
      "\n",
      "MAE is the average of the absolute differences between the predicted and actual targets:\n",
      "\n",
      "MAE is similar in principle to MSE, but it does not punish large deviations as much.\n",
      "\n",
      "Our function to compute MAE is as follows:\n",
      "\n",
      "    def abs_error(actual, pred):\n",
      "        return np.abs(pred - actual)\n",
      "\n",
      "## Root Mean Squared Log Error\n",
      "\n",
      "This measurement is not as widely used as MSE and MAE, but it is used as the metric for the Kaggle competition that uses the bike sharing dataset. It is effectively the RMSE of the log-transformed predicted and target values. This measurement is useful when there is a wide range in the target variable, and you do not necessarily want to penalize large errors when the predicted and target values are themselves high. It is also effective when you care about percentage errors rather than the absolute value of errors.\n",
      "\n",
      "### Note\n",
      "\n",
      "The Kaggle competition evaluation page can be found at <https://www.kaggle.com/c/bike-sharing-demand/details/evaluation>.\n",
      "\n",
      "The function to compute RMSLE is shown here:\n",
      "\n",
      "    def squared_log_error(pred, actual):\n",
      "        return (np.log(pred + 1) - np.log(actual + 1))**2\n",
      "\n",
      "## The R-squared coefficient\n",
      "\n",
      "The R-squared coefficient, also known as the coefficient of determination, is a measure of how well a model fits a dataset. It is commonly used in statistics. It measures the degree of variation in the target variable; this is explained by the variation in the input features. An R-squared coefficient generally takes a value between 0 and 1, where 1 equates to a perfect fit of the model.\n",
      "\n",
      "## Computing performance metrics on the bike sharing dataset\n",
      "\n",
      "Given the functions we defined earlier, we can now compute the various evaluation metrics on our bike sharing data.\n",
      "\n",
      "### Linear model\n",
      "\n",
      "Our approach will be to apply the relevant error function to each record in the `RDD` we computed earlier, which is `true_vs_predicted` for our linear model:\n",
      "\n",
      "    mse = true_vs_predicted.map(lambda (t, p): squared_error(t, p)).mean()\n",
      "    mae = true_vs_predicted.map(lambda (t, p): abs_error(t, p)).mean()\n",
      "    rmsle = np.sqrt(true_vs_predicted.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "    print \"Linear Model - Mean Squared Error: %2.4f\" % mse\n",
      "    print \"Linear Model - Mean Absolute Error: %2.4f\" % mae\n",
      "    print \"Linear Model - Root Mean Squared Log Error: %2.4f\" % rmsle\n",
      "\n",
      "This outputs the following metrics:\n",
      "\n",
      "    **Linear Model - Mean Squared Error: 28166.3824**\n",
      "    **Linear Model - Mean Absolute Error: 129.4506**\n",
      "    **Linear Model - Root Mean Squared Log Error: 1.4974**\n",
      "\n",
      "### Decision tree\n",
      "\n",
      "We will use the same approach for the decision tree model, using the `true_vs_predicted_dt` RDD:\n",
      "\n",
      "    mse_dt = true_vs_predicted_dt.map(lambda (t, p): squared_error(t, p)).mean()\n",
      "    mae_dt = true_vs_predicted_dt.map(lambda (t, p): abs_error(t, p)).mean()\n",
      "    rmsle_dt = np.sqrt(true_vs_predicted_dt.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "    print \"Decision Tree - Mean Squared Error: %2.4f\" % mse_dt\n",
      "    print \"Decision Tree - Mean Absolute Error: %2.4f\" % mae_dt\n",
      "    print \"Decision Tree - Root Mean Squared Log Error: %2.4f\" % rmsle_dt\n",
      "\n",
      "You should see output similar to this:\n",
      "\n",
      "    **Decision Tree - Mean Squared Error: 11560.7978**\n",
      "    **Decision Tree - Mean Absolute Error: 71.0969**\n",
      "    **Decision Tree - Root Mean Squared Log Error: 0.6259**\n",
      "\n",
      "Looking at the results, we can see that our initial guess about the decision tree model being the better performer is indeed true.\n",
      "\n",
      "### Note\n",
      "\n",
      "The Kaggle competition leaderboard lists the Mean Value Benchmark score on the test set at about 1.58. So, we see that our linear model performance is not much better. However, the decision tree with default settings achieves a performance of 0.63.\n",
      "\n",
      "The winning score at the time of writing this book is listed as 0.29504.\n",
      "\n",
      "# Improving model performance and tuning parameters\n",
      "\n",
      "In Chapter 5, _Building a Classification Model with Spark_ , we showed how feature transformation and selection can make a large difference to the performance of a model. In this chapter, we will focus on another type of transformation that can be applied to a dataset: transforming the target variable itself.\n",
      "\n",
      "## Transforming the target variable\n",
      "\n",
      "Recall that many machine learning models, including linear models, make assumptions regarding the distribution of the input data as well as target variables. In particular, linear regression assumes a normal distribution.\n",
      "\n",
      "In many real-world cases, the distributional assumptions of linear regression do not hold. In this case, for example, we know that the number of bike rentals can never be negative. This alone should indicate that the assumption of normality might be problematic. To get a better idea of the target distribution, it is often a good idea to plot a histogram of the target values.\n",
      "\n",
      "In this section, if you are using IPython Notebook, enter the magic function, `%pylab inline`, to import `pylab` (that is, the `numpy` and `matplotlib` plotting functions) into the workspace. This will also create any figures and plots inline within the `Notebook` cell.\n",
      "\n",
      "If you are using the standard IPython console, you can use `%pylab` to import the necessary functionality (your plots will appear in a separate window).\n",
      "\n",
      "We will now create a plot of the target variable distribution in the following piece of code:\n",
      "\n",
      "    targets = records.map(lambda r: float(r[-1])).collect()\n",
      "    hist(targets, bins=40, color='lightblue', normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "Looking at the histogram plot, we can see that the distribution is highly skewed and certainly does not follow a normal distribution:\n",
      "\n",
      "Distribution of raw target variable values\n",
      "\n",
      "One way in which we might deal with this situation is by applying a transformation to the target variable, such that we take the logarithm of the target value instead of the raw value. This is often referred to as log-transforming the target variable (this transformation can also be applied to feature values).\n",
      "\n",
      "We will apply a log transformation to the following target variable and plot a histogram of the log-transformed values:\n",
      "\n",
      "    log_targets = records.map(lambda r: np.log(float(r[-1]))).collect()\n",
      "    hist(log_targets, bins=40, color='lightblue', normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "Distribution of log-transformed target variable values\n",
      "\n",
      "A second type of transformation that is useful in the case of target values that do not take on negative values and, in addition, might take on a very wide range of values, is to take the square root of the variable.\n",
      "\n",
      "We will apply the square root transform in the following code, once more plotting the resulting target variable distribution:\n",
      "\n",
      "    sqrt_targets = records.map(lambda r: np.sqrt(float(r[-1]))).collect()\n",
      "    hist(sqrt_targets, bins=40, color='lightblue', normed=True)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    fig.set_size_inches(16, 10)\n",
      "\n",
      "From the plots of the log and square root transformations, we can see that both result in a more even distribution relative to the raw values. While they are still not normally distributed, they are a lot closer to a normal distribution when compared to the original target variable.\n",
      "\n",
      "Distribution of square-root-transformed target variable values\n",
      "\n",
      "### Impact of training on log-transformed targets\n",
      "\n",
      "So, does applying these transformations have any impact on model performance? Let's evaluate the various metrics we used previously on log-transformed data as an example.\n",
      "\n",
      "We will do this first for the linear model by applying the `numpy log` function to the `label` field of each `LabeledPoint` RDD. Here, we will only transform the target variable, and we will not apply any transformations to the features:\n",
      "\n",
      "    data_log = data.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\n",
      "\n",
      "We will then train a model on this transformed data and form the RDD of predicted versus true values:\n",
      "\n",
      "    model_log = LinearRegressionWithSGD.train(data_log, iterations=10, step=0.1)\n",
      "\n",
      "Note that now that we have transformed the target variable, the predictions of the model will be on the log scale, as will the target values of the transformed dataset. Therefore, in order to use our model and evaluate its performance, we must first transform the log data back into the original scale by taking the exponent of both the predicted and true values using the `numpy exp` function. We will show you how to do this in the code here:\n",
      "\n",
      "    true_vs_predicted_log = data_log.map(lambda p: (np.exp(p.label), np.exp(model_log.predict(p.features))))\n",
      "\n",
      "Finally, we will compute the MSE, MAE, and RMSLE metrics for the model:\n",
      "\n",
      "    mse_log = true_vs_predicted_log.map(lambda (t, p): squared_error(t, p)).mean()\n",
      "    mae_log = true_vs_predicted_log.map(lambda (t, p): abs_error(t, p)).mean()\n",
      "    rmsle_log = np.sqrt(true_vs_predicted_log.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "    print \"Mean Squared Error: %2.4f\" % mse_log\n",
      "    print \"Mean Absolue Error: %2.4f\" % mae_log\n",
      "    print \"Root Mean Squared Log Error: %2.4f\" % rmsle_log\n",
      "    print \"Non log-transformed predictions:\\n\" + str(true_vs_predicted.take(3))\n",
      "    print \"Log-transformed predictions:\\n\" + str(true_vs_predicted_log.take(3))\n",
      "\n",
      "You should see output similar to the following:\n",
      "\n",
      "    **Mean Squared Error: 38606.0875**\n",
      "    **Mean Absolue Error: 135.2726**\n",
      "    **Root Mean Squared Log Error: 1.3516**\n",
      "    **Non log-transformed predictions:**\n",
      "    **[(16.0, 119.30920003093594), (40.0, 116.95463511937378), (32.0, 116.57294610647752)]**\n",
      "    **Log-transformed predictions:**\n",
      "    **[(15.999999999999998, 45.860944832110015), (40.0, 43.255903592233274), (32.0, 42.311306147884252)]**\n",
      "\n",
      "If we compare these results to the results on the raw target variable, we see that while we did not improve the MSE or MAE, we improved the RMSLE.\n",
      "\n",
      "We will perform the same analysis for the decision tree model:\n",
      "\n",
      "    data_dt_log = data_dt.map(lambda lp: LabeledPoint(np.log(lp.label), lp.features))\n",
      "    dt_model_log = DecisionTree.trainRegressor(data_dt_log,{})\n",
      "\n",
      "    preds_log = dt_model_log.predict(data_dt_log.map(lambda p: p.features))\n",
      "    actual_log = data_dt_log.map(lambda p: p.label)\n",
      "    true_vs_predicted_dt_log = actual_log.zip(preds_log).map(lambda (t, p): (np.exp(t), np.exp(p)))\n",
      "\n",
      "    mse_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): squared_error(t, p)).mean()\n",
      "    mae_log_dt = true_vs_predicted_dt_log.map(lambda (t, p): abs_error(t, p)).mean()\n",
      "    rmsle_log_dt = np.sqrt(true_vs_predicted_dt_log.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "    print \"Mean Squared Error: %2.4f\" % mse_log_dt\n",
      "    print \"Mean Absolue Error: %2.4f\" % mae_log_dt\n",
      "    print \"Root Mean Squared Log Error: %2.4f\" % rmsle_log_dt\n",
      "    print \"Non log-transformed predictions:\\n\" + str(true_vs_predicted_dt.take(3))\n",
      "    print \"Log-transformed predictions:\\n\" + str(true_vs_predicted_dt_log.take(3))\n",
      "\n",
      "From the results here, we can see that we actually made our metrics slightly worse for the decision tree:\n",
      "\n",
      "    **Mean Squared Error: 14781.5760**\n",
      "    **Mean Absolue Error: 76.4131**\n",
      "    **Root Mean Squared Log Error: 0.6406**\n",
      "    **Non log-transformed predictions:**\n",
      "    **[(16.0, 54.913223140495866), (40.0, 54.913223140495866), (32.0, 53.171052631578945)]**\n",
      "    **Log-transformed predictions:**\n",
      "    **[(15.999999999999998, 37.530779787154508), (40.0, 37.530779787154508), (32.0, 7.2797070993907287)]**\n",
      "\n",
      "### Tip\n",
      "\n",
      "It is probably not surprising that the log transformation results in a better RMSLE performance for the linear model. As we are minimizing the squared error, once we have transformed the target variable to log values, we are effectively minimizing a loss function that is very similar to the RMSLE.\n",
      "\n",
      "This is good for Kaggle competition purposes, since we can more directly optimize against the competition-scoring metric.\n",
      "\n",
      "It might or might not be as useful in a real-world situation. This depends on how important larger absolute errors are (recall that RMSLE essentially penalizes relative errors rather than absolute magnitude of errors).\n",
      "\n",
      "## Tuning model parameters\n",
      "\n",
      "So far in this chapter, we have illustrated the concepts of model training and evaluation for MLlib's regression models by training and testing on the same dataset. We will now use a similar cross-validation approach that we used previously to evaluate the effect on performance of different parameter settings for our models.\n",
      "\n",
      "### Creating training and testing sets to evaluate parameters\n",
      "\n",
      "The first step is to create a test and training set for cross-validation purposes. Spark's Python API does not yet provide the `randomSplit` convenience method that is available in Scala. Hence, we will need to create a training and test dataset manually.\n",
      "\n",
      "One relatively easy way to do this is by first taking a random sample of, say, 20 percent of our data as our test set. We will then define our training set as the elements of the original RDD that are not in the test set RDD.\n",
      "\n",
      "We can achieve this using the `sample` method to take a random sample for our test set, followed by using the `subtractByKey` method, which takes care of returning the elements in one RDD where the keys do not overlap with the other RDD.\n",
      "\n",
      "Note that `subtractByKey`, as the name suggests, works on the keys of the RDD elements that consist of key-value pairs. Therefore, here we will use `zipWithIndex` on our RDD of extracted training examples. This creates an RDD of `(LabeledPoint, index)` pairs.\n",
      "\n",
      "We will then reverse the keys and values so that we can operate on the index keys:\n",
      "\n",
      "    data_with_idx = data.zipWithIndex().map(lambda (k, v): (v, k)) \n",
      "    test = data_with_idx.sample(False, 0.2, 42)\n",
      "    train = data_with_idx.subtractByKey(test)\n",
      "\n",
      "Once we have the two RDDs, we will recover just the `LabeledPoint` instances we need for training and test data, using `map` to extract the value from the key-value pairs:\n",
      "\n",
      "    train_data = train.map(lambda (idx, p): p)\n",
      "    test_data = test.map(lambda (idx, p) : p)\n",
      "    train_size = train_data.count()\n",
      "    test_size = test_data.count()\n",
      "    print \"Training data size: %d\" % train_size\n",
      "    print \"Test data size: %d\" % test_size\n",
      "    print \"Total data size: %d \" % num_data\n",
      "    print \"Train + Test size : %d\" % (train_size + test_size)\n",
      "\n",
      "We can confirm that we now have two distinct datasets that add up to the original dataset in total:\n",
      "\n",
      "    **Training data size: 13934**\n",
      "    **Test data size: 3445**\n",
      "    **Total data size: 17379**\n",
      "    **Train + Test size : 17379**\n",
      "\n",
      "The final step is to apply the same approach to the features extracted for the decision tree model:\n",
      "\n",
      "    data_with_idx_dt = data_dt.zipWithIndex().map(lambda (k, v): (v, k))\n",
      "    test_dt = data_with_idx_dt.sample(False, 0.2, 42)\n",
      "    train_dt = data_with_idx_dt.subtractByKey(test_dt)\n",
      "    train_data_dt = train_dt.map(lambda (idx, p): p)\n",
      "    test_data_dt = test_dt.map(lambda (idx, p) : p)\n",
      "\n",
      "### The impact of parameter settings for linear models\n",
      "\n",
      "Now that we have prepared our training and test sets, we are ready to investigate the impact of different parameter settings on model performance. We will first carry out this evaluation for the linear model. We will create a convenience function to evaluate the relevant performance metric by training the model on the training set and evaluating it on the test set for different parameter settings.\n",
      "\n",
      "We will use the RMSLE evaluation metric, as it is the one used in the Kaggle competition with this dataset, and this allows us to compare our model results against the competition leaderboard to see how we perform.\n",
      "\n",
      "The evaluation function is defined here:\n",
      "\n",
      "    def evaluate(train, test, iterations, step, regParam, regType, intercept):\n",
      "        model = LinearRegressionWithSGD.train(train, iterations, step, regParam=regParam, regType=regType, intercept=intercept)\n",
      "        tp = test.map(lambda p: (p.label, model.predict(p.features)))\n",
      "        rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "        return rmsle\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that in the following sections, you might get slightly different results due to some random initialization for SGD. However, your results will be comparable.\n",
      "\n",
      "#### Iterations\n",
      "\n",
      "As we saw when evaluating our classification models, we generally expect that a model trained with SGD will achieve better performance as the number of iterations increases, although the increase in performance will slow down as the number of iterations goes above some minimum number. Note that here, we will set the step size to 0.01 to better illustrate the impact at higher iteration numbers:\n",
      "\n",
      "    params = [1, 5, 10, 20, 50, 100]\n",
      "    metrics = [evaluate(train_data, test_data, **param** , 0.01, 0.0, 'l2', False) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "\n",
      "The output shows that the error metric indeed decreases as the number of iterations increases. It also does so at a decreasing rate, again as expected. What is interesting is that eventually, the SGD optimization tends to overshoot the optimal solution, and the RMSLE eventually starts to increase slightly:\n",
      "\n",
      "    **[1, 5, 10, 20, 50, 100]**\n",
      "    **[2.3532904530306888, 1.6438528499254723, 1.4869656275309227, 1.4149741941240344, 1.4159641262731959, 1.4539667094611679]**\n",
      "\n",
      "Here, we will use the `matplotlib` library to plot a graph of the RMSLE metric against the number of iterations. We will use a log scale for the _x_ axis to make the output easier to visualize:\n",
      "\n",
      "    plot(params, metrics)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    pyplot.xscale('log')\n",
      "\n",
      "Metrics for varying number of iterations\n",
      "\n",
      "#### Step size\n",
      "\n",
      "We will perform a similar analysis for step size in the following code:\n",
      "\n",
      "    params = [0.01, 0.025, 0.05, 0.1, 1.0]\n",
      "    metrics = [evaluate(train_data, test_data, 10, **param** , 0.0, 'l2', False) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "\n",
      "The output of the preceding code:\n",
      "\n",
      "    **[0.01, 0.025, 0.05, 0.1, 0.5]**\n",
      "    **[1.4869656275309227, 1.4189071944747715, 1.5027293911925559, 1.5384660954019973, nan]**\n",
      "\n",
      "Now, we can see why we avoided using the default step size when training the linear model originally. The default is set to _1.0_ , which, in this case, results in a `nan` output for the RMSLE metric. This typically means that the SGD model has converged to a very poor local minimum in the error function that it is optimizing. This can happen when the step size is relatively large, as it is easier for the optimization algorithm to overshoot good solutions.\n",
      "\n",
      "We can also see that for low step sizes and a relatively low number of iterations (we used 10 here), the model performance is slightly poorer. However, in the preceding _Iterations_ section, we saw that for the lower step-size setting, a higher number of iterations will generally converge to a better solution.\n",
      "\n",
      "Generally speaking, setting step size and number of iterations involves a trade-off. A lower step size means that convergence is slower but slightly more assured. However, it requires a higher number of iterations, which is more costly in terms of computation and time, in particular at a very large scale.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Selecting the best parameter settings can be an intensive process that involves training a model on many combinations of parameter settings and selecting the best outcome. Each instance of model training involves a number of iterations, so this process can be very expensive and time consuming when performed on very large datasets.\n",
      "\n",
      "The output is plotted here, again using a log scale for the step-size axis:\n",
      "\n",
      "Metrics for varying values of step size\n",
      "\n",
      "#### L2 regularization\n",
      "\n",
      "In Chapter 5, _Building a Classification Model with Spark_ , we saw that regularization has the effect of penalizing model complexity in the form of an additional loss term that is a function of the model weight vector. L2 regularization penalizes the L2-norm of the weight vector, while L1 regularization penalizes the L1-norm.\n",
      "\n",
      "We expect training set performance to deteriorate with increasing regularization, as the model cannot fit the dataset well. However, we would also expect some amount of regularization that will result in optimal generalization performance as evidenced by the best performance on the test set.\n",
      "\n",
      "We will evaluate the impact of different levels of L2 regularization in this code:\n",
      "\n",
      "    params = [0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]\n",
      "    metrics = [evaluate(train_data, test_data, 10, 0.1, **param** , **'l2'** , False) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "    plot(params, metrics)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    pyplot.xscale('log')\n",
      "\n",
      "As expected, there is an optimal setting of the regularization parameter with respect to the test set RMSLE:\n",
      "\n",
      "    **[0.0, 0.01, 0.1, 1.0, 5.0, 10.0, 20.0]**\n",
      "    **[1.5384660954019971, 1.5379108106882864, 1.5329809395123755, 1.4900275345312988, 1.4016676336981468, 1.40998359211149, 1.5381771283158705]**\n",
      "\n",
      "This is easiest to see in the following plot (where we once more use the log scale for the regularization parameter axis):\n",
      "\n",
      "Metrics for varying levels of L2 regularization\n",
      "\n",
      "#### L1 regularization\n",
      "\n",
      "We can apply the same approach for differing levels of L1 regularization:\n",
      "\n",
      "    params = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
      "    metrics = [evaluate(train_data, test_data, 10, 0.1, **param** , **'l1'** , False) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "    plot(params, metrics)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "    pyplot.xscale('log')\n",
      "\n",
      "Again, the results are more clearly seen when plotted in the following graph. We see that there is a much more subtle decline in RMSLE, and it takes a very high value to cause a jump back up. Here, the level of L1 regularization required is much higher than that for the L2 form; however, the overall performance is poorer:\n",
      "\n",
      "    **[0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]**\n",
      "    **[1.5384660954019971, 1.5384518080419873, 1.5383237472930684, 1.5372017600929164, 1.5303809928601677, 1.4352494587433793, 4.7551250073268614]**\n",
      "\n",
      "Metrics for varying levels of L1 regularization\n",
      "\n",
      "Using L1 regularization can encourage sparse weight vectors. Does this hold true in this case? We can find out by examining the number of entries in the weight vector that are zero, with increasing levels of regularization:\n",
      "\n",
      "    model_l1 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=1.0, regType='l1', intercept=False)\n",
      "    model_l1_10 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=10.0, regType='l1', intercept=False)\n",
      "    model_l1_100 = LinearRegressionWithSGD.train(train_data, 10, 0.1, regParam=100.0, regType='l1', intercept=False)\n",
      "    print \"L1 (1.0) number of zero weights: \" + str(sum(model_l1.weights.array == 0))\n",
      "    print \"L1 (10.0) number of zeros weights: \" + str(sum(model_l1_10.weights.array == 0))\n",
      "    print \"L1 (100.0) number of zeros weights: \" + str(sum(model_l1_100.weights.array == 0))\n",
      "\n",
      "We can see from the results that as we might expect, the number of zero feature weights in the model weight vector increases as greater levels of L1 regularization are applied:\n",
      "\n",
      "    **L1 (1.0) number of zero weights: 4**\n",
      "    **L1 (10.0) number of zeros weights: 20**\n",
      "    **L1 (100.0) number of zeros weights: 55**\n",
      "\n",
      "#### Intercept\n",
      "\n",
      "The final parameter option for the linear model is whether to use an intercept or not. An intercept is a constant term that is added to the weight vector and effectively accounts for the mean value of the target variable. If the data is already centered or normalized, an intercept is not necessary; however, it often does not hurt to use one in any case.\n",
      "\n",
      "We will evaluate the effect of adding an intercept term to the model here:\n",
      "\n",
      "    params = [False, True]\n",
      "    metrics = [evaluate(train_data, test_data, 10, 0.1, 1.0, 'l2', **param** ) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "    bar(params, metrics, color='lightblue')\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "\n",
      "We can see from the result and plot that adding the intercept term results in a very slight increase in RMSLE:\n",
      "\n",
      "    **[False, True]**\n",
      "    **[1.4900275345312988, 1.506469812020645]**\n",
      "\n",
      "Metrics without and with an intercept\n",
      "\n",
      "### The impact of parameter settings for the decision tree\n",
      "\n",
      "Decision trees provide two main parameters: maximum tree depth and the maximum number of bins. We will now perform the same evaluation of the effect of parameter settings for the decision tree model. Our starting point is to create an evaluation function for the model, similar to the one used for the linear regression earlier. This function is provided here:\n",
      "\n",
      "    def evaluate_dt(train, test, maxDepth, maxBins):\n",
      "        model = DecisionTree.trainRegressor(train, {}, impurity='variance', maxDepth=maxDepth, maxBins=maxBins)\n",
      "        preds = model.predict(test.map(lambda p: p.features))\n",
      "        actual = test.map(lambda p: p.label)\n",
      "        tp = actual.zip(preds)\n",
      "        rmsle = np.sqrt(tp.map(lambda (t, p): squared_log_error(t, p)).mean())\n",
      "        return rmsle\n",
      "\n",
      "#### Tree depth\n",
      "\n",
      "We would generally expect performance to increase with more complex trees (that is, trees of greater depth). Having a lower tree depth acts as a form of regularization, and it might be the case that as with L2 or L1 regularization in linear models, there is a tree depth that is optimal with respect to the test set performance.\n",
      "\n",
      "Here, we will try to increase the depths of trees to see what impact they have on test set RMSLE, keeping the number of bins at the default level of `32`:\n",
      "\n",
      "    params = [1, 2, 3, 4, 5, 10, 20]\n",
      "    metrics = [evaluate_dt(train_data_dt, test_data_dt, param, 32) for param in params] \n",
      "    print params\n",
      "    print metrics\n",
      "    plot(params, metrics)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "\n",
      "In this case, it appears that the decision tree starts over-fitting at deeper tree levels. An optimal tree depth appears to be around 10 on this dataset.\n",
      "\n",
      "### Note\n",
      "\n",
      "Notice that our best RMSLE of 0.42 is now quite close to the Kaggle winner of around 0.29!\n",
      "\n",
      "The output of the tree depth is as follows:\n",
      "\n",
      "    **[1, 2, 3, 4, 5, 10, 20]**\n",
      "    **[1.0280339660196287, 0.92686672078778276, 0.81807794023407532, 0.74060228537329209, 0.63583503599563096, 0.42851360418692447, 0.45500008049779139]**\n",
      "\n",
      "Metrics for different tree depths\n",
      "\n",
      "#### Maximum bins\n",
      "\n",
      "Finally, we will perform our evaluation on the impact of setting the number of bins for the decision tree. As with the tree depth, a larger number of bins should allow the model to become more complex and might help performance with larger feature dimensions. After a certain point, it is unlikely that it will help any more and might, in fact, hinder performance on the test set due to over-fitting:\n",
      "\n",
      "    params = [2, 4, 8, 16, 32, 64, 100]\n",
      "    metrics = [evaluate_dt(train_data_dt, test_data_dt, 5, param) for param in params]\n",
      "    print params\n",
      "    print metrics\n",
      "    plot(params, metrics)\n",
      "    fig = matplotlib.pyplot.gcf()\n",
      "\n",
      "Here, we will show the output and plot to vary the number of bins (while keeping the tree depth at the default level of 5). In this case, using a small number of bins hurts performance, while there is no impact when we use around 32 bins (the default setting) or more. There seems to be an optimal setting for test set performance at around 16-20 bins:\n",
      "\n",
      "    **[2, 4, 8, 16, 32, 64, 100]**\n",
      "    **[1.3069788763726049, 0.81923394899750324, 0.75745322513058744, 0.62328384445223795, 0.63583503599563096, 0.63583503599563096, 0.63583503599563096]**\n",
      "\n",
      "Metrics for different maximum bins\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, you saw how to use MLlib's linear model and decision tree functionality in Python within the context of regression models. We explored categorical feature extraction and the impact of applying transformations to the target variable in a regression problem. Finally, we implemented various performance-evaluation metrics and used them to implement a cross-validation exercise that explores the impact of the various parameter settings available in both linear models and decision trees on test set model performance.\n",
      "\n",
      "In the next chapter, we will cover a different approach to machine learning, that is unsupervised learning, specifically in clustering models.\n",
      "\n",
      "# Chapter 7. Building a Clustering Model with Spark\n",
      "\n",
      "In the last few chapters, we covered supervised learning methods, where the training data is labeled with the true outcome that we would like to predict (for example, a rating for recommendations and class assignment for classification or real target variable in the case of regression).\n",
      "\n",
      "Next, we will consider the case when we do not have labeled data available. This is called unsupervised learning, as the model is not supervised with the true target label. The unsupervised case is very common in practice, since obtaining labeled training data can be very difficult or expensive in many real-world scenarios (for example, having humans label training data with class labels for classification). However, we would still like to learn some underlying structure in the data and use these to make predictions.\n",
      "\n",
      "This is where unsupervised learning approaches can be useful. Unsupervised learning models are also often combined with supervised models, for example, applying unsupervised techniques to create new input features for supervised models.\n",
      "\n",
      "Clustering models are, in many ways, the unsupervised equivalent of classification models. With classification, we tried to learn a model that would predict which class a given training example belonged to. The model was essentially a mapping from a set of features to the class.\n",
      "\n",
      "In clustering, we would like to segment the data such that each training example is assigned to a segment called a **cluster**. The clusters act much like classes, except that the true class assignments are unknown.\n",
      "\n",
      "Clustering models have many use cases that are the same as classification; these include the following:\n",
      "\n",
      "  * Segmenting users or customers into different groups based on behavior characteristics and metadata\n",
      "  * Grouping content on a website or products in a retail business\n",
      "  * Finding clusters of similar genes\n",
      "  * Segmenting communities in ecology\n",
      "  * Creating image segments for use in image analysis applications such as object detection\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Briefly explore a few types of clustering models\n",
      "  * Extract features from data specifically using the output of one model as input features for our clustering model\n",
      "  * Train a clustering model and use it to make predictions\n",
      "  * Apply performance-evaluation and parameter-selection techniques to select the optimal number of clusters to use\n",
      "\n",
      "# Types of clustering models\n",
      "\n",
      "There are many different forms of clustering models available, ranging from simple to extremely complex ones. The MLlib library currently provides K-means clustering, which is among the simplest approaches available. However, it is often very effective, and its simplicity means it is relatively easy to understand and is scalable.\n",
      "\n",
      "## K-means clustering\n",
      "\n",
      "K-means attempts to partition a set of data points into K distinct clusters (where K is an input parameter for the model).\n",
      "\n",
      "More formally, K-means tries to find clusters so as to minimize the sum of squared errors (or distances) within each cluster. This objective function is known as the **within cluster sum of squared errors** ( **WCSS** ).\n",
      "\n",
      "It is the sum, over each cluster, of the squared errors between each point and the cluster center.\n",
      "\n",
      "Starting with a set of K initial cluster centers (which are computed as the mean vector for all data points in the cluster), the standard method for K-means iterates between two steps:\n",
      "\n",
      "  1. Assign each data point to the cluster that minimizes the WCSS. The sum of squares is equivalent to the squared Euclidean distance; therefore, this equates to assigning each point to the **closest** cluster center as measured by the Euclidean distance metric.\n",
      "  2. Compute the new cluster centers based on the cluster assignments from the first step.\n",
      "\n",
      "The algorithm proceeds until either a maximum number of iterations has been reached or **convergence** has been achieved. Convergence means that the cluster assignments no longer change during the first step; therefore, the value of the WCSS objective function does not change either.\n",
      "\n",
      "### Tip\n",
      "\n",
      "For more details, refer to Spark's documentation on clustering at <http://spark.apache.org/docs/latest/mllib-clustering.html> or refer to <http://en.wikipedia.org/wiki/K-means_clustering>.\n",
      "\n",
      "To illustrate the basics of K-means, we will use the simple dataset we showed in our multiclass classification example in Chapter 5, _Building a Classification Model with Spark_. Recall that we have five classes, which are shown in the following figure:\n",
      "\n",
      "Multiclass dataset\n",
      "\n",
      "However, assume that we don't actually know the true classes. If we use K-means with five clusters, then after the first step, the model's cluster assignments might look like this:\n",
      "\n",
      "Cluster assignments after the first K-means iteration\n",
      "\n",
      "We can see that K-means has already picked out the centers of each cluster fairly well. After the next iteration, the assignments might look like those shown in the following figure:\n",
      "\n",
      "Cluster assignments after the second K-means iteration\n",
      "\n",
      "Things are starting to stabilize, but the overall cluster assignments are broadly the same as they were after the first iteration. Once the model has converged, the final assignments could look like this:\n",
      "\n",
      "Final cluster assignments for K-means\n",
      "\n",
      "As we can see, the model has done a decent job of separating the five clusters. The leftmost three are fairly accurate (with a few incorrect points). However, the two clusters in the bottom-right corner are less accurate.\n",
      "\n",
      "This illustrates:\n",
      "\n",
      "  * The iterative nature of K-means\n",
      "  * The model's dependency on the method of initially selecting clusters' centers (here, we will use a random approach)\n",
      "  * That the final cluster assignments can be very good for well-separated data but can be poor for data that is more difficult\n",
      "\n",
      "### Initialization methods\n",
      "\n",
      "The standard initialization method for K-means, usually simply referred to as the random method, starts by randomly assigning each data point to a cluster before proceeding with the first update step.\n",
      "\n",
      "MLlib provides a parallel variant for this initialization method, called K-means ||, which is the default initialization method used.\n",
      "\n",
      "MLlib provides a parallel variant called **K-means** **||** , **||** , for this initialization method; this is the default initialization method used.\n",
      "\n",
      "### Note\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/K-means_clustering#Initialization_methods> and <http://en.wikipedia.org/wiki/K-means%2B%2B> for more information.\n",
      "\n",
      "The results of using K-means++ are shown here. Note that this time, the difficult lower-right points have been mostly correctly clustered.\n",
      "\n",
      "Final cluster assignments for K-means++\n",
      "\n",
      "### Variants\n",
      "\n",
      "There are many other variants of K-means; they focus on initialization methods or the core model. One of the more common variants is fuzzy K-means. This model does not assign each point to one cluster as K-means does (a so-called hard assignment). Instead, it is a soft version of K-means, where each point can belong to many clusters, and is represented by the relative membership to each cluster. So, for K clusters, each point is represented as a K-dimensional membership vector, with each entry in this vector indicating the membership proportion in each cluster.\n",
      "\n",
      "## Mixture models\n",
      "\n",
      "A **mixture model** is essentially an extension of the idea behind fuzzy K-means; however, it makes an assumption that there is an underlying probability distribution that generates the data. For example, we might assume that the data points are drawn from a set of K-independent Gaussian (normal) probability distributions. The cluster assignments are also soft, so each point is represented by K membership weights in each of the K underlying probability distributions.\n",
      "\n",
      "### Note\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Mixture_model> for further details and for a mathematical treatment of mixture models.\n",
      "\n",
      "## Hierarchical clustering\n",
      "\n",
      " **Hierarchical clustering** is a structured clustering approach that results in a multilevel hierarchy of clusters, where each cluster might contain many subclusters (or child clusters). Each child cluster is, thus, linked to the parent cluster. This form of clustering is often also called tree clustering.\n",
      "\n",
      "Agglomerative clustering is a bottom-up approach where:\n",
      "\n",
      "  * Each data point begins in its own cluster\n",
      "  * The similarity (or distance) between each pair of clusters is evaluated\n",
      "  * The pair of clusters that are most similar are found; this pair is then merged to form a new cluster\n",
      "  * The process is repeated until only one top-level cluster remains\n",
      "\n",
      " **Divisive** clustering is a top-down approach that works in reverse, starting with one cluster and at each stage, splitting a cluster into two, until all data points are allocated to their own bottom-level cluster.\n",
      "\n",
      "### Note\n",
      "\n",
      "You can find more information at <http://en.wikipedia.org/wiki/Hierarchical_clustering>.\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "Like most of the machine learning models we have encountered so far, K-means clustering requires numerical vectors as input. The same feature extraction and transformation approaches that we have seen for classification and regression are applicable for clustering.\n",
      "\n",
      "As K-means, like least squares regression, uses a squared error function as the optimization objective, it tends to be impacted by outliers and features with large variance.\n",
      "\n",
      "As for regression and classification cases, input data can be normalized and standardized to overcome this, which might improve accuracy. In some cases, however, it might be desirable not to standardize data, if, for example, the objective is to find segmentations according to certain specific features.\n",
      "\n",
      "## Extracting features from the MovieLens dataset\n",
      "\n",
      "For this example, we will return to the movie rating dataset we used in Chapter 4, _Building a Recommendation Engine with Spark_. Recall that we have three main datasets: one that contains the movie ratings (in the `u.data` file), a second one with user data (`u.user`), and a third one with movie data (`u.item`). We will also be using the genre data file to extract the genres for each movie (`u.genre`).\n",
      "\n",
      "We will start by looking at the movie data:\n",
      "\n",
      "    val movies = sc.textFile(\"/PATH/ml-100k/u.item\")\n",
      "    println(movies.first)\n",
      "\n",
      "This should output the first line of the dataset:\n",
      "\n",
      "    **1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0**\n",
      "\n",
      "So, we have access to the move title, and we already have the movies categorized into genres. Why do we need to apply a clustering model to the movies? Clustering the movies is a useful exercise for two reasons:\n",
      "\n",
      "  * First, because we have access to the true genre labels, we can use these to evaluate the quality of the clusters that the model finds\n",
      "  * Second, we might wish to segment the movies based on some other attributes or features, apart from their genres\n",
      "\n",
      "For example, in this case, it seems that we don't have a lot of data to use for clustering, apart from the genres and title. However, this is not true--we also have the ratings data. Previously, we created a matrix factorization model from the ratings data. The model is made up of a set of user and movie factor vectors.\n",
      "\n",
      "We can think of the movie factors as representing each movie in a new latent feature space, where each latent feature, in turn, represents some form of structure in the ratings matrix. While it is not possible to directly interpret each latent feature, they might represent some hidden structure that influences the ratings behavior between users and movies. One factor could represent genre preference, another could refer to actors or directors, while yet another could represent the theme of the movie, and so on.\n",
      "\n",
      "So, if we use these factor vector representations of each movie as inputs to our clustering model, we will end up with a clustering that is based on the actual rating behavior of users rather than manual genre assignments.\n",
      "\n",
      "The same logic applies to the user factors--they represent users in the latent feature space of rating behavior, so clustering the user vectors should result in a clustering based on user rating behavior.\n",
      "\n",
      "### Extracting movie genre labels\n",
      "\n",
      "Before proceeding further, let's extract the genre mappings from the `u.genre` file. As you can see from the first line of the preceding dataset, we will need to map from the numerical genre assignments to the textual version so that they are readable.\n",
      "\n",
      "Take a look at the first few lines of `u.genre`:\n",
      "\n",
      "    val genres = sc.textFile(\"/PATH/ml-100k/u.genre\")\n",
      "    genres.take(5).foreach(println)\n",
      "\n",
      "You should see the following output displayed:\n",
      "\n",
      "    **unknown|0**\n",
      "    **Action|1**\n",
      "    **Adventure|2**\n",
      "    **Animation|3**\n",
      "    **Children's|4**\n",
      "\n",
      "Here, `0` is the index of the relevant genre, while `unknown` is the genre assigned for this index. The indices correspond to the indices of the binary subvector that will represent the genres for each movie (that is, the 0s and 1s in the preceding movie data).\n",
      "\n",
      "To extract the genre mappings, we will split each line and extract a key-value pair, where the key is the text genre and the value is the index. Note that we have to filter out an empty line at the end; this will, otherwise, throw an error when we try to split the line (see the code highlighted here):\n",
      "\n",
      "    val genreMap = genres.filter( **!_.isEmpty** ).map(line => line.split(\"\\\\|\")).map(array => (array(1), array(0))).collectAsMap\n",
      "    println(genreMap)\n",
      "\n",
      "The preceding code will provide the following output:\n",
      "\n",
      "    **Map(2 - > Adventure, 5 -> Comedy, 12 -> Musical, 15 -> Sci-Fi, 8 -> Drama, 18 -> Western, ...**\n",
      "\n",
      "Next, we'll create a new RDD from the movie data and our genre mapping; this RDD contains the movie ID, title, and genres. We will use this later to create a more readable output when we evaluate the clusters assigned to each movie by our clustering model.\n",
      "\n",
      "In the following code section, we will map over each movie and extract the genres subvector (which will still contain `Strings` rather than `Int` indexes). We will then apply the `zipWithIndex` method to create a new collection that contains the indices of the genre subvector, and we will filter this collection so that we are left only with the positive assignments (that is, the 1s that denote a genre assignment for the relevant index). We can then use our extracted genre mapping to map these indices to the textual genres. Finally, we will inspect the first record of the new `RDD` to see the result of these operations:\n",
      "\n",
      "    val titlesAndGenres = movies.map(_.split(\"\\\\|\")).map { array =>\n",
      "      val genres = array.toSeq.slice(5, array.size)\n",
      "      val genresAssigned = genres.zipWithIndex.filter { case (g, idx) => \n",
      "        g == \"1\" \n",
      "      }.map { case (g, idx) => \n",
      "        genreMap(idx.toString) \n",
      "      }\n",
      "      (array(0).toInt, (array(1), genresAssigned))\n",
      "    }\n",
      "    println(titlesAndGenres.first)\n",
      "\n",
      "This should output the following result:\n",
      "\n",
      "    **(1,(Toy Story (1995),ArrayBuffer(Animation, Children's, Comedy)))**\n",
      "\n",
      "### Training the recommendation model\n",
      "\n",
      "To get the user and movie factor vectors, we first need to train another recommendation model. Fortunately, we have already done this in Chapter 4, _Building a Recommendation Engine with Spark_ , so we will follow the same procedure:\n",
      "\n",
      "    import org.apache.spark.mllib.recommendation.ALS\n",
      "    import org.apache.spark.mllib.recommendation.Rating\n",
      "    val rawData = sc.textFile(\"/PATH/ml-100k/u.data\")\n",
      "    val rawRatings = rawData.map(_.split(\"\\t\").take(3))\n",
      "    val ratings = rawRatings.map{ case Array(user, movie, rating) => Rating(user.toInt, movie.toInt, rating.toDouble) }\n",
      "    ratings.cache\n",
      "    val alsModel = ALS.train(ratings, 50, 10, 0.1)\n",
      "\n",
      "Recall from Chapter 4, _Building a Recommendation Engine with Spark_ , that the ALS model returned contains the factors in two RDDs of key-value pairs (called `userFeatures` and `productFeatures`) with the user or movie ID as the key and the factor as the value. We will need to extract just the factors and transform each one of them into an MLlib `Vector` to use as training input for our clustering model.\n",
      "\n",
      "We will do this for both users and movies as follows:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.Vectors\n",
      "    val movieFactors = alsModel.productFeatures.map { case (id, factor) => (id, Vectors.dense(factor)) }\n",
      "    val movieVectors = movieFactors.map(_._2)\n",
      "    val userFactors = alsModel.userFeatures.map { case (id, factor) => (id, Vectors.dense(factor)) }\n",
      "    val userVectors = userFactors.map(_._2)\n",
      "\n",
      "### Normalization\n",
      "\n",
      "Before we train our clustering model, it might be useful to look into the distribution of the input data in the form of the factor vectors. This will tell us whether we need to normalize the training data.\n",
      "\n",
      "We will follow the same approach as we did in Chapter 5, _Building a Classification Model with Spark_ , using MLlib's summary statistics available in the distributed `RowMatrix` class:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "    val movieMatrix = new RowMatrix(movieVectors)\n",
      "    val movieMatrixSummary = movieMatrix.computeColumnSummaryStatistics()\n",
      "    val userMatrix = new RowMatrix(userVectors)\n",
      "    val userMatrixSummary = \n",
      "    userMatrix.computeColumnSummaryStatistics()\n",
      "    println(\"Movie factors mean: \" + movieMatrixSummary.mean)\n",
      "    println(\"Movie factors variance: \" + movieMatrixSummary.variance)\n",
      "    println(\"User factors mean: \" + userMatrixSummary.mean)\n",
      "    println(\"User factors variance: \" + userMatrixSummary.variance)\n",
      "\n",
      "You should see output similar to the one here:\n",
      "\n",
      "    **Movie factors mean: [0.28047737659519767,0.26886479057520024,0.2935579964446398,0.27821738264113755, ...**\n",
      "    **Movie factors variance: [0.038242041794064895,0.03742229118854288,0.044116961097355877,0.057116244055791986, ...**\n",
      "    **User factors mean: [0.2043520841572601,0.22135773814655782,0.2149706318418221,0.23647602029329481, ...**\n",
      "    **User factors variance: [0.037749421148850396,0.02831191551960241,0.032831876953314174,0.036775110657850954, ...**\n",
      "\n",
      "If we look at the output, we will see that there do not appear to be any important outliers that might skew the clustering results, so normalization should not be required in this case.\n",
      "\n",
      "# Training a clustering model\n",
      "\n",
      "Training for K-means in MLlib takes an approach similar to the other models--we pass an RDD that contains our training data to the `train` method of the `KMeans` object. Note that here we do not use `LabeledPoint` instances, as the labels are not used in clustering; they are used only in the feature vectors. Thus, we use a RDD `[Vector]` as input to the `train` method.\n",
      "\n",
      "## Training a clustering model on the MovieLens dataset\n",
      "\n",
      "We will train a model for both the movie and user factors that we generated by running our recommendation model. We need to pass in the number of clusters K and the maximum number of iterations for the algorithm to run. Model training might run for less than the maximum number of iterations if the change in the objective function from one iteration to the next is less than the tolerance level (the default for this tolerance is 0.0001).\n",
      "\n",
      "MLlib's K-means provides random and K-means || initialization, with the default being K-means ||. As both of these initialization methods are based on random selection to some extent, each model training run will return a different result.\n",
      "\n",
      "K-means does not generally converge to a global optimum model, so performing multiple training runs and selecting the most optimal model from these runs is a common practice. MLlib's training methods expose an option to complete multiple model training runs. The best training run, as measured by the evaluation of the loss function, is selected as the final model.\n",
      "\n",
      "We will first set up the required imports, as well as model parameters: K, maximum iterations, and number of runs:\n",
      "\n",
      "    import org.apache.spark.mllib.clustering.KMeans\n",
      "    val numClusters = 5\n",
      "    val numIterations = 10\n",
      "    val numRuns = 3\n",
      "\n",
      "We will then run K-means on the movie factor vectors:\n",
      "\n",
      "    val movieClusterModel = KMeans.train(movieVectors, numClusters, numIterations, numRuns)\n",
      "\n",
      "Once the model has completed training, we should see output that looks something like this:\n",
      "\n",
      "    **...**\n",
      "    **14/09/02 21:53:58 INFO SparkContext: Job finished: collectAsMap at KMeans.scala:193, took 0.02043 s**\n",
      "    **14/09/02 21:53:58 INFO KMeans: Iterations took 0.331 seconds.**\n",
      "    **14/09/02 21:53:58 INFO KMeans: KMeans reached the max number of iterations: 10.**\n",
      "    **14/09/02 21:53:58 INFO KMeans: The cost for the best run is 2586.298785925147**\n",
      "    **.**\n",
      "    **...**\n",
      "    **movieClusterModel: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@71c6f512**\n",
      "\n",
      "As can be seen from the highlighted text, the model training output tells us that the maximum number of iterations was reached, so the training process did not stop early based on the convergence criterion. It also shows the training set error (that is, the value of the K-means objective function) for the best run.\n",
      "\n",
      "We can try a much larger setting for the maximum iterations and use only one training run to see an example where the K-means model converges:\n",
      "\n",
      "    val movieClusterModelConverged = KMeans.train(movieVectors, numClusters, 100)\n",
      "\n",
      "You should be able to see the `KMeans converged in ... iterations` text in the model output; this text indicates that after so many iterations, the K-means objective function did not decrease more than the tolerance level:\n",
      "\n",
      "    **...**\n",
      "    **14/09/02 22:04:38 INFO SparkContext: Job finished: collectAsMap at KMeans.scala:193, took 0.040685 s**\n",
      "    **14/09/02 22:04:38 INFO KMeans: Run 0 finished in 34 iterations**\n",
      "    **14/09/02 22:04:38 INFO KMeans: Iterations took 0.812 seconds.**\n",
      "    **14/09/02 22:04:38 INFO KMeans: KMeans converged in 34 iterations.**\n",
      "    **14/09/02 22:04:38 INFO KMeans: The cost for the best run is 2584.9354332904104.**\n",
      "    **...**\n",
      "    **movieClusterModelConverged: org.apache.spark.mllib.clustering.KMeansModel = org.apache.spark.mllib.clustering.KMeansModel@6bb28fb5**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Notice that when we use a lower number of iterations but use multiple training runs, we typically get a training error (called cost above) that is very similar to the one we obtain by running the model to convergence. Using the multiple runs option can, therefore, be a very effective method to find the best possible model.\n",
      "\n",
      "Finally, we will also train a K-means model on the user factor vectors:\n",
      "\n",
      "    val userClusterModel = KMeans.train(userVectors, numClusters, numIterations, numRuns)\n",
      "\n",
      "# Making predictions using a clustering model\n",
      "\n",
      "Using the trained K-means model is straightforward and similar to the other models we have encountered so far, such as classification and regression. We can make a prediction for a single `Vector` instance as follows:\n",
      "\n",
      "    val movie1 = movieVectors.first\n",
      "    val movieCluster = movieClusterModel.predict(movie1)\n",
      "    println(movieCluster)\n",
      "\n",
      "We can also make predictions for multiple inputs by passing a RDD `[Vector]` to the `predict` method of the model:\n",
      "\n",
      "    val predictions = movieClusterModel.predict(movieVectors)\n",
      "    println(predictions.take(10).mkString(\",\"))\n",
      "\n",
      "The resulting output is a cluster assignment for each data point:\n",
      "\n",
      "    **0,0,1,1,2,1,0,1,1,1**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that due to random initialization, the cluster assignments might change from one run of the model to another, so your results might differ from those shown earlier. The cluster ID themselves have no inherent meaning; they are simply arbitrarily labeled, starting from 0.\n",
      "\n",
      "## Interpreting cluster predictions on the MovieLens dataset\n",
      "\n",
      "We have covered how to make predictions for a set of input vectors, but how do we evaluate how good the predictions are? We will cover performance metrics a little later; however, here, we will see how to manually inspect and interpret the cluster assignments made by our K-means model.\n",
      "\n",
      "While unsupervised techniques have the advantage that they do not require us to provide labeled data for training, the disadvantage is that often, the results need to be manually interpreted. Often, we would like to further examine the clusters that are found and possibly try to interpret them and assign some sort of labeling or categorization to them.\n",
      "\n",
      "For example, we can examine the clustering of movies we have found to try to see whether there is some meaningful interpretation of each cluster, such as a common genre or theme among the movies in the cluster. There are many approaches we can use, but we will start by taking a few movies in each cluster that are closest to the center of the cluster. These movies, we assume, would be the ones that are least likely to be marginal in terms of their cluster assignment, and so, they should be among the most representative of the movies in the cluster. By examining these sets of movies, we can see what attributes are shared by the movies in each cluster.\n",
      "\n",
      "### Interpreting the movie clusters\n",
      "\n",
      "To begin, we need to decide what we mean by \"closest to the center of each cluster\". The objective function that is minimized by K-means is the sum of Euclidean distances between each point and the cluster center, summed over all clusters. Therefore, it is natural to use the Euclidean distance as our measure.\n",
      "\n",
      "We will define this function here. Note that we will need access to certain imports from the **Breeze** library (a dependency of MLlib) for linear algebra and vector-based numerical functions:\n",
      "\n",
      "    import breeze.linalg._\n",
      "    import breeze.numerics.pow\n",
      "    def computeDistance(v1: DenseVector[Double], v2: DenseVector[Double]) = pow(v1 - v2, 2).sum\n",
      "\n",
      "### Tip\n",
      "\n",
      "The preceding `pow` function is a Breeze universal function. This function is the same as the `pow` function from `scala.math`, except that it operates element-wise on the vector that is returned from the minus operation between the two input vectors.\n",
      "\n",
      "Now, we will use this function to compute, for each movie, the distance of the relevant movie factor vector from the center vector of the assigned cluster. We will also join our cluster assignments and distances data with the movie titles and genres so that we can output the results in a more readable way:\n",
      "\n",
      "    val titlesWithFactors = titlesAndGenres.join(movieFactors)\n",
      "    val moviesAssigned = titlesWithFactors.map { case (id, ((title, genres), vector)) => \n",
      "      val pred = movieClusterModel.predict(vector)\n",
      "      val clusterCentre = movieClusterModel.clusterCenters(pred)\n",
      "      val dist = computeDistance(DenseVector(clusterCentre.toArray), DenseVector(vector.toArray))\n",
      "      (id, title, genres.mkString(\" \"), pred, dist) \n",
      "    }\n",
      "    val clusterAssignments = moviesAssigned.groupBy { case (id, title, genres, cluster, dist) => cluster }.collectAsMap\n",
      "\n",
      "After running the preceding code snippet, we have an RDD that contains a set of key-value pairs for each cluster; here, the key is the numeric cluster identifier, and the value is made up of a set of movies and related information. The movie information we have is the movie ID, title, genres, cluster index, and distance of the movie's factor vector from the cluster center.\n",
      "\n",
      "Finally, we will iterate through each cluster and output the top 20 movies, ranked by distance from closest to the cluster center:\n",
      "\n",
      "    for ( (k, v) <- clusterAssignments.toSeq.sortBy(_._1)) {\n",
      "      println(s\"Cluster $k:\")\n",
      "      val m = v.toSeq.sortBy(_._5)\n",
      "      println(m.take(20).map { case (_, title, genres, _, d) => (title, genres, d) }.mkString(\"\\n\")) \n",
      "      println(\"=====\\n\")\n",
      "    }\n",
      "\n",
      "The following screenshot is an example output. Note that your output might differ due to random initializations of both the recommendation and clustering model.\n",
      "\n",
      "The first cluster\n",
      "\n",
      "The first cluster, labeled 0, seems to contain a lot of old movies from the 1940s, 1950s, and 1960s, as well as a scattering of recent dramas.\n",
      "\n",
      "The second cluster\n",
      "\n",
      "The second cluster has a few horror movies in a prominent position, while the rest of the movies are less clear, but dramas are common too.\n",
      "\n",
      "The third cluster\n",
      "\n",
      "The third cluster is not clear-cut but has a fair number of comedy and drama movies.\n",
      "\n",
      "The fourth cluster\n",
      "\n",
      "The next cluster is more clearly associated with dramas and contains some foreign language films in particular.\n",
      "\n",
      "The last cluster\n",
      "\n",
      "The final cluster seems to be related predominantly to action and thrillers as well as romance movies, and seems to contain a number of relatively popular movies.\n",
      "\n",
      "As you can see, it is not always straightforward to determine exactly what each cluster represents. However, there is some evidence here that the clustering is picking out attributes or commonalities between groups of movies, which might not be immediately obvious based only on the movie titles and genres (such as a foreign language segment, a classic movie segment, and so on). If we had more metadata available, such as directors, actors, and so on, we might find out more details about the defining features of each cluster.\n",
      "\n",
      "### Tip\n",
      "\n",
      "We leave it as an exercise for you to perform a similar investigation into the clustering of the user factors. We have already created the input vectors in the `userVectors` variable, so you can train a K-means model on these vectors. After that, in order to evaluate the clusters, you would need to investigate the closest users for each cluster center (as we did for movies) and see if some common characteristics can be identified from the movies they have rated or the user metadata available.\n",
      "\n",
      "# Evaluating the performance of clustering models\n",
      "\n",
      "Like models such as regression, classification, and recommendation engines, there are many evaluation metrics that can be applied to clustering models to analyze their performance and the goodness of the clustering of the data points. Clustering evaluation is generally divided into either internal or external evaluation. Internal evaluation refers to the case where the same data used to train the model is used for evaluation. External evaluation refers to using data external to the training data for evaluation purposes.\n",
      "\n",
      "## Internal evaluation metrics\n",
      "\n",
      "Common internal evaluation metrics include the WCSS we covered earlier (which is exactly the K-means objective function), the Davies-Bouldin index, the Dunn Index, and the silhouette coefficient. All these measures tend to reward clusters where elements within a cluster are relatively close together, while elements in different clusters are relatively far away from each other.\n",
      "\n",
      "### Note\n",
      "\n",
      "The Wikipedia page on clustering evaluation at <http://en.wikipedia.org/wiki/Cluster_analysis#Internal_evaluation> has more details.\n",
      "\n",
      "## External evaluation metrics\n",
      "\n",
      "Since clustering can be thought of as unsupervised classification, if we have some form of labeled (or partially labeled) data available, we could use these labels to evaluate a clustering model. We can make predictions of clusters (that is, the class labels) using the model and evaluate the predictions against the true labels using metrics similar to some that we saw for classification evaluation (that is, based on true positive and negative and false positive and negative rates).\n",
      "\n",
      "These include the Rand measure, F-measure, Jaccard index, and others.\n",
      "\n",
      "### Note\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Cluster_analysis#External_evaluation> for more information on external evaluation for clustering.\n",
      "\n",
      "## Computing performance metrics on the MovieLens dataset\n",
      "\n",
      "MLlib provides a convenient `computeCost` function to compute the WCSS objective function given a RDD `[Vector]`. We will compute this metric for the following movie and user training data:\n",
      "\n",
      "    val movieCost = movieClusterModel.computeCost(movieVectors)\n",
      "    val userCost = userClusterModel.computeCost(userVectors)\n",
      "    println(\"WCSS for movies: \" + movieCost)\n",
      "    println(\"WCSS for users: \" + userCost)\n",
      "\n",
      "This should output the result similar to the following one:\n",
      "\n",
      "    **WCSS for movies: 2586.0777166339426**\n",
      "    **WCSS for users: 1403.4137493396831**\n",
      "\n",
      "# Tuning parameters for clustering models\n",
      "\n",
      "In contrast to many of the other models we have come across so far, K-means only has one parameter that can be tuned. This is K, the number of cluster centers chosen.\n",
      "\n",
      "## Selecting K through cross-validation\n",
      "\n",
      "As we have done with classification and regression models, we can apply cross-validation techniques to select the optimal number of clusters for our model. This works in much the same way as for supervised learning methods. We will split the dataset into a training set and a test set. We will then train a model on the training set and compute the evaluation metric of interest on the test set.\n",
      "\n",
      "We will do this for the movie clustering using the built-in WCSS evaluation metric provided by MLlib in the following code, using a 60 percent / 40 percent split between the training set and test set:\n",
      "\n",
      "    val trainTestSplitMovies = movieVectors.randomSplit(Array(0.6, 0.4), 123)\n",
      "    val trainMovies = trainTestSplitMovies(0)\n",
      "    val testMovies = trainTestSplitMovies(1)\n",
      "    val costsMovies = Seq(2, 3, 4, 5, 10, 20).map { k => (k, KMeans.train(trainMovies, numIterations, k, numRuns).computeCost(testMovies)) }\n",
      "    println(\"Movie clustering cross-validation:\")\n",
      "    costsMovies.foreach { case (k, cost) => println(f\"WCSS for K=$k id $cost%2.2f\") }\n",
      "\n",
      "This should give results that look something like the ones shown here.\n",
      "\n",
      "The output of movie clustering cross-validation is:\n",
      "\n",
      "    **Movie clustering cross-validation**\n",
      "    **WCSS for K=2 id 942.06**\n",
      "    **WCSS for K=3 id 942.67**\n",
      "    **WCSS for K=4 id 950.35**\n",
      "    **WCSS for K=5 id 948.20**\n",
      "    **WCSS for K=10 id 943.26**\n",
      "    **WCSS for K=20 id 947.10**\n",
      "\n",
      "We can observe that the WCSS decreases as the number of clusters increases, up to a point. It then begins to increase. Another common pattern observed in the WCSS in cross-validation for K-means is that the metric continues to decrease as K increases, but at a certain point, the rate of decrease flattens out substantially. The value of K at which this occurs is generally selected as the optimal K parameter (this is sometimes called the elbow point, as this is where the line kinks when drawn as a graph).\n",
      "\n",
      "In our case, we might select a value of 10 for K, based on the preceding results. Also, note that the clusters that are computed by the model are often used for purposes that require some human interpretation (such as the cases of movie and customer segmentation we mentioned earlier). Therefore, this consideration also impacts the choice of K, as although a higher value of K might be more optimal from the mathematical point of view, it might be more difficult to understand and interpret many clusters.\n",
      "\n",
      "For completeness, we will also compute the cross-validation metrics for user clustering:\n",
      "\n",
      "    val trainTestSplitUsers = userVectors.randomSplit(Array(0.6, 0.4), 123)\n",
      "    val trainUsers = trainTestSplitUsers(0)\n",
      "    val testUsers = trainTestSplitUsers(1)\n",
      "    val costsUsers = Seq(2, 3, 4, 5, 10, 20).map { k => (k, KMeans.train(trainUsers, numIterations, k, numRuns).computeCost(testUsers)) }\n",
      "    println(\"User clustering cross-validation:\")\n",
      "    costsUsers.foreach { case (k, cost) => println(f\"WCSS for K=$k id $cost%2.2f\") }\n",
      "\n",
      "We will see a pattern that is similar to the movie case:\n",
      "\n",
      "    **User clustering cross-validation:**\n",
      "    **WCSS for K=2 id 544.02**\n",
      "    **WCSS for K=3 id 542.18**\n",
      "    **WCSS for K=4 id 542.38**\n",
      "    **WCSS for K=5 id 542.33**\n",
      "    **WCSS for K=10 id 539.68**\n",
      "    **WCSS for K=20 id 541.21**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that your results may differ slightly due to random initialization of the clustering models.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we explored a new class of model that learns structure from unlabeled data--unsupervised learning. We worked through required input data, feature extraction, and saw how to use the output of one model (a recommendation model in our example) as the input to another model (our K-means clustering model). Finally, we evaluated the performance of the clustering model, both using manual interpretation of the cluster assignments and using mathematical performance metrics.\n",
      "\n",
      "In the next chapter, we will cover another type of unsupervised learning used to reduce our data down to its most important features or components--dimensionality reduction models.\n",
      "\n",
      "# Chapter 8. Dimensionality Reduction with Spark\n",
      "\n",
      "Over the course of this chapter, we will continue our exploration of unsupervised learning models in the form of **dimensionality reduction**.\n",
      "\n",
      "Unlike the models we have covered so far, such as regression, classification, and clustering, dimensionality reduction does not focus on making predictions. Instead, it tries to take a set of input data with a feature dimension _D_ (that is, the length of our feature vector) and extract a representation of the data of dimension _k_ , where _k_ is usually significantly smaller than _D_. It is, therefore, a form of preprocessing or feature transformation rather than a predictive model in its own right.\n",
      "\n",
      "It is important that the representation that is extracted should still be able to capture a large proportion of the variability or structure of the original data. The idea behind this is that most data sources will contain some form of underlying structure. This structure is typically unknown (often called latent features or latent factors), but if we can uncover some of this structure, our models could learn this structure and make predictions from it rather than from the data in its raw form, which might be noisy or contain many irrelevant features. In other words, dimensionality reduction throws away some of the noise in the data and keeps the hidden structure that is present.\n",
      "\n",
      "In some cases, the dimensionality of the raw data is far higher than the number of data points we have, so without dimensionality reduction, it would be difficult for other machine learning models, such as classification and regression, to learn anything, as they need to fit a number of parameters that is far larger than the number of training examples (in this sense, these methods bear some similarity to the regularization approaches that we have seen used in classification and regression).\n",
      "\n",
      "A few use cases of dimensionality reduction techniques include:\n",
      "\n",
      "  * Exploratory data analysis\n",
      "  * Extracting features to train other machine learning models\n",
      "  * Reducing storage and computation requirements for very large models in the prediction phase (for example, a production system that makes predictions)\n",
      "  * Reducing a large group of text documents down to a set of hidden topics or concepts\n",
      "  * Making learning and generalization of models easier when our data has a very large number of features (for example, when working with text, sound, images, or video data, which tends to be very high-dimensional)\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Introduce the types of dimensionality reduction models available in MLlib\n",
      "  * Work with images of faces to extract features suitable for dimensionality reduction\n",
      "  * Train a dimensionality reduction model using MLlib\n",
      "  * Visualize and evaluate the results\n",
      "  * Perform parameter selection for our dimensionality reduction model\n",
      "\n",
      "# Types of dimensionality reduction\n",
      "\n",
      "MLlib provides two models for dimensionality reduction; these models are closely related to each other. They are **Principal Components Analysis** ( **PCA** ) and **Singular Value Decomposition** ( **SVD** ).\n",
      "\n",
      "## Principal Components Analysis\n",
      "\n",
      "PCA operates on a data matrix _X_ and seeks to extract a set of _k_ principal components from _X_. The principal components are each uncorrelated to each other and are computed such that the first principal component accounts for the largest variation in the input data. Each subsequent principal component is, in turn, computed such that it accounts for the largest variation, provided that it is independent of the principal components computed so far.\n",
      "\n",
      "In this way, the _k_ principal components returned are guaranteed to account for the highest amount of variation in the input data possible. Each principal component, in fact, has the same feature dimensionality as the original data matrix. Hence, a projection step is required in order to actually perform dimensionality reduction, where the original data is projected into the _k-dimensional_ space represented by the principal components.\n",
      "\n",
      "## Singular Value Decomposition\n",
      "\n",
      "SVD seeks to decompose a matrix _X_ of dimension _m x n_ into three component matrices:\n",
      "\n",
      "  *  _U_ of dimension _m x m_\n",
      "  *  _S_ , a diagonal matrix of size _m x n_ ; the entries of _S_ are referred to as the **singular values**\n",
      "  *  _V T_ of dimension _n x n_\n",
      "\n",
      "    X = U * S * V T\n",
      "\n",
      "Looking at the preceding formula, it appears that we have not reduced the dimensionality of the problem at all, as by multiplying _U_ , _S_ , and _V_ , we reconstruct the original matrix. In practice, the truncated SVD is usually computed. That is, only the top _k_ singular values, which represent the most variation in the data, are kept, while the rest are discarded. The formula to reconstruct _X_ based on the component matrices is then approximate:\n",
      "\n",
      "    X ~ Uk * Sk * Vk T\n",
      "\n",
      "An illustration of the truncated SVD is shown here:\n",
      "\n",
      "The truncated Singular Value Decomposition\n",
      "\n",
      "Keeping the top _k_ singular values is similar to keeping the top _k_ principal components in PCA. In fact, SVD and PCA are directly related, as we will see a little later in this chapter.\n",
      "\n",
      "### Note\n",
      "\n",
      "A detailed mathematical treatment of both PCA and SVD is beyond the scope of this book.\n",
      "\n",
      "An overview of dimensionality reduction can be found in the Spark documentation at <http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html>.\n",
      "\n",
      "The following links contain a more in-depth mathematical overview of PCA and SVD, respectively: <http://en.wikipedia.org/wiki/Principal_component_analysis> and <http://en.wikipedia.org/wiki/Singular_value_decomposition>.\n",
      "\n",
      "## Relationship with matrix factorization\n",
      "\n",
      "PCA and SVD are both matrix factorization techniques, in the sense that they decompose a data matrix into subcomponent matrices, each of which has a lower dimension (or rank) than the original matrix. Many other dimensionality reduction techniques are based on matrix factorization.\n",
      "\n",
      "You might remember another example of matrix factorization, that is, collaborative filtering, that we have already seen in Chapter 4, _Building a Recommendation Engine with Spark_. Matrix factorization approaches to collaborative filtering work by factorizing the ratings matrix into two components: the user factor matrix and the item factor matrix. Each of these has a lower dimension than the original data, so these methods also act as dimensionality reduction models.\n",
      "\n",
      "### Note\n",
      "\n",
      "Many of the best performing approaches to collaborative filtering include models based on SVD. Simon Funk's approach to the Netflix prize is a famous example. You can look at it at <http://sifter.org/~simon/journal/20061211.html>.\n",
      "\n",
      "## Clustering as dimensionality reduction\n",
      "\n",
      "The clustering models we covered in the previous chapter can also be used for a form of dimensionality reduction. This works in the following way:\n",
      "\n",
      "  * Assume that we cluster our high-dimensional feature vectors using a K-means clustering model, with _k_ clusters. The result is a set of _k_ cluster centers.\n",
      "  * We can represent each of our original data points in terms of how far it is from each of these cluster centers. That is, we can compute the distance of a data point to each cluster center. The result is a set of _k_ distances for each data point.\n",
      "  * These _k_ distances can form a new vector of dimension _k_. We can now represent our original data as a new vector of lower dimension, relative to the original feature dimension.\n",
      "\n",
      "Depending on the distance metric used, this can result in both dimensionality reduction and a form of nonlinear transformation of the data, allowing us to learn a more complex model while still benefiting from the speed and scalability of a linear model. For example, using a Gaussian or exponential distance function can approximate a very complex nonlinear feature transformation.\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "As with all machine learning models we have explored so far, dimensionality reduction models also operate on a feature vector representation of our data.\n",
      "\n",
      "For this chapter, we will dive into the world of image processing, using the **Labeled Faces in the Wild** ( **LFW** ) dataset of facial images. This dataset contains over 13,000 images of faces generally taken from the Internet and belonging to well-known public figures. The faces are labeled with the person's name.\n",
      "\n",
      "## Extracting features from the LFW dataset\n",
      "\n",
      "In order to avoid having to download and process a very large dataset, we will work with a subset of the images, using people who have names that start with an \"A\". This dataset can be downloaded from <http://vis-www.cs.umass.edu/lfw/lfw-a.tgz>.\n",
      "\n",
      "### Note\n",
      "\n",
      "For more details and other variants of the data, visit <http://vis-www.cs.umass.edu/lfw/>.\n",
      "\n",
      "The original research paper reference is:\n",
      "\n",
      " _Gary B. Huang_ , _Manu Ramesh_ , _Tamara Berg_ , and _Erik Learned-Miller_. _Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments_. University of Massachusetts, Amherst, Technical Report 07-49, October, 2007.\n",
      "\n",
      "It can be downloaded from <http://vis-www.cs.umass.edu/lfw/lfw.pdf>.\n",
      "\n",
      "Unzip the data using the following command:\n",
      "\n",
      "    **> tar xfvz lfw-a.tgz**\n",
      "\n",
      "This will create a folder called `lfw`, which contains a number of subfolders, one for each person.\n",
      "\n",
      "### Exploring the face data\n",
      "\n",
      "Start up your Spark Scala console by ensuring that you allocate sufficient memory, as dimensionality reduction methods can be quite computationally expensive:\n",
      "\n",
      "    **>./SPARK_HOME/bin/spark-shell --driver-memory 2g**\n",
      "\n",
      "Now that we've unzipped the data, we face a small challenge. Spark provides us with a way to read text files and custom Hadoop input data sources. However, there is no built-in functionality to allow us to read images.\n",
      "\n",
      "Spark provides a method called `wholeTextFiles`, which allows us to operate on entire files at once, compared to the `textFile` method that we have been using so far, which operates on the individual lines within a text file (or multiple files).\n",
      "\n",
      "We will use the `wholeTextFiles` method to access the location of each file. Using these file paths, we will write custom code to load and process the images. In the following example code, we will use `PATH` to refer to the directory in which you extracted the `lfw` subdirectory.\n",
      "\n",
      "We can use a wildcard path specification (using the `*` character highlighted in the following code snippet) to tell Spark to look in each directory under the `lfw` directory for files:\n",
      "\n",
      "    val path = \"/ **PATH** /lfw/ ***** \"\n",
      "    val rdd = sc.wholeTextFiles(path)\n",
      "    val first = rdd.first\n",
      "    println(first)\n",
      "\n",
      "Running the `first` command might take a little time, as Spark first scans the specified directory structure for all available files. Once completed, you should see output similar to the one shown here:\n",
      "\n",
      "    **first: (String, String) =  (file:/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg, ����??JFIF????? ...**\n",
      "\n",
      "You will see that `wholeTextFiles` returns an RDD that contains key-value pairs, where the key is the file location while the value is the content of the entire text file. For our purposes, we only care about the file path, as we cannot work directly with the image data as a string (notice that it is displayed as \"binary nonsense\" in the shell output).\n",
      "\n",
      "Let's extract the file paths from the RDD. Note that earlier, the file path starts with the `file:` text. This is used by Spark when reading files in order to differentiate between different filesystems (for example, `file://` for the local filesystem, `hdfs://` for HDFS, `s3n://` for Amazon S3, and so on).\n",
      "\n",
      "In our case, we will be using custom code to read the images, so we don't need this part of the path. Thus, we will remove it with the following `map` function:\n",
      "\n",
      "    val files = rdd.map { case (fileName, content) => fileName.replace(\"file:\", \"\") }\n",
      "    println(files.first)\n",
      "\n",
      "This should display the file location with the `file:` prefix removed:\n",
      "\n",
      "    **/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg**\n",
      "\n",
      "Next, we will see how many files we are dealing with:\n",
      "\n",
      "    println(files.count)\n",
      "\n",
      "Running these commands creates a lot of noisy output in the Spark shell, as it outputs all the file paths that are read to the console. Ignore this part, but after the command has completed, the output should look something like this:\n",
      "\n",
      "    **..., /PATH/lfw/Azra_Akin/Azra_Akin_0003.jpg:0+19927, /PATH/lfw/Azra_Akin/Azra_Akin_0004.jpg:0+16030**\n",
      "    **...**\n",
      "    **14/09/18 20:36:25 INFO SparkContext: Job finished: count at <console>:19, took 1.151955 s**\n",
      "    **1055**\n",
      "\n",
      "So, we can see that we have 1055 images to work with.\n",
      "\n",
      "### Visualizing the face data\n",
      "\n",
      "Although there are a few tools available in Scala or Java to display images, this is one area where Python and the matplotlib library shine. We will use Scala to process and extract the images and run our models and IPython to display the actual images.\n",
      "\n",
      "You can run a separate IPython Notebook by opening a new terminal window and launching a new notebook:\n",
      "\n",
      "    **> ipython notebook**\n",
      "\n",
      "### Note\n",
      "\n",
      "Note that if using Python Notebook, you should first execute the following code snippet to ensure that the images are displayed inline after each notebook cell (including the `%` character): `%pylab inline`.\n",
      "\n",
      "Alternatively, you can launch a plain IPython console without the web notebook, enabling the `pylab` plotting functionality using the following command:\n",
      "\n",
      "    **> ipython --pylab**\n",
      "\n",
      "The dimensionality reduction techniques in MLlib are only available in Scala or Java at the time of writing this book, so we will continue to use the Scala Spark shell to run the models. Therefore, you won't need to run a PySpark console.\n",
      "\n",
      "### Tip\n",
      "\n",
      "We have provided the full Python code with this chapter as a Python script as well as in the IPython Notebook format. For instructions on installing IPython, see the code bundle.\n",
      "\n",
      "Let's display the image given by the first path we extracted earlier using matplotlib's `imread` and `imshow` functions:\n",
      "\n",
      "    path = \"/PATH/lfw/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\"\n",
      "    ae = imread(path)\n",
      "    imshow(ae)\n",
      "\n",
      "### Note\n",
      "\n",
      "You should see the image displayed in your Notebook (or in a pop-up window if you are using the standard IPython shell). Note that we have not shown the image here.\n",
      "\n",
      "### Extracting facial images as vectors\n",
      "\n",
      "While a full treatment of image processing is beyond the scope of this book, we will need to know a few basics to proceed. Each color image can be represented as a three-dimensional array, or matrix, of pixels. The first two dimensions, that is the _x_ and _y_ axes, represent the position of each pixel, while the third dimension represents the **red, blue, and green** ( **RGB** ) color values for each pixel.\n",
      "\n",
      "A grayscale image only requires one value per pixel (there are no RGB values), so it can be represented as a plain two-dimensional matrix. For many image-processing and machine learning tasks related to images, it is common to operate on grayscale images. We will do this here by converting the color images to grayscale first.\n",
      "\n",
      "It is also a common practice in machine learning tasks to represent an image as a vector, instead of a matrix. We do this by concatenating each row (or alternatively, each column) of the matrix together to form a long vector (this is known as **reshaping** ). In this way, each raw, grayscale image matrix is transformed into a feature vector that is usable as input to a machine learning model.\n",
      "\n",
      "Fortunately for us, the built-in Java **Abstract Window Toolkit** ( **AWT** ) contains various basic image-processing functions. We will define a few utility functions to perform this processing using the `java.awt` classes.\n",
      "\n",
      "#### Loading images\n",
      "\n",
      "The first of these is a function to read an image from a file:\n",
      "\n",
      "    import java.awt.image.BufferedImage\n",
      "    def loadImageFromFile(path: String): BufferedImage = { \n",
      "      import javax.imageio.ImageIO\n",
      "      import java.io.File\n",
      "      ImageIO.read(new File(path))\n",
      "    }\n",
      "\n",
      "This returns an instance of a `java.awt.image.BufferedImage` class, which stores the image data and provides a number of useful methods. Let's test it out by loading the first image into our Spark shell:\n",
      "\n",
      "    val aePath = \"/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\"\n",
      "    val aeImage = loadImageFromFile(aePath)\n",
      "\n",
      "You should see the image details displayed in the shell:\n",
      "\n",
      "    **aeImage: java.awt.image.BufferedImage = BufferedImage@f41266e: type = 5 ColorModel: #pixelBits = 24 numComponents = 3 color space = java.awt.color.ICC_ColorSpace@7e420794 transparency = 1 has alpha = false isAlphaPre = false ByteInterleavedRaster: width = 250 height = 250 #numDataElements 3 dataOff[0] = 2**\n",
      "\n",
      "There is quite a lot of information here. Of particular interest to us is that the image width and height are 250 pixels, and as we can see, there are three components (that is, the RGB values) that are highlighted in the preceding output.\n",
      "\n",
      "#### Converting to grayscale and resizing the images\n",
      "\n",
      "The next function we will define will take the image that we have loaded with our preceding function, convert the image from color to grayscale, and resize the image's width and height.\n",
      "\n",
      "These steps are not strictly necessary, but both steps are done in many cases for efficiency purposes. Using RGB color images instead of grayscale increases the amount of data to be processed by a factor of 3. Similarly, larger images increase the processing and storage overhead significantly. Our raw 250 x 250 images represent 187,500 data points per image using three color components. For a set of 1055 images, this is 197,812,500 data points. Even if stored as integer values, each value stored takes 4 bytes of memory, so just 1055 images represent around 800 MB of memory! As you can see, image-processing tasks can quickly become extremely memory intensive.\n",
      "\n",
      "If we convert to grayscale and resize the images to, say, 50 x 50 pixels, we only require 2500 data points per image. For our 1055 images, this equates to 10 MB of memory, which is far more manageable for illustrative purposes.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Another reason to resize is that MLlib's PCA model works best on _tall and skinny_ matrices with less than 10,000 columns. We will have 2500 columns (that is, each pixel becomes an entry in our feature vector), so we will come in well below this restriction.\n",
      "\n",
      "Let's define our processing function. We will do the grayscale conversion and resizing in one step, using the `java.awt.image` package:\n",
      "\n",
      "    def processImage(image: BufferedImage, width: Int, height: Int): BufferedImage = {\n",
      "      val bwImage = new BufferedImage(width, height, BufferedImage.TYPE_BYTE_GRAY)\n",
      "      val g = bwImage.getGraphics()\n",
      "      g.drawImage(image, 0, 0, width, height, null)\n",
      "      g.dispose()\n",
      "      bwImage\n",
      "    }\n",
      "\n",
      "The first line of the function creates a new image of the desired width and height and specifies a grayscale color model. The third line draws the original image onto this newly created image. The `drawImage` method takes care of the color conversion and resizing for us! Finally, we return the new, processed image.\n",
      "\n",
      "Let's test this out on our sample image. We will convert it to grayscale and resize it to 100 x 100 pixels:\n",
      "\n",
      "    val grayImage = processImage(aeImage, 100, 100)\n",
      "\n",
      "You should see the following output in the console:\n",
      "\n",
      "    **grayImage: java.awt.image.BufferedImage = BufferedImage@21f8ea3b: type = 10 ColorModel: #pixelBits = 8 numComponents = 1 color space = java.awt.color.ICC_ColorSpace@5cd9d8e9 transparency = 1 has alpha = false isAlphaPre = false ByteInterleavedRaster: width = 100 height = 100 #numDataElements 1 dataOff[0] = 0**\n",
      "\n",
      "As you can see from the highlighted output, the image's width and height are indeed 100, and the number of color components is 1.\n",
      "\n",
      "Next, we will save the processed image to a temporary location so that we can read it back and display it in our IPython console:\n",
      "\n",
      "    import javax.imageio.ImageIO\n",
      "    import java.io.File\n",
      "    ImageIO.write(grayImage, \"jpg\", new File(\"/tmp/aeGray.jpg\"))\n",
      "\n",
      "You should see a result of `true` displayed in your console, indicating that we successfully saved the image to the `aeGray.jpg` file in our `/tmp` directory.\n",
      "\n",
      "Finally, we will read the image in Python and use matplotlib to display the image. Type the following code into your IPython Notebook or shell (remember that this should be open in a new terminal window):\n",
      "\n",
      "    tmpPath = \"/tmp/aeGray.jpg\"\n",
      "    aeGary = imread(tmpPath)\n",
      "    imshow(aeGary, cmap=plt.cm.gray)\n",
      "\n",
      "This should display the image (note again, we haven't shown the image here). You should see that it is grayscale and of slightly worse quality as compared to the original image. Furthermore, you will notice that the scale of the axes are different, representing the new 100 x 100 dimension instead of the original 250 x 250 size.\n",
      "\n",
      "#### Extracting feature vectors\n",
      "\n",
      "The final step in the processing pipeline is to extract the actual feature vectors that will be the input to our dimensionality reduction model. As we mentioned earlier, the raw grayscale pixel data will be our features. We will form the vectors by flattening out the two-dimensional pixel matrix. The `BufferedImage` class provides a utility method to do just this, which we will use in our function:\n",
      "\n",
      "    def getPixelsFromImage(image: BufferedImage): Array[Double] = {\n",
      "      val width = image.getWidth\n",
      "      val height = image.getHeight\n",
      "      val pixels = Array.ofDim[Double](width * height)\n",
      "      image.getData.getPixels(0, 0, width, height, pixels)\n",
      "    }\n",
      "\n",
      "We can then combine these three functions into one utility function that takes a file location together with the desired image's width and height and returns the raw `Array[Double]` value that contains the pixel data:\n",
      "\n",
      "    def extractPixels(path: String, width: Int, height: Int): Array[Double] = {\n",
      "      val raw = loadImageFromFile(path)\n",
      "      val processed = processImage(raw, width, height)\n",
      "      getPixelsFromImage(processed)\n",
      "    }\n",
      "\n",
      "Applying this function to each element of the RDD that contains all the image file paths will give us a new RDD that contains the pixel data for each image. Let's do this and inspect the first few elements:\n",
      "\n",
      "    val pixels = files.map(f => extractPixels(f, 50, 50))\n",
      "    println(pixels.take(10).map(_.take(10).mkString(\"\", \",\", \", ...\")).mkString(\"\\n\"))\n",
      "\n",
      "You should see output similar to this:\n",
      "\n",
      "    **0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0, ...**\n",
      "    **241.0,243.0,245.0,244.0,231.0,205.0,177.0,160.0,150.0,147.0, ...**\n",
      "    **253.0,253.0,253.0,253.0,253.0,253.0,254.0,254.0,253.0,253.0, ...**\n",
      "    **244.0,244.0,243.0,242.0,241.0,240.0,239.0,239.0,237.0,236.0, ...**\n",
      "    **44.0,47.0,47.0,49.0,62.0,116.0,173.0,223.0,232.0,233.0, ...**\n",
      "    **0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...**\n",
      "    **1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0, ...**\n",
      "    **26.0,26.0,27.0,26.0,24.0,24.0,25.0,26.0,27.0,27.0, ...**\n",
      "    **240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0, ...**\n",
      "    **0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...**\n",
      "\n",
      "The final step is to create an MLlib `Vector` instance for each image. We will cache the RDD to speed up our later computations:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.Vectors\n",
      "    val vectors = pixels.map(p => Vectors.dense(p))\n",
      "    vectors.setName(\"image-vectors\")\n",
      "    vectors.cache\n",
      "\n",
      "### Tip\n",
      "\n",
      "We used the `setName` function earlier to assign an RDD a name. In this case, we called it `image-vectors`. This is so that we can later identify it more easily when looking at the Spark web interface.\n",
      "\n",
      "### Normalization\n",
      "\n",
      "It is a common practice to standardize input data prior to running dimensionality reduction models, in particular for PCA. As we did in Chapter 5, _Building a Classification Model with Spark_ , we will do this using the built-in `StandardScaler` provided by MLlib's `feature` package. We will only subtract the mean from the data in this case:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.Matrix\n",
      "    import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "    import org.apache.spark.mllib.feature.StandardScaler\n",
      "    val scaler = new StandardScaler(withMean = true, withStd = false).fit(vectors)\n",
      "\n",
      "Calling `fit` triggers a computation on our `RDD[Vector]`. You should see output similar to the one shown here:\n",
      "\n",
      "    **...**\n",
      "    **14/09/21 11:46:58 INFO SparkContext: Job finished: reduce at RDDFunctions.scala:111, took 0.495859 s**\n",
      "    **scaler: org.apache.spark.mllib.feature.StandardScalerModel = org.apache.spark.mllib.feature.StandardScalerModel@6bb1a1a1**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that subtracting the mean works for dense input data. However, for sparse vectors, subtracting the mean vector from each input will transform the sparse data into dense data. For very high-dimensional input, this will likely exhaust the available memory resources, so it is not advisable.\n",
      "\n",
      "Finally, we will use the returned `scaler` to transform the raw image vectors to vectors with the column means subtracted:\n",
      "\n",
      "    val scaledVectors = vectors.map(v => scaler.transform(v))\n",
      "\n",
      "We mentioned earlier that the resized grayscale images would take up around 10 MB of memory. Indeed, you can take a look at the memory usage in the Spark application monitor storage page by going to `http://localhost:4040/storage/` ` `in your web browser.\n",
      "\n",
      "Since we gave our RDD of image vectors a friendly name of `image-vectors`, you should see something like the following screenshot (note that as we are using `Vector[Double]`, each element takes up 8 bytes instead of 4 bytes; hence, we actually use 20 MB of memory):\n",
      "\n",
      "Size of image vectors in memory\n",
      "\n",
      "# Training a dimensionality reduction model\n",
      "\n",
      "Dimensionality reduction models in MLlib require vectors as inputs. However, unlike clustering that operated on an `RDD[Vector]`, PCA and SVD computations are provided as methods on a distributed `RowMatrix` (this difference is largely down to syntax, as a `RowMatrix` is simply a wrapper around an `RDD[Vector]`).\n",
      "\n",
      "## Running PCA on the LFW dataset\n",
      "\n",
      "Now that we have extracted our image pixel data into vectors, we can instantiate a new `RowMatrix` and call the `computePrincipalComponents` method to compute the top `K` principal components of our distributed matrix:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.Matrix\n",
      "    import org.apache.spark.mllib.linalg.distributed.RowMatrix\n",
      "    val matrix = new RowMatrix(scaledVectors)\n",
      "    val K = 10\n",
      "    val pc = matrix.computePrincipalComponents(K)\n",
      "\n",
      "You will likely see quite a lot of output in your console while the model runs.\n",
      "\n",
      "### Tip\n",
      "\n",
      "If you see warnings such as **WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK** or **WARN LAPACK: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK** , you can safely ignore these.\n",
      "\n",
      "This means that the underlying linear algebra libraries used by MLlib could not load native routines. In this case, a Java-based fallback will be used, which is slower, but there is nothing to worry about for the purposes of this example.\n",
      "\n",
      "Once the model training is complete, you should see a result displayed in the console that looks similar to the following one:\n",
      "\n",
      "    **pc: org.apache.spark.mllib.linalg.Matrix =**\n",
      "    **-0.023183157256614906  -0.010622723054037303  ... (10 total)**\n",
      "    **-0.023960537953442107  -0.011495966728461177  ...**\n",
      "    **-0.024397470862198022  -0.013512219690177352  ...**\n",
      "    **-0.02463158818330343   -0.014758658113862178  ...**\n",
      "    **-0.024941633606137027  -0.014878858729655142  ...**\n",
      "    **-0.02525998879466241   -0.014602750644394844  ...**\n",
      "    **-0.025494722450369593  -0.014678013626511024  ...**\n",
      "    **-0.02604194423255582   -0.01439561589951032   ...**\n",
      "    **-0.025942214214865228  -0.013907665261197633  ...**\n",
      "    **-0.026151551334429365  -0.014707035797934148  ...**\n",
      "    **-0.026106572186134578  -0.016701471378568943  ...**\n",
      "    **-0.026242986173995755  -0.016254664123732318  ...**\n",
      "    **-0.02573628754284022   -0.017185663918352894  ...**\n",
      "    **-0.02545319635905169   -0.01653357295561698   ...**\n",
      "    **-0.025325893980995124  -0.0157082218373399...**\n",
      "\n",
      "### Visualizing the Eigenfaces\n",
      "\n",
      "Now that we have trained our PCA model, what is the result? Let's inspect the dimensions of the resulting matrix:\n",
      "\n",
      "    val rows = pc.numRows\n",
      "    val cols = pc.numCols\n",
      "    println(rows, cols)\n",
      "\n",
      "As you should see from your console output, the matrix of principal components has 2500 rows and 10 columns:\n",
      "\n",
      "    **(2500,10)**\n",
      "\n",
      "Recall that the dimension of each image is 50 x 50, so here, we have the top 10 principal components, each with a dimension identical to that of the input images. These principal components can be thought of as the set of latent (or hidden) features that capture the greatest variation in the original data.\n",
      "\n",
      "### Note\n",
      "\n",
      "In facial recognition and image processing, these principal components are often referred to as **Eigenfaces** , as PCA is closely related to the eigenvalue decomposition of the covariance matrix of the original data.\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Eigenface> for more details.\n",
      "\n",
      "Since each principal component is of the same dimension as the original images, each component can itself be thought of and represented as an image, making it possible to visualize the Eigenfaces as we would the input images.\n",
      "\n",
      "As we have often done in this book, we will use functionality from the Breeze linear algebra library as well as Python's numpy and matplotlib to visualize the Eigenfaces.\n",
      "\n",
      "First, we will extract the `pc` variable (an MLlib matrix) into a Breeze `DenseMatrix`:\n",
      "\n",
      "    import breeze.linalg.DenseMatrix\n",
      "    val pcBreeze = new DenseMatrix(rows, cols, pc.toArray)\n",
      "\n",
      "Breeze provides a useful function within the `linalg` package to write the matrix out as a CSV file. We will use this to save the principal components to a temporary CSV file:\n",
      "\n",
      "    import breeze.linalg.csvwrite\n",
      "    csvwrite(new File(\"/tmp/pc.csv\"), pcBreeze)\n",
      "\n",
      "Next, we will load the matrix in IPython and visualize the principal components as images. Fortunately, numpy provides a utility function to read the matrix from the CSV file we created:\n",
      "\n",
      "    pcs = np.loadtxt(\"/tmp/pc.csv\", delimiter=\",\")\n",
      "    print(pcs.shape)\n",
      "\n",
      "You should see the following output, confirming that the matrix we read has the same dimensions as the one we saved:\n",
      "\n",
      "    **(2500, 10)**\n",
      "\n",
      "We will need a utility function to display the images, which we define here:\n",
      "\n",
      "    def plot_gallery(images, h, w, n_row=2, n_col=5):\n",
      "        \"\"\"Helper function to plot a gallery of portraits\"\"\"\n",
      "        plt.figure(figsize=(1.8 * n_col, 2.4 * n_row))\n",
      "        plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90, hspace=.35)\n",
      "        for i in range(n_row * n_col):\n",
      "            plt.subplot(n_row, n_col, i + 1)\n",
      "            plt.imshow(images[:, i].reshape((h, w)), cmap=plt.cm.gray)\n",
      "            plt.title(\"Eigenface %d\" % (i + 1), size=12)\n",
      "            plt.xticks(())\n",
      "            plt.yticks(())\n",
      "\n",
      "### Note\n",
      "\n",
      "This function is adapted from the LFW dataset example code in the scikit-learn documentation available at <http://scikit-learn.org/stable/auto_examples/applications/face_recognition.html>.\n",
      "\n",
      "We will now use this function to plot the top 10 Eigenfaces:\n",
      "\n",
      "    plot_gallery(pcs, 50, 50)\n",
      "\n",
      "This should display the following plot:\n",
      "\n",
      "Top 10 Eigenfaces\n",
      "\n",
      "### Interpreting the Eigenfaces\n",
      "\n",
      "Looking at the preceding images, we can see that the PCA model has effectively extracted recurring patterns of variation, which represent various features of the facial images. Each principal component can, as with clustering models, be interpreted. Again, like clustering, it is not always straightforward to interpret precisely what each principal component represents.\n",
      "\n",
      "We can see from these images that there appear to be some images that pick up directional factors (for example, images 6 and 9), some hone in on hair patterns (such as images 4, 5, 7, and 10), while others seem to be somewhat more related to facial features such as eyes, nose, and mouth (such as images 1, 7, and 9).\n",
      "\n",
      "# Using a dimensionality reduction model\n",
      "\n",
      "It is interesting to be able to visualize the outcome of a model in this way; however, the overall purpose of using dimensionality reduction is to create a more compact representation of the data that still captures the important features and variability in the raw dataset. To do this, we need to use a trained model to transform our raw data by projecting it into the new, lower-dimensional space represented by the principal components.\n",
      "\n",
      "## Projecting data using PCA on the LFW dataset\n",
      "\n",
      "We will illustrate this concept by projecting each LFW image into a ten-dimensional vector. This is done through a matrix multiplication of the image matrix with the matrix of principal components. As the image matrix is a distributed MLlib `RowMatrix`, Spark takes care of distributing this computation for us through the `multiply` function:\n",
      "\n",
      "    val projected = matrix.multiply(pc)\n",
      "    println(projected.numRows, projected.numCols)\n",
      "\n",
      "This will give you the following output:\n",
      "\n",
      "    **(1055,10)**\n",
      "\n",
      "Observe that each image that was of dimension 2500 has been transformed into a vector of size 10. Let's take a look at the first few vectors:\n",
      "\n",
      "    println(projected.rows.take(5).mkString(\"\\n\"))\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **[2648.9455749636277,1340.3713412351376,443.67380716760965,-353.0021423043161,52.53102289832631,423.39861446944354,413.8429065865399,-484.18122999722294,87.98862070273545,-104.62720604921965]**\n",
      "    **[172.67735747311974,663.9154866829355,261.0575622447282,-711.4857925259682,462.7663154755333,167.3082231097332,-71.44832640530836,624.4911488194524,892.3209964031695,-528.0056327351435]**\n",
      "    **[-1063.4562028554978,388.3510869550539,1508.2535609357597,361.2485590837186,282.08588829583596,-554.3804376922453,604.6680021092125,-224.16600191143075,-228.0771984153961,-110.21539201855907]**\n",
      "    **[-4690.549692385103,241.83448841252638,-153.58903325799685,-28.26215061165965,521.8908276360171,-442.0430200747375,-490.1602309367725,-456.78026845649435,-78.79837478503592,70.62925170688868]**\n",
      "    **[-2766.7960144161225,612.8408888724891,-405.76374113178616,-468.56458995613974,863.1136863614743,-925.0935452709143,69.24586949009642,-777.3348492244131,504.54033662376435,257.0263568009851]**\n",
      "\n",
      "As the projected data is in the form of vectors, we can use the projection as input to another machine learning model. For example, we could use these projected inputs together with a set of input data generated from various images without faces to train a facial recognition model. Alternatively, we could train a multiclass classifier where each person is a class, thus creating a model that learns to identify the particular person that a face belongs to.\n",
      "\n",
      "## The relationship between PCA and SVD\n",
      "\n",
      "We mentioned earlier that there is a close relationship between PCA and SVD. In fact, we can recover the same principal components and also apply the same projection into the space of principal components using SVD.\n",
      "\n",
      "In our example, the right singular vectors derived from computing the SVD will be equivalent to the principal components we have calculated. We can see that this is the case by first computing the SVD on our image matrix and comparing the right singular vectors to the result of PCA. As was the case with PCA, SVD computation is provided as a function on a distributed `RowMatrix`:\n",
      "\n",
      "    val svd = matrix.computeSVD(10, computeU = true)\n",
      "    println(s\"U dimension: (${svd.U.numRows}, ${svd.U.numCols})\")\n",
      "    println(s\"S dimension: (${svd.s.size}, )\")\n",
      "    println(s\"V dimension: (${svd.V.numRows}, ${svd.V.numCols})\")\n",
      "\n",
      "We can see that SVD returns a matrix `U` of dimension 1055 x 10, a vector `S` of the singular values of length `10`, and a matrix `V` of the right singular vectors of dimension 2500 x 10:\n",
      "\n",
      "    **U dimension: (1055, 10)**\n",
      "    **S dimension: (10, )**\n",
      "    **V dimension: (2500, 10)**\n",
      "\n",
      "The matrix `V` is exactly equivalent to the result of PCA (ignoring the sign of the values and floating point tolerance). We can verify this with a utility function to compare the two by approximately comparing the data arrays of each matrix:\n",
      "\n",
      "    def approxEqual(array1: Array[Double], array2: Array[Double], tolerance: Double = 1e-6): Boolean = {\n",
      "      // note we ignore sign of the principal component / singular vector elements\n",
      "      val bools = array1.zip(array2).map { case (v1, v2) => if (math.abs(math.abs(v1) - math.abs(v2)) > 1e-6) false else true }\n",
      "      bools.fold(true)(_ & _)\n",
      "    }\n",
      "\n",
      "We will test the function on some test data:\n",
      "\n",
      "    println(approxEqual(Array(1.0, 2.0, 3.0), Array(1.0, 2.0, 3.0)))\n",
      "\n",
      "This will give you the following output:\n",
      "\n",
      "    **true**\n",
      "\n",
      "Let's try another test data:\n",
      "\n",
      "    println(approxEqual(Array(1.0, 2.0, 3.0), Array(3.0, 2.0, 1.0)))\n",
      "\n",
      "This will give you the following output:\n",
      "\n",
      "    **false**\n",
      "\n",
      "Finally, we can apply our equality function as follows:\n",
      "\n",
      "    println(approxEqual(svd.V.toArray, pc.toArray))\n",
      "\n",
      "Here is the output:\n",
      "\n",
      "    **true**\n",
      "\n",
      "The other relationship that holds is that the multiplication of the matrix `U` and vector `S` (or, strictly speaking, the diagonal matrix `S`) is equivalent to the PCA projection of our original image data into the space of the top 10 principal components.\n",
      "\n",
      "We will now show that this is indeed the case. We will first use Breeze to multiply each vector in `U` by `S`, element-wise. We will then compare each vector in our PCA projected vectors with the equivalent vector in our SVD projection, and sum up the number of equal cases:\n",
      "\n",
      "    val breezeS = breeze.linalg.DenseVector(svd.s.toArray)\n",
      "    val projectedSVD = svd.U.rows.map { v => \n",
      "      val breezeV = breeze.linalg.DenseVector(v.toArray)\n",
      "      val multV = breezeV **:*** breezeS\n",
      "      Vectors.dense(multV.data)\n",
      "    }\n",
      "    projected.rows.zip(projectedSVD).map { case (v1, v2) => approxEqual(v1.toArray, v2.toArray) }.filter(b => true).count\n",
      "\n",
      "This should display a result of 1055, as we would expect, confirming that each row of `projected` is equal to each row of `projectedSVD`.\n",
      "\n",
      "### Note\n",
      "\n",
      "Note that the **:*** operator highlighted in the preceding code represents element-wise multiplication of the vectors.\n",
      "\n",
      "# Evaluating dimensionality reduction models\n",
      "\n",
      "Both PCA and SVD are deterministic models. That is, given a certain input dataset, they will always produce the same result. This is in contrast to many of the models we have seen so far, which depend on some random element (most often for the initialization of model weight vectors and so on).\n",
      "\n",
      "Both models are also guaranteed to return the top principal components or singular values, and hence, the only parameter is _k_. Like clustering models, increasing _k_ always improves the model performance (for clustering, the relevant error function, while for PCA and SVD, the total amount of variability explained by the _k_ components). Therefore, selecting a value for _k_ is a trade-off between capturing as much structure of the data as possible while keeping the dimensionality of projected data low.\n",
      "\n",
      "## Evaluating k for SVD on the LFW dataset\n",
      "\n",
      "We will examine the singular values obtained from computing the SVD on our image data. We can verify that the singular values are the same for each run and that they are returned in decreasing order, as follows:\n",
      "\n",
      "    val sValues = (1 to 5).map { i => matrix.computeSVD(i, computeU = false).s }\n",
      "    sValues.foreach(println)\n",
      "\n",
      "This should show us output similar to the following:\n",
      "\n",
      "    **[54091.00997110354]**\n",
      "    **[54091.00997110358,33757.702867982436]**\n",
      "    **[54091.00997110357,33757.70286798241,24541.193694775946]**\n",
      "    **[54091.00997110358,33757.70286798242,24541.19369477593,23309.58418888302]**\n",
      "    **[54091.00997110358,33757.70286798242,24541.19369477593,23309.584188882982,21803.09841158358]**\n",
      "\n",
      "As with evaluating values of _k_ for clustering, in the case of SVD (and PCA), it is often useful to plot the singular values for a larger range of _k_ and see where the point on the graph is where the amount of additional variance accounted for by each additional singular value starts to flatten out considerably.\n",
      "\n",
      "We will do this by first computing the top 300 singular values:\n",
      "\n",
      "    val svd300 = matrix.computeSVD(300, computeU = false)\n",
      "    val sMatrix = new DenseMatrix(1, 300, svd300.s.toArray)\n",
      "    csvwrite(new File(\"/tmp/s.csv\"), sMatrix)\n",
      "\n",
      "We will write out the vector `S` of singular values to a temporary CSV file (as we did for our matrix of Eigenfaces previously) and then read it back in our IPython console, plotting the singular values for each _k_ :\n",
      "\n",
      "    s = np.loadtxt(\"/tmp/s.csv\", delimiter=\",\")\n",
      "    print(s.shape)\n",
      "    plot(s)\n",
      "\n",
      "You should see an image displayed similar to the one shown here:\n",
      "\n",
      "Top 300 singular values\n",
      "\n",
      "A similar pattern is seen in the cumulative variation accounted for by the top 300 singular values (which we will plot on a log scale for the _y_ axis):\n",
      "\n",
      "    plot(cumsum(s))\n",
      "    plt.yscale('log')\n",
      "\n",
      "Cumulative sum of top 300 singular values\n",
      "\n",
      "We can see that after a certain value range for _k_ (around 100 in this case), the graph flattens considerably. This indicates that a number of singular values (or principal components) equivalent to this value of _k_ probably explains enough of the variation of the original data.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Of course, if we are using dimensionality reduction to help improve the performance of another model, we could use the same evaluation methods used for that model to help us choose a value for _k_.\n",
      "\n",
      "For example, we could use the AUC metric, together with cross-validation, to choose both the model parameters for a classification model as well as the value of _k_ for our dimensionality reduction model. This does come at the expense of higher computation cost, however, as we would have to recompute the full model training and testing pipeline.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we explored two new unsupervised learning methods, PCA and SVD, for dimensionality reduction. We saw how to extract features for and train these models using facial image data. We visualized the results of the model in the form of Eigenfaces, saw how to apply the models to transform our original data into a reduced dimensionality representation, and investigated the close link between PCA and SVD.\n",
      "\n",
      "In the next chapter, we will delve more deeply into techniques for text processing and analysis with Spark.\n",
      "\n",
      "# Chapter 9. Advanced Text Processing with Spark\n",
      "\n",
      "In Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ , we covered various topics related to feature extraction and data processing, including the basics of extracting features from text data. In this chapter, we will introduce more advanced text processing techniques available in MLlib to work with large-scale text datasets.\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Work through detailed examples that illustrate data processing, feature extraction, and the modeling pipeline, as they relate to text data\n",
      "  * Evaluate the similarity between two documents based on the words in the documents\n",
      "  * Use the extracted text features as inputs for a classification model\n",
      "  * Cover a recent development in natural language processing to model words themselves as vectors and illustrate the use of Spark's **Word2Vec** model to evaluate the similarity between two words, based on their meaning\n",
      "\n",
      "# What's so special about text data?\n",
      "\n",
      "Text data can be complex to work with for two main reasons. First, text and language have an inherent structure that is not easily captured using the raw words as is (for example, meaning, context, different types of words, sentence structure, and different languages, to highlight a few). Therefore, naive feature extraction is usually relatively ineffective.\n",
      "\n",
      "Second, the effective dimensionality of text data is extremely large and potentially limitless. Think about the number of words in the English language alone and add all kinds of special words, characters, slang, and so on to this. Then, throw in other languages and all the types of text one might find across the Internet. The dimension of text data can easily exceed tens or even hundreds of millions of words, even in relatively small datasets. For example, the Common Crawl dataset of billions of websites contains over 840 billion individual words.\n",
      "\n",
      "To deal with these issues, we need ways of extracting more structured features and methods to handle the huge dimensionality of text data.\n",
      "\n",
      "# Extracting the right features from your data\n",
      "\n",
      "The field of **natural language processing** ( **NLP** ) covers a wide range of techniques to work with text, from text processing and feature extraction through to modeling and machine learning. In this chapter, we will focus on two feature extraction techniques available within MLlib: the TF-IDF term weighting scheme and feature hashing.\n",
      "\n",
      "Working through an example of TF-IDF, we will also explore the ways in which processing, tokenization, and filtering during feature extraction can help reduce the dimensionality of our input data as well as improve the information content and usefulness of the features we extract.\n",
      "\n",
      "## Term weighting schemes\n",
      "\n",
      "In Chapter 3, _Obtaining, Processing, and Preparing Data with Spark_ , we looked at vector representation where text features are mapped to a simple binary vector called the **bag-of-words** model. Another representation used commonly in practice is called **term frequency-inverse document frequency** ( **TF-IDF** ).\n",
      "\n",
      "TF-IDF weights each term in a piece of text (referred to as a **document** ) based on its frequency in the document (the **term frequency** ). A global normalization, called the **inverse document frequency** , is then applied based on the frequency of this term among all documents (the set of documents in a dataset is commonly referred to as a **corpus** ). The standard definition of TF-IDF is shown here:\n",
      "\n",
      "    tf-idf(t,d) = tf(t,d) x idf(t)\n",
      "\n",
      "Here, _tf(t,d)_ is the frequency (number of occurrences) of term _t_ in document _d_ and _idf(t)_ is the inverse document frequency of term _t_ in the corpus; this is defined as follows:\n",
      "\n",
      "    idf(t) = log(N / d)\n",
      "\n",
      "Here, _N_ is the total number of documents, and _d_ is the number of documents in which the term _t_ occurs.\n",
      "\n",
      "The TF-IDF formulation means that terms occurring many times in a document receive a higher weighting in the vector representation relative to those that occur few times in the document. However, the IDF normalization has the effect of reducing the weight of terms that are very common across all documents. The end result is that truly rare or important terms should be assigned higher weighting, while more common terms (which are assumed to have less importance) should have less impact in terms of weighting.\n",
      "\n",
      "### Note\n",
      "\n",
      "A good resource to learn more about the bag-of-words model (or **vector space model** ) is the book _Introduction to Information Retrieval_ , _Christopher D. Manning, Prabhakar Raghavan and Hinrich Sch utze_, _Cambridge University Press_ (available in HTML form at <http://nlp.stanford.edu/IR-book/html/htmledition/irbook.html>).\n",
      "\n",
      "It contains sections on text processing techniques, including tokenization, stop word removal, stemming, and the vector space model, as well as weighting schemes such as TF-IDF.\n",
      "\n",
      "An overview can also be found at <http://en.wikipedia.org/wiki/Tf%E2%80%93idf>.\n",
      "\n",
      "## Feature hashing\n",
      "\n",
      " **Feature hashing** is a technique to deal with high-dimensional data and is often used with text and categorical datasets where the features can take on many unique values (often many millions of values). In the previous chapters, we often used the _1-of-K_ encoding approach for categorical features, including text. While this approach is simple and effective, it can break down in the face of extremely high-dimensional data.\n",
      "\n",
      "Building and using _1-of-K_ feature encoding requires us to keep a mapping of each possible feature value to an index in a vector. Furthermore, the process of creating the mapping itself requires at least one additional pass through the dataset and can be tricky to do in parallel scenarios. Up until now, we have often used a simple approach of collecting the distinct feature values and zipping this collection with a set of indices to create a map of feature value to index. This mapping is then broadcast (either explicitly in our code or implicitly by Spark) to each worker.\n",
      "\n",
      "However, when dealing with huge feature dimensions in the tens of millions or more that are common when working with text, this approach can be slow and can require significant memory and network resources, both on the Spark master (to collect the unique values) and workers (to broadcast the resulting mapping to each worker, which keeps it in memory to allow it to apply the feature encoding to its local piece of the input data).\n",
      "\n",
      "Feature hashing works by assigning the vector index for a feature based on the value obtained by hashing this feature to a number (usually, an integer value) using a hash function. For example, let's say the hash value of a categorical feature for the geolocation of `United States` is `342`. We will use the hashed value as the vector index, and the value at this index will be `1.0` to indicate the presence of the `United States` feature. The hash function used must be consistent (that is, for a given input, it returns the same output each time).\n",
      "\n",
      "This encoding works the same way as mapping-based encoding, except that we choose a size for our feature vector upfront. As the most common hash functions return values in the entire range of integers, we will use a _modulo_ operation to restrict the index values to the size of our vector, which is typically much smaller (a few tens of thousands to a few million, depending on our requirements).\n",
      "\n",
      "Feature hashing has the advantage that we do not need to build a mapping and keep it in memory. It is also easy to implement, very fast, and can be done online and in real time, thus not requiring a pass through our dataset first. Finally, because we selected a feature vector dimension that is significantly smaller than the raw dimensionality of our dataset, we bound the memory usage of our model both in training and production; hence, memory usage does not scale with the size and dimensionality of our data.\n",
      "\n",
      "However, there are two important drawbacks, which are as follows:\n",
      "\n",
      "  * As we don't create a mapping of features to index values, we also cannot do the reverse mapping of feature index to value. This makes it harder to, for example, determine which features are most informative in our models.\n",
      "  * As we are restricting the size of our feature vectors, we might experience **hash collisions**. This happens when two different features are hashed into the same index in our feature vector. Surprisingly, this doesn't seem to have a severe impact on model performance as long as we choose a reasonable feature vector dimension relative to the dimension of the input data.\n",
      "\n",
      "### Note\n",
      "\n",
      "Further information on hashing can be found at <http://en.wikipedia.org/wiki/Hash_function>.\n",
      "\n",
      "A key paper that introduced the use of hashing for feature extraction and machine learning is:\n",
      "\n",
      " _Kilian Weinberger_ , _Anirban Dasgupta_ , _John Langford_ , _Alex Smola_ , and _Josh Attenberg_. _Feature Hashing for Large Scale Multitask Learning_. _Proc. ICML 2009_ , which is available at <http://alex.smola.org/papers/2009/Weinbergeretal09.pdf>.\n",
      "\n",
      "## Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "\n",
      "To illustrate the concepts in this chapter, we will use a well-known text dataset called **20 Newsgroups** ; this dataset is commonly used for text-classification tasks. This is a collection of newsgroup messages posted across 20 different topics. There are various forms of data available. For our purposes, we will use the `bydate` version of the dataset, which is available at <http://qwone.com/~jason/20Newsgroups>.\n",
      "\n",
      "This dataset splits up the available data into training and test sets that comprise 60 percent and 40 percent of the original data, respectively. Here, the messages in the test set occur after those in the training set. This dataset also excludes some of the message headers that identify the actual newsgroup; hence, it is an appropriate dataset to test the real-world performance of classification models.\n",
      "\n",
      "### Note\n",
      "\n",
      "Further information on the original dataset can be found in the _UCI Machine Learning Repository_ page at <http://kdd.ics.uci.edu/databases/20newsgroups/20newsgroups.data.html>.\n",
      "\n",
      "To get started, download the data and unzip the file using the following command:\n",
      "\n",
      "    **> tar xfvz 20news-bydate.tar.gz**\n",
      "\n",
      "This will create two folders: one called `20news-bydate-train` and another one called `20news-bydate-test`. Let's take a look at the directory structure under the training dataset folder:\n",
      "\n",
      "    **> cd 20news-bydate-train/**\n",
      "    **> ls**\n",
      "\n",
      "You will see that it contains a number of subfolders, one for each newsgroup:\n",
      "\n",
      "    **alt.atheism              comp.windows.x           rec.sport.hockey         soc.religion.christian**\n",
      "    **comp.graphics            misc.forsale             sci.crypt                talk.politics.guns**\n",
      "    **comp.os.ms-windows.misc  rec.autos                sci.electronics          talk.politics.mideast**\n",
      "    **comp.sys.ibm.pc.hardware rec.motorcycles          sci.med                  talk.politics.misc**\n",
      "    **comp.sys.mac.hardware    rec.sport.baseball       sci.space                talk.religion.misc**\n",
      "\n",
      "There are a number of files under each newsgroup folder; each file contains an individual message posting:\n",
      "\n",
      "    **> ls rec.sport.hockey**\n",
      "    **52550 52580 52610 52640 53468 53550 53580 53610 53640 53670 53700 53731 53761 53791**\n",
      "    **...**\n",
      "\n",
      "We can take a look at a part of one of these messages to see the format:\n",
      "\n",
      "    **> head -20 rec.sport.hockey/52550**\n",
      "    **From: dchhabra@stpl.ists.ca (Deepak Chhabra)**\n",
      "    **Subject: Superstars and attendance (was Teemu Selanne, was +/- leaders)**\n",
      "    **Nntp-Posting-Host: stpl.ists.ca**\n",
      "    **Organization: Solar Terresterial Physics Laboratory, ISTS**\n",
      "    **Distribution: na**\n",
      "    **Lines: 115**\n",
      "\n",
      "    **Dean J. Falcione (posting from jrmst+8@pitt.edu) writes:**\n",
      "    **[I wrote:]**\n",
      "\n",
      "    **> >When the Pens got Mario, granted there was big publicity, etc, etc,**\n",
      "    **> >and interest was immediately generated.  Gretzky did the same thing for LA.**\n",
      "    **> >However, imnsho, neither team would have seen a marked improvement in**\n",
      "    **> >attendance if the team record did not improve.  In the year before Lemieux**\n",
      "    **> >came, Pittsburgh finished with 38 points.  Following his arrival, the Pens**\n",
      "    **> >finished with 53, 76, 72, 81, 87, 72, 88, and 87 points, with a couple of**\n",
      "    **^^**\n",
      "    **> >Stanley Cups thrown in.**\n",
      "    **...**\n",
      "\n",
      "As we can see, each message contains some header fields that contain the sender, subject, and other metadata, followed by the raw content of the message.\n",
      "\n",
      "### Exploring the 20 Newsgroups data\n",
      "\n",
      "Now, we will start up our Spark Scala console, ensuring that we make enough memory available:\n",
      "\n",
      "    **>./SPARK_HOME/bin/spark-shell --driver-memory 4g**\n",
      "\n",
      "Looking at the directory structure, you might recognize that once again, we have data contained in individual text files (one text file per message). Therefore, we will again use Spark's `wholeTextFiles` method to read the content of each file into a record in our RDD.\n",
      "\n",
      "In the code that follows, `PATH` refers to the directory in which you extracted the `20news-bydate` ZIP file:\n",
      "\n",
      "    val path = \"/PATH/20news-bydate-train/*\"\n",
      "    val rdd = sc.wholeTextFiles(path)\n",
      "    val text = rdd.map { case (file, text) => text }\n",
      "    println(text.count)\n",
      "\n",
      "The first time you run this command, it might take quite a bit of time, as Spark needs to scan the directory structure. You will also see quite a lot of console output, as Spark logs all the file paths that are being processed. During the processing, you will see the following line displayed, indicating the total number of files that Spark has detected:\n",
      "\n",
      "    **...**\n",
      "    **14/10/12 14:27:54 INFO FileInputFormat: Total input paths to process : 11314**\n",
      "    **...**\n",
      "\n",
      "After the command has finished running, you will see the total record count, which should be the same as the preceding **Total input paths to process** screen output:\n",
      "\n",
      "    **11314**\n",
      "\n",
      "Next, we will take a look at the newsgroup topics available:\n",
      "\n",
      "    val newsgroups = rdd.map { case (file, text) => file.split(\"/\").takeRight(2).head }\n",
      "    val countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_ + _).collect.sortBy(-_._2).mkString(\"\\n\")\n",
      "    println(countByGroup)\n",
      "\n",
      "This will display the following result:\n",
      "\n",
      "    **(rec.sport.hockey,600)**\n",
      "    **(soc.religion.christian,599)**\n",
      "    **(rec.motorcycles,598)**\n",
      "    **(rec.sport.baseball,597)**\n",
      "    **(sci.crypt,595)**\n",
      "    **(rec.autos,594)**\n",
      "    **(sci.med,594)**\n",
      "    **(comp.windows.x,593)**\n",
      "    **(sci.space,593)**\n",
      "    **(sci.electronics,591)**\n",
      "    **(comp.os.ms-windows.misc,591)**\n",
      "    **(comp.sys.ibm.pc.hardware,590)**\n",
      "    **(misc.forsale,585)**\n",
      "    **(comp.graphics,584)**\n",
      "    **(comp.sys.mac.hardware,578)**\n",
      "    **(talk.politics.mideast,564)**\n",
      "    **(talk.politics.guns,546)**\n",
      "    **(alt.atheism,480)**\n",
      "    **(talk.politics.misc,465)**\n",
      "    **(talk.religion.misc,377)**\n",
      "\n",
      "We can see that the number of messages is roughly even between the topics.\n",
      "\n",
      "### Applying basic tokenization\n",
      "\n",
      "The first step in our text processing pipeline is to split up the raw text content in each document into a collection of terms (also referred to as **tokens** ). This is known as **tokenization**. We will start by applying a simple **whitespace** tokenization, together with converting each token to lowercase for each document:\n",
      "\n",
      "    val text = rdd.map { case (file, text) => text }\n",
      "    val whiteSpaceSplit = text.flatMap(t => t.split(\" \").map(_.toLowerCase))\n",
      "    println(whiteSpaceSplit.distinct.count)\n",
      "\n",
      "### Tip\n",
      "\n",
      "In the preceding code, we used the `flatMap` function instead of `map`, as for now, we want to inspect all the tokens together for exploratory analysis. Later in this chapter, we will apply our tokenization scheme on a per-document basis, so we will use the `map` function.\n",
      "\n",
      "After running this code snippet, you will see the total number of unique tokens after applying our tokenization:\n",
      "\n",
      "    **402978**\n",
      "\n",
      "As you can see, for even a relatively small set of text, the number of raw tokens (and, therefore, the dimensionality of our feature vectors) can be very high.\n",
      "\n",
      "Let's take a look at a randomly selected document:\n",
      "\n",
      "    println(whiteSpaceSplit.sample(true, 0.3, 42).take(100).mkString(\",\"))\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that we set the third parameter to the `sample` function, which is the random seed. We set this function to `42` so that we get the same results from each call to `sample` so that your results match those in this chapter.\n",
      "\n",
      "This will display the following result:\n",
      "\n",
      "    **atheist,resources**\n",
      "    **summary:,addresses,,to,atheism**\n",
      "    **keywords:,music,,thu,,11:57:19,11:57:19,gmt**\n",
      "    **distribution:,cambridge.,290**\n",
      "\n",
      "    **archive-name:,atheism/resources**\n",
      "    **alt-atheism-archive-name:,december,,,,,,,,,,,,,,,,,,,,,,addresses,addresses,,,,,,,religion,to:,to:,,p.o.,53701.**\n",
      "    **telephone:,sell,the,,fish,on,their,cars,,with,and,written**\n",
      "    **inside.,3d,plastic,plastic,,evolution,evolution,7119,,,,,san,san,san,mailing,net,who,to,atheist,press**\n",
      "\n",
      "    **aap,various,bible,,and,on.,,,one,book,is:**\n",
      "\n",
      "    **\"the,w.p.,american,pp.,,1986.,bible,contains,ball,,based,based,james,of**\n",
      "\n",
      "### Improving our tokenization\n",
      "\n",
      "The preceding simple approach results in a lot of tokens and does not filter out many nonword characters (such as punctuation). Most tokenization schemes will remove these characters. We can do this by splitting each raw document on **nonword characters** using a regular expression pattern:\n",
      "\n",
      "    val nonWordSplit = text.flatMap(t => t.split(\"\"\"\\W+\"\"\").map(_.toLowerCase))\n",
      "    println(nonWordSplit.distinct.count)\n",
      "\n",
      "This reduces the number of unique tokens significantly:\n",
      "\n",
      "    **130126**\n",
      "\n",
      "If we inspect the first few tokens, we will see that we have eliminated most of the less useful characters in the text:\n",
      "\n",
      "    println(nonWordSplit.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))\n",
      "\n",
      "You will see the following result displayed:\n",
      "\n",
      "    **bone,k29p,w1w3s1,odwyer,dnj33n,bruns,_congressional,mmejv5,mmejv5,artur,125215,entitlements,beleive,1pqd9hinnbmi,**\n",
      "    **jxicaijp,b0vp,underscored,believiing,qsins,1472,urtfi,nauseam,tohc4,kielbasa,ao,wargame,seetex,museum,typeset,pgva4,**\n",
      "    **dcbq,ja_jp,ww4ewa4g,animating,animating,10011100b,10011100b,413,wp3d,wp3d,cannibal,searflame,ets,1qjfnv,6jx,6jx,**\n",
      "    **detergent,yan,aanp,unaskable,9mf,bowdoin,chov,16mb,createwindow,kjznkh,df,classifieds,hour,cfsmo,santiago,santiago,**\n",
      "    **1r1d62,almanac_,almanac_,chq,nowadays,formac,formac,bacteriophage,barking,barking,barking,ipmgocj7b,monger,projector,**\n",
      "    **hama,65e90h8y,homewriter,cl5,1496,zysec,homerific,00ecgillespie,00ecgillespie,mqh0,suspects,steve_mullins,io21087,**\n",
      "    **funded,liberated,canonical,throng,0hnz,exxon,xtappcontext,mcdcup,mcdcup,5seg,biscuits**\n",
      "\n",
      "While our nonword pattern to split text works fairly well, we are still left with numbers and tokens that contain numeric characters. In some cases, numbers can be an important part of a corpus. For our purposes, the next step in our pipeline will be to filter out numbers and tokens that are words mixed with numbers.\n",
      "\n",
      "We can do this by applying another regular expression pattern and using this to filter out tokens that _do not match_ the pattern:\n",
      "\n",
      "    val regex = \"\"\"[^0-9]*\"\"\".r\n",
      "    val filterNumbers = nonWordSplit.filter(token => regex.pattern.matcher(token).matches)\n",
      "    println(filterNumbers.distinct.count)\n",
      "\n",
      "This further reduces the size of the token set:\n",
      "\n",
      "    **84912**\n",
      "\n",
      "Let's take a look at another random sample of the filtered tokens:\n",
      "\n",
      "    println(filterNumbers.distinct.sample(true, 0.3, 42).take(100).mkString(\",\"))\n",
      "\n",
      "You will see output like the following one:\n",
      "\n",
      "    **reunion,wuair,schwabam,eer,silikian,fuller,sloppiness,crying,crying,beckmans,leymarie,fowl,husky,rlhzrlhz,ignore,**\n",
      "    **loyalists,goofed,arius,isgal,dfuller,neurologists,robin,jxicaijp,majorly,nondiscriminatory,akl,sively,adultery,**\n",
      "    **urtfi,kielbasa,ao,instantaneous,subscriptions,collins,collins,za_,za_,jmckinney,nonmeasurable,nonmeasurable,**\n",
      "    **seetex,kjvar,dcbq,randall_clark,theoreticians,theoreticians,congresswoman,sparcstaton,diccon,nonnemacher,**\n",
      "    **arresed,ets,sganet,internship,bombay,keysym,newsserver,connecters,igpp,aichi,impute,impute,raffle,nixdorf,**\n",
      "    **nixdorf,amazement,butterfield,geosync,geosync,scoliosis,eng,eng,eng,kjznkh,explorers,antisemites,bombardments,**\n",
      "    **abba,caramate,tully,mishandles,wgtn,springer,nkm,nkm,alchoholic,chq,shutdown,bruncati,nowadays,mtearle,eastre,**\n",
      "    **discernible,bacteriophage,paradijs,systematically,rluap,rluap,blown,moderates**\n",
      "\n",
      "We can see that we have removed all the numeric characters. This still leaves us with a few strange _words_ , but we will not worry about these too much here.\n",
      "\n",
      "### Removing stop words\n",
      "\n",
      " **Stop words** refer to common words that occur many times across almost all documents in a corpus (and across most corpuses). Examples of typical English stop words include and, but, the, of, and so on. It is a standard practice in text feature extraction to exclude stop words from the extracted tokens.\n",
      "\n",
      "When using TF-IDF weighting, the weighting scheme actually takes care of this for us. As stop words have a very low IDF score, they will tend to have very low TF-IDF weightings and thus less importance. In some cases, for information retrieval and search tasks, it might be desirable to include stop words. However, it can still be beneficial to exclude stop words during feature extraction, as it reduces the dimensionality of the final feature vectors as well as the size of the training data.\n",
      "\n",
      "We can take a look at some of the tokens in our corpus that have the highest occurrence across all documents to get an idea about some other stop words to exclude:\n",
      "\n",
      "    val tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + _)\n",
      "    val oreringDesc = Ordering.by[(String, Int), Int](_._2)\n",
      "    println(tokenCounts.top(20)(oreringDesc).mkString(\"\\n\"))\n",
      "\n",
      "In the preceding code, we took the tokens after filtering out numeric characters and generated a count of the occurrence of each token across the corpus. We can now use Spark's `top` function to retrieve the top 20 tokens by count. Notice that we need to provide the `top` function with an ordering that tells Spark how to order the elements of our RDD. In this case, we want to order by the count, so we will specify the second element of our key-value pair.\n",
      "\n",
      "Running the preceding code snippet will result in the following top tokens:\n",
      "\n",
      "    **(the,146532)**\n",
      "    **(to,75064)**\n",
      "    **(of,69034)**\n",
      "    **(a,64195)**\n",
      "    **(ax,62406)**\n",
      "    **(and,57957)**\n",
      "    **(i,53036)**\n",
      "    **(in,49402)**\n",
      "    **(is,43480)**\n",
      "    **(that,39264)**\n",
      "    **(it,33638)**\n",
      "    **(for,28600)**\n",
      "    **(you,26682)**\n",
      "    **(from,22670)**\n",
      "    **(s,22337)**\n",
      "    **(edu,21321)**\n",
      "    **(on,20493)**\n",
      "    **(this,20121)**\n",
      "    **(be,19285)**\n",
      "    **(t,18728)**\n",
      "\n",
      "As we might expect, there are a lot of common words in this list that we could potentially label as stop words. Let's create a set of stop words with some of these as well as other common words. We will then look at the tokens after filtering out these stop words:\n",
      "\n",
      "    val stopwords = Set(\n",
      "      \"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\", \"is\", \"not\", \"with\", \"as\", \"was\", \"if\",\n",
      "      \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\", \"be\", \"that\", \"to\"\n",
      "    )\n",
      "    val tokenCountsFilteredStopwords = tokenCounts.filter { case (k, v) => !stopwords.contains(k) }\n",
      "    println(tokenCountsFilteredStopwords.top(20)(oreringDesc).mkString(\"\\n\"))\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **(ax,62406)**\n",
      "    **(i,53036)**\n",
      "    **(you,26682)**\n",
      "    **(s,22337)**\n",
      "    **(edu,21321)**\n",
      "    **(t,18728)**\n",
      "    **(m,12756)**\n",
      "    **(subject,12264)**\n",
      "    **(com,12133)**\n",
      "    **(lines,11835)**\n",
      "    **(can,11355)**\n",
      "    **(organization,11233)**\n",
      "    **(re,10534)**\n",
      "    **(what,9861)**\n",
      "    **(there,9689)**\n",
      "    **(x,9332)**\n",
      "    **(all,9310)**\n",
      "    **(will,9279)**\n",
      "    **(we,9227)**\n",
      "    **(one,9008)**\n",
      "\n",
      "You might notice that there are still quite a few common words in this top list. In practice, we might have a much larger set of stop words. However, we will keep a few (partly to illustrate the impact of common words when using TF-IDF weighting a little later).\n",
      "\n",
      "One other filtering step that we will use is removing any tokens that are only one character in length. The reasoning behind this is similar to removing stop words--these single-character tokens are unlikely to be informative in our text model and can further reduce the feature dimension and model size. We will do this with another filtering step:\n",
      "\n",
      "    val tokenCountsFilteredSize = tokenCountsFilteredStopwords.filter { case (k, v) => k.size >= 2 }\n",
      "    println(tokenCountsFilteredSize.top(20)(oreringDesc).mkString(\"\\n\"))\n",
      "\n",
      "Again, we will examine the tokens remaining after this filtering step:\n",
      "\n",
      "    **(ax,62406)**\n",
      "    **(you,26682)**\n",
      "    **(edu,21321)**\n",
      "    **(subject,12264)**\n",
      "    **(com,12133)**\n",
      "    **(lines,11835)**\n",
      "    **(can,11355)**\n",
      "    **(organization,11233)**\n",
      "    **(re,10534)**\n",
      "    **(what,9861)**\n",
      "    **(there,9689)**\n",
      "    **(all,9310)**\n",
      "    **(will,9279)**\n",
      "    **(we,9227)**\n",
      "    **(one,9008)**\n",
      "    **(would,8905)**\n",
      "    **(do,8674)**\n",
      "    **(he,8441)**\n",
      "    **(about,8336)**\n",
      "    **(writes,7844)**\n",
      "\n",
      "Apart from some of the common words that we have not excluded, we see that a few potentially more informative words are starting to appear.\n",
      "\n",
      "### Excluding terms based on frequency\n",
      "\n",
      "It is also a common practice to exclude terms during tokenization when their overall occurrence in the corpus is very low. For example, let's examine the least occurring terms in the corpus (notice the different ordering we use here to return the results sorted in ascending order):\n",
      "\n",
      "    val oreringAsc = Ordering.by[(String, Int), Int](-_._2)\n",
      "    println(tokenCountsFilteredSize.top(20)(oreringAsc).mkString(\"\\n\"))\n",
      "\n",
      "You will get the following results:\n",
      "\n",
      "    **(lennips,1)**\n",
      "    **(bluffing,1)**\n",
      "    **(preload,1)**\n",
      "    **(altina,1)**\n",
      "    **(dan_jacobson,1)**\n",
      "    **(vno,1)**\n",
      "    **(actu,1)**\n",
      "    **(donnalyn,1)**\n",
      "    **(ydag,1)**\n",
      "    **(mirosoft,1)**\n",
      "    **(xiconfiywindow,1)**\n",
      "    **(harger,1)**\n",
      "    **(feh,1)**\n",
      "    **(bankruptcies,1)**\n",
      "    **(uncompression,1)**\n",
      "    **(d_nibby,1)**\n",
      "    **(bunuel,1)**\n",
      "    **(odf,1)**\n",
      "    **(swith,1)**\n",
      "    **(lantastic,1)**\n",
      "\n",
      "As we can see, there are many terms that only occur once in the entire corpus. Since typically we want to use our extracted features for other tasks such as document similarity or machine learning models, tokens that only occur once are not useful to learn from, as we will not have enough training data relative to these tokens. We can apply another filter to exclude these rare tokens:\n",
      "\n",
      "    val rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map { case (k, v) => k }.collect.toSet\n",
      "    val tokenCountsFilteredAll = tokenCountsFilteredSize.filter { case (k, v) => !rareTokens.contains(k) }\n",
      "    println(tokenCountsFilteredAll.top(20)(oreringAsc).mkString(\"\\n\"))\n",
      "\n",
      "We can see that we are left with tokens that occur at least twice in the corpus:\n",
      "\n",
      "    **(sina,2)**\n",
      "    **(akachhy,2)**\n",
      "    **(mvd,2)**\n",
      "    **(hizbolah,2)**\n",
      "    **(wendel_clark,2)**\n",
      "    **(sarkis,2)**\n",
      "    **(purposeful,2)**\n",
      "    **(feagans,2)**\n",
      "    **(wout,2)**\n",
      "    **(uneven,2)**\n",
      "    **(senna,2)**\n",
      "    **(multimeters,2)**\n",
      "    **(bushy,2)**\n",
      "    **(subdivided,2)**\n",
      "    **(coretest,2)**\n",
      "    **(oww,2)**\n",
      "    **(historicity,2)**\n",
      "    **(mmg,2)**\n",
      "    **(margitan,2)**\n",
      "    **(defiance,2)**\n",
      "\n",
      "Now, let's count the number of unique tokens:\n",
      "\n",
      "    println(tokenCountsFilteredAll.count)\n",
      "\n",
      "You will see the following output:\n",
      "\n",
      "    **51801**\n",
      "\n",
      "As we can see, by applying all the filtering steps in our tokenization pipeline, we have reduced the feature dimension from 402,978 to 51,801.\n",
      "\n",
      "We can now combine all our filtering logic into one function, which we can apply to each document in our RDD:\n",
      "\n",
      "    def tokenize(line: String): Seq[String] = {\n",
      "      line.split(\"\"\"\\W+\"\"\")\n",
      "        .map(_.toLowerCase)\n",
      "        .filter(token => regex.pattern.matcher(token).matches)\n",
      "        .filterNot(token => stopwords.contains(token))\n",
      "        .filterNot(token => rareTokens.contains(token))\n",
      "        .filter(token => token.size >= 2)\n",
      "        .toSeq\n",
      "    }\n",
      "\n",
      "We can check whether this function gives us the same result with the following code snippet:\n",
      "\n",
      "    println(text.flatMap(doc => tokenize(doc)).distinct.count)\n",
      "\n",
      "This will output `51801`, giving us the same unique token count as our step-by-step pipeline.\n",
      "\n",
      "We can tokenize each document in our RDD as follows:\n",
      "\n",
      "    val tokens = text.map(doc => tokenize(doc))\n",
      "    println(tokens.first.take(20))\n",
      "\n",
      "You will see output similar to the following, showing the first part of the tokenized version of our first document:\n",
      "\n",
      "    **WrappedArray(mathew, mathew, mantis, co, uk, subject, alt, atheism, faq, atheist, resources, summary, books, addresses, music, anything, related, atheism, keywords, faq)**\n",
      "\n",
      "### A note about stemming\n",
      "\n",
      "A common step in text processing and tokenization is **stemming**. This is the conversion of whole words to a **base form** (called a **word stem** ). For example, plurals might be converted to singular ( _dogs_ becomes _dog_ ), and forms such as _walking_ and _walker_ might become _walk_. Stemming can become quite complex and is typically handled with specialized NLP or search engine software (such as NLTK, OpenNLP, and Lucene, for example). We will ignore stemming for the purpose of our example here.\n",
      "\n",
      "### Note\n",
      "\n",
      "A full treatment of stemming is beyond the scope of this book. You can find more details at <http://en.wikipedia.org/wiki/Stemming>.\n",
      "\n",
      "### Training a TF-IDF model\n",
      "\n",
      "We will now use MLlib to transform each document, in the form of processed tokens, into a vector representation. The first step will be to use the `HashingTF` implementation, which makes use of feature hashing to map each token in the input text to an index in the vector of term frequencies. Then, we will compute the global IDF and use it to transform the term frequency vectors into TF-IDF vectors.\n",
      "\n",
      "For each token, the index will thus be the hash of the token (mapped in turn onto the dimension of the feature vector). The value for each token will be the TF-IDF weighting for that token (that is, the term frequency multiplied by the inverse document frequency).\n",
      "\n",
      "First, we will import the classes we need and create our `HashingTF` instance, passing in a `dim` dimension parameter. While the default feature dimension is 220 (or around 1 million), we will choose 218 (or around 260,000), since with about 50,000 tokens, we should not experience a significant number of hash collisions, and a smaller dimension will be more memory and processing friendly for illustrative purposes:\n",
      "\n",
      "    import org.apache.spark.mllib.linalg.{ SparseVector => SV }\n",
      "    import org.apache.spark.mllib.feature.HashingTF\n",
      "    import org.apache.spark.mllib.feature.IDF\n",
      "    val dim = math.pow(2, 18).toInt\n",
      "    val hashingTF = new HashingTF(dim)\n",
      "    val tf = hashingTF.transform(tokens)\n",
      "    tf.cache\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that we imported MLlib's `SparseVector` using an alias of `SV`. This is because later, we will use Breeze's `linalg` module, which itself also imports `SparseVector`. This way, we will avoid namespace collisions.\n",
      "\n",
      "The `transform` function of `HashingTF` maps each input document (that is, a sequence of tokens) to an MLlib `Vector`. We will also call `cache` to pin the data in memory to speed up subsequent operations.\n",
      "\n",
      "Let's inspect the first element of our transformed dataset:\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that `HashingTF.transform` returns an `RDD[Vector]`, so we will cast the result returned to an instance of an MLlib `SparseVector`.\n",
      "\n",
      "The `transform` method can also work on an individual document by taking an `Iterable` argument (for example, a document as a `Seq[String]`). This returns a single vector.\n",
      "\n",
      "    val v = tf.first.asInstanceOf[SV]\n",
      "    println(v.size)\n",
      "    println(v.values.size)\n",
      "    println(v.values.take(10).toSeq)\n",
      "    println(v.indices.take(10).toSeq)\n",
      "\n",
      "You will see the following output displayed:\n",
      "\n",
      "    **262144**\n",
      "    **706**\n",
      "    **WrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)**\n",
      "    **WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)**\n",
      "\n",
      "We can see that the dimension of each sparse vector of term frequencies is 262,144 (or 218 as we specified). However, the number on non-zero entries in the vector is only 706. The last two lines of the output show the frequency counts and vector indexes for the first few entries in the vector.\n",
      "\n",
      "We will now compute the inverse document frequency for each term in the corpus by creating a new `IDF` instance and calling `fit` with our RDD of term frequency vectors as the input. We will then transform our term frequency vectors to TF-IDF vectors through the `transform` function of `IDF`:\n",
      "\n",
      "    val idf = new IDF().fit(tf)\n",
      "    val tfidf = idf.transform(tf)\n",
      "    val v2 = tfidf.first.asInstanceOf[SV]\n",
      "    println(v2.values.size)\n",
      "    println(v2.values.take(10).toSeq)\n",
      "    println(v2.indices.take(10).toSeq)\n",
      "\n",
      "When you examine the first element in the RDD of TF-IDF transformed vectors, you will see output similar to the one shown here:\n",
      "\n",
      "    **706**\n",
      "    **WrappedArray(2.3869085659322193, 4.670445463955571, 6.561295835827856, 4.597686109673142,  ...**\n",
      "    **WrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115, 3166)**\n",
      "\n",
      "We can see that the number of non-zero entries hasn't changed (at 706), nor have the vector indices for the terms. What has changed are the values for each term. Earlier, these represented the frequency of each term in the document, but now, the new values represent the frequencies weighted by the `IDF`.\n",
      "\n",
      "### Analyzing the TF-IDF weightings\n",
      "\n",
      "Next, let's investigate the TF-IDF weighting for a few terms to illustrate the impact of the commonality or rarity of a term.\n",
      "\n",
      "First, we can compute the minimum and maximum TF-IDF weights across the entire corpus:\n",
      "\n",
      "    val minMaxVals = tfidf.map { v =>\n",
      "      val sv = v.asInstanceOf[SV]\n",
      "      (sv.values.min, sv.values.max)\n",
      "    }\n",
      "    val globalMinMax = minMaxVals.reduce { case ((min1, max1), (min2, max2)) =>\n",
      "      (math.min(min1, min2), math.max(max1, max2))\n",
      "    }\n",
      "    println(globalMinMax)\n",
      "\n",
      "As we can see, the minimum TF-IDF is zero, while the maximum is significantly larger:\n",
      "\n",
      "    **(0.0,66155.39470409753)**\n",
      "\n",
      "We will now explore the TF-IDF weight attached to various terms. In the previous section on stop words, we filtered out many common terms that occur frequently. Recall that we did not remove all such potential stop words. Instead, we kept a few in the corpus so that we could illustrate the impact of applying the TF-IDF weighting scheme on these terms.\n",
      "\n",
      "TF-IDF weighting will tend to assign a lower weighting to common terms. To see this, we can compute the TF-IDF representation for a few of the terms that appear in the list of top occurrences that we previously computed, such as `you`, `do`, and `we`:\n",
      "\n",
      "    val common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\")))\n",
      "    val tfCommon = hashingTF.transform(common)\n",
      "    val tfidfCommon = idf.transform(tfCommon)\n",
      "    val commonVector = tfidfCommon.first.asInstanceOf[SV]\n",
      "    println(commonVector.values.toSeq)\n",
      "\n",
      "If we form a TF-IDF vector representation of this document, we would see the following values assigned to each term. Note that because of feature hashing, we are not sure exactly which term represents what. However, the values illustrate that the weighting applied to these terms is relatively low:\n",
      "\n",
      "    **WrappedArray(0.9965359935704624, 1.3348773448236835, 0.5457486182039175)**\n",
      "\n",
      "Now, let's apply the same transformation to a few less common terms that we might intuitively associate with being more linked to specific topics or concepts:\n",
      "\n",
      "    val uncommon = sc.parallelize(Seq(Seq(\"telescope\", \"legislation\", \"investment\")))\n",
      "    val tfUncommon = hashingTF.transform(uncommon)\n",
      "    val tfidfUncommon = idf.transform(tfUncommon)\n",
      "    val uncommonVector = tfidfUncommon.first.asInstanceOf[SV]\n",
      "    println(uncommonVector.values.toSeq)\n",
      "\n",
      "We can see from the following results that the TF-IDF weightings are indeed significantly higher than for the more common terms:\n",
      "\n",
      "    **WrappedArray(5.3265513728351666, 5.308532867332488, 5.483736956357579)**\n",
      "\n",
      "# Using a TF-IDF model\n",
      "\n",
      "While we often refer to training a TF-IDF model, it is actually a feature extraction process or transformation rather than a machine learning model. TF-IDF weighting is often used as a preprocessing step for other models, such as dimensionality reduction, classification, or regression.\n",
      "\n",
      "To illustrate the potential uses of TF-IDF weighting, we will explore two examples. The first is using the TF-IDF vectors to compute document similarity, while the second involves training a multilabel classification model with the TF-IDF vectors as input features.\n",
      "\n",
      "## Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "\n",
      "You might recall from Chapter 4, _Building a Recommendation Engine with Spark_ , that the similarity between two vectors can be computed using a distance metric. The closer two vectors are (that is, the lower the distance metric), the more similar they are. One such metric that we used to compute similarity between movies is cosine similarity.\n",
      "\n",
      "Just like we did for movies, we can also compute the similarity between two documents. Using TF-IDF, we have transformed each document into a vector representation. Hence, we can use the same techniques as we used for movie vectors to compare two documents.\n",
      "\n",
      "Intuitively, we might expect two documents to be more similar to each other if they share many terms. Conversely, we might expect two documents to be less similar if they each contain many terms that are different from each other. As we compute cosine similarity by computing a dot product of the two vectors and each vector is made up of the terms in each document, we can see that documents with a high overlap of terms will tend to have a higher cosine similarity.\n",
      "\n",
      "Now, we can see TF-IDF at work. We might reasonably expect that even very different documents might contain many overlapping terms that are relatively common (for example, our stop words). However, due to a low TF-IDF weighting, these terms will not have a significant impact on the dot product and, therefore, will not have much impact on the similarity computed.\n",
      "\n",
      "For example, we might expect two randomly chosen messages from the `hockey` newsgroup to be relatively similar to each other. Let's see if this is the case:\n",
      "\n",
      "    val hockeyText = rdd.filter { case (file, text) => file.contains(\"hockey\") }\n",
      "    val hockeyTF = hockeyText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
      "    val hockeyTfIdf = idf.transform(hockeyTF.map(_._2))\n",
      "\n",
      "In the preceding code, we first filtered our raw input RDD to keep only the messages within the hockey topic. We then applied our tokenization and term frequency transformation functions. Note that the `transform` method used is the version that works on a single document (in the form of a `Seq[String]`) rather than the version that works on an RDD of documents.\n",
      "\n",
      "Finally, we applied the `IDF` transform (note that we use the same IDF that we have already computed on the whole corpus).\n",
      "\n",
      "Once we have our `hockey` document vectors, we can select two of these vectors at random and compute the cosine similarity between them (as we did earlier, we will use Breeze for the linear algebra functionality, in particular converting our MLlib vectors to Breeze `SparseVector` instances first):\n",
      "\n",
      "    import breeze.linalg._\n",
      "    val hockey1 = hockeyTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
      "    val breeze1 = new SparseVector(hockey1.indices, hockey1.values, hockey1.size)\n",
      "    val hockey2 = hockeyTfIdf.sample(true, 0.1, 43).first.asInstanceOf[SV]\n",
      "    val breeze2 = new SparseVector(hockey2.indices, hockey2.values, hockey2.size)\n",
      "    val cosineSim = breeze1.dot(breeze2) / (norm(breeze1) * norm(breeze2))\n",
      "    println(cosineSim)\n",
      "\n",
      "We can see that the cosine similarity between the documents is around 0.06:\n",
      "\n",
      "    **0.060250114361164626**\n",
      "\n",
      "While this might seem quite low, recall that the effective dimensionality of our features is high due to the large number of unique terms that is typical when dealing with text data. Hence, we can expect that any two documents might have a relatively low overlap of terms even if they are about the same topic, and therefore would have a lower absolute similarity score.\n",
      "\n",
      "By contrast, we can compare this similarity score to the one computed between one of our `hockey` documents and another document chosen randomly from the `comp.graphics` newsgroup, using the same methodology:\n",
      "\n",
      "    val graphicsText = rdd.filter { case (file, text) => file.contains(\"comp.graphics\") }\n",
      "    val graphicsTF = graphicsText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
      "    val graphicsTfIdf = idf.transform(graphicsTF.map(_._2))\n",
      "    val graphics = graphicsTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
      "    val breezeGraphics = new SparseVector(graphics.indices, graphics.values, graphics.size)\n",
      "    val cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * norm(breezeGraphics))\n",
      "    println(cosineSim2)\n",
      "\n",
      "The cosine similarity is significantly lower at 0.0047:\n",
      "\n",
      "    **0.004664850323792852**\n",
      "\n",
      "Finally, it is likely that a document from another sports-related topic might be more similar to our `hockey` document than one from a computer-related topic. However, we would probably expect a `baseball` document to not be as similar as our `hockey` document. Let's see whether this is the case by computing the similarity between a random message from the `baseball` newsgroup and our `hockey` document:\n",
      "\n",
      "    val baseballText = rdd.filter { case (file, text) => file.contains(\"baseball\") }\n",
      "    val baseballTF = baseballText.mapValues(doc => hashingTF.transform(tokenize(doc)))\n",
      "    val baseballTfIdf = idf.transform(baseballTF.map(_._2))\n",
      "    val baseball = baseballTfIdf.sample(true, 0.1, 42).first.asInstanceOf[SV]\n",
      "    val breezeBaseball = new SparseVector(baseball.indices, baseball.values, baseball.size)\n",
      "    val cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * norm(breezeBaseball))\n",
      "    println(cosineSim3)\n",
      "\n",
      "Indeed, as we expected, we found that the `baseball` and `hockey` documents have a cosine similarity of 0.05, which is significantly higher than the `comp.graphics` document, but also somewhat lower than the other `hockey` document:\n",
      "\n",
      "    **0.05047395039466008**\n",
      "\n",
      "## Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "\n",
      "When using TF-IDF vectors, we expected that the cosine similarity measure would capture the similarity between documents, based on the overlap of terms between them. In a similar way, we would expect that a machine learning model, such as a classifier, would be able to learn weightings for individual terms; this would allow it to distinguish between documents from different classes. That is, it should be possible to learn a mapping between the presence (and weighting) of certain terms and a specific topic.\n",
      "\n",
      "In the 20 Newsgroups example, each newsgroup topic is a class, and we can train a classifier using our TF-IDF transformed vectors as input.\n",
      "\n",
      "Since we are dealing with a multiclass classification problem, we will use the naive Bayes model in MLlib, which supports multiple classes. As the first step, we will import the Spark classes that we will be using:\n",
      "\n",
      "    import org.apache.spark.mllib.regression.LabeledPoint\n",
      "    import org.apache.spark.mllib.classification.NaiveBayes\n",
      "    import org.apache.spark.mllib.evaluation.MulticlassMetrics\n",
      "\n",
      "Next, we will need to extract the 20 topics and convert them to class mappings. We can do this in exactly the same way as we might for 1-of-K feature encoding, by assigning a numeric index to each class:\n",
      "\n",
      "    val newsgroupsMap = newsgroups.distinct.collect().zipWithIndex.toMap\n",
      "    val zipped = newsgroups.zip(tfidf)\n",
      "    val train = zipped.map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\n",
      "    train.cache\n",
      "\n",
      "In the preceding code snippet, we took the `newsgroups` RDD, where each element is the topic, and used the `zip` function to combine it with each element in our `tfidf` RDD of TF-IDF vectors. We then mapped over each key-value element in our new `zipped` RDD and created a `LabeledPoint` instance, where `label` is the class index and `features` is the TF-IDF vector.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that the `zip` operator assumes that each RDD has the same number of partitions as well as the same number of elements in each partition. It will fail if this is not the case. We can make this assumption here because we have effectively created both our `tfidf` RDD and `newsgroups` RDD from a series of `map` transformations on the same original RDD that preserved the partitioning structure.\n",
      "\n",
      "Now that we have an input RDD in the correct form, we can simply pass it to the naive Bayes `train` function:\n",
      "\n",
      "    val model = NaiveBayes.train(train, lambda = 0.1)\n",
      "\n",
      "Let's evaluate the performance of the model on the test dataset. We will load the raw test data from the `20news-bydate-test` directory, again using `wholeTextFiles` to read each message into an RDD element. We will then extract the class labels from the file paths in the same way as we did for the `newsgroups` RDD:\n",
      "\n",
      "    val testPath = \"/PATH/20news-bydate-test/*\"\n",
      "    val testRDD = sc.wholeTextFiles(testPath)\n",
      "    val testLabels = testRDD.map { case (file, text) =>\n",
      "      val topic = file.split(\"/\").takeRight(2).head\n",
      "      newsgroupsMap(topic)\n",
      "    }\n",
      "\n",
      "Transforming the text in the test dataset follows the same procedure as for the training data--we will apply our `tokenize` function followed by the term frequency transformation, and we will again use the same IDF computed from the training data to transform the TF vectors into TF-IDF vectors. Finally, we will zip the test class labels with the TF-IDF vectors and create our test `RDD[LabeledPoint]`:\n",
      "\n",
      "    val testTf = testRDD.map { case (file, text) => hashingTF.transform(tokenize(text)) }\n",
      "    val testTfIdf = idf.transform(testTf)\n",
      "    val zippedTest = testLabels.zip(testTfIdf)\n",
      "    val test = zippedTest.map { case (topic, vector) => LabeledPoint(topic, vector) }\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that it is important that we use the training set IDF to transform the test data, as this creates a more realistic estimation of model performance on new data, which might potentially contain terms that the model has not yet been trained on. It would be \"cheating\" to recompute the IDF vector based on the test dataset and, more importantly, would potentially lead to incorrect estimates of optimal model parameters selected through cross-validation.\n",
      "\n",
      "Now, we're ready to compute the predictions and true class labels for our model. We will use this RDD to compute accuracy and the multiclass weighted F-measure for our model:\n",
      "\n",
      "    val predictionAndLabel = test.map(p => (model.predict(p.features), p.label))\n",
      "    val accuracy = 1.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / test.count()\n",
      "    val metrics = new MulticlassMetrics(predictionAndLabel)\n",
      "    println(accuracy)\n",
      "    println(metrics.weightedFMeasure)\n",
      "\n",
      "### Tip\n",
      "\n",
      "The weighted F-measure is an overall measure of precision and recall performance (where, like area under an ROC curve, values closer to 1.0 indicate better performance), which is then combined through a weighted averaged across the classes.\n",
      "\n",
      "We can see that our simple multiclass naive Bayes model has achieved close to 80 percent for both accuracy and F-measure:\n",
      "\n",
      "    **0.7915560276155071**\n",
      "    **0.7810675969031116**\n",
      "\n",
      "# Evaluating the impact of text processing\n",
      "\n",
      "Text processing and TF-IDF weighting are examples of feature extraction techniques designed to both reduce the dimensionality of and extract some structure from raw text data. We can see the impact of applying these processing techniques by comparing the performance of a model trained on raw text data with one trained on processed and TF-IDF weighted text data.\n",
      "\n",
      "## Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset\n",
      "\n",
      "In this example, we will simply apply the hashing term frequency transformation to the raw text tokens obtained using a simple whitespace splitting of the document text. We will train a model on this data and evaluate the performance on the test set as we did for the model trained with TF-IDF features:\n",
      "\n",
      "    val rawTokens = rdd.map { case (file, text) => text.split(\" \") }\n",
      "    val rawTF = texrawTokenst.map(doc => hashingTF.transform(doc))\n",
      "    val rawTrain = newsgroups.zip(rawTF).map { case (topic, vector) => LabeledPoint(newsgroupsMap(topic), vector) }\n",
      "    val rawModel = NaiveBayes.train(rawTrain, lambda = 0.1)\n",
      "    val rawTestTF = testRDD.map { case (file, text) => hashingTF.transform(text.split(\" \")) }\n",
      "    val rawZippedTest = testLabels.zip(rawTestTF)\n",
      "    val rawTest = rawZippedTest.map { case (topic, vector) => LabeledPoint(topic, vector) }\n",
      "    val rawPredictionAndLabel = rawTest.map(p => (rawModel.predict(p.features), p.label))\n",
      "    val rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 == x._2).count() / rawTest.count()\n",
      "    println(rawAccuracy)\n",
      "    val rawMetrics = new MulticlassMetrics(rawPredictionAndLabel)\n",
      "    println(rawMetrics.weightedFMeasure)\n",
      "\n",
      "Perhaps surprisingly, the raw model does quite well, although both accuracy and F-measure are a few percentage points lower than those of the TF-IDF model. This is also partly a reflection of the fact that the naive Bayes model is well suited to data in the form of raw frequency counts:\n",
      "\n",
      "    **0.7661975570897503**\n",
      "    **0.7628947184990661**\n",
      "\n",
      "# Word2Vec models\n",
      "\n",
      "Until now, we have used a bag-of-words vector, optionally with some weighting scheme such as TF-IDF to represent the text in a document. Another recent class of models that has become popular is related to representing individual words as vectors.\n",
      "\n",
      "These are generally based in some way on the co-occurrence statistics between the words in a corpus. Once the vector representation is computed, we can use these vectors in ways similar to how we might use TF-IDF vectors (such as using them as features for other machine learning models). One such common use case is computing the similarity between two words with respect to their meanings, based on their vector representations.\n",
      "\n",
      "Word2Vec refers to a specific implementation of one of these models, often referred to as **distributed vector representations**. The MLlib model uses a **skip-gram** model, which seeks to learn vector representations that take into account the contexts in which words occur.\n",
      "\n",
      "### Note\n",
      "\n",
      "While a detailed treatment of Word2Vec is beyond the scope of this book, Spark's documentation at <http://spark.apache.org/docs/latest/mllib-feature-extraction.html#word2vec> contains some further details on the algorithm as well as links to the reference implementation.\n",
      "\n",
      "One of the main academic papers underlying Word2Vec is _Tomas Mikolov_ , _Kai Chen_ , _Greg Corrado_ , and _Jeffrey Dean_. _Efficient Estimation of Word Representations in Vector Space_. _In Proceedings of Workshop at ICLR_ , _2013_.\n",
      "\n",
      "It is available at <http://arxiv.org/pdf/1301.3781.pdf>.\n",
      "\n",
      "Another recent model in the area of word vector representations is GloVe at <http://www-nlp.stanford.edu/projects/glove/>.\n",
      "\n",
      "## Word2Vec on the 20 Newsgroups dataset\n",
      "\n",
      "Training a Word2Vec model in Spark is relatively simple. We will pass in an RDD where each element is a sequence of terms. We can use the RDD of tokenized documents we have already created as input to the model:\n",
      "\n",
      "    import org.apache.spark.mllib.feature.Word2Vec\n",
      "    val word2vec = new Word2Vec()\n",
      "    word2vec.setSeed(42)\n",
      "    val word2vecModel = word2vec.fit(tokens)\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that we used `setSeed` to set the random seed for model training so that you can see the same results each time the model is trained.\n",
      "\n",
      "You will see some output similar to the following while the model is being trained:\n",
      "\n",
      "    **...**\n",
      "    **14/10/25 14:21:59 INFO Word2Vec: wordCount = 2133172, alpha = 0.0011868763094487506**\n",
      "    **14/10/25 14:21:59 INFO Word2Vec: wordCount = 2144172, alpha = 0.0010640806039941193**\n",
      "    **14/10/25 14:21:59 INFO Word2Vec: wordCount = 2155172, alpha = 9.412848985394907E-4**\n",
      "    **14/10/25 14:21:59 INFO Word2Vec: wordCount = 2166172, alpha = 8.184891930848592E-4**\n",
      "    **14/10/25 14:22:00 INFO Word2Vec: wordCount = 2177172, alpha = 6.956934876302307E-4**\n",
      "    **14/10/25 14:22:00 INFO Word2Vec: wordCount = 2188172, alpha = 5.728977821755993E-4**\n",
      "    **14/10/25 14:22:00 INFO Word2Vec: wordCount = 2199172, alpha = 4.501020767209707E-4**\n",
      "    **14/10/25 14:22:00 INFO Word2Vec: wordCount = 2210172, alpha = 3.2730637126634213E-4**\n",
      "    **14/10/25 14:22:01 INFO Word2Vec: wordCount = 2221172, alpha = 2.0451066581171076E-4**\n",
      "    **14/10/25 14:22:01 INFO Word2Vec: wordCount = 2232172, alpha = 8.171496035708214E-5**\n",
      "    **...**\n",
      "    **14/10/25 14:22:02 INFO SparkContext: Job finished: collect at Word2Vec.scala:368, took 56.585983 s**\n",
      "    **14/10/25 14:22:02 INFO MappedRDD: Removing RDD 200 from persistence list**\n",
      "    **14/10/25 14:22:02 INFO BlockManager: Removing RDD 200**\n",
      "    **14/10/25 14:22:02 INFO BlockManager: Removing block rdd_200_0**\n",
      "    **14/10/25 14:22:02 INFO MemoryStore: Block rdd_200_0 of size 9008840 dropped from memory (free 1755596828)**\n",
      "    **word2vecModel: org.apache.spark.mllib.feature.Word2VecModel = org.apache.spark.mllib.feature.Word2VecModel@2b94e480**\n",
      "\n",
      "Once trained, we can easily find the top 20 synonyms for a given term (that is, the most similar term to the input term, computed by cosine similarity between the word vectors). For example, to find the 20 most similar terms to _hockey_ , use the following lines of code:\n",
      "\n",
      "    word2vecModel.findSynonyms(\"hockey\", 20).foreach(println)\n",
      "\n",
      "As we can see from the following output, most of the terms relate to hockey or other sports topics:\n",
      "\n",
      "    **(sport,0.6828256249427795)**\n",
      "    **(ecac,0.6718048453330994)**\n",
      "    **(hispanic,0.6519884467124939)**\n",
      "    **(glens,0.6447514891624451)**\n",
      "    **(woofers,0.6351765394210815)**\n",
      "    **(boxscores,0.6009076237678528)**\n",
      "    **(tournament,0.6006366014480591)**\n",
      "    **(champs,0.5957855582237244)**\n",
      "    **(aargh,0.584071934223175)**\n",
      "    **(playoff,0.5834275484085083)**\n",
      "    **(ahl,0.5784651637077332)**\n",
      "    **(ncaa,0.5680188536643982)**\n",
      "    **(pool,0.5612311959266663)**\n",
      "    **(olympic,0.5552600026130676)**\n",
      "    **(champion,0.5549421310424805)**\n",
      "    **(filinuk,0.5528956651687622)**\n",
      "    **(yankees,0.5502706170082092)**\n",
      "    **(motorcycles,0.5484763979911804)**\n",
      "    **(calder,0.5481109023094177)**\n",
      "    **(rec,0.5432182550430298)**\n",
      "\n",
      "As another example, we can find 20 synonyms for the term _legislation_ as follows:\n",
      "\n",
      "    word2vecModel.findSynonyms(\"legislation\", 20).foreach(println)\n",
      "\n",
      "In this case, we observe the terms related to _regulation_ , _politics_ , and _business_ feature prominently:\n",
      "\n",
      "    **(accommodates,0.8149217963218689)**\n",
      "    **(briefed,0.7582570314407349)**\n",
      "    **(amended,0.7310371994972229)**\n",
      "    **(telephony,0.7139414548873901)**\n",
      "    **(aclu,0.7080780863761902)**\n",
      "    **(pitted,0.7062571048736572)**\n",
      "    **(licensee,0.6981208324432373)**\n",
      "    **(agency,0.6880651712417603)**\n",
      "    **(policies,0.6828961372375488)**\n",
      "    **(senate,0.6821110844612122)**\n",
      "    **(businesses,0.6814320087432861)**\n",
      "    **(permit,0.6797110438346863)**\n",
      "    **(cpsr,0.6764014959335327)**\n",
      "    **(cooperation,0.6733141541481018)**\n",
      "    **(surveillance,0.6670728325843811)**\n",
      "    **(restricted,0.6666574478149414)**\n",
      "    **(congress,0.6661365628242493)**\n",
      "    **(procure,0.6655452251434326)**\n",
      "    **(industry,0.6650314927101135)**\n",
      "    **(inquiry,0.6644254922866821)**\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we took a deeper look into more complex text processing and explored MLlib's text feature extraction capabilities, in particular the TF-IDF term weighting schemes. We covered examples of using the resulting TF-IDF feature vectors to compute document similarity and train a newsgroup topic classification model. Finally, you learned how to use MLlib's cutting-edge Word2Vec model to compute a vector representation of words in a corpus of text and use the trained model to find words with contextual meaning that is similar to a given word.\n",
      "\n",
      "In the next chapter, we will take a look at online learning, and you will learn how Spark Streaming relates to online learning models.\n",
      "\n",
      "# Chapter 10. Real-time Machine Learning with Spark Streaming\n",
      "\n",
      "So far in this book, we have focused on **batch** data processing. That is, all our analysis, feature extraction, and model training has been applied to a fixed set of data that does not change. This fits neatly into Spark's core abstraction of RDDs, which are immutable distributed datasets. Once created, the data underlying the RDD does not change, although we might create new RDDs from the original RDD through Spark's transformation and action operators.\n",
      "\n",
      "Our attention has also been on batch machine learning models where we train a model on a fixed batch of training data that is usually represented as an RDD of feature vectors (and labels, in the case of supervised learning models).\n",
      "\n",
      "In this chapter, we will:\n",
      "\n",
      "  * Introduce the concept of online learning, where models are trained and updated on new data as it becomes available\n",
      "  * Explore stream processing using Spark Streaming\n",
      "  * See how Spark Streaming fits together with the online learning approach\n",
      "\n",
      "# Online learning\n",
      "\n",
      "The batch machine learning methods that we have applied in this book focus on processing an existing fixed set of training data. Typically, these techniques are also iterative, and we have performed multiple passes over our training data in order to converge to an optimal model.\n",
      "\n",
      "By contrast, online learning is based on performing only one sequential pass through the training data in a fully incremental fashion (that is, one training example at a time). After seeing each training example, the model makes a prediction for this example and then receives the true outcome (for example, the label for classification or real target for regression). The idea behind online learning is that the model continually updates as new information is received instead of being retrained periodically in batch training.\n",
      "\n",
      "In some settings, when data volume is very large or the process that generates the data is changing rapidly, online learning methods can adapt more quickly and in near real time, without needing to be retrained in an expensive batch process.\n",
      "\n",
      "However, online learning methods do not have to be used in a purely online manner. In fact, we have already seen an example of using an online learning model in the batch setting when we used **stochastic gradient descent** optimization to train our classification and regression models. SGD updates the model after each training example. However, we still made use of multiple passes over the training data in order to converge to a better result.\n",
      "\n",
      "In the pure online setting, we do not (or perhaps cannot) make multiple passes over the training data; hence, we need to process each input as it arrives. Online methods also include mini-batch methods where, instead of processing one input at a time, we process a small batch of training data.\n",
      "\n",
      "Online and batch methods can also be combined in real-world situations. For example, we can periodically retrain our models offline (say, every day) using batch methods. We can then deploy the trained model to production and update it using online methods in real time (that is, during the day, in between batch retraining) to adapt to any changes in the environment.\n",
      "\n",
      "As we will see in this chapter, the online learning setting can fit neatly into stream processing and the Spark Streaming framework.\n",
      "\n",
      "### Note\n",
      "\n",
      "See <http://en.wikipedia.org/wiki/Online_machine_learning> for more details on online machine learning.\n",
      "\n",
      "# Stream processing\n",
      "\n",
      "Before covering online learning with Spark, we will first explore the basics of stream processing and introduce the Spark Streaming library.\n",
      "\n",
      "In addition to the core Spark API and functionality, the Spark project contains another major library (in the same way as MLlib is a major project library) called **Spark Streaming** , which focuses on processing data streams in real time.\n",
      "\n",
      "A data stream is a continuous sequence of records. Common examples include activity stream data from a web or mobile application, time-stamped log data, transactional data, and event streams from sensor or device networks.\n",
      "\n",
      "The batch processing approach typically involves saving the data stream to an intermediate storage system (for example, HDFS or a database) and running a batch process on the saved data. In order to generate up-to-date results, the batch process must be run periodically (for example, daily, hourly, or even every few minutes) on the latest data available.\n",
      "\n",
      "By contrast, the stream-based approach applies processing to the data stream as it is generated. This allows near real-time processing (of the order of a subsecond to a few tenths of a second time frames rather than minutes, hours, days, or even weeks with typical batch processing).\n",
      "\n",
      "## An introduction to Spark Streaming\n",
      "\n",
      "There are a few different general techniques to deal with stream processing. Two of the most common ones are as follows:\n",
      "\n",
      "  * Treat each record individually and process it as soon as it is seen.\n",
      "  * Combine multiple records into **mini-batches**. These mini-batches can be delineated either by time or by the number of records in a batch.\n",
      "\n",
      "Spark Streaming takes the second approach. The core primitive in Spark Streaming is the **discretized stream** , or **DStream**. A DStream is a sequence of mini-batches, where each mini-batch is represented as a Spark RDD:\n",
      "\n",
      "The discretized stream abstraction\n",
      "\n",
      "A DStream is defined by its input source and a time window called the **batch interval**. The stream is broken up into time periods equal to the batch interval (beginning from the starting time of the application). Each RDD in the stream will contain the records that are received by the Spark Streaming application during a given batch interval. If no data is present in a given interval, the RDD will simply be empty.\n",
      "\n",
      "### Input sources\n",
      "\n",
      "Spark Streaming **receivers** are responsible for receiving data from an **input source** and converting the raw data into a DStream made up of Spark RDDs.\n",
      "\n",
      "Spark Streaming supports various input sources, including file-based sources (where the receiver watches for new files arriving at the input location and creates the DStream from the contents read from each new file) and network-based sources (such as receivers that communicate with socket-based sources, the Twitter API stream, Akka actors, or message queues and distributed stream and log transfer frameworks, such Flume, Kafka, and Amazon Kinesis).\n",
      "\n",
      "### Note\n",
      "\n",
      "See the documentation on input sources at <http://spark.apache.org/docs/latest/streaming-programming-guide.html#input-dstreams> for more details and for links to various advanced sources.\n",
      "\n",
      "### Transformations\n",
      "\n",
      "As we saw in Chapter 1, _Getting Up and Running with Spark_ , and throughout this book, Spark allows us to apply powerful transformations to RDDs. As DStreams are made up of RDDs, Spark Streaming provides a set of transformations available on DStreams; these transformations are similar to those available on RDDs. These include `map`, `flatMap`, `filter`, `join`, and `reduceByKey`.\n",
      "\n",
      "Spark Streaming transformations, such as those applicable to RDDs, operate on each element of a DStream's underlying data. That is, the transformations are effectively applied to each RDD in the DStream, which, in turn, applies the transformation to the elements of the RDD.\n",
      "\n",
      "Spark Streaming also provides operators such as `reduce` and `count`. These operators return a DStream made up of a single element (for example, the count value for each batch). Unlike the equivalent operators on RDDs, these do not trigger computation on DStreams directly. That is, they are not **actions** , but they are still transformations, as they return another DStream.\n",
      "\n",
      "#### Keeping track of state\n",
      "\n",
      "When we were dealing with batch processing of RDDs, keeping and updating a state variable was relatively straightforward. We could start with a certain state (for example, a count or sum of values) and then use broadcast variables or accumulators to update this state in parallel. Usually, we would then use an RDD action to collect the updated state to the driver and, in turn, update the global state.\n",
      "\n",
      "With DStreams, this is a little more complex, as we need to keep track of states across batches in a fault-tolerant manner. Conveniently, Spark Streaming provides the `updateStateByKey` function on a DStream of key-value pairs, which takes care of this for us, allowing us to create a stream of arbitrary state information and update it with each batch of data seen. For example, the state could be a global count of the number of times each key has been seen. The state could, thus, represent the number of visits per web page, clicks per advert, tweets per user, or purchases per product, for example.\n",
      "\n",
      "#### General transformations\n",
      "\n",
      "The Spark Streaming API also exposes a general `transform` function that gives us access to the underlying RDD for each batch in the stream. That is, where the higher level functions such as `map` transform a DStream to another DStream, `transform` allows us to apply functions from an RDD to another RDD. For example, we can use the RDD `join` operator to join each batch of the stream to an existing RDD that we computed separately from our streaming application (perhaps, in Spark or some other system).\n",
      "\n",
      "### Note\n",
      "\n",
      "The full list of transformations and further information on each of them is provided in the Spark documentation at <http://spark.apache.org/docs/latest/streaming-programming-guide.html#transformations-on-dstreams>.\n",
      "\n",
      "### Actions\n",
      "\n",
      "While some of the operators we have seen in Spark Streaming, such as `count`, are not actions as in the batch RDD case, Spark Streaming has the concept of **actions** on DStreams. Actions are **output** operators that, when invoked, trigger computation on the DStream. They are as follows:\n",
      "\n",
      "  * `print`: This prints the first 10 elements of each batch to the console and is typically used for debugging and testing.\n",
      "  * `saveAsObjectFile`, `saveAsTextFiles`, and `saveAsHadoopFiles`: These functions output each batch to a Hadoop-compatible filesystem with a filename (if applicable) derived from the batch start timestamp.\n",
      "  * `forEachRDD`: This operator is the most generic and allows us to apply any arbitrary processing to the RDDs within each batch of a DStream. It is used to apply _side effects_ , such as saving data to an external system, printing it for testing, exporting it to a dashboard, and so on.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that like batch processing with Spark, DStream operators are **lazy**. In the same way in which we need to call an action, such as `count`, on an RDD to ensure that processing takes place, we need to call one of the preceding action operators in order to trigger computation on a DStream. Otherwise, our streaming application will not actually perform any computation.\n",
      "\n",
      "### Window operators\n",
      "\n",
      "As Spark Streaming operates on time-ordered batched streams of data, it introduces a new concept, which is that of **windowing**. A `window` function computes a transformation over a sliding window applied to the stream.\n",
      "\n",
      "A window is defined by the length of the window and the sliding interval. For example, with a 10-second window and a 5-second sliding interval, we will compute results every 5 seconds, based on the latest 10 seconds of data in the DStream. For example, we might wish to calculate the top websites by page view numbers over the last 10 seconds and recompute this metric every 5 seconds using a sliding window.\n",
      "\n",
      "The following figure illustrates a windowed DStream:\n",
      "\n",
      "A windowed DStream\n",
      "\n",
      "## Caching and fault tolerance with Spark Streaming\n",
      "\n",
      "Like Spark RDDs, DStreams can be cached in memory. The use cases for caching are similar to those for RDDs--if we expect to access the data in a DStream multiple times (perhaps performing multiple types of analysis or aggregation or outputting to multiple external systems), we will benefit from caching the data. Stateful operators, which include `window` functions and `updateStateByKey`, do this automatically for efficiency.\n",
      "\n",
      "Recall that RDDs are immutable datasets and are defined by their input data source and **lineage** --that is, the set of transformations and actions that are applied to the RDD. Fault tolerance in RDDs works by recreating the RDD (or partition of an RDD) that is lost due to the failure of a worker node.\n",
      "\n",
      "As DStreams are themselves batches of RDDs, they can also be recomputed as required to deal with worker node failure. However, this depends on the input data still being available. If the data source itself is fault-tolerant and persistent (such as HDFS or some other fault-tolerant data store), then the DStream can be recomputed.\n",
      "\n",
      "If data stream sources are delivered over a network (which is a common case with stream processing), Spark Streaming's default persistence behavior is to replicate data to two worker nodes. This allows network DStreams to be recomputed in the case of failure. Note, however, that any data received by a node but _not yet replicated_ might be lost when a node fails.\n",
      "\n",
      "Spark Streaming also supports recovery of the driver node in the event of failure. However, currently, for network-based sources, data in the memory of worker nodes will be lost in this case. Hence, Spark Streaming is not fully fault-tolerant in the face of failure of the driver node or application.\n",
      "\n",
      "### Note\n",
      "\n",
      "See http://spark.apache.org/docs/latest/streaming-programming-guide.html#caching--persistence and <http://spark.apache.org/docs/latest/streaming-programming-guide.html#fault-tolerance-properties> for more details.\n",
      "\n",
      "# Creating a Spark Streaming application\n",
      "\n",
      "We will now work through creating our first Spark Streaming application to illustrate some of the basic concepts around Spark Streaming that we introduced earlier.\n",
      "\n",
      "We will expand on the example applications used in Chapter 1, _Getting Up and Running with Spark_ , where we used a small example dataset of product purchase events. For this example, instead of using a static set of data, we will create a simple producer application that will randomly generate events and send them over a network connection. We will then create a few Spark Streaming consumer applications that will process this event stream.\n",
      "\n",
      "The sample project for this chapter contains the code you will need. It is called `scala-spark-streaming-app`. It consists of a Scala SBT project definition file, the example application source code, and a `\\src\\main\\resources` directory that contains a file called `names.csv`.\n",
      "\n",
      "The `build.sbt` file for the project contains the following project definition:\n",
      "\n",
      "    name := \"scala-spark-streaming-app\"\n",
      "\n",
      "    version := \"1.0\"\n",
      "\n",
      "    scalaVersion := \"2.10.4\"\n",
      "\n",
      "    libraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"1.1.0\"\n",
      "\n",
      "    libraryDependencies += \"org.apache.spark\" %% \"spark-streaming\" % \"1.1.0\"\n",
      "\n",
      "Note that we added a dependency on Spark MLlib and Spark Streaming, which includes the dependency on the Spark core.\n",
      "\n",
      "The `names.csv` file contains a set of 20 randomly generated user names. We will use these names as part of our data generation function in our producer application:\n",
      "\n",
      "    **Miguel,Eric,James,Juan,Shawn,James,Doug,Gary,Frank,Janet,Michael,James,Malinda,Mike,Elaine,Kevin,Janet,Richard,Saul,Manuela**\n",
      "\n",
      "## The producer application\n",
      "\n",
      "Our producer needs to create a network connection and generate some random purchase event data to send over this connection. First, we will define our object and main method definition. We will then read the random names from the `names.csv` resource and create a set of products with prices, from which we will generate our random product events:\n",
      "\n",
      "    /**\n",
      "     * A producer application that generates random \"product events\", up to 5 per second, and sends them over a\n",
      "     * network connection\n",
      "     */\n",
      "    object StreamingProducer {\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val random = new Random()\n",
      "\n",
      "        // Maximum number of events per second\n",
      "        val MaxEvents = 6\n",
      "\n",
      "        // Read the list of possible names\n",
      "        val namesResource = this.getClass.getResourceAsStream(\"/names.csv\")\n",
      "        val names = scala.io.Source.fromInputStream(namesResource)\n",
      "          .getLines()\n",
      "          .toList\n",
      "          .head\n",
      "          .split(\",\")\n",
      "          .toSeq\n",
      "\n",
      "        // Generate a sequence of possible products\n",
      "        val products = Seq(\n",
      "          \"iPhone Cover\" -> 9.99,\n",
      "          \"Headphones\" -> 5.49,\n",
      "          \"Samsung Galaxy Cover\" -> 8.95,\n",
      "          \"iPad Cover\" -> 7.49\n",
      "        )\n",
      "\n",
      "Using the list of names and map of product name to price, we will create a function that will randomly pick a product and name from these sources, generating a specified number of product events:\n",
      "\n",
      "        /** Generate a number of random product events */\n",
      "        def generateProductEvents(n: Int) = {\n",
      "          (1 to n).map { i =>\n",
      "            val (product, price) = products(random.nextInt(products.size))\n",
      "            val user = random.shuffle(names).head\n",
      "            (user, product, price)\n",
      "          }\n",
      "        }\n",
      "\n",
      "Finally, we will create a network socket and set our producer to listen on this socket. As soon as a connection is made (which will come from our consumer streaming application), the producer will start generating random events at a random rate between 0 and 5 per second:\n",
      "\n",
      "        // create a network producer\n",
      "        val listener = new ServerSocket(9999)\n",
      "        println(\"Listening on port: 9999\")\n",
      "\n",
      "        while (true) {\n",
      "          val socket = listener.accept()\n",
      "          new Thread() {\n",
      "            override def run = {\n",
      "              println(\"Got client connected from: \" + socket.getInetAddress)\n",
      "              val out = new PrintWriter(socket.getOutputStream(), true)\n",
      "\n",
      "              while (true) {\n",
      "                Thread.sleep(1000)\n",
      "                val num = random.nextInt(MaxEvents)\n",
      "                val productEvents = generateProductEvents(num)\n",
      "                productEvents.foreach{ event =>\n",
      "                  out.write(event.productIterator.mkString(\",\"))\n",
      "                  out.write(\"\\n\")\n",
      "                }\n",
      "                out.flush()\n",
      "                println(s\"Created $num events...\")\n",
      "              }\n",
      "              socket.close()\n",
      "            }\n",
      "          }.start()\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "### Note\n",
      "\n",
      "This producer example is based on the `PageViewGenerator` example in the Spark Streaming examples.\n",
      "\n",
      "The producer can be run by changing into the base directory of `scala-spark-streaming-app` and using SBT to run the application, as we did in Chapter 1, _Getting Up and Running with Spark_ :\n",
      "\n",
      "    **> cd scala-spark-streaming-app**\n",
      "    **> sbt**\n",
      "    **[info] ...**\n",
      "    **>**\n",
      "\n",
      "Use the `run` command to execute the application:\n",
      "\n",
      "    **> run**\n",
      "\n",
      "You should see output similar to the following:\n",
      "\n",
      "    **...**\n",
      "    **Multiple main classes detected, select one to run:**\n",
      "\n",
      "    **[1] StreamingProducer**\n",
      "    **[2] SimpleStreamingApp**\n",
      "    **[3] StreamingAnalyticsApp**\n",
      "    **[4] StreamingStateApp**\n",
      "    **[5] StreamingModelProducer**\n",
      "    **[6] SimpleStreamingModel**\n",
      "    **[7] MonitoringStreamingModel**\n",
      "\n",
      "    **Enter number:**\n",
      "\n",
      "Select the `StreamingProducer` option. The application will start running, and you should see the following output:\n",
      "\n",
      "    **[info] Running StreamingProducer**\n",
      "    **Listening on port: 9999**\n",
      "\n",
      "We can see that the producer is listening on port `9999`, waiting for our consumer application to connect.\n",
      "\n",
      "## Creating a basic streaming application\n",
      "\n",
      "Next, we will create our first streaming program. We will simply connect to the producer and print out the contents of each batch. Our streaming code looks like this:\n",
      "\n",
      "    /**\n",
      "     * A simple Spark Streaming app in Scala\n",
      "     */\n",
      "    object SimpleStreamingApp {\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n",
      "        val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
      "\n",
      "        // here we simply print out the first few elements of each\n",
      "        // batch\n",
      "        stream.print()\n",
      "        ssc.start()\n",
      "        ssc.awaitTermination()\n",
      "\n",
      "      }\n",
      "    }\n",
      "\n",
      "It looks fairly simple, and it is mostly due to the fact that Spark Streaming takes care of all the complexity for us. First, we initialized a `StreamingContext` (which is the streaming equivalent of the `SparkContext` we have used so far), specifying similar configuration options that are used to create a `SparkContext`. Notice, however, that here we are required to provide the batch interval, which we set to 10 seconds.\n",
      "\n",
      "We then created our data stream using a predefined streaming source, `socketTextStream`, which reads text from a socket host and port and creates a `DStream[String]`. We then called the `print` function on the DStream; this function prints out the first few elements of each batch.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Calling `print` on a DStream is similar to calling `take` on an RDD. It displays only the first few elements.\n",
      "\n",
      "We can run this program using SBT. Open a second terminal window, leaving the producer program running, and run `sbt`:\n",
      "\n",
      "    **> sbt**\n",
      "    **[info] ...**\n",
      "    **> run**\n",
      "    **....**\n",
      "\n",
      "Again, you should see a few options to select:\n",
      "\n",
      "    **Multiple main classes detected, select one to run:**\n",
      "\n",
      "    **[1] StreamingProducer**\n",
      "    **[2] SimpleStreamingApp**\n",
      "    **[3] StreamingAnalyticsApp**\n",
      "    **[4] StreamingStateApp**\n",
      "    **[5] StreamingModelProducer**\n",
      "    **[6] SimpleStreamingModel**\n",
      "    **[7] MonitoringStreamingModel**\n",
      "\n",
      "Run the `SimpleStreamingApp` main class. You should see the streaming program start up, displaying output similar to the one shown here:\n",
      "\n",
      "    **...**\n",
      "    **14/11/15 21:02:23 INFO scheduler.ReceiverTracker: ReceiverTracker started**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: metadataCleanupDelay = -1**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: metadataCleanupDelay = -1**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: Slide time = 10000 ms**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: Storage level = StorageLevel(false, false, false, false, 1)**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: Checkpoint interval = null**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: Remember duration = 10000 ms**\n",
      "    **14/11/15 21:02:23 INFO dstream.SocketInputDStream: Initialized and validated org.apache.spark.streaming.dstream.SocketInputDStream@ff3436d**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: Slide time = 10000 ms**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: Storage level = StorageLevel(false, false, false, false, 1)**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: Checkpoint interval = null**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: Remember duration = 10000 ms**\n",
      "    **14/11/15 21:02:23 INFO dstream.ForEachDStream: Initialized and validated org.apache.spark.streaming.dstream.ForEachDStream@5a10b6e8**\n",
      "    **14/11/15 21:02:23 INFO scheduler.ReceiverTracker: Starting 1 receivers**\n",
      "    **14/11/15 21:02:23 INFO spark.SparkContext: Starting job: runJob at ReceiverTracker.scala:275**\n",
      "    **...**\n",
      "\n",
      "At the same time, you should see that the terminal window running the producer displays something like the following:\n",
      "\n",
      "    **...**\n",
      "    **Got client connected from: /127.0.0.1**\n",
      "    **Created 2 events...**\n",
      "    **Created 2 events...**\n",
      "    **Created 3 events...**\n",
      "    **Created 1 events...**\n",
      "    **Created 5 events...**\n",
      "    **...**\n",
      "\n",
      "After about 10 seconds, which is the time of our streaming batch interval, Spark Streaming will trigger a computation on the stream due to our use of the `print` operator. This should display the first few events in the batch, which will look something like the following output:\n",
      "\n",
      "    **...**\n",
      "    **14/11/15 21:02:30 INFO spark.SparkContext: Job finished: take at DStream.scala:608, took 0.05596 s**\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416078150000 ms**\n",
      "    **-------------------------------------------**\n",
      "    **Michael,Headphones,5.49**\n",
      "    **Frank,Samsung Galaxy Cover,8.95**\n",
      "    **Eric,Headphones,5.49**\n",
      "    **Malinda,iPad Cover,7.49**\n",
      "    **James,iPhone Cover,9.99**\n",
      "    **James,Headphones,5.49**\n",
      "    **Doug,iPhone Cover,9.99**\n",
      "    **Juan,Headphones,5.49**\n",
      "    **James,iPhone Cover,9.99**\n",
      "    **Richard,iPad Cover,7.49**\n",
      "    **...**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that you might see different results, as the producer generates a random number of random events each second.\n",
      "\n",
      "You can terminate the streaming app by pressing _Ctrl_ \\+ _C_. If you want to, you can also terminate the producer (if you do, you will need to restart it again before starting the next streaming programs that we will create).\n",
      "\n",
      "## Streaming analytics\n",
      "\n",
      "Next, we will create a slightly more complex streaming program. In Chapter 1, _Getting Up and Running with Spark_ , we calculated a few metrics on our dataset of product purchases. These included the total number of purchases, the number of unique users, the total revenue, and the most popular product (together with its number of purchases and total revenue).\n",
      "\n",
      "In this example, we will compute the same metrics on our stream of purchase events. The key difference is that these metrics will be computed per batch and printed out.\n",
      "\n",
      "We will define our streaming application code here:\n",
      "\n",
      "    /**\n",
      "     * A more complex Streaming app, which computes statistics and prints the results for each batch in a DStream\n",
      "     */\n",
      "    object StreamingAnalyticsApp {\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n",
      "        val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
      "\n",
      "        // create stream of events from raw text elements\n",
      "        val events = stream.map { record =>\n",
      "          val event = record.split(\",\")\n",
      "          (event(0), event(1), event(2))\n",
      "        }\n",
      "\n",
      "First, we created exactly the same `StreamingContext` and socket stream as we did earlier. Our next step is to apply a `map` transformation to the raw text, where each record is a comma-separated string representing the purchase event. The `map` function splits the text and creates a tuple of `(user, product, price)`. This illustrates the use of `map` on a DStream and how it is the same as if we had been operating on an RDD.\n",
      "\n",
      "Next, we will use `foreachRDD` to apply arbitrary processing on each RDD in the stream to compute our desired metrics and print them to the console:\n",
      "\n",
      "        /*\n",
      "          We compute and print out stats for each batch.\n",
      "          Since each batch is an RDD, we call forEeachRDD on the DStream, and apply the usual RDD functions\n",
      "          we used in Chapter 1.\n",
      "         */\n",
      "        events.foreachRDD { (rdd, time) =>\n",
      "          val numPurchases = rdd.count()\n",
      "          val uniqueUsers = rdd.map { case (user, _, _) => user }.distinct().count()\n",
      "          val totalRevenue = rdd.map { case (_, _, price) => price.toDouble }.sum()\n",
      "          val productsByPopularity = rdd\n",
      "            .map { case (user, product, price) => (product, 1) }\n",
      "            .reduceByKey(_ + _)\n",
      "            .collect()\n",
      "            .sortBy(-_._2)\n",
      "          val mostPopular = productsByPopularity(0)\n",
      "\n",
      "          val formatter = new SimpleDateFormat\n",
      "          val dateStr = formatter.format(new Date(time.milliseconds))\n",
      "          println(s\"== Batch start time: $dateStr ==\")\n",
      "          println(\"Total purchases: \" + numPurchases)\n",
      "          println(\"Unique users: \" + uniqueUsers)\n",
      "          println(\"Total revenue: \" + totalRevenue)\n",
      "          println(\"Most popular product: %s with %d purchases\".format(mostPopular._1, mostPopular._2))\n",
      "        }\n",
      "\n",
      "        // start the context\n",
      "        ssc.start()\n",
      "        ssc.awaitTermination()\n",
      "\n",
      "      }\n",
      "\n",
      "    }\n",
      "\n",
      "If you compare the code operating on the RDDs inside the preceding `foreachRDD` block with that used in Chapter 1, _Getting Up and Running with Spark_ , you will notice that it is virtually the same code. This shows that we can apply any RDD-related processing we wish within the streaming setting by operating on the underlying RDDs, as well as using the built-in higher level streaming operations.\n",
      "\n",
      "Let's run the streaming program again by calling `sbt run` and selecting `StreamingAnalyticsApp`.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Remember that you might also need to restart the producer if you previously terminated the program. This should be done before starting the streaming application.\n",
      "\n",
      "After about 10 seconds, you should see output from the streaming program similar to the following:\n",
      "\n",
      "    **...**\n",
      "    **14/11/15 21:27:30 INFO spark.SparkContext: Job finished: collect at Streaming.scala:125, took 0.071145 s**\n",
      "    **== Batch start time: 2014/11/15 9:27 PM ==**\n",
      "    **Total purchases: 16**\n",
      "    **Unique users: 10**\n",
      "    **Total revenue: 123.72**\n",
      "    **Most popular product: iPad Cover with 6 purchases**\n",
      "    **...**\n",
      "\n",
      "You can again terminate the streaming program using _Ctrl_ \\+ _C_.\n",
      "\n",
      "## Stateful streaming\n",
      "\n",
      "As a final example, we will apply the concept of **stateful** streaming using the `updateStateByKey` function to compute a global state of revenue and number of purchases per user, which will be updated with new data from each 10-second batch. Our `StreamingStateApp` app is shown here:\n",
      "\n",
      "    object StreamingStateApp {\n",
      "      import org.apache.spark.streaming.StreamingContext._\n",
      "\n",
      "We will first define an `updateState` function that will compute the new state from the running state value and the new data in the current batch. Our state, in this case, is a tuple of `(number of products, revenue)` pairs, which we will keep for each user. We will compute the new state given the set of `(product, revenue)` pairs for the current batch and the accumulated state at the current time.\n",
      "\n",
      "Notice that we will deal with an `Option` value for the current state, as it might be empty (which will be the case for the first batch), and we need to define a default value, which we will do using `getOrElse` as shown here:\n",
      "\n",
      "      def updateState(prices: Seq[(String, Double)], currentTotal: Option[(Int, Double)]) = {\n",
      "        val currentRevenue = prices.map(_._2).sum\n",
      "        val currentNumberPurchases = prices.size\n",
      "        val state = currentTotal.getOrElse((0, 0.0))\n",
      "        Some((currentNumberPurchases + state._1, currentRevenue + state._2))\n",
      "      }\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n",
      "        // for stateful operations, we need to set a checkpoint\n",
      "        // location\n",
      "        ssc.checkpoint(\"/tmp/sparkstreaming/\")\n",
      "        val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
      "\n",
      "        // create stream of events from raw text elements\n",
      "        val events = stream.map { record =>\n",
      "          val event = record.split(\",\")\n",
      "          (event(0), event(1), event(2).toDouble)\n",
      "        }\n",
      "\n",
      "        val users = events.map{ case (user, product, price) => (user, (product, price)) }\n",
      "        val revenuePerUser = users.updateStateByKey(updateState)\n",
      "        revenuePerUser.print()\n",
      "\n",
      "        // start the context\n",
      "        ssc.start()\n",
      "        ssc.awaitTermination()\n",
      "\n",
      "      }\n",
      "    }\n",
      "\n",
      "After applying the same string split transformation we used in our previous example, we called `updateStateByKey` on our DStream, passing in our defined `updateState` function. We then printed the results to the console.\n",
      "\n",
      "Start the streaming example using `sbt run` and by selecting `[4] StreamingStateApp` (also restart the producer program if necessary).\n",
      "\n",
      "After around 10 seconds, you will start to see the first set of state output. We will wait another 10 seconds to see the next set of output. You will see the overall global state being updated:\n",
      "\n",
      "    **...**\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416080440000 ms**\n",
      "    **-------------------------------------------**\n",
      "    **(Janet,(2,10.98))**\n",
      "    **(Frank,(1,5.49))**\n",
      "    **(James,(2,12.98))**\n",
      "    **(Malinda,(1,9.99))**\n",
      "    **(Elaine,(3,29.97))**\n",
      "    **(Gary,(2,12.98))**\n",
      "    **(Miguel,(3,20.47))**\n",
      "    **(Saul,(1,5.49))**\n",
      "    **(Manuela,(2,18.939999999999998))**\n",
      "    **(Eric,(2,18.939999999999998))**\n",
      "    **...**\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416080441000 ms**\n",
      "    **-------------------------------------------**\n",
      "    **(Janet,(6,34.94))**\n",
      "    **(Juan,(4,33.92))**\n",
      "    **(Frank,(2,14.44))**\n",
      "    **(James,(7,48.93000000000001))**\n",
      "    **(Malinda,(1,9.99))**\n",
      "    **(Elaine,(7,61.89))**\n",
      "    **(Gary,(4,28.46))**\n",
      "    **(Michael,(1,8.95))**\n",
      "    **(Richard,(2,16.439999999999998))**\n",
      "    **(Miguel,(5,35.95))**\n",
      "    **...**\n",
      "\n",
      "We can see that the number of purchases and revenue totals for each user are added to with each batch of data.\n",
      "\n",
      "### Tip\n",
      "\n",
      "Now, see if you can adapt this example to use Spark Streaming's `window` functions. For example, you can compute similar statistics per user over the past minute, sliding every 30 seconds.\n",
      "\n",
      "# Online learning with Spark Streaming\n",
      "\n",
      "As we have seen, Spark Streaming makes it easy to work with data streams in a way that should be familiar to us from working with RDDs. Using Spark's stream processing primitives combined with the online learning capabilities of MLlib's SGD-based methods, we can create real-time machine learning models that we can update on new data in the stream as it arrives.\n",
      "\n",
      "## Streaming regression\n",
      "\n",
      "Spark provides a built-in streaming machine learning model in the `StreamingLinearAlgorithm` class. Currently, only a linear regression implementation is available--`StreamingLinearRegressionWithSGD`--but future versions will include classification.\n",
      "\n",
      "The streaming regression model provides two methods for usage:\n",
      "\n",
      "  * `trainOn`: This takes `DStream[LabeledPoint]` as its argument. This tells the model to train on every batch in the input DStream. It can be called multiple times to train on different streams.\n",
      "  * `predictOn`: This also takes `DStream[LabeledPoint]`. This tells the model to make predictions on the input DStream, returning a new `DStream[Double]` that contains the model predictions.\n",
      "\n",
      "Under the hood, the streaming regression model uses `foreachRDD` and `map` to accomplish this. It also updates the model variable after each batch and exposes the latest trained model, which allows us to use this model in other applications or save it to an external location.\n",
      "\n",
      "The streaming regression model can be configured with parameters for step size and number of iterations in the same way as standard batch regression--the model class used is the same. We can also set the initial model weight vector.\n",
      "\n",
      "When we first start training a model, we can set the initial weights to a zero vector, or a random vector, or perhaps load the latest model from the result of an offline batch process. We can also decide to save the model periodically to an external system and use the latest model state as the starting point (for example, in the case of a restart after a node or application failure).\n",
      "\n",
      "## A simple streaming regression program\n",
      "\n",
      "To illustrate the use of streaming regression, we will create a simple example similar to the preceding one, which uses simulated data. We will write a producer program that generates random feature vectors and target variables, given a fixed, known weight vector, and writes each training example to a network stream.\n",
      "\n",
      "Our consumer application will run a streaming regression model, training and then testing on our simulated data stream. Our first example consumer will simply print its predictions to the console.\n",
      "\n",
      "### Creating a streaming data producer\n",
      "\n",
      "The data producer operates in a manner similar to our product event producer example. Recall from Chapter 5, _Building a Classification Model with Spark_ , that a linear model is a linear combination (or vector dot product) of a weight vector, _w_ , and a feature vector, _x_ (that is, _wTx_ ). Our producer will generate synthetic data using a fixed, known weight vector and randomly generated feature vectors. This data fits the linear model formulation exactly, so we will expect our regression model to learn the true weight vector fairly easily.\n",
      "\n",
      "First, we will set up a maximum number of events per second (say, 100) and the number of features in our feature vector (also 100 in this example):\n",
      "\n",
      "    /**\n",
      "     * A producer application that generates random linear regression data.\n",
      "     */\n",
      "    object StreamingModelProducer {\n",
      "      import breeze.linalg._\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        // Maximum number of events per second\n",
      "        val MaxEvents = 100\n",
      "        val NumFeatures = 100\n",
      "\n",
      "        val random = new Random()\n",
      "\n",
      "The `generateRandomArray` function creates an array of the specified size where the entries are randomly generated from a normal distribution. We will use this function initially to generate our known weight vector, `w`, which will be fixed throughout the life of the producer. We will also create a random `intercept` value that will also be fixed. The weight vector and `intercept` will be used to generate each data point in our stream:\n",
      "\n",
      "        /** Function to generate a normally distributed dense vector */\n",
      "        def generateRandomArray(n: Int) = Array.tabulate(n)(_ => random.nextGaussian())\n",
      "\n",
      "        // Generate a fixed random model weight vector\n",
      "        val w = new DenseVector(generateRandomArray(NumFeatures))\n",
      "        val intercept = random.nextGaussian() * 10\n",
      "\n",
      "We will also need a function to generate a specified number of random data points. Each event is made up of a random feature vector and the target that we get from computing the dot product of our known weight vector with the random feature vector and adding the `intercept` value:\n",
      "\n",
      "        /** Generate a number of random data events*/\n",
      "        def generateNoisyData(n: Int) = {\n",
      "          (1 to n).map { i =>\n",
      "            val x = new DenseVector(generateRandomArray(NumFeatures))\n",
      "            val y: Double = w.dot(x)\n",
      "            val noisy = y + intercept\n",
      "            (noisy, x)\n",
      "          }\n",
      "        }\n",
      "\n",
      "Finally, we will use code similar to our previous producer to instantiate a network connection and send a random number of data points (between 0 and 100) in text format over the network each second:\n",
      "\n",
      "        // create a network producer\n",
      "        val listener = new ServerSocket(9999)\n",
      "        println(\"Listening on port: 9999\")\n",
      "\n",
      "        while (true) {\n",
      "          val socket = listener.accept()\n",
      "          new Thread() {\n",
      "            override def run = {\n",
      "              println(\"Got client connected from: \" + socket.getInetAddress)\n",
      "              val out = new PrintWriter(socket.getOutputStream(), true)\n",
      "\n",
      "              while (true) {\n",
      "                Thread.sleep(1000)\n",
      "                val num = random.nextInt(MaxEvents)\n",
      "                val data = generateNoisyData(num)\n",
      "                data.foreach { case (y, x) =>\n",
      "                  val xStr = x.data.mkString(\",\")\n",
      "                  val eventStr = s\"$y\\t$xStr\"\n",
      "                  out.write(eventStr)\n",
      "                  out.write(\"\\n\")\n",
      "                }\n",
      "                out.flush()\n",
      "                println(s\"Created $num events...\")\n",
      "              }\n",
      "              socket.close()\n",
      "            }\n",
      "          }.start()\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "\n",
      "You can start the producer using `sbt run`, followed by choosing to execute the `StreamingModelProducer` main method. This should result in the following output, thus indicating that the producer program is waiting for connections from our streaming regression application:\n",
      "\n",
      "    **[info] Running StreamingModelProducer**\n",
      "    **Listening on port: 9999**\n",
      "\n",
      "### Creating a streaming regression model\n",
      "\n",
      "In the next step in our example, we will create a streaming regression program. The basic layout and setup is the same as our previous streaming analytics examples:\n",
      "\n",
      "    /**\n",
      "     * A simple streaming linear regression that prints out predicted value for each batch\n",
      "     */\n",
      "    object SimpleStreamingModel {\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n",
      "        val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
      "\n",
      "Here, we will set up the number of features to match the records in our input data stream. We will then create a zero vector to use as the initial weight vector of our streaming regression model. Finally, we will select the number of iterations and step size:\n",
      "\n",
      "    val NumFeatures = 100\n",
      "        val zeroVector = DenseVector.zeros[Double](NumFeatures)\n",
      "        val model = new StreamingLinearRegressionWithSGD()\n",
      "          .setInitialWeights(Vectors.dense(zeroVector.data))\n",
      "          .setNumIterations(1)\n",
      "          .setStepSize(0.01)\n",
      "\n",
      "Next, we will again use the `map` function to transform the input DStream, where each record is a string representation of our input data, into a `LabeledPoint` instance that contains the target value and feature vector:\n",
      "\n",
      "        // create a stream of labeled points\n",
      "        val labeledStream = stream.map { event =>\n",
      "          val split = event.split(\"\\t\")\n",
      "          val y = split(0).toDouble\n",
      "          val features = split(1).split(\",\").map(_.toDouble)\n",
      "          LabeledPoint(label = y, features = Vectors.dense(features))\n",
      "        }\n",
      "\n",
      "The final step is to tell the model to train and test on our transformed DStream and also to print out the first few elements of each batch in the DStream of predicted values:\n",
      "\n",
      "        // train and test model on the stream, and print predictions\n",
      "        // for illustrative purposes\n",
      "        model.trainOn(labeledStream)\n",
      "        model.predictOn(labeledStream).print()\n",
      "\n",
      "        ssc.start()\n",
      "        ssc.awaitTermination()\n",
      "\n",
      "      }\n",
      "    }\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note that because we are using the same MLlib model classes for streaming as we did for batch processing, we can, if we choose, perform multiple iterations over the training data in each batch (which is just an RDD of `LabeledPoint` instances).\n",
      "\n",
      "Here, we will set the number of iterations to `1` to simulate purely online learning. In practice, you can set the number of iterations higher, but note that the training time per batch will go up. If the training time per batch is much higher than the batch interval, the streaming model will start to lag behind the velocity of the data stream.\n",
      "\n",
      "This can be handled by decreasing the number of iterations, increasing the batch interval, or increasing the parallelism of our streaming program by adding more Spark workers.\n",
      "\n",
      "Now, we're ready to run `SimpleStreamingModel` in our second terminal window using `sbt run` in the same way as we did for the producer (remember to select the correct main method for SBT to execute). Once the streaming program starts running, you should see the following output in the producer console:\n",
      "\n",
      "    **Got client connected from: /127.0.0.1**\n",
      "    **...**\n",
      "    **Created 10 events...**\n",
      "    **Created 83 events...**\n",
      "    **Created 75 events...**\n",
      "    **...**\n",
      "\n",
      "After about 10 seconds, you should start seeing the model predictions being printed to the streaming application console, similar to those shown here:\n",
      "\n",
      "    **14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Model updated at time 1416142440000 ms**\n",
      "    **14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Current model: weights, [0.05160959387864821,0.05122747155689144,-0.17224086785756998,0.05822993392274008,0.07848094246845688,-0.1298315806501979,0.006059323642394124, ...**\n",
      "    **...**\n",
      "    **14/11/16 14:54:00 INFO JobScheduler: Finished job streaming job 1416142440000 ms.0 from job set of time 1416142440000 ms**\n",
      "    **14/11/16 14:54:00 INFO JobScheduler: Starting job streaming job 1416142440000 ms.1 from job set of time 1416142440000 ms**\n",
      "    **14/11/16 14:54:00 INFO SparkContext: Starting job: take at DStream.scala:608**\n",
      "    **14/11/16 14:54:00 INFO DAGScheduler: Got job 3 (take at DStream.scala:608) with 1 output partitions (allowLocal=true)**\n",
      "    **14/11/16 14:54:00 INFO DAGScheduler: Final stage: Stage 3(take at DStream.scala:608)**\n",
      "    **14/11/16 14:54:00 INFO DAGScheduler: Parents of final stage: List()**\n",
      "    **14/11/16 14:54:00 INFO DAGScheduler: Missing parents: List()**\n",
      "    **14/11/16 14:54:00 INFO DAGScheduler: Computing the requested partition locally**\n",
      "    **14/11/16 14:54:00 INFO SparkContext: Job finished: take at DStream.scala:608, took 0.014064 s**\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416142440000 ms**\n",
      "    **-------------------------------------------**\n",
      "    **-2.0851430248312526**\n",
      "    **4.609405228401022**\n",
      "    **2.817934589675725**\n",
      "    **3.3526557917118813**\n",
      "    **4.624236379848475**\n",
      "    **-2.3509098272485156**\n",
      "    **-0.7228551577759544**\n",
      "    **2.914231548990703**\n",
      "    **0.896926579927631**\n",
      "    **1.1968162940541283**\n",
      "    **...**\n",
      "\n",
      "Congratulations! You've created your first streaming online learning model!\n",
      "\n",
      "You can shut down the streaming application (and, optionally, the producer) by pressing _Ctrl_ \\+ _C_ in each terminal window.\n",
      "\n",
      "## Streaming K-means\n",
      "\n",
      "MLlib also includes a streaming version of K-means clustering; this is called `StreamingKMeans`. This model is an extension of the mini-batch K-means algorithm where the model is updated with each batch based on a combination between the cluster centers computed from the previous batches and the cluster centers computed for the current batch.\n",
      "\n",
      "`StreamingKMeans` supports a _forgetfulness_ parameter _alpha_ (set using the `setDecayFactor` method); this controls how aggressive the model is in giving weight to newer data. An alpha value of 0 means the model will only use new data, while with an alpha value of `1`, all data since the beginning of the streaming application will be used.\n",
      "\n",
      "We will not cover streaming K-means further here (the Spark documentation at <http://spark.apache.org/docs/latest/mllib-clustering.html#streaming-clustering> contains further detail and an example). However, perhaps you could try to adapt the preceding streaming regression data producer to generate input data for a `StreamingKMeans` model. You could also adapt the streaming regression application to use `StreamingKMeans`.\n",
      "\n",
      "You can create the clustering data producer by first selecting a number of clusters, _K_ , and then generating each data point by:\n",
      "\n",
      "  * Randomly selecting a cluster index.\n",
      "  * Generating a random vector using specific normal distribution parameters for each cluster. That is, each of the _K_ clusters will have a mean and variance parameter, from which the random vectors will be generated using an approach similar to our preceding `generateRandomArray` function.\n",
      "\n",
      "In this way, each data point that belongs to the same cluster will be drawn from the same distribution, so our streaming clustering model should be able to learn the correct cluster centers over time.\n",
      "\n",
      "# Online model evaluation\n",
      "\n",
      "Combining machine learning with Spark Streaming has many potential applications and use cases, including keeping a model or set of models up to date on new training data as it arrives, thus enabling them to adapt quickly to changing situations or contexts.\n",
      "\n",
      "Another useful application is to track and compare the performance of multiple models in an online manner and, possibly, also perform model selection in real time so that the best performing model is always used to generate predictions on live data.\n",
      "\n",
      "This can be used to do real-time \"A/B testing\" of models, or combined with more advanced online selection and learning techniques, such as Bayesian update approaches and bandit algorithms. It can also be used simply to monitor model performance in real time, thus being able to respond or adapt if performance degrades for some reason.\n",
      "\n",
      "In this section, we will walk through a simple extension to our streaming regression example. In this example, we will compare the evolving error rate of two models with different parameters as they see more and more data in our input stream.\n",
      "\n",
      "## Comparing model performance with Spark Streaming\n",
      "\n",
      "As we have used a known weight vector and intercept to generate the training data in our producer application, we would expect our model to eventually learn this underlying weight vector (in the absence of random noise, which we do not add for this example).\n",
      "\n",
      "Therefore, we should see the model's error rate decrease over time, as it sees more and more data. We can also use standard regression error metrics to compare the performance of multiple models.\n",
      "\n",
      "In this example, we will create two models with different learning rates, training them both on the same data stream. We will then make predictions for each model and measure the **mean-squared error** ( **MSE** ) and **root mean-squared error** ( **RMSE** ) metrics for each batch.\n",
      "\n",
      "Our new monitored streaming model code is shown here:\n",
      "\n",
      "    /**\n",
      "     * A streaming regression model that compares the model performance of two models, printing out metrics for\n",
      "     * each batch\n",
      "     */\n",
      "    object MonitoringStreamingModel {\n",
      "      import org.apache.spark.SparkContext._\n",
      "\n",
      "      def main(args: Array[String]) {\n",
      "\n",
      "        val ssc = new StreamingContext(\"local[2]\", \"First Streaming App\", Seconds(10))\n",
      "        val stream = ssc.socketTextStream(\"localhost\", 9999)\n",
      "\n",
      "        val NumFeatures = 100\n",
      "        val zeroVector = DenseVector.zeros[Double](NumFeatures)\n",
      "        val model1 = new StreamingLinearRegressionWithSGD()\n",
      "          .setInitialWeights(Vectors.dense(zeroVector.data))\n",
      "          .setNumIterations(1)\n",
      "          .setStepSize(0.01)\n",
      "\n",
      "        val model2 = new StreamingLinearRegressionWithSGD()\n",
      "          .setInitialWeights(Vectors.dense(zeroVector.data))\n",
      "          .setNumIterations(1)\n",
      "          .setStepSize(1.0)\n",
      "    // create a stream of labeled points\n",
      "        val labeledStream = stream.map { event =>\n",
      "          val split = event.split(\"\\t\")\n",
      "          val y = split(0).toDouble\n",
      "          val features = split(1).split(\",\").map(_.toDouble)\n",
      "          LabeledPoint(label = y, features = Vectors.dense(features))\n",
      "        }\n",
      "\n",
      "Note that most of the preceding setup code is the same as our simple streaming model example. However, we created two instances of `StreamingLinearRegressionWithSGD`: one with a learning rate of `0.01` and one with the learning rate set to `1.0`.\n",
      "\n",
      "Next, we will train each model on our input stream, and using Spark Streaming's `transform` function, we will create a new DStream that contains the error rates for each model:\n",
      "\n",
      "        // train both models on the same stream\n",
      "        model1.trainOn(labeledStream)\n",
      "        model2.trainOn(labeledStream)\n",
      "\n",
      "        // use transform to create a stream with model error rates\n",
      "        val predsAndTrue = labeledStream.transform { rdd =>\n",
      "          val latest1 = model1.latestModel()\n",
      "          val latest2 = model2.latestModel()\n",
      "          rdd.map { point =>\n",
      "            val pred1 = latest1.predict(point.features)\n",
      "            val pred2 = latest2.predict(point.features)\n",
      "            (pred1 - point.label, pred2 - point.label)\n",
      "          }\n",
      "        }\n",
      "\n",
      "Finally, we will use `foreachRDD` to compute the MSE and RMSE metrics for each model and print them to the console:\n",
      "\n",
      "        // print out the MSE and RMSE metrics for each model per batch\n",
      "        predsAndTrue.foreachRDD { (rdd, time) =>\n",
      "          val mse1 = rdd.map { case (err1, err2) => err1 * err1 }.mean()\n",
      "          val rmse1 = math.sqrt(mse1)\n",
      "          val mse2 = rdd.map { case (err1, err2) => err2 * err2 }.mean()\n",
      "          val rmse2 = math.sqrt(mse2)\n",
      "          println(\n",
      "            s\"\"\"\n",
      "               |-------------------------------------------\n",
      "               |Time: $time\n",
      "               |-------------------------------------------\n",
      "             \"\"\".stripMargin)\n",
      "          println(s\"MSE current batch: Model 1: $mse1; Model 2: $mse2\")\n",
      "          println(s\"RMSE current batch: Model 1: $rmse1; Model 2: $rmse2\")\n",
      "          println(\"...\\n\")\n",
      "        }\n",
      "\n",
      "        ssc.start()\n",
      "        ssc.awaitTermination()\n",
      "\n",
      "      }\n",
      "    }\n",
      "\n",
      "If you terminated the producer earlier, start it again by executing `sbt run` and selecting `StreamingModelProducer`. Once the producer is running again, in your second terminal window, execute `sbt run` and choose the main class for `MonitoringStreamingModel`.\n",
      "\n",
      "You should see the streaming program startup, and after about 10 seconds, the first batch will be processed, printing output similar to the following:\n",
      "\n",
      "    **...**\n",
      "    **14/11/16 14:56:11 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.09122 s**\n",
      "\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416142570000 ms**\n",
      "    **-------------------------------------------**\n",
      "\n",
      "    **MSE current batch: Model 1: 97.9475827857361; Model 2: 97.9475827857361**\n",
      "    **RMSE current batch: Model 1: 9.896847113385965; Model 2: 9.896847113385965**\n",
      "    **...**\n",
      "\n",
      "Since both models start with the same initial weight vector, we see that they both make the same predictions on this first batch and, therefore, have the same error.\n",
      "\n",
      "If we leave the streaming program running for a few minutes, we should eventually see that one of the models has started converging, leading to a lower and lower error, while the other model has tended to diverge to a poorer model due to the overly high learning rate:\n",
      "\n",
      "    **...**\n",
      "    **14/11/16 14:57:30 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.069175 s**\n",
      "\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416142650000 ms**\n",
      "    **-------------------------------------------**\n",
      "\n",
      "    **MSE current batch: Model 1: 75.54543031658632; Model 2: 10318.213926882852**\n",
      "    **RMSE current batch: Model 1: 8.691687426304878; Model 2: 101.57860959317593**\n",
      "    **...**\n",
      "\n",
      "If you leave the program running for a number of minutes, you should eventually see the first model's error rate getting quite small:\n",
      "\n",
      "    **...**\n",
      "    **14/11/16 17:27:00 INFO SparkContext: Job finished: mean at StreamingModel.scala:159, took 0.037856 s**\n",
      "\n",
      "    **-------------------------------------------**\n",
      "    **Time: 1416151620000 ms**\n",
      "    **-------------------------------------------**\n",
      "\n",
      "    **MSE current batch: Model 1: 6.551475362521364; Model 2: 1.057088005456417E26**\n",
      "    **RMSE current batch: Model 1: 2.559584998104451; Model 2: 1.0281478519436867E13**\n",
      "    **...**\n",
      "\n",
      "### Tip\n",
      "\n",
      "Note again that due to random data generation, you might see different results, but the overall result should be the same--in the first batch, the models will have the same error, and subsequently, the first model should start to generate to a smaller and smaller error.\n",
      "\n",
      "# Summary\n",
      "\n",
      "In this chapter, we connected some of the dots between online machine learning and streaming data analysis. We introduced the Spark Streaming library and API for continuous processing of data streams based on familiar RDD functionality and worked through examples of streaming analytics applications that illustrate this functionality.\n",
      "\n",
      "Finally, we used MLlib's streaming regression model in a streaming application that involves computing and comparing model performance on a stream of input feature vectors.\n",
      "\n",
      "# Index\n",
      "\n",
      "## A\n",
      "\n",
      "  * Abstract Window Toolkit (AWT) / Extracting facial images as vectors\n",
      "  * accumulators / Broadcast variables and accumulators\n",
      "  * additive smoothing\n",
      "    * URL / The naïve Bayes model\n",
      "  * agglomerative clustering\n",
      "    * about / Hierarchical clustering\n",
      "  * alpha parameter / Training a model using implicit feedback data\n",
      "  * Alternating Least Squares (ALS) / Alternating least squares\n",
      "  * Amazon AWS public datasets\n",
      "    * URL / Accessing publicly available datasets\n",
      "    * about / Accessing publicly available datasets\n",
      "  * Amazon EC2\n",
      "    * Spark, running on / Getting Spark running on Amazon EC2\n",
      "    * EC2 Spark cluster, launching / Launching an EC2 Spark cluster\n",
      "  * Amazon Web Services account\n",
      "    * URL / Getting Spark running on Amazon EC2\n",
      "  * Anaconda\n",
      "    * URL / Exploring and visualizing your data\n",
      "  * analytics\n",
      "    * streaming / Streaming analytics\n",
      "  * architecture, machine learning system / An architecture for a machine learning system\n",
      "  * area under ROC curve (AUC) / Evaluating the performance of classification models\n",
      "  * AUC, classification models / ROC curve and AUC\n",
      "  * AWS console\n",
      "    * URL / Getting Spark running on Amazon EC2\n",
      "\n",
      "## B\n",
      "\n",
      "  * bad data\n",
      "    * filling / Filling in bad or missing data\n",
      "  * bag-of-words model\n",
      "    * about / Term weighting schemes\n",
      "  * base form / A note about stemming\n",
      "  * basic streaming application\n",
      "    * creating / Creating a basic streaming application\n",
      "  * batch interval\n",
      "    * about / An introduction to Spark Streaming\n",
      "  * bike sharing dataset\n",
      "    * features, extracting from / Extracting features from the bike sharing dataset\n",
      "    * regression model, training on / Training a regression model on the bike sharing dataset\n",
      "    * performance metrics, computing on / Computing performance metrics on the bike sharing dataset\n",
      "  * Breeze library / Interpreting the movie clusters\n",
      "  * broadcast variable / Broadcast variables and accumulators\n",
      "  * built-in evaluation functions\n",
      "    * using / Using MLlib's built-in evaluation functions\n",
      "    * RMSE / RMSE and MSE\n",
      "    * MSE / RMSE and MSE\n",
      "    * MAP / MAP\n",
      "  * business use cases, machine learning system\n",
      "    * about / Business use cases for a machine learning system\n",
      "    * personalization / Personalization\n",
      "    * targeted marketing / Targeted marketing and customer segmentation\n",
      "    * customer segmentation / Targeted marketing and customer segmentation\n",
      "    * predictive modelling and analytics / Predictive modeling and analytics\n",
      "\n",
      "## C\n",
      "\n",
      "  * categorical features / Categorical features\n",
      "    * timestamps, transforming into / Transforming timestamps into categorical features\n",
      "  * classification model\n",
      "    * about / Predictive modeling and analytics\n",
      "  * classification models\n",
      "    * types / Types of classification models\n",
      "    * linear models / Linear models\n",
      "    * naïve Bayes model / The naïve Bayes model\n",
      "    * decision trees / Decision trees\n",
      "    * training / Training classification models\n",
      "    * training, on Kaggle/StumbleUpon evergreen classification dataset / Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * using / Using classification models\n",
      "    * predictions generating, for Kaggle/StumbleUpon evergreen classification dataset / Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "  * clustering evaluation\n",
      "    * URL / Internal evaluation metrics\n",
      "  * clustering model\n",
      "    * training / Training a clustering model\n",
      "    * training, on MovieLens dataset / Training a clustering model on the MovieLens dataset\n",
      "    * used, for making predictions / Making predictions using a clustering model\n",
      "  * clustering models\n",
      "    * types / Types of clustering models\n",
      "    * K-means clustering / K-means clustering\n",
      "    * mixture model / Mixture models\n",
      "    * hierarchical clustering / Hierarchical clustering\n",
      "    * parameters, tuning for / Tuning parameters for clustering models\n",
      "    * K, selecting through cross-validation / Selecting K through cross-validation\n",
      "  * cluster predictions\n",
      "    * interpreting, on MovieLens dataset / Interpreting cluster predictions on the MovieLens dataset\n",
      "  * collaborative filtering\n",
      "    * about / Collaborative filtering\n",
      "    * matrix factorization / Matrix factorization\n",
      "  * comma-separated-value (CSV) / The first step to a Spark program in Scala\n",
      "  * components, data-driven machine learning system\n",
      "    * about / The components of a data-driven machine learning system\n",
      "    * data ingestion / Data ingestion and storage\n",
      "    * data storage / Data ingestion and storage\n",
      "    * data cleansing / Data cleansing and transformation\n",
      "    * data transformation / Data cleansing and transformation\n",
      "    * model training / Model training and testing loop\n",
      "    * testing loop / Model training and testing loop\n",
      "    * model deployment / Model deployment and integration\n",
      "    * model integration / Model deployment and integration\n",
      "    * model monitoring / Model monitoring and feedback\n",
      "    * model feedback / Model monitoring and feedback\n",
      "    * batch, versus real time / Batch versus real time\n",
      "  * content-based filtering / Content-based filtering\n",
      "  * convergence\n",
      "    * about / K-means clustering\n",
      "  * corpus\n",
      "    * about / Term weighting schemes\n",
      "  * correct form of data\n",
      "    * using / Using the correct form of data\n",
      "  * cross-validation\n",
      "    * K, selecting through / Selecting K through cross-validation\n",
      "  * cross validation\n",
      "    * about / Model training and testing loop, Cross-validation\n",
      "    * URL / Cross-validation\n",
      "  * customer segmentation\n",
      "    * about / Targeted marketing and customer segmentation\n",
      "\n",
      "## D\n",
      "\n",
      "  * data\n",
      "    * exploring / Exploring and visualizing your data\n",
      "    * visualizing / Exploring and visualizing your data\n",
      "    * user dataset, exploring / Exploring the user dataset\n",
      "    * movie dataset, exploring / Exploring the movie dataset\n",
      "    * rating dataset, exploring / Exploring the rating dataset\n",
      "    * processing / Processing and transforming your data\n",
      "    * transforming / Processing and transforming your data\n",
      "    * features, extracting from / Extracting useful features from your data, Extracting the right features from your data, Extracting the right features from your data, Extracting the right features from your data\n",
      "    * projecting, PCA used / Projecting data using PCA on the LFW dataset\n",
      "  * data-driven machine learning system\n",
      "    * components / The components of a data-driven machine learning system, Data ingestion and storage, Data cleansing and transformation, Model training and testing loop, Model monitoring and feedback, Batch versus real time\n",
      "  * data cleansing / Data cleansing and transformation\n",
      "  * data ingestion / Data ingestion and storage\n",
      "  * datasets\n",
      "    * accessing / Accessing publicly available datasets\n",
      "    * MovieLens 100k dataset / The MovieLens 100k dataset\n",
      "  * data sources\n",
      "    * UCI Machine Learning Repository / Accessing publicly available datasets\n",
      "    * Amazon AWS public datasets / Accessing publicly available datasets\n",
      "    * Kaggle / Accessing publicly available datasets\n",
      "    * KDnuggets / Accessing publicly available datasets\n",
      "  * data storage / Data ingestion and storage\n",
      "  * data transformation / Data cleansing and transformation\n",
      "  * decision tree / Decision tree\n",
      "  * decision trees / Decision trees\n",
      "    * about / Decision trees\n",
      "    * tree depth, tuning / Tuning tree depth and impurity\n",
      "    * impurity, tuning / Tuning tree depth and impurity\n",
      "    * used, for regression / Decision trees for regression\n",
      "  * derived features\n",
      "    * about / Derived features\n",
      "    * timestamps, transforming into categorical features / Transforming timestamps into categorical features\n",
      "  * dimensionality reduction\n",
      "    * types / Types of dimensionality reduction\n",
      "    * PCA / Principal Components Analysis\n",
      "    * SVD / Singular Value Decomposition\n",
      "    * relationship, to matrix factorization / Relationship with matrix factorization\n",
      "    * clustering as / Clustering as dimensionality reduction\n",
      "  * dimensionality reduction model\n",
      "    * training / Training a dimensionality reduction model\n",
      "    * PCA running, on LFW dataset / Running PCA on the LFW dataset\n",
      "    * using / Using a dimensionality reduction model\n",
      "    * data projecting, PCA used / Projecting data using PCA on the LFW dataset\n",
      "    * PCA and SVD, relationship between / The relationship between PCA and SVD\n",
      "  * dimensionality reduction models\n",
      "    * evaluating / Evaluating dimensionality reduction models\n",
      "    * k, evaluating for SVD / Evaluating k for SVD on the LFW dataset\n",
      "  * discretized stream\n",
      "    * about / An introduction to Spark Streaming\n",
      "  * distributed vector representations\n",
      "    * about / Word2Vec models\n",
      "  * divisive clustering\n",
      "    * about / Hierarchical clustering\n",
      "  * document similarity\n",
      "    * with 20 Newsgroups dataset / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "    * with TF-IDF features / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "  * DStream\n",
      "    * about / An introduction to Spark Streaming\n",
      "    * actions / Actions\n",
      "\n",
      "## E\n",
      "\n",
      "  * EC2 Spark cluster\n",
      "    * launching / Launching an EC2 Spark cluster\n",
      "  * Eigenfaces\n",
      "    * visualizing / Visualizing the Eigenfaces\n",
      "    * about / Visualizing the Eigenfaces\n",
      "    * URL / Visualizing the Eigenfaces\n",
      "    * interpreting / Interpreting the Eigenfaces\n",
      "  * ensemble methods / Model training and testing loop\n",
      "  * evaluation metrics\n",
      "    * about / Evaluating the performance of recommendation models\n",
      "  * explicit matrix factorization / Explicit matrix factorization\n",
      "  * external evaluation metrics / External evaluation metrics\n",
      "\n",
      "## F\n",
      "\n",
      "  * face data\n",
      "    * exploring / Exploring the face data\n",
      "    * visualizing / Visualizing the face data\n",
      "  * facial images, as vectors\n",
      "    * extracting / Extracting facial images as vectors\n",
      "    * images, loading / Loading images\n",
      "    * grayscale, converting to / Converting to grayscale and resizing the images\n",
      "    * images, resizing / Converting to grayscale and resizing the images\n",
      "    * feature vectors, extracting / Extracting feature vectors\n",
      "  * false positive rate (FPR) / ROC curve and AUC\n",
      "  * feature extraction\n",
      "    * packages, used for / Using packages for feature extraction\n",
      "  * feature extraction techniques\n",
      "    * term weighting schemes / Term weighting schemes\n",
      "    * feature hashing / Feature hashing\n",
      "    * TF-IDF features, extracting from 20 Newsgroups dataset / Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "  * feature hashing / Feature hashing\n",
      "  * features\n",
      "    * extracting, from data / Extracting useful features from your data, Extracting the right features from your data, Extracting the right features from your data, Extracting the right features from your data, Extracting the right features from your data, Extracting the right features from your data\n",
      "    * about / Extracting useful features from your data\n",
      "    * numerical features / Extracting useful features from your data, Numerical features\n",
      "    * categorical features / Extracting useful features from your data, Categorical features\n",
      "    * text features / Extracting useful features from your data, Text features\n",
      "    * derived features / Derived features\n",
      "    * normalizing features / Normalizing features\n",
      "    * extracting / Extracting the right features from your data\n",
      "    * extracting, from MovieLens 100k dataset / Extracting features from the MovieLens 100k dataset\n",
      "    * extracting, from Kaggle/StumbleUpon evergreen classification dataset / Extracting features from the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * extracting, from bike sharing dataset / Extracting features from the bike sharing dataset\n",
      "    * extracting, from MovieLens dataset / Extracting features from the MovieLens dataset\n",
      "    * extracting, from LFW dataset / Extracting features from the LFW dataset\n",
      "  * features, extracting\n",
      "    * feature vectors, creating for linear model / Creating feature vectors for the linear model\n",
      "    * feature vectors, creating for decision tree / Creating feature vectors for the decision tree\n",
      "  * features, MovieLens dataset\n",
      "    * movie genre labels, extracting / Extracting movie genre labels\n",
      "    * recommendation model, training / Training the recommendation model\n",
      "    * normalization / Normalization\n",
      "  * feature standardization, model performance / Feature standardization\n",
      "  * feature vector\n",
      "    * about / Extracting the right features from your data\n",
      "  * feature vectors\n",
      "    * creating, for linear model / Creating feature vectors for the linear model\n",
      "    * creating, for decision tree / Creating feature vectors for the decision tree\n",
      "    * extracting / Extracting feature vectors\n",
      "\n",
      "## G\n",
      "\n",
      "  * generalized linear models\n",
      "    * URL / Linear models\n",
      "  * general regularization\n",
      "    * URL / Regularization\n",
      "  * grayscale\n",
      "    * converting to / Converting to grayscale and resizing the images\n",
      "\n",
      "## H\n",
      "\n",
      "  * Hadoop Distributed File System (HDFS) / Installing and setting up Spark locally\n",
      "  * hash collisions\n",
      "    * about / Feature hashing\n",
      "  * hierarchical clustering / Hierarchical clustering\n",
      "  * hinge loss\n",
      "    * about / Linear support vector machines\n",
      "\n",
      "## I\n",
      "\n",
      "  * images\n",
      "    * loading / Loading images\n",
      "    * resizing / Converting to grayscale and resizing the images\n",
      "  * implicit feedback data\n",
      "    * used, for training model / Training a model using implicit feedback data\n",
      "  * implicit matrix factorization / Implicit matrix factorization\n",
      "  * initialization methods, K-means clustering / Initialization methods\n",
      "  * internal evaluation metrics / Internal evaluation metrics\n",
      "  * inverse document frequency\n",
      "    * about / Term weighting schemes\n",
      "  * IPython\n",
      "    * about / Exploring and visualizing your data\n",
      "  * IPython Notebook\n",
      "    * URL / Exploring and visualizing your data\n",
      "  * item recommendations\n",
      "    * about / Item recommendations\n",
      "    * similar movies, generating for MovieLens 100K dataset / Generating similar movies for the MovieLens 100k dataset\n",
      "\n",
      "## J\n",
      "\n",
      "  * Java\n",
      "    * Spark program, writing in / The first step to a Spark program in Java\n",
      "  * Java Development Kit (JDK) / Installing and setting up Spark locally\n",
      "  * Java Runtime Environment (JRE) / Installing and setting up Spark locally\n",
      "\n",
      "## K\n",
      "\n",
      "  * K\n",
      "    * selecting, through cross-validation / Selecting K through cross-validation\n",
      "  * k\n",
      "    * evaluating, for SVD on LFW dataset / Evaluating k for SVD on the LFW dataset\n",
      "  * K-means\n",
      "    * streaming / Streaming K-means\n",
      "  * K-means clustering\n",
      "    * about / K-means clustering\n",
      "    * initialization methods / Initialization methods\n",
      "    * variants / Variants\n",
      "  * K-means ||\n",
      "    * about / Initialization methods\n",
      "  * Kaggle\n",
      "    * about / Accessing publicly available datasets\n",
      "    * URL / Accessing publicly available datasets\n",
      "  * Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * features, extracting from / Extracting features from the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * URL / Extracting features from the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * classification models, training on / Training a classification model on the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * predictions, generating for / Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "  * Kaggle competition evaluation page\n",
      "    * URL / Root Mean Squared Log Error\n",
      "  * KDnuggets\n",
      "    * about / Accessing publicly available datasets\n",
      "    * URL / Accessing publicly available datasets\n",
      "\n",
      "## L\n",
      "\n",
      "  * L1 regularization / L1 regularization\n",
      "  * L2 regularization\n",
      "    * URL / Regularization\n",
      "/ L2 regularization\n",
      "  * label\n",
      "    * about / Extracting the right features from your data\n",
      "  * Labeled Faces in the Wild (LFW)\n",
      "    * about / Extracting the right features from your data\n",
      "  * lasso\n",
      "    * about / Least squares regression\n",
      "  * latent feature models\n",
      "    * about / Explicit matrix factorization\n",
      "  * Least Squares Regression / Least squares regression\n",
      "  * LFW dataset\n",
      "    * features, extracting from / Extracting features from the LFW dataset\n",
      "    * face data, exploring / Exploring the face data\n",
      "    * face data, visualizing / Visualizing the face data\n",
      "    * facial images, extracting as vectors / Extracting facial images as vectors\n",
      "    * normalization / Normalization\n",
      "    * PCA, running on / Running PCA on the LFW dataset\n",
      "    * Eigenfaces, visualizing / Visualizing the Eigenfaces\n",
      "    * Eigenfaces, interpreting / Interpreting the Eigenfaces\n",
      "    * data projecting, PCA used / Projecting data using PCA on the LFW dataset\n",
      "    * k evaluating, for SVD / Evaluating k for SVD on the LFW dataset\n",
      "  * line => line.size syntax\n",
      "    * about / Spark operations\n",
      "  * linear model / Linear model\n",
      "  * linear models\n",
      "    * about / Linear models, Linear models\n",
      "    * logistic regression / Logistic regression\n",
      "    * linear support vector machines / Linear support vector machines\n",
      "    * iterations / Iterations\n",
      "    * step size parameter / Step size\n",
      "    * regularization / Regularization\n",
      "  * linear support vector machines / Linear support vector machines\n",
      "  * log-transformed targets\n",
      "    * training, impact / Impact of training on log-transformed targets\n",
      "  * logistic regression\n",
      "    * about / Linear models, Logistic regression\n",
      "\n",
      "## M\n",
      "\n",
      "  * machine learning models, types\n",
      "    * about / Types of machine learning models\n",
      "    * supervised learning / Types of machine learning models\n",
      "    * unsupervised learning / Types of machine learning models\n",
      "  * machine learning system\n",
      "    * business use cases / Business use cases for a machine learning system, Personalization, Targeted marketing and customer segmentation\n",
      "    * architecture / An architecture for a machine learning system\n",
      "  * MAE / Mean Absolute Error\n",
      "  * MAP\n",
      "    * about / MAP\n",
      "    * calculating / MAP\n",
      "  * map function / Broadcast variables and accumulators\n",
      "  * MAPK\n",
      "    * URL / Mean average precision at K\n",
      "  * matrix factorization\n",
      "    * about / Matrix factorization, Relationship with matrix factorization\n",
      "    * explicit matrix factorization / Explicit matrix factorization\n",
      "    * implicit matrix factorization / Implicit matrix factorization\n",
      "    * Alternating Least Squares (ALS) / Alternating least squares\n",
      "  * mean-squared error (MSE) / Comparing model performance with Spark Streaming\n",
      "  * Mean average precision at K (MAPK) / Mean average precision at K\n",
      "  * Mean Squared Error (MSE) / Mean Squared Error\n",
      "  * mini-batches\n",
      "    * about / An introduction to Spark Streaming\n",
      "  * missing data\n",
      "    * filling / Filling in bad or missing data\n",
      "  * mixture model / Mixture models\n",
      "  * MLlib\n",
      "    * used, for normalizing features / Using MLlib for feature normalization\n",
      "  * model\n",
      "    * training, on MovieLens 100k dataset / Training a model on the MovieLens 100k dataset\n",
      "    * training, implicit feedback data used / Training a model using implicit feedback data\n",
      "  * model deployment / Model deployment and integration\n",
      "  * model feedback\n",
      "    * about / Model monitoring and feedback\n",
      "  * model fitting\n",
      "    * about / Linear models\n",
      "  * model inputs\n",
      "    * rank / Training a model on the MovieLens 100k dataset\n",
      "    * iterations / Training a model on the MovieLens 100k dataset\n",
      "    * lambda / Training a model on the MovieLens 100k dataset\n",
      "  * model integration / Model deployment and integration\n",
      "  * model monitoring / Model monitoring and feedback\n",
      "  * model parameters\n",
      "    * tuning / Tuning model parameters, Tuning model parameters\n",
      "    * linear models / Linear models\n",
      "    * decision trees / Decision trees\n",
      "    * naïve Bayes model / The naïve Bayes model\n",
      "    * testing set, creating to evaluate parameters / Creating training and testing sets to evaluate parameters\n",
      "    * training set, creating to evaluate parameters / Creating training and testing sets to evaluate parameters\n",
      "    * parameter settings, impact for linear models / The impact of parameter settings for linear models\n",
      "    * parameter settings, impact for decision tree / The impact of parameter settings for the decision tree\n",
      "  * model performance\n",
      "    * improving / Improving model performance and tuning parameters, Improving model performance and tuning parameters\n",
      "    * feature standardization / Feature standardization\n",
      "    * additional features / Additional features\n",
      "    * correct form of data, using / Using the correct form of data\n",
      "    * comparing, with Spark Streaming / Comparing model performance with Spark Streaming\n",
      "  * model selection\n",
      "    * about / Model training and testing loop\n",
      "  * model training / Model training and testing loop\n",
      "  * movie clusters\n",
      "    * interpreting / Interpreting the movie clusters\n",
      "  * movie dataset\n",
      "    * exploring / Exploring the movie dataset\n",
      "  * movie genre labels\n",
      "    * extracting / Extracting movie genre labels\n",
      "  * MovieLens 100K dataset\n",
      "    * similar movies, generating for / Generating similar movies for the MovieLens 100k dataset\n",
      "  * MovieLens 100k dataset / The MovieLens 100k dataset\n",
      "    * URL / The MovieLens 100k dataset\n",
      "    * features, extracting from / Extracting features from the MovieLens 100k dataset\n",
      "    * movie recommendations, generating from / Generating movie recommendations from the MovieLens 100k dataset\n",
      "  * MovieLens dataset\n",
      "    * about / Accessing publicly available datasets\n",
      "    * features, extracting from / Extracting features from the MovieLens dataset\n",
      "    * clustering model, training on / Training a clustering model on the MovieLens dataset\n",
      "    * cluster predictions, interpreting on / Interpreting cluster predictions on the MovieLens dataset\n",
      "    * performance metrics, computing on / Computing performance metrics on the MovieLens dataset\n",
      "  * movie recommendations\n",
      "    * generating, from MovieLens 100k dataset / Generating movie recommendations from the MovieLens 100k dataset\n",
      "  * MovieStream\n",
      "    * about / Introducing MovieStream\n",
      "  * MSE / RMSE and MSE, Mean Squared Error and Root Mean Squared Error\n",
      "\n",
      "## N\n",
      "\n",
      "  * 20 Newsgroups\n",
      "    * about / Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "    * URL / Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "  * 20 Newsgroups data\n",
      "    * exploring / Exploring the 20 Newsgroups data\n",
      "  * 20 Newsgroups dataset\n",
      "    * TF-IDF features, extracting from / Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "    * document similarity, used with / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "    * text classifier, training on / Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "    * Word2Vec models, used on / Word2Vec on the 20 Newsgroups dataset\n",
      "  * natural language processing (NLP)\n",
      "    * about / Extracting the right features from your data\n",
      "  * naïve Bayes model / The naïve Bayes model, The naïve Bayes model\n",
      "  * nominal variables\n",
      "    * about / Categorical features\n",
      "  * nonword characters / Improving our tokenization\n",
      "  * normalization\n",
      "    * normalize a feature / Normalizing features\n",
      "    * normalize a feature vector / Normalizing features\n",
      "  * normalization, LFW dataset / Normalization\n",
      "  * normalization, MovieLens dataset / Normalization\n",
      "  * normalizing features\n",
      "    * about / Normalizing features\n",
      "    * MLlib, used for / Using MLlib for feature normalization\n",
      "  * numerical features / Numerical features\n",
      "\n",
      "## O\n",
      "\n",
      "  * 1-of-k encoding\n",
      "    * about / Categorical features\n",
      "  * online learning / Batch versus real time\n",
      "    * about / Online learning\n",
      "  * online learning, with Spark Streaming\n",
      "    * about / Online learning with Spark Streaming\n",
      "    * streaming regression model / Streaming regression\n",
      "    * streaming regression program / A simple streaming regression program\n",
      "    * K-means, streaming / Streaming K-means\n",
      "  * online machine learning\n",
      "    * URL / Online learning\n",
      "  * online model evaluation\n",
      "    * about / Online model evaluation\n",
      "    * model performance, comparing with Spark Streaming / Comparing model performance with Spark Streaming\n",
      "  * optimization\n",
      "    * about / Linear models\n",
      "  * options, data transformation\n",
      "    * about / Processing and transforming your data\n",
      "  * ordinal variables\n",
      "    * about / Categorical features\n",
      "  * Oryx\n",
      "    * URL / Explicit matrix factorization\n",
      "  * over-fitting and under-fitting\n",
      "    * URL / Regularization\n",
      "\n",
      "## P\n",
      "\n",
      "  * packages\n",
      "    * used, for feature extraction / Using packages for feature extraction\n",
      "  * parameters\n",
      "    * tuning / Improving model performance and tuning parameters, Improving model performance and tuning parameters\n",
      "    * tuning, for clustering models / Tuning parameters for clustering models\n",
      "  * parameter settings impact, for decision tree\n",
      "    * about / The impact of parameter settings for the decision tree\n",
      "    * tree depth / Tree depth\n",
      "    * maximum bins / Maximum bins\n",
      "  * parameter settings impact, for linear models\n",
      "    * about / The impact of parameter settings for linear models\n",
      "    * iterations / Iterations\n",
      "    * step size / Step size\n",
      "    * L2 regularization / L2 regularization\n",
      "    * L1 regularization / L1 regularization\n",
      "    * intercept, using / Intercept\n",
      "  * PCA / Principal Components Analysis\n",
      "    * running, on LFW dataset / Running PCA on the LFW dataset\n",
      "    * and SVD, relationship between / The relationship between PCA and SVD\n",
      "  * performance, classification models\n",
      "    * evaluating / Evaluating the performance of classification models\n",
      "    * accuracy, calculating / Accuracy and prediction error\n",
      "    * prediction error / Accuracy and prediction error\n",
      "    * precision / Precision and recall\n",
      "    * recall / Precision and recall\n",
      "    * ROC curve / ROC curve and AUC\n",
      "    * AUC / ROC curve and AUC\n",
      "  * performance, clustering models\n",
      "    * evaluating / Evaluating the performance of clustering models\n",
      "    * internal evaluation metrics / Internal evaluation metrics\n",
      "    * external evaluation metrics / External evaluation metrics\n",
      "    * performance metrics, computing on MovieLens dataset / Computing performance metrics on the MovieLens dataset\n",
      "  * performance, recommendation models\n",
      "    * evaluating / Evaluating the performance of recommendation models\n",
      "    * Mean Squared Error (MSE) / Mean Squared Error\n",
      "    * Mean average precision at K (MAPK) / Mean average precision at K\n",
      "    * built-in evaluation functions, using / Using MLlib's built-in evaluation functions\n",
      "  * performance, regression models\n",
      "    * evaluating / Evaluating the performance of regression models\n",
      "    * MSE / Mean Squared Error and Root Mean Squared Error\n",
      "    * RMSE / Mean Squared Error and Root Mean Squared Error\n",
      "    * MAE / Mean Absolute Error\n",
      "    * Root Mean Squared Log Error / Root Mean Squared Log Error\n",
      "    * R-squared coefficient / The R-squared coefficient\n",
      "    * performance metrics, computing on bike sharing dataset / Computing performance metrics on the bike sharing dataset\n",
      "  * performance metrics\n",
      "    * computing, on bike sharing dataset / Computing performance metrics on the bike sharing dataset\n",
      "    * linear model / Linear model\n",
      "    * decision tree / Decision tree\n",
      "    * computing, on MovieLens dataset / Computing performance metrics on the MovieLens dataset\n",
      "  * personalization / Personalization\n",
      "  * precision, classification models / Precision and recall\n",
      "  * precision-recall (PR) curve / Precision and recall\n",
      "  * Prediction.io\n",
      "    * URL / Explicit matrix factorization\n",
      "  * prediction error, classification models / Accuracy and prediction error\n",
      "  * predictions\n",
      "    * generating, for Kaggle/StumbleUpon evergreen / Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * generating, for Kaggle/StumbleUpon evergreen classification dataset / Generating predictions for the Kaggle/StumbleUpon evergreen classification dataset\n",
      "    * making, clustering model used / Making predictions using a clustering model\n",
      "  * predictive modeling\n",
      "    * about / Predictive modeling and analytics\n",
      "  * producer application / The producer application\n",
      "  * pylab\n",
      "    * about / Exploring and visualizing your data\n",
      "  * Python\n",
      "    * Spark program, writing in / The first step to a Spark program in Python\n",
      "\n",
      "## R\n",
      "\n",
      "  * R-squared coefficient / The R-squared coefficient\n",
      "  * rating dataset\n",
      "    * exploring / Exploring the rating dataset\n",
      "  * RDD caching\n",
      "    * URL / Caching RDDs\n",
      "  * RDDs\n",
      "    * about / Resilient Distributed Datasets\n",
      "    * creating / Creating RDDs\n",
      "    * Spark operations / Spark operations\n",
      "    * caching / Caching RDDs\n",
      "  * Readme.txt file\n",
      "    * about / Extracting features from the bike sharing dataset\n",
      "    * variables / Extracting features from the bike sharing dataset\n",
      "  * recall, classification models / Precision and recall\n",
      "  * receiver operating characteristic (ROC) / Evaluating the performance of classification models\n",
      "  * recommendation model\n",
      "    * training / Training the recommendation model, Training the recommendation model\n",
      "    * model, training on MovieLens 100k dataset / Training a model on the MovieLens 100k dataset\n",
      "    * using / Using the recommendation model\n",
      "    * user recommendations / User recommendations\n",
      "    * item recommendations / Item recommendations\n",
      "  * recommendation models\n",
      "    * about / Types of recommendation models\n",
      "    * types / Types of recommendation models\n",
      "    * content-based filtering / Content-based filtering\n",
      "    * collaborative filtering / Collaborative filtering\n",
      "  * recommendations / Personalization\n",
      "    * inspecting / Inspecting the recommendations\n",
      "  * red, blue, and green (RGB) / Extracting facial images as vectors\n",
      "  * regression model\n",
      "    * about / Predictive modeling and analytics\n",
      "  * regression models\n",
      "    * types / Types of regression models\n",
      "    * Least Squares Regression / Least squares regression\n",
      "    * decision trees, for regression / Decision trees for regression\n",
      "    * training / Training and using regression models\n",
      "    * using / Training and using regression models\n",
      "    * training, on bike sharing dataset / Training a regression model on the bike sharing dataset\n",
      "  * regularization forms\n",
      "    * SimpleUpdater / Regularization\n",
      "    * SquaredL2Updater / Regularization\n",
      "    * L1Updater / Regularization\n",
      "  * REPL (Read-Eval-Print-Loop)\n",
      "    * about / The Spark shell\n",
      "  * reshaping / Extracting facial images as vectors\n",
      "  * RMSE\n",
      "    * about / Mean Squared Error\n",
      "/ RMSE and MSE, Mean Squared Error and Root Mean Squared Error\n",
      "  * ROC curve\n",
      "    * URL / ROC curve and AUC\n",
      "  * ROC curve, classification models / ROC curve and AUC\n",
      "  * root mean-squared error (RMSE) / Comparing model performance with Spark Streaming\n",
      "  * Root Mean Squared Log Error / Root Mean Squared Log Error\n",
      "\n",
      "## S\n",
      "\n",
      "  * Scala\n",
      "    * Spark program, writing in / The first step to a Spark program in Scala\n",
      "  * Scala Build Tool (sbt) / The first step to a Spark program in Scala\n",
      "  * similar items\n",
      "    * inspecting / Inspecting the similar items\n",
      "  * singular values\n",
      "    * about / Singular Value Decomposition\n",
      "  * skip-gram model\n",
      "    * about / Word2Vec models\n",
      "  * Spark\n",
      "    * installing / Installing and setting up Spark locally\n",
      "    * setting up / Installing and setting up Spark locally\n",
      "    * running, on Amazon EC2 / Getting Spark running on Amazon EC2\n",
      "  * Spark clusters\n",
      "    * about / Spark clusters\n",
      "    * URL / Spark clusters\n",
      "  * SparkConf / SparkContext and SparkConf\n",
      "  * SparkContext / SparkContext and SparkConf\n",
      "  * Spark documentation\n",
      "    * URL / Linear models, Decision trees for regression, General transformations\n",
      "  * Spark documentation, for EC2\n",
      "    * URL / Getting Spark running on Amazon EC2\n",
      "  * Spark operations / Spark operations\n",
      "  * Spark program\n",
      "    * in Scala / The first step to a Spark program in Scala\n",
      "    * in Java / The first step to a Spark program in Java\n",
      "    * in Python / The first step to a Spark program in Python\n",
      "  * Spark programming guide\n",
      "    * URL / The first step to a Spark program in Python\n",
      "  * Spark Programming Guide\n",
      "    * URL / Broadcast variables and accumulators\n",
      "  * Spark programming model\n",
      "    * about / The Spark programming model\n",
      "    * SparkContext / SparkContext and SparkConf\n",
      "    * SparkConf / SparkContext and SparkConf\n",
      "    * Spark shell / The Spark shell\n",
      "    * RDDs / Resilient Distributed Datasets\n",
      "    * broadcast variable / Broadcast variables and accumulators\n",
      "    * accumulators / Broadcast variables and accumulators\n",
      "  * Spark project documentation website\n",
      "    * URL / Installing and setting up Spark locally\n",
      "  * Spark project website\n",
      "    * URL / Installing and setting up Spark locally\n",
      "  * Spark Quick Start\n",
      "    * URL / The Spark programming model\n",
      "  * Spark shell / The Spark shell\n",
      "  * Spark Streaming\n",
      "    * about / Batch versus real time, An introduction to Spark Streaming\n",
      "    * input sources / Input sources\n",
      "    * transformations / Transformations\n",
      "    * actions / Actions\n",
      "    * window operators / Window operators\n",
      "    * model performance, comparing with / Comparing model performance with Spark Streaming\n",
      "  * Spark Streaming application\n",
      "    * creating / Creating a Spark Streaming application\n",
      "    * producer application / The producer application\n",
      "    * basic streaming application, creating / Creating a basic streaming application\n",
      "    * analytics, streaming / Streaming analytics\n",
      "    * stateful streaming / Stateful streaming\n",
      "  * stateful streaming / Stateful streaming\n",
      "  * stemming\n",
      "    * about / A note about stemming\n",
      "    * URL / A note about stemming\n",
      "  * stochastic gradient descent\n",
      "    * about / Online learning\n",
      "  * Stochastic Gradient Descent (SGD) / Linear models\n",
      "  * stop words\n",
      "    * removing / Removing stop words\n",
      "  * streaming data producer\n",
      "    * creating / Creating a streaming data producer\n",
      "  * streaming regression model / Streaming regression\n",
      "    * trainOn method / Streaming regression\n",
      "    * predictOn method / Streaming regression\n",
      "    * creating / Creating a streaming regression model\n",
      "  * streaming regression program\n",
      "    * about / A simple streaming regression program\n",
      "    * streaming data producer, creating / Creating a streaming data producer\n",
      "    * streaming regression model, creating / Creating a streaming regression model\n",
      "  * Stream processing\n",
      "    * about / Stream processing\n",
      "    * Spark Streaming / An introduction to Spark Streaming\n",
      "    * caching, with Spark Streaming / Caching and fault tolerance with Spark Streaming\n",
      "    * fault tolerance, with Spark Streaming / Caching and fault tolerance with Spark Streaming\n",
      "  * supervised learning\n",
      "    * about / Types of machine learning models\n",
      "  * Support Vector Machine (SVM)\n",
      "    * about / Linear models\n",
      "  * SVD\n",
      "    * about / Singular Value Decomposition\n",
      "    * and PCA, relationship between / The relationship between PCA and SVD\n",
      "\n",
      "## T\n",
      "\n",
      "  * targeted marketing\n",
      "    * about / Targeted marketing and customer segmentation\n",
      "  * target variable\n",
      "    * transforming / Transforming the target variable\n",
      "    * training on log-transformed targets, impact / Impact of training on log-transformed targets\n",
      "  * term frequency\n",
      "    * about / Term weighting schemes\n",
      "  * term frequency-inverse document frequency (TF-IDF)\n",
      "    * about / Term weighting schemes\n",
      "  * terms based on frequency\n",
      "    * excluding / Excluding terms based on frequency\n",
      "  * term weighting schemes / Term weighting schemes\n",
      "  * testing loop / Model training and testing loop\n",
      "  * testing set\n",
      "    * creating, to evaluate parameters / Creating training and testing sets to evaluate parameters\n",
      "  * text classifier\n",
      "    * training, on 20 Newsgroups dataset / Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "  * text data\n",
      "    * about / What's so special about text data?\n",
      "  * text features\n",
      "    * about / Text features\n",
      "    * extraction / Simple text feature extraction\n",
      "  * text processing impact\n",
      "    * evaluating / Evaluating the impact of text processing\n",
      "    * raw features, comparing / Comparing raw features with processed TF-IDF features on the 20 Newsgroups dataset\n",
      "  * TF-IDF\n",
      "    * used, for training text classifier / Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "  * TF-IDF features\n",
      "    * extracting, from 20 Newsgroups dataset / Extracting the TF-IDF features from the 20 Newsgroups dataset\n",
      "    * document similarity, used with / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "  * TF-IDF model\n",
      "    * training / Training a TF-IDF model\n",
      "    * using / Using a TF-IDF model\n",
      "    * document similarity, with 20 Newsgroups dataset / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "    * document similarity, with TF-IDF features / Document similarity with the 20 Newsgroups dataset and TF-IDF features\n",
      "    * text classifier, training on 20 Newsgroups dataset / Training a text classifier on the 20 Newsgroups dataset using TF-IDF\n",
      "  * TF-IDF weightings\n",
      "    * analyzing / Analyzing the TF-IDF weightings\n",
      "  * timestamps\n",
      "    * transforming, into categorical features / Transforming timestamps into categorical features\n",
      "  * tokenization\n",
      "    * applying / Applying basic tokenization\n",
      "    * improving / Improving our tokenization\n",
      "  * training\n",
      "    * about / Linear models\n",
      "  * training set\n",
      "    * creating, to evaluate parameters / Creating training and testing sets to evaluate parameters\n",
      "  * transformations\n",
      "    * about / Transformations\n",
      "    * state, tracking / Keeping track of state\n",
      "    * general transformations / General transformations\n",
      "  * true positive rate (TPR) / ROC curve and AUC\n",
      "\n",
      "## U\n",
      "\n",
      "  * UCI Machine Learning Repository\n",
      "    * about / Accessing publicly available datasets\n",
      "    * URL / Accessing publicly available datasets\n",
      "  * unsupervised learning\n",
      "    * about / Types of machine learning models\n",
      "  * user dataset\n",
      "    * exploring / Exploring the user dataset\n",
      "  * user recommendations\n",
      "    * about / User recommendations\n",
      "    * movie recommendations, generating / Generating movie recommendations from the MovieLens 100k dataset\n",
      "\n",
      "## V\n",
      "\n",
      "  * variance\n",
      "    * about / Decision trees for regression\n",
      "  * variants, K-means clustering / Variants\n",
      "  * vector\n",
      "    * about / Extracting useful features from your data\n",
      "  * vector space model\n",
      "    * about / Term weighting schemes\n",
      "\n",
      "## W\n",
      "\n",
      "  * whitespace tokenization\n",
      "    * URL / Applying basic tokenization\n",
      "  * window\n",
      "    * about / Window operators\n",
      "  * windowing\n",
      "    * about / Window operators\n",
      "  * window operators / Window operators\n",
      "  * within cluster sum of squared errors (WCSS) / K-means clustering\n",
      "  * Word2Vec models\n",
      "    * about / Word2Vec models\n",
      "    * on 20 Newsgroups dataset / Word2Vec on the 20 Newsgroups dataset\n",
      "  * word stem / A note about stemming\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data[\"train\"][0][\"context\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example 0  # Table of Contents\n",
      "example 1  David Forsyth\n",
      "example 2  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 3  Use R!\n",
      "example 4  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 5  # Introducing Regular Expressions\n",
      "example 6  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 7  David Forsyth\n",
      "example 8  Table of Contents\n",
      "example 9  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 10  David Forsyth\n",
      "example 11  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 12  # Introducing Regular Expressions\n",
      "example 13  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 14  Joe Celko's SQL Programming Style\n",
      "example 15  Table of Contents\n",
      "example 16  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 17  # Introducing Regular Expressions\n",
      "example 18  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 19  Table of Contents\n",
      "example 20  Table of Contents\n",
      "example 21  David Forsyth\n",
      "example 22  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 23  # Table of Contents\n",
      "example 24  Joe Celko's SQL Programming Style\n",
      "example 25  Table of Contents\n",
      "example 26  # Table of Contents\n",
      "example 27  # Introducing Regular Expressions\n",
      "example 28  Joe Celko's SQL Programming Style\n",
      "example 29  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 30  Table of Contents\n",
      "example 31  Table of Contents\n",
      "example 32  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 33  # Table of Contents\n",
      "example 34  # Introducing Regular Expressions\n",
      "example 35  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 36  Table of Contents\n",
      "example 37  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 38  David Forsyth\n",
      "example 39  Use R!\n",
      "example 40  Table of Contents\n",
      "example 41  Table of Contents\n",
      "example 42  David Forsyth\n",
      "example 43  Joe Celko's SQL Programming Style\n",
      "example 44  Table of Contents\n",
      "example 45  # Introducing Regular Expressions\n",
      "example 46  Joe Celko's SQL Programming Style\n",
      "example 47  # Introducing Regular Expressions\n",
      "example 48  Table of Contents\n",
      "example 49  Table of Contents\n",
      "example 50  Table of Contents\n",
      "example 51  Joe Celko's SQL Programming Style\n",
      "example 52  Use R!\n",
      "example 53  Joe Celko's SQL Programming Style\n",
      "example 54  David Forsyth\n",
      "example 55  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 56  Table of Contents\n",
      "example 57  # Introducing Regular Expressions\n",
      "example 58  Use R!\n",
      "example 59  # Table of Contents\n",
      "example 60  # Introducing Regular Expressions\n",
      "example 61  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 62  Joe Celko's SQL Programming Style\n",
      "example 63  Table of Contents\n",
      "example 64  Table of Contents\n",
      "example 65  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 66  Table of Contents\n",
      "example 67  Table of Contents\n",
      "example 68  Use R!\n",
      "example 69  David Forsyth\n",
      "example 70  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 71  Table of Contents\n",
      "example 72  Use R!\n",
      "example 73  James T. Streib and Takako SomaUndergraduate Topics in Computer ScienceGuide to Java2014A Concise Introduction to Programming10.1007/978-1-4471-6317-6© Springer-Verlag London 2014\n",
      "example 74  Use R!\n",
      "example 75  # Introducing Regular Expressions\n",
      "example 76  Table of Contents\n",
      "example 77  Joe Celko's SQL Programming Style\n",
      "example 78  Use R!\n",
      "example 79  # Table of Contents\n",
      "example 80  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 81  Joe Celko's SQL Programming Style\n",
      "example 82  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 83  Table of Contents\n",
      "example 84  Table of Contents\n",
      "example 85  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 86  # Table of Contents\n",
      "example 87  Table of Contents\n",
      "example 88  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 89  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 90  Rami RosenLinux Kernel NetworkingImplementation and Theory10.1007/978-1-4302-6197-1© Apress 2014\n",
      "example 91  Table of Contents\n",
      "example 92  David Forsyth\n",
      "example 93  # Introducing Regular Expressions\n",
      "example 94  Table of Contents\n",
      "example 95  Joe Celko's SQL Programming Style\n",
      "example 96  # Table of Contents\n",
      "example 97  Gilles Dowek and Jean-Jacques LévyUndergraduate Topics in Computer ScienceIntroduction to the Theory of Programming Languages10.1007/978-0-85729-076-2© Springer-Verlag London Limited 2011\n",
      "example 98  Use R!\n",
      "example 99  # Introducing Regular Expressions\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for example in data[\"train\"]:\n",
    "    lines= example[\"context\"].splitlines()\n",
    "    lines=[line for line in lines if line.strip() != \"\"]\n",
    "    print(f\"example {count}  {lines[0]}\")\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lightrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
